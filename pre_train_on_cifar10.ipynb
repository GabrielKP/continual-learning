{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre_train_on_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jJyPBbDRTAc9"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOAoiNUrTxbn0jMfHbjPRe0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fc8955c72e1542259251541b957d9584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_84d3bfb6294e4fa7b040487f08895bb1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d5460c6ad13c4d78af1ced946f488402",
              "IPY_MODEL_af5ba11eb998435c94f733dc71157a36"
            ]
          }
        },
        "84d3bfb6294e4fa7b040487f08895bb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d5460c6ad13c4d78af1ced946f488402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2a236358a0374dc39592b065a04c1698",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe6d503b443a44d9ad1b4a22524c5870"
          }
        },
        "af5ba11eb998435c94f733dc71157a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f271438a9aad42688d5d804e49b6327e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:11&lt;00:00, 15336546.42it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd5569a359ff49af8bae056468207923"
          }
        },
        "2a236358a0374dc39592b065a04c1698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe6d503b443a44d9ad1b4a22524c5870": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f271438a9aad42688d5d804e49b6327e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd5569a359ff49af8bae056468207923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbT1OcF162un"
      },
      "source": [
        "# Pre-train 5 CNN layers on CIFAR10\n",
        "Author: Gabriel Kressin Palacios\n",
        "\n",
        "[this notebook is not needed if you have the model already]\n",
        "\n",
        "Pre-train a CNN with following properties:\n",
        "- 100 Epochs on [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "(Krizhevsky 2009)\n",
        "- 5 convolutional layers with\n",
        "  - 16,32,64,128,256 output Channels respectively\n",
        "  - padding of 1\n",
        "  - with bias\n",
        "  - a stride of 1, 2, 2, 2, 2\n",
        "  - a kernel size of 3\n",
        "  - a BatchNormalization after each convolution\n",
        "  - a ReLU nonlinearity after each convolution\n",
        "\n",
        "\n",
        "This is to make our approach comparable to other approaches\n",
        "(van de Ven et al. 2020, Vogelstein et al. 2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIiQreFq4tJm"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uuyU1xG4Qsm"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import itertools"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lAC05aSaOS_"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0001\n",
        "BETAS = (0.9, 0.999)\n",
        "CONFIG = {\n",
        "    \"dataset\": \"cifar10\",\n",
        "    \"img_size\": 32,\n",
        "    \"n_labels\": 10,\n",
        "    \"channels\": 3,\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_6cg58xHUWp"
      },
      "source": [
        "#### ConvLayers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmlI4X7D7JVd"
      },
      "source": [
        "class ConvLayers(nn.Module):\n",
        "    \"\"\" Contains 5 Convolutional Layers \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, img_size):\n",
        "        \"\"\"\n",
        "        Creates 5 conv layers with batchnorm and ReLU activation\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channels: int\n",
        "            Amount of channels the input data has\n",
        "        img_size: int\n",
        "            Amount of pixels in one axis\n",
        "        \"\"\"\n",
        "        super(ConvLayers, self).__init__()\n",
        "\n",
        "        list_out_channels = [16,32,64,128,256]\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.out_size = int(np.ceil(img_size / 2**4))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = list_out_channels[-1]\n",
        "        self.out_units = self.out_channels * self.out_size**2\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, list_out_channels[0],\n",
        "                               kernel_size=3, stride=1, bias=True, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(list_out_channels[0])\n",
        "        self.nl1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(list_out_channels[0], list_out_channels[1],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(list_out_channels[1])\n",
        "        self.nl2 = nn.ReLU()\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(list_out_channels[1], list_out_channels[2],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(list_out_channels[2])\n",
        "        self.nl3 = nn.ReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(list_out_channels[2], list_out_channels[3],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(list_out_channels[3])\n",
        "        self.nl4 = nn.ReLU()\n",
        "\n",
        "        self.conv5 = nn.Conv2d(list_out_channels[3], list_out_channels[4],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(list_out_channels[4])\n",
        "        self.nl5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X1 = self.nl1(self.bn1(self.conv1(X)))\n",
        "        X2 = self.nl2(self.bn2(self.conv2(X1)))\n",
        "        X3 = self.nl3(self.bn3(self.conv3(X2)))\n",
        "        X4 = self.nl4(self.bn4(self.conv4(X3)))\n",
        "        X5 = self.nl5(self.bn5(self.conv5(X4)))\n",
        "\n",
        "        return X5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WXIn5hsJBRe"
      },
      "source": [
        "#### LinLayers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPRft0SN62Wv"
      },
      "source": [
        "class LinLayers(nn.Module):\n",
        "    \"\"\"\n",
        "    Contains 2 fully connected layers with 2000 units each and a ReLU.\n",
        "    Additional output layer on top\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_features: int\n",
        "            input dimensionality of data\n",
        "        out_features: int\n",
        "            output dimensionality of data\n",
        "        \"\"\"\n",
        "        super(LinLayers, self).__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        hidden_dims = [2000, 2000]\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_dims[0])\n",
        "        self.nl1 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.nl2 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], out_features)\n",
        "\n",
        "    def forward(self, X):\n",
        "\n",
        "        X1 = self.nl1(self.fc1(X))\n",
        "        X2 = self.nl2(self.fc2(X1))\n",
        "        X3 = self.fc3(X2)\n",
        "\n",
        "        return X3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g2jkj2S4v2H"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxakK1ojQ1zG"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJyPBbDRTAc9"
      },
      "source": [
        "#### Compute Norm and std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-VfXD0URPBw"
      },
      "source": [
        "# un_ds_train_cifar10 = datasets.CIFAR10(\"data\", train=True,\n",
        "#                                        transform=transforms.ToTensor(),\n",
        "#                                        download=True)\n",
        "# un_dl_train_cifar10 = DataLoader(un_ds_train_cifar10,\n",
        "#                                  batch_size=len(un_ds_train_cifar10))\n",
        "# data, _ = next(iter(un_dl_train_cifar10))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvDxdDpNSQDK"
      },
      "source": [
        "# means = []\n",
        "# stds = []\n",
        "# for channel in range(data.shape[1]):\n",
        "#     means.append(data[:,channel].mean().item())\n",
        "#     stds.append(data[:,channel].std().item())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzV7JBxBXOMN"
      },
      "source": [
        "# print(means)\n",
        "# print(stds)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBeiWF463207"
      },
      "source": [
        "#### Load Norm and Std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onAx0PSAYvVz"
      },
      "source": [
        "means = [0.491400808095932, 0.48215898871421814, 0.44653093814849854]\n",
        "stds = [0.24703224003314972, 0.24348513782024384, 0.26158785820007324]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZC9RXRGYe0W"
      },
      "source": [
        "#### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106,
          "referenced_widgets": [
            "fc8955c72e1542259251541b957d9584",
            "84d3bfb6294e4fa7b040487f08895bb1",
            "d5460c6ad13c4d78af1ced946f488402",
            "af5ba11eb998435c94f733dc71157a36",
            "2a236358a0374dc39592b065a04c1698",
            "fe6d503b443a44d9ad1b4a22524c5870",
            "f271438a9aad42688d5d804e49b6327e",
            "dd5569a359ff49af8bae056468207923"
          ]
        },
        "id": "rai3U66vYMa5",
        "outputId": "192c898c-4391-4d4c-8e26-6231210167aa"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(means, stds)])\n",
        "\n",
        "ds_train_cifar10 = datasets.CIFAR10(\"data\", train=True,\n",
        "                                  transform=transform,\n",
        "                                  download=True)\n",
        "ds_test_cifar10 = datasets.CIFAR10(\"data\", train=False,\n",
        "                                 transform=transform,\n",
        "                                 download=False)\n",
        "\n",
        "dl_train_cifar10 = DataLoader(ds_train_cifar10, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "dl_test_cifar10 = DataLoader(ds_test_cifar10, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc8955c72e1542259251541b957d9584",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting data/cifar-10-python.tar.gz to data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Oro5cYbXBK"
      },
      "source": [
        "#### Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXPauws6zgdr"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Computes minutes and seconds given start and end time\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start_time: float\n",
        "        systemtime in ms at the start of interval to be measured\n",
        "    end_time: float\n",
        "        systemtime in ms at the end of interval to be measured\n",
        "    \"\"\"\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4_iGoCCZ2nw"
      },
      "source": [
        "def train_cifar10(convLayers, linLayers, dl, opt, crit, clip):\n",
        "    \"\"\"\n",
        "    trains convlayers on cifar10 dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    convLayers: ConvLayers\n",
        "        ConvLayers object that will be trained\n",
        "    linLayers: LinLayers\n",
        "        LinLayers object that will be trained\n",
        "    dl: DataLoader\n",
        "        dataloader containing batched data\n",
        "    opt: optim.Optimizer\n",
        "        optimizer for linLayers and convLayers\n",
        "    crit: nn.Criterion\n",
        "        criterion taking outputs from linLayers and labels\n",
        "    clip: int\n",
        "        clips if gradients are too high\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    epoch_loss: float\n",
        "        Accumulated loss over epochs\n",
        "    \"\"\"\n",
        "\n",
        "    convLayers.train()\n",
        "    linLayers.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for X, y in dl:\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        conv_out = convLayers(X)\n",
        "        output = linLayers(torch.flatten(conv_out, start_dim=1))\n",
        "\n",
        "        loss = crit(output, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(convLayers.parameters(), clip)\n",
        "        torch.nn.utils.clip_grad_norm_(linLayers.parameters(), clip)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dl)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElVRHm0X8Qnb"
      },
      "source": [
        "def hitsLoss(outputs, labels):\n",
        "    \"\"\"\n",
        "    Computes the amount of labels correct by taking the\n",
        "    argmax of the output.\n",
        "    Cannot be used for training!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    outputs: torch.tensor\n",
        "        model outputs\n",
        "    labels: torch.tensor\n",
        "        true labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    hits: float [0,1]\n",
        "        A number between 0 and 1 indicating how many labels\n",
        "        were classified correctly\n",
        "    \"\"\"\n",
        "    #outputs = [batch_size, n_classes]\n",
        "    #labels = [batch_size]\n",
        "    \n",
        "    batch_size = outputs.shape[0]\n",
        "    \n",
        "    preds = outputs.detach().argmax(dim=-1)\n",
        "    \n",
        "    hits = sum(preds == labels)/batch_size\n",
        "    \n",
        "    return hits"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sBkIaf5869C"
      },
      "source": [
        "def eval_cifar10(convLayers, linLayers, dl, crit):\n",
        "    \"\"\"\n",
        "    Evaluates convLayers together with linLayers on given DataLoader\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    convLayers: ConvLayers\n",
        "        ConvLayers object that will be trained\n",
        "    linLayers: LinLayers\n",
        "        LinLayers object that will be trained\n",
        "    dl: DataLoader\n",
        "        dataloader containing batched data\n",
        "    crit: nn.Criterion\n",
        "        criterion taking outputs from linLayers and labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    epoch_loss: float\n",
        "        Accumulated loss over epochs\n",
        "\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for X, y in dataloader:\n",
        "            #X = [batch_size, input_dim]\n",
        "            #y = [batch_size]\n",
        "\n",
        "            output = model(X[:,permutation])\n",
        "            #output = [batch_size, n_classes]\n",
        "            \n",
        "            loss = criterion(output, y)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYBWBlcgcZ1p"
      },
      "source": [
        "#### Pre-train Conv Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eFAjz0NcoQP"
      },
      "source": [
        "convLayers = ConvLayers(in_channels=CONFIG[\"channels\"],\n",
        "                        img_size=CONFIG[\"img_size\"])\n",
        "linLayers = LinLayers(in_features=convLayers.out_units,\n",
        "                      out_features=CONFIG[\"n_labels\"])\n",
        "\n",
        "opt = optim.Adam(list(convLayers.parameters()) + list(linLayers.parameters()),\n",
        "                 lr=LEARNING_RATE, betas=BETAS)\n",
        "\n",
        "crit = nn.CrossEntropyLoss()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_RL52lRcOKJ",
        "outputId": "54689644-eb98-47ef-c059-9f28685bcbb7"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss = train_cifar10(convLayers, linLayers, dl_train_cifar10, opt,\n",
        "                               crit, 1)\n",
        "    end_time = time.time()\n",
        "\n",
        "    e_mins, e_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {e_mins}m {e_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 57s\n",
            "\tTrain Loss: 1.576\n",
            "Epoch: 02 | Time: 0m 59s\n",
            "\tTrain Loss: 1.207\n",
            "Epoch: 03 | Time: 0m 57s\n",
            "\tTrain Loss: 1.023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn_X1aVE0PCe"
      },
      "source": [
        "torch.save(convLayers, \"convLayers_trained_cifar10.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuMWgbmW_GaI"
      },
      "source": [
        "## References\n",
        "\n",
        "Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, Technical Report, 2009, https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\n",
        "\n",
        "van de Ven, G.M., Siegelmann, H.T. & Tolias, A.S. Brain-inspired replay for continual learning with artificial neural networks. Nat Commun 11, 4069 (2020). https://doi.org/10.1038/s41467-020-17866-2\n",
        "\n",
        "Joshua T. Vogelstein, Jayanta Dey, Hayden S. Helm, Will LeVine, Ronak D. Mehta, Ali Geisa, Gido M. van de Ven, Emily Chang, Chenyu Gao, Weiwei Yang, Bryan Tower, Jonathan Larson, Christopher M. White, Carey E. Priebe, Omnidirectional Transfer for Quasilinear Lifelong Learning, 2021, [https://arxiv.org/abs/2004.12908v7](https://arxiv.org/abs/2004.12908v7)"
      ]
    }
  ]
}