{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pre_train_on_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jJyPBbDRTAc9"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNPg3G2fkKFaNCxF/fEmOpq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "374d049d88604955a74b4b2a2e14fae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6788421f7b064a77b6dcaedb1d76afb0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_de86cd0d82b6408cbd6eb7033cd40f11",
              "IPY_MODEL_50ef93519888425ca152a133e72ece7a",
              "IPY_MODEL_1ce172af8a9049b0a2a6bc6a7e9ef88c"
            ]
          }
        },
        "6788421f7b064a77b6dcaedb1d76afb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de86cd0d82b6408cbd6eb7033cd40f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b4c28a987a046f688b3910494434306",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2f792521f3234102a816ed0bc72f1d88"
          }
        },
        "50ef93519888425ca152a133e72ece7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2cfad9a05dd14aafa7efa18d587de768",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8cc7d86cb1f4919b5d14db82c82d8e1"
          }
        },
        "1ce172af8a9049b0a2a6bc6a7e9ef88c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72dfc8b01e4649efb6184a9a056f34c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 49205203.14it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7d0e6446d77c4f1e929e1b2a4c1899c1"
          }
        },
        "6b4c28a987a046f688b3910494434306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2f792521f3234102a816ed0bc72f1d88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2cfad9a05dd14aafa7efa18d587de768": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8cc7d86cb1f4919b5d14db82c82d8e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72dfc8b01e4649efb6184a9a056f34c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7d0e6446d77c4f1e929e1b2a4c1899c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielKP/continual-learning/blob/main/pre_train_on_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbT1OcF162un"
      },
      "source": [
        "# Pre-train 5 CNN layers on CIFAR10\n",
        "Author: Gabriel Kressin Palacios\n",
        "\n",
        "[this notebook is not needed if you have the model already]\n",
        "\n",
        "Pre-train a CNN with following properties:\n",
        "- 100 Epochs on [cifar10](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
        "(Krizhevsky 2009)\n",
        "- 5 convolutional layers with\n",
        "  - 16,32,64,128,256 output Channels respectively\n",
        "  - padding of 1\n",
        "  - with bias\n",
        "  - a stride of 1, 2, 2, 2, 2\n",
        "  - a kernel size of 3\n",
        "  - a BatchNormalization after each convolution\n",
        "  - a ReLU nonlinearity after each convolution\n",
        "\n",
        "\n",
        "This is to make our approach comparable to other approaches\n",
        "(van de Ven et al. 2020, Vogelstein et al. 2021)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIiQreFq4tJm"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uuyU1xG4Qsm"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lAC05aSaOS_"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "N_EPOCHS = 100\n",
        "LEARNING_RATE = 0.0001\n",
        "BETAS = (0.9, 0.999)\n",
        "CONFIG = {\n",
        "    \"dataset\": \"cifar10\",\n",
        "    \"img_size\": 32,\n",
        "    \"n_labels\": 10,\n",
        "    \"channels\": 3,\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_6cg58xHUWp"
      },
      "source": [
        "#### ConvLayers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmlI4X7D7JVd"
      },
      "source": [
        "class ConvLayers(nn.Module):\n",
        "    \"\"\"\n",
        "    5 torch convolutional layers with batchnormalization\n",
        "    and ReLU nonlinearity. No pooling is used. Inherits from nn.Module.\n",
        "    Requires images with the same height and width.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward(X) -> None\n",
        "        Applies convolutions on input X\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_size : int\n",
        "        number indicating the height of input images\n",
        "    in_channels : int\n",
        "        channels of input data\n",
        "    out_channels : int\n",
        "        amount of channels for last layer\n",
        "    out_size : int\n",
        "        height of the output data\n",
        "    out_units : int\n",
        "        total amount of units outputting\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, img_size):\n",
        "        \"\"\"\n",
        "        Creates 5 conv layers with batchnorm and ReLU activation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_channels : int\n",
        "            Amount of channels the input data has\n",
        "        img_size : int\n",
        "            Amount of pixels in one axis\n",
        "        \"\"\"\n",
        "        super(ConvLayers, self).__init__()\n",
        "\n",
        "        list_out_channels = [16,32,64,128,256]\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.out_size = int(math.ceil(img_size / 2**4))\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = list_out_channels[-1]\n",
        "        self.out_units = self.out_channels * self.out_size**2\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, list_out_channels[0],\n",
        "                               kernel_size=3, stride=1, bias=True, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(list_out_channels[0])\n",
        "        self.nl1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(list_out_channels[0], list_out_channels[1],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(list_out_channels[1])\n",
        "        self.nl2 = nn.ReLU()\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(list_out_channels[1], list_out_channels[2],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(list_out_channels[2])\n",
        "        self.nl3 = nn.ReLU()\n",
        "\n",
        "        self.conv4 = nn.Conv2d(list_out_channels[2], list_out_channels[3],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(list_out_channels[3])\n",
        "        self.nl4 = nn.ReLU()\n",
        "\n",
        "        self.conv5 = nn.Conv2d(list_out_channels[3], list_out_channels[4],\n",
        "                               kernel_size=3, stride=2, bias=True, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(list_out_channels[4])\n",
        "        self.nl5 = nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Applies convolutions on input X\n",
        "        \"\"\"\n",
        "\n",
        "        X1 = self.nl1(self.bn1(self.conv1(X)))\n",
        "        X2 = self.nl2(self.bn2(self.conv2(X1)))\n",
        "        X3 = self.nl3(self.bn3(self.conv3(X2)))\n",
        "        X4 = self.nl4(self.bn4(self.conv4(X3)))\n",
        "        X5 = self.nl5(self.bn5(self.conv5(X4)))\n",
        "\n",
        "        return X5"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WXIn5hsJBRe"
      },
      "source": [
        "#### LinLayers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPRft0SN62Wv"
      },
      "source": [
        "class LinLayers(nn.Module):\n",
        "    \"\"\"\n",
        "    Contains 2 fully connected layers with 2000 units each and a ReLU.\n",
        "    Additional output layer on top. Inherits from nn.Module\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    forward(X) -> None\n",
        "        Applies both layers on input data X\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    in_features : int\n",
        "        dimensionality of input to layers\n",
        "    out_features : int\n",
        "        dimensioality of output of layers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        in_features: int\n",
        "            input dimensionality of data\n",
        "        out_features: int\n",
        "            output dimensionality of data\n",
        "        \"\"\"\n",
        "        super(LinLayers, self).__init__()\n",
        "\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        hidden_dims = [2000, 2000]\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features, hidden_dims[0])\n",
        "        self.nl1 = nn.ReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.nl2 = nn.ReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], out_features)\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Applies both layers on input data X\n",
        "        \"\"\"\n",
        "\n",
        "        X1 = self.nl1(self.fc1(X))\n",
        "        X2 = self.nl2(self.fc2(X1))\n",
        "        X3 = self.fc3(X2)\n",
        "\n",
        "        return X3"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g2jkj2S4v2H"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxakK1ojQ1zG"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJyPBbDRTAc9"
      },
      "source": [
        "#### Compute Norm and std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-VfXD0URPBw"
      },
      "source": [
        "# un_ds_train_cifar10 = datasets.CIFAR10(\"data\", train=True,\n",
        "#                                        transform=transforms.ToTensor(),\n",
        "#                                        download=True)\n",
        "# un_dl_train_cifar10 = DataLoader(un_ds_train_cifar10,\n",
        "#                                  batch_size=len(un_ds_train_cifar10))\n",
        "# data, _ = next(iter(un_dl_train_cifar10))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvDxdDpNSQDK"
      },
      "source": [
        "# means = []\n",
        "# stds = []\n",
        "# for channel in range(data.shape[1]):\n",
        "#     means.append(data[:,channel].mean().item())\n",
        "#     stds.append(data[:,channel].std().item())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzV7JBxBXOMN"
      },
      "source": [
        "# print(means)\n",
        "# print(stds)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBeiWF463207"
      },
      "source": [
        "#### Load Norm and Std"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onAx0PSAYvVz"
      },
      "source": [
        "means = [0.491400808095932, 0.48215898871421814, 0.44653093814849854]\n",
        "stds = [0.24703224003314972, 0.24348513782024384, 0.26158785820007324]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZC9RXRGYe0W"
      },
      "source": [
        "#### DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "374d049d88604955a74b4b2a2e14fae7",
            "6788421f7b064a77b6dcaedb1d76afb0",
            "de86cd0d82b6408cbd6eb7033cd40f11",
            "50ef93519888425ca152a133e72ece7a",
            "1ce172af8a9049b0a2a6bc6a7e9ef88c",
            "6b4c28a987a046f688b3910494434306",
            "2f792521f3234102a816ed0bc72f1d88",
            "2cfad9a05dd14aafa7efa18d587de768",
            "d8cc7d86cb1f4919b5d14db82c82d8e1",
            "72dfc8b01e4649efb6184a9a056f34c6",
            "7d0e6446d77c4f1e929e1b2a4c1899c1"
          ]
        },
        "id": "rai3U66vYMa5",
        "outputId": "486809d7-5256-4f3f-8b3a-81bc012e4d62"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize(means, stds)])\n",
        "\n",
        "ds_train_cifar10 = datasets.CIFAR10(\"data\", train=True,\n",
        "                                  transform=transform,\n",
        "                                  download=True)\n",
        "ds_test_cifar10 = datasets.CIFAR10(\"data\", train=False,\n",
        "                                 transform=transform,\n",
        "                                 download=False)\n",
        "\n",
        "dl_train_cifar10 = DataLoader(ds_train_cifar10, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True)\n",
        "dl_test_cifar10 = DataLoader(ds_test_cifar10, batch_size=BATCH_SIZE,\n",
        "                             shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "374d049d88604955a74b4b2a2e14fae7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Oro5cYbXBK"
      },
      "source": [
        "#### Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXPauws6zgdr"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    \"\"\"\n",
        "    Computes minutes and seconds given start and end time\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    start_time: float\n",
        "        systemtime in ms at the start of interval to be measured\n",
        "    end_time: float\n",
        "        systemtime in ms at the end of interval to be measured\n",
        "    \"\"\"\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4_iGoCCZ2nw"
      },
      "source": [
        "def train_cifar10(convLayers, linLayers, dl, opt, crit, clip):\n",
        "    \"\"\"\n",
        "    trains convlayers on cifar10 dataset\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    convLayers: ConvLayers\n",
        "        ConvLayers object that will be trained\n",
        "    linLayers: LinLayers\n",
        "        LinLayers object that will be trained\n",
        "    dl: DataLoader\n",
        "        dataloader containing batched data\n",
        "    opt: optim.Optimizer\n",
        "        optimizer for linLayers and convLayers\n",
        "    crit: nn.Criterion\n",
        "        criterion taking outputs from linLayers and labels\n",
        "    clip: int\n",
        "        clips if gradients are too high\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    epoch_loss: float\n",
        "        Accumulated loss over epochs\n",
        "    \"\"\"\n",
        "\n",
        "    convLayers.train()\n",
        "    linLayers.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for X, y in dl:\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        conv_out = convLayers(X)\n",
        "        output = linLayers(torch.flatten(conv_out, start_dim=1))\n",
        "\n",
        "        loss = crit(output, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(convLayers.parameters(), clip)\n",
        "        torch.nn.utils.clip_grad_norm_(linLayers.parameters(), clip)\n",
        "\n",
        "        opt.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dl)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElVRHm0X8Qnb"
      },
      "source": [
        "def hitsLoss(outputs, labels):\n",
        "    \"\"\"\n",
        "    Computes the amount of labels correct by taking the\n",
        "    argmax of the output.\n",
        "    Cannot be used for training!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    outputs: torch.tensor\n",
        "        model outputs\n",
        "    labels: torch.tensor\n",
        "        true labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    hits: float [0,1]\n",
        "        A number between 0 and 1 indicating how many labels\n",
        "        were classified correctly\n",
        "    \"\"\"\n",
        "    #outputs = [batch_size, n_classes]\n",
        "    #labels = [batch_size]\n",
        "    \n",
        "    batch_size = outputs.shape[0]\n",
        "    \n",
        "    preds = outputs.detach().argmax(dim=-1)\n",
        "    \n",
        "    hits = sum(preds == labels)/batch_size\n",
        "    \n",
        "    return hits"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sBkIaf5869C"
      },
      "source": [
        "def eval_cifar10(convLayers, linLayers, dl, crit):\n",
        "    \"\"\"\n",
        "    Evaluates convLayers together with linLayers on given DataLoader\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    convLayers: ConvLayers\n",
        "        ConvLayers object that will be trained\n",
        "    linLayers: LinLayers\n",
        "        LinLayers object that will be trained\n",
        "    dl: DataLoader\n",
        "        dataloader containing batched data\n",
        "    crit: nn.Criterion\n",
        "        criterion taking outputs from linLayers and labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    epoch_loss: float\n",
        "        Accumulated loss over epochs\n",
        "\n",
        "    \"\"\"\n",
        "    convLayers.eval()\n",
        "    linLayers.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for X, y in dl:\n",
        "\n",
        "            conv_out = convLayers(X)\n",
        "            output = linLayers(torch.flatten(conv_out, start_dim=1))\n",
        "            \n",
        "            loss = crit(output, y)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dl)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYBWBlcgcZ1p"
      },
      "source": [
        "#### Pre-train Conv Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eFAjz0NcoQP"
      },
      "source": [
        "convLayers = ConvLayers(in_channels=CONFIG[\"channels\"],\n",
        "                        img_size=CONFIG[\"img_size\"])\n",
        "linLayers = LinLayers(in_features=convLayers.out_units,\n",
        "                      out_features=CONFIG[\"n_labels\"])\n",
        "\n",
        "opt = optim.Adam(list(convLayers.parameters()) + list(linLayers.parameters()),\n",
        "                 lr=LEARNING_RATE, betas=BETAS)\n",
        "\n",
        "crit = nn.CrossEntropyLoss()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAqeP-0YSXZO",
        "outputId": "255adf86-390f-4df1-8a3b-afca81795f37"
      },
      "source": [
        "t_acc = eval_cifar10(convLayers, linLayers, dl_train_cifar10, hitsLoss)\n",
        "te_acc = eval_cifar10(convLayers, linLayers, dl_test_cifar10, hitsLoss)\n",
        "print(f\"Accuracy before Training | Train {t_acc} | Test {te_acc}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy before Training | Train 0.09993223853561344 | Test 0.10205078125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_RL52lRcOKJ",
        "outputId": "f46769a2-b059-4c8e-9a62-bf8f6cacdc67"
      },
      "source": [
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss = train_cifar10(convLayers, linLayers, dl_train_cifar10, opt,\n",
        "                               crit, 1)\n",
        "    end_time = time.time()\n",
        "\n",
        "    e_mins, e_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {e_mins}m {e_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 13s\n",
            "\tTrain Loss: 1.603\n",
            "Epoch: 02 | Time: 1m 13s\n",
            "\tTrain Loss: 1.238\n",
            "Epoch: 03 | Time: 1m 13s\n",
            "\tTrain Loss: 1.061\n",
            "Epoch: 04 | Time: 1m 14s\n",
            "\tTrain Loss: 0.917\n",
            "Epoch: 05 | Time: 1m 14s\n",
            "\tTrain Loss: 0.778\n",
            "Epoch: 06 | Time: 1m 12s\n",
            "\tTrain Loss: 0.646\n",
            "Epoch: 07 | Time: 1m 12s\n",
            "\tTrain Loss: 0.520\n",
            "Epoch: 08 | Time: 1m 12s\n",
            "\tTrain Loss: 0.399\n",
            "Epoch: 09 | Time: 1m 12s\n",
            "\tTrain Loss: 0.290\n",
            "Epoch: 10 | Time: 1m 12s\n",
            "\tTrain Loss: 0.204\n",
            "Epoch: 11 | Time: 1m 12s\n",
            "\tTrain Loss: 0.143\n",
            "Epoch: 12 | Time: 1m 13s\n",
            "\tTrain Loss: 0.099\n",
            "Epoch: 13 | Time: 1m 12s\n",
            "\tTrain Loss: 0.073\n",
            "Epoch: 14 | Time: 1m 12s\n",
            "\tTrain Loss: 0.061\n",
            "Epoch: 15 | Time: 1m 12s\n",
            "\tTrain Loss: 0.051\n",
            "Epoch: 16 | Time: 1m 12s\n",
            "\tTrain Loss: 0.047\n",
            "Epoch: 17 | Time: 1m 12s\n",
            "\tTrain Loss: 0.041\n",
            "Epoch: 18 | Time: 1m 12s\n",
            "\tTrain Loss: 0.036\n",
            "Epoch: 19 | Time: 1m 13s\n",
            "\tTrain Loss: 0.040\n",
            "Epoch: 20 | Time: 1m 12s\n",
            "\tTrain Loss: 0.034\n",
            "Epoch: 21 | Time: 1m 13s\n",
            "\tTrain Loss: 0.034\n",
            "Epoch: 22 | Time: 1m 12s\n",
            "\tTrain Loss: 0.032\n",
            "Epoch: 23 | Time: 1m 12s\n",
            "\tTrain Loss: 0.032\n",
            "Epoch: 24 | Time: 1m 12s\n",
            "\tTrain Loss: 0.030\n",
            "Epoch: 25 | Time: 1m 12s\n",
            "\tTrain Loss: 0.031\n",
            "Epoch: 26 | Time: 1m 12s\n",
            "\tTrain Loss: 0.025\n",
            "Epoch: 27 | Time: 1m 12s\n",
            "\tTrain Loss: 0.028\n",
            "Epoch: 28 | Time: 1m 12s\n",
            "\tTrain Loss: 0.026\n",
            "Epoch: 29 | Time: 1m 12s\n",
            "\tTrain Loss: 0.027\n",
            "Epoch: 30 | Time: 1m 11s\n",
            "\tTrain Loss: 0.026\n",
            "Epoch: 31 | Time: 1m 11s\n",
            "\tTrain Loss: 0.025\n",
            "Epoch: 32 | Time: 1m 11s\n",
            "\tTrain Loss: 0.024\n",
            "Epoch: 33 | Time: 1m 11s\n",
            "\tTrain Loss: 0.023\n",
            "Epoch: 34 | Time: 1m 11s\n",
            "\tTrain Loss: 0.022\n",
            "Epoch: 35 | Time: 1m 12s\n",
            "\tTrain Loss: 0.023\n",
            "Epoch: 36 | Time: 1m 12s\n",
            "\tTrain Loss: 0.023\n",
            "Epoch: 37 | Time: 1m 12s\n",
            "\tTrain Loss: 0.024\n",
            "Epoch: 38 | Time: 1m 12s\n",
            "\tTrain Loss: 0.020\n",
            "Epoch: 39 | Time: 1m 12s\n",
            "\tTrain Loss: 0.021\n",
            "Epoch: 40 | Time: 1m 12s\n",
            "\tTrain Loss: 0.019\n",
            "Epoch: 41 | Time: 1m 11s\n",
            "\tTrain Loss: 0.018\n",
            "Epoch: 42 | Time: 1m 12s\n",
            "\tTrain Loss: 0.020\n",
            "Epoch: 43 | Time: 1m 12s\n",
            "\tTrain Loss: 0.017\n",
            "Epoch: 44 | Time: 1m 11s\n",
            "\tTrain Loss: 0.018\n",
            "Epoch: 45 | Time: 1m 11s\n",
            "\tTrain Loss: 0.020\n",
            "Epoch: 46 | Time: 1m 11s\n",
            "\tTrain Loss: 0.018\n",
            "Epoch: 47 | Time: 1m 11s\n",
            "\tTrain Loss: 0.020\n",
            "Epoch: 48 | Time: 1m 11s\n",
            "\tTrain Loss: 0.018\n",
            "Epoch: 49 | Time: 1m 11s\n",
            "\tTrain Loss: 0.017\n",
            "Epoch: 50 | Time: 1m 11s\n",
            "\tTrain Loss: 0.018\n",
            "Epoch: 51 | Time: 1m 11s\n",
            "\tTrain Loss: 0.017\n",
            "Epoch: 52 | Time: 1m 11s\n",
            "\tTrain Loss: 0.017\n",
            "Epoch: 53 | Time: 1m 10s\n",
            "\tTrain Loss: 0.017\n",
            "Epoch: 54 | Time: 1m 11s\n",
            "\tTrain Loss: 0.016\n",
            "Epoch: 55 | Time: 1m 11s\n",
            "\tTrain Loss: 0.015\n",
            "Epoch: 56 | Time: 1m 11s\n",
            "\tTrain Loss: 0.015\n",
            "Epoch: 57 | Time: 1m 11s\n",
            "\tTrain Loss: 0.015\n",
            "Epoch: 58 | Time: 1m 11s\n",
            "\tTrain Loss: 0.014\n",
            "Epoch: 59 | Time: 1m 11s\n",
            "\tTrain Loss: 0.016\n",
            "Epoch: 60 | Time: 1m 11s\n",
            "\tTrain Loss: 0.015\n",
            "Epoch: 61 | Time: 1m 11s\n",
            "\tTrain Loss: 0.018\n",
            "Epoch: 62 | Time: 1m 11s\n",
            "\tTrain Loss: 0.014\n",
            "Epoch: 63 | Time: 1m 11s\n",
            "\tTrain Loss: 0.016\n",
            "Epoch: 64 | Time: 1m 11s\n",
            "\tTrain Loss: 0.015\n",
            "Epoch: 65 | Time: 1m 11s\n",
            "\tTrain Loss: 0.013\n",
            "Epoch: 66 | Time: 1m 11s\n",
            "\tTrain Loss: 0.015\n",
            "Epoch: 67 | Time: 1m 11s\n",
            "\tTrain Loss: 0.013\n",
            "Epoch: 68 | Time: 1m 11s\n",
            "\tTrain Loss: 0.015\n",
            "Epoch: 69 | Time: 1m 11s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 70 | Time: 1m 11s\n",
            "\tTrain Loss: 0.014\n",
            "Epoch: 71 | Time: 1m 11s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 72 | Time: 1m 12s\n",
            "\tTrain Loss: 0.013\n",
            "Epoch: 73 | Time: 1m 12s\n",
            "\tTrain Loss: 0.011\n",
            "Epoch: 74 | Time: 1m 12s\n",
            "\tTrain Loss: 0.013\n",
            "Epoch: 75 | Time: 1m 12s\n",
            "\tTrain Loss: 0.013\n",
            "Epoch: 76 | Time: 1m 12s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 77 | Time: 1m 12s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 78 | Time: 1m 12s\n",
            "\tTrain Loss: 0.011\n",
            "Epoch: 79 | Time: 1m 12s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 80 | Time: 1m 12s\n",
            "\tTrain Loss: 0.013\n",
            "Epoch: 81 | Time: 1m 12s\n",
            "\tTrain Loss: 0.010\n",
            "Epoch: 82 | Time: 1m 12s\n",
            "\tTrain Loss: 0.009\n",
            "Epoch: 83 | Time: 1m 12s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 84 | Time: 1m 12s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 85 | Time: 1m 12s\n",
            "\tTrain Loss: 0.010\n",
            "Epoch: 86 | Time: 1m 12s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 87 | Time: 1m 12s\n",
            "\tTrain Loss: 0.011\n",
            "Epoch: 88 | Time: 1m 12s\n",
            "\tTrain Loss: 0.011\n",
            "Epoch: 89 | Time: 1m 12s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 90 | Time: 1m 11s\n",
            "\tTrain Loss: 0.011\n",
            "Epoch: 91 | Time: 1m 11s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 92 | Time: 1m 11s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 93 | Time: 1m 11s\n",
            "\tTrain Loss: 0.009\n",
            "Epoch: 94 | Time: 1m 11s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 95 | Time: 1m 11s\n",
            "\tTrain Loss: 0.011\n",
            "Epoch: 96 | Time: 1m 11s\n",
            "\tTrain Loss: 0.008\n",
            "Epoch: 97 | Time: 1m 11s\n",
            "\tTrain Loss: 0.010\n",
            "Epoch: 98 | Time: 1m 11s\n",
            "\tTrain Loss: 0.012\n",
            "Epoch: 99 | Time: 1m 11s\n",
            "\tTrain Loss: 0.010\n",
            "Epoch: 100 | Time: 1m 11s\n",
            "\tTrain Loss: 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8tqKZImRno1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c18a944-eee6-456a-ecd1-164196e1016d"
      },
      "source": [
        "t_acc2 = eval_cifar10(convLayers, linLayers, dl_train_cifar10, hitsLoss)\n",
        "te_acc2 = eval_cifar10(convLayers, linLayers, dl_test_cifar10, hitsLoss)\n",
        "print(f\"Accuracy after Training | Train {t_acc2} | Test {te_acc2}\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy after Training | Train 0.9977838010812292 | Test 0.61318359375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn_X1aVE0PCe"
      },
      "source": [
        "torch.save(convLayers, \"convLayers_trained_cifar10.pt\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuMWgbmW_GaI"
      },
      "source": [
        "## References\n",
        "\n",
        "Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, Technical Report, 2009, https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\n",
        "\n",
        "van de Ven, G.M., Siegelmann, H.T. & Tolias, A.S. Brain-inspired replay for continual learning with artificial neural networks. Nat Commun 11, 4069 (2020). https://doi.org/10.1038/s41467-020-17866-2\n",
        "\n",
        "Joshua T. Vogelstein, Jayanta Dey, Hayden S. Helm, Will LeVine, Ronak D. Mehta, Ali Geisa, Gido M. van de Ven, Emily Chang, Chenyu Gao, Weiwei Yang, Bryan Tower, Jonathan Larson, Christopher M. White, Carey E. Priebe, Omnidirectional Transfer for Quasilinear Lifelong Learning, 2021, [https://arxiv.org/abs/2004.12908v7](https://arxiv.org/abs/2004.12908v7)"
      ]
    }
  ]
}