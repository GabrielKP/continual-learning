{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "agl_autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "metadata": {
      "interpreter": {
        "hash": "fa0c181e15994ab92b77e0a9eeb7815659805982e17e67ed50eb685ba3cdee26"
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnant-urt22Y"
      },
      "source": [
        "# AGL Autoencoder\n",
        "\n",
        "Based on https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjgT9Azct22a"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import itertools\n",
        "\n",
        "import data"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVDD0NqSt22c"
      },
      "source": [
        "Seed for reproducability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rytihYxWt22d"
      },
      "source": [
        "SEED = 54321\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvhH1uQsvnV7"
      },
      "source": [
        "## Grammar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV39YqA5vnV8"
      },
      "source": [
        "class GrammarGen():\n",
        "    \"\"\"\n",
        "    Generates Grammar sequences from grammars, and offers other functionalities\n",
        "    Grammars are dictionaries:\n",
        "    - always have START\n",
        "    - all paths lead eventually to END\n",
        "    - Entries starting with the same letter\n",
        "      have same output\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, grammar=None):\n",
        "        if grammar is None:\n",
        "            self.grammar = data.g0()\n",
        "        else:\n",
        "            self.grammar = grammar\n",
        "\n",
        "        # find how many letters in grammar\n",
        "        self.len = len(set([token[0] for token in self.grammar if (token != 'START' and token != 'END')]))\n",
        "\n",
        "        # variable to check how many sequences have been generated for the grammaticality test\n",
        "        self.grammCheckMaxLen = -1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def generate(self, n):\n",
        "        \"\"\"Generates n tokens\"\"\"\n",
        "        ret = []\n",
        "        count = 0\n",
        "        hashtrack = set()\n",
        "        while count < n:\n",
        "            token = []\n",
        "            current = 'START'\n",
        "            while current != 'END':\n",
        "                # Append current\n",
        "                if current != 'START':\n",
        "                    token.append(current[0])\n",
        "                # Choose next\n",
        "                r = random.randint(0, len(self.grammar[current]) - 1)\n",
        "                current = self.grammar[current][r]\n",
        "            # Check if seq is already inside\n",
        "            tokenhash = ''.join([str(x) for x in token])\n",
        "            if tokenhash not in hashtrack:\n",
        "                hashtrack.add(tokenhash)\n",
        "                ret.append((token, ))\n",
        "                count += 1\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def generateAllGrammatical(self, maxlen=float('inf')):\n",
        "        \"\"\"Generates all grammatical sequences until length maxlen\"\"\"\n",
        "        def genAllHelp(seq, current):\n",
        "            if current == 'END':\n",
        "                return [seq]\n",
        "            if len(seq) >= maxlen:\n",
        "                return []\n",
        "            # Append Current\n",
        "            if current != 'START':\n",
        "                seq.append(current[0])\n",
        "            # Generate next possibilities\n",
        "            options = range(len(self.grammar[current]))\n",
        "            ret = [(genAllHelp(copy.copy(seq), self.grammar[current][i]))\n",
        "                   for i in options]\n",
        "            return itertools.chain(*ret)\n",
        "        return set([tuple(seq) for seq in genAllHelp([], 'START')])\n",
        "\n",
        "    def isGrammatical(self, seqs):\n",
        "        \"\"\"Check for grammaticality of sequences in seqs\"\"\"\n",
        "        maxlen = max([len(seq) for seq in seqs])\n",
        "        if self.grammCheckMaxLen < maxlen:\n",
        "            self.allGrammatical = self.generateAllGrammatical(maxlen)\n",
        "            self.grammCheckMaxLen = maxlen\n",
        "\n",
        "        return [tuple(seq) in self.allGrammatical for seq in seqs]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLVj3zdDt22d"
      },
      "source": [
        "## Data\n",
        "\n",
        "First, get the training and test sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH6UqaslvnV_"
      },
      "source": [
        "Define a Dataset for Sequences:\n",
        "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBmNkT78t22d"
      },
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Sequences\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, seqs):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            size (int): amount of sequences generated\n",
        "        \"\"\"\n",
        "        self.seqs = seqs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.seqs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.seqs[idx]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x7ocJD0vnWB"
      },
      "source": [
        "Define collate_batch for the Dataloader: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
        "\n",
        "Sequences are padded and their non-padded lengths are returned.\n",
        "Since pack_padded_sequences requires sequences to be sorted, they are sorted too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOBuBfPPt22e"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    seq_lens = []\n",
        "    processed_seqs = []\n",
        "    # Sort in descending order\n",
        "    batch.sort(reverse=True, key=(lambda x: len(x)))\n",
        "    # append start and end token\n",
        "    for seq in batch:\n",
        "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
        "        seq_lens.append(len(seq))\n",
        "        processed_seqs.append(torch.tensor(seq))\n",
        "    # pad\n",
        "    padded_seqs = pad_sequence(processed_seqs)\n",
        "    seq_lens = torch.tensor(seq_lens)\n",
        "    return padded_seqs.to(device), seq_lens.to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7eCduf-t22m"
      },
      "source": [
        "## Model\n",
        "\n",
        "\n",
        "### Encoder\n",
        "\n",
        "Next up, we define the encoder.\n",
        "\n",
        "The changes here all within the `forward` method. It now accepts the lengths of the source sentences as well as the sentences themselves. \n",
        "\n",
        "After the source sentence (padded automatically within the iterator) has been embedded, we can then use `pack_padded_sequence` on it with the lengths of the sentences. Note that the tensor containing the lengths of the sequences must be a CPU tensor as of the latest version of PyTorch, which we explicitly do so with `to('cpu')`. `packed_embedded` will then be our packed padded sequence. This can be then fed to our RNN as normal which will return `packed_outputs`, a packed tensor containing all of the hidden states from the sequence, and `hidden` which is simply the final hidden state from our sequence. `hidden` is a standard tensor and not packed in any way, the only difference is that as the input was a packed sequence, this tensor is from the final **non-padded element** in the sequence.\n",
        "\n",
        "We then unpack our `packed_outputs` using `pad_packed_sequence` which returns the `outputs` and the lengths of each, which we don't need. \n",
        "\n",
        "The first dimension of `outputs` is the padded sequence lengths however due to using a packed padded sequence the values of tensors when a padding token was the input will be all zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P57X-q51t22n"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src, src_len):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "                \n",
        "        #need to explicitly put lengths on cpu!\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
        "                \n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "                                 \n",
        "        #packed_outputs is a packed sequence containing all hidden states\n",
        "        #hidden is now from the final non-padded element in the batch\n",
        "            \n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
        "            \n",
        "        #outputs is now a non-packed sequence, all hidden states obtained\n",
        "        #  when the input is a pad token are all zeros\n",
        "            \n",
        "        #outputs = [src len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "        \n",
        "        #outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        \n",
        "        return outputs, hidden"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le1kHQOIt22o"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_t5icjNt22o"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "  \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        #attention = [batch size, src len]\n",
        "        \n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        return F.softmax(attention, dim = 1)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v371q1Tkt22q"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECjR4jZot22q"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs, mask):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        #mask = [batch size, src len]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs, mask)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "        \n",
        "        #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [seq len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden.squeeze(0), a.squeeze(1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toSHWJmEt22q"
      },
      "source": [
        "### Seq2Seq\n",
        "\n",
        "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
        "\n",
        "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
        "\n",
        "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
        "\n",
        "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
        "\n",
        "The attention at each time-step is stored in the `attentions` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkcTBfr-t22s"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "                    \n",
        "        batch_size = src.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "\n",
        "        #mask = [batch size, src len]\n",
        "                \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
        "            #  and mask\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        return outputs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKt9wtL6t22s"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Deyfd3OByLSC"
      },
      "source": [
        "### Cosine Loss, Init_weights, count_parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X9xZXFQvnWF"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "class CosineLoss():\n",
        "    def __init__(self, vocabsize, ignore_index):\n",
        "        self.vocabsize = vocabsize\n",
        "        self.ignore_index = ignore_index\n",
        "        self.eye = torch.eye(self.vocabsize).to(device)\n",
        "        self.cosSim = nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def __call__(self, outputs, labels):\n",
        "        maxlen = outputs.shape[0]\n",
        "        bs = outputs.shape[1]\n",
        "        ignore_positions = (labels == self.ignore_index).to(device)\n",
        "        outputs[ignore_positions] = self.eye[self.ignore_index]\n",
        "        labels_onehot = torch.empty((maxlen, bs, self.vocabsize)).to(device)\n",
        "        for idx in range(maxlen):\n",
        "            labels_onehot[idx,:,:] = self.eye[labels[idx,:]]\n",
        "\n",
        "        batch_first_labels = labels_onehot.permute(1,0,2)\n",
        "        processed_labels = batch_first_labels.reshape(-1, maxlen * self.vocabsize)\n",
        "        batch_first_outputs = outputs.permute(1,0,2)\n",
        "        processed_outputs = batch_first_outputs.reshape(-1, maxlen * self.vocabsize)\n",
        "        return (1 - self.cosSim(processed_labels, processed_outputs)).mean()"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3d2C9Rvt22w"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fut2CtQrt22w"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src, src_len = batch\n",
        "        trg = src\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(src, src_len, trg)\n",
        "        \n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        \n",
        "        if isinstance(criterion, CosineLoss):\n",
        "            loss = criterion(output, trg)\n",
        "        else:\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "            \n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIT6a8uqt22w"
      },
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for seq, seq_len in dataloader:\n",
        "\n",
        "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
        "            \n",
        "            #seq = [seq_len, batch_size]\n",
        "            #output = [seq_len, batch_size, output_dim]\n",
        "\n",
        "            if isinstance(criterion, CosineLoss):\n",
        "                loss = criterion(output, seq)\n",
        "            else:\n",
        "                output_dim = output.shape[-1]\n",
        "                \n",
        "                output = output[1:].view(-1, output_dim)\n",
        "                trg = seq[1:].view(-1)\n",
        "                \n",
        "                #trg = [(trg len - 1) * batch size]\n",
        "                #output = [(trg len - 1) * batch size, output dim]\n",
        "                \n",
        "                loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "    return epoch_loss / len(dataloader)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOtABRKLvnWI"
      },
      "source": [
        "def evaluate_extra(model, dataloader, loss_func):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    loss_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for seqs, seqs_len in dataloader:\n",
        "\n",
        "            outputs = model(seqs, seqs_len, seqs, 0)\n",
        "\n",
        "            loss = loss_func(outputs, seqs) / seqs.shape[1]\n",
        "\n",
        "            loss_total += loss.item()\n",
        "        \n",
        "#        if loss_func == allOrNoneLoss:\n",
        "#            return loss_total\n",
        "\n",
        "        return loss_total / len(dataloader)\n",
        "\n",
        "def cutEndToken(seq):\n",
        "    ret = []\n",
        "    for stim in seq:\n",
        "        if stim == END_TOKEN:\n",
        "            break\n",
        "        ret.append(stim)\n",
        "    return ret\n",
        "\n",
        "\n",
        "def allOrNoneLoss(output, trg):\n",
        "    bs = output.shape[1]\n",
        "    ret = 0\n",
        "    pred = output.argmax(-1)[1:]\n",
        "    trg = trg[1:]\n",
        "    for b in range(bs):\n",
        "        p = cutEndToken(pred[:,b].tolist())\n",
        "        t = cutEndToken(trg[:,b].tolist())\n",
        "        ret += not p == t\n",
        "    return torch.tensor(ret)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW3r-pjXt22x"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUY7o5eGt22x"
      },
      "source": [
        "During Training in addition to collecting the train/validation loss, collect the amount of entirely correct predicted sequences on the train and test gr/ugr set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAVqiZ3kvnWJ"
      },
      "source": [
        "def fit(model, task_id, epochs, step_size_evaluation, clip ):\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
        "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
        "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
        "    # [task_id, dataset, evaluations]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_loss = train(model, train_dls[task_id], optimizer, criterion, clip)\n",
        "        valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), SAVENAME)\n",
        "\n",
        "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
        "            idx = epoch//STEP_SIZE_EVALUATION\n",
        "            for other_id in range(task_id + 1):\n",
        "                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], criterion)\n",
        "                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], criterion)\n",
        "                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], criterion)\n",
        "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
        "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
        "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
        "\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        \n",
        "    return total_loss, total_hits"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wml725COvnWJ"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "1. Load in all the data\n",
        "2. Set parameters\n",
        "3. Define plotting functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4DkjyxLvnWJ"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HREfj6IuvnWJ"
      },
      "source": [
        "SEED = 54321\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pLZpHqVvnWJ"
      },
      "source": [
        "Set base tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K5p-0jyvnWK"
      },
      "source": [
        "PAD_TOKEN = 0\n",
        "START_TOKEN = 1\n",
        "END_TOKEN = 2"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhSjYo6vnWK"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWwIlfPcvnWL"
      },
      "source": [
        "train_seqs = data.g1_train()\n",
        "valid_seqs = data.g1_train()\n",
        "test_seqs = data.g1_test_gr()\n",
        "test_ugr_seqs = data.g1_test_ugr_balanced()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5I5lDz8vnWL"
      },
      "source": [
        "Sort for better perfomance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocjMQF-RvnWL"
      },
      "source": [
        "train_seqs.sort(key=(lambda x: len(x)))\n",
        "valid_seqs.sort(key=(lambda x: len(x)))\n",
        "test_seqs.sort(key=(lambda x: len(x)))\n",
        "test_ugr_seqs.sort(key=(lambda x: len(x)))"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMSM652MvnWL"
      },
      "source": [
        "def buildVocab(letterset):\n",
        "    vocab = {'<pad>': PAD_TOKEN, '<sos>': START_TOKEN, '<eos>': END_TOKEN}\n",
        "    counter = len(vocab)\n",
        "    for letter in letterset:\n",
        "        vocab[letter] = counter\n",
        "        counter +=1\n",
        "    return vocab"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBCWnEM1vnWM"
      },
      "source": [
        "letters = set()\n",
        "for seq in train_seqs:\n",
        "    [letters.add(letter) for letter in seq]\n",
        "letters = list(letters)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPjDWlVPvnWM"
      },
      "source": [
        "Make all tasks, creates an additional task with all sequences from all tasks mashed together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtuaqeKNvnWM"
      },
      "source": [
        "N_TASKS = 2\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "train_dls = []\n",
        "valid_dls = []\n",
        "test_dls = []\n",
        "test_ugr_dls = []\n",
        "vocabs = []\n",
        "rvocabs = []\n",
        "let2idxs = []\n",
        "idx2lets = []\n",
        "\n",
        "usedpermutations = set()\n",
        "train_convs = []\n",
        "valid_convs = []\n",
        "test_convs = []\n",
        "test_ugr_convs = []\n",
        "for t in range(N_TASKS + 1):\n",
        "    # Create normal tasks\n",
        "    if t != N_TASKS:\n",
        "        temp_letters = copy.copy(letters)\n",
        "\n",
        "        while str(temp_letters) in usedpermutations:\n",
        "            random.shuffle(temp_letters)\n",
        " \n",
        "        usedpermutations.add(str(temp_letters))\n",
        "\n",
        "        # Vocab\n",
        "        vocabs.append(buildVocab(temp_letters))\n",
        "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
        "\n",
        "        # Conversion Functions\n",
        "        let2idxs.append(lambda seq: [vocabs[-1][let] for let in seq])\n",
        "        idx2lets.append(lambda seq: [rvocabs[-1][let] for let in seq])\n",
        "\n",
        "        # Convert to indices\n",
        "        train_conv = [let2idxs[-1](seq) for seq in train_seqs]\n",
        "        valid_conv = [let2idxs[-1](seq) for seq in valid_seqs]\n",
        "        test_conv = [let2idxs[-1](seq) for seq in test_seqs]\n",
        "        test_ugr_conv = [let2idxs[-1](seq) for seq in test_ugr_seqs]\n",
        "\n",
        "        # Add conv seq to sequence collection\n",
        "        train_convs.extend(train_conv)\n",
        "        valid_convs.extend(valid_conv)\n",
        "        test_convs.extend(test_conv)\n",
        "        test_ugr_convs.extend(test_ugr_conv)\n",
        "\n",
        "    # Create joint task\n",
        "    else:\n",
        "        train_conv = train_convs\n",
        "        valid_conv = valid_convs\n",
        "        test_conv = test_convs\n",
        "        test_ugr_conv = test_ugr_convs\n",
        "\n",
        "    # Datasets\n",
        "    train_ds = SequenceDataset(train_conv)\n",
        "    valid_ds = SequenceDataset(valid_conv)\n",
        "    test_ds = SequenceDataset(test_conv)\n",
        "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
        "    \n",
        "    # Dataloader\n",
        "    train_dls.append(\n",
        "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                   shuffle=True, collate_fn=collate_batch))\n",
        "    valid_dls.append(\n",
        "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
        "                   shuffle=False, collate_fn=collate_batch))\n",
        "    test_dls.append(\n",
        "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
        "                   shuffle=False, collate_fn=collate_batch))\n",
        "    test_ugr_dls.append(\n",
        "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
        "                   shuffle=False, collate_fn=collate_batch))    "
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVaRvD2svnWN",
        "outputId": "a0b24358-99af-46e5-b1e8-43b10696c440"
      },
      "source": [
        "print(vocabs[0])\n",
        "print(vocabs[1])\n",
        "for i in range(len(valid_dls) - 1):\n",
        "    for seqs, _ in valid_dls[i]:\n",
        "        print(f\"\\nFirst Batch of Task {i}:\")\n",
        "        print(seqs)\n",
        "        break"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'Z': 3, 'W': 4, 'P': 5, 'S': 6, 'N': 7}\n",
            "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'W': 3, 'P': 4, 'N': 5, 'Z': 6, 'S': 7}\n",
            "\n",
            "First Batch of Task 0:\n",
            "tensor([[1, 1, 1, 1],\n",
            "        [4, 4, 4, 7],\n",
            "        [6, 4, 6, 7],\n",
            "        [6, 6, 4, 3],\n",
            "        [4, 7, 3, 2],\n",
            "        [3, 3, 2, 0],\n",
            "        [2, 2, 0, 0]], device='cuda:0')\n",
            "\n",
            "First Batch of Task 1:\n",
            "tensor([[1, 1, 1, 1],\n",
            "        [3, 3, 3, 5],\n",
            "        [7, 3, 7, 5],\n",
            "        [7, 7, 3, 6],\n",
            "        [3, 5, 6, 2],\n",
            "        [6, 6, 2, 0],\n",
            "        [2, 2, 0, 0]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xoKXlA1vnWQ"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc0kqVE-vnWQ"
      },
      "source": [
        "INPUT_DIM = max(vocabs[0].values()) + 1\n",
        "OUTPUT_DIM = max(vocabs[0].values()) + 1\n",
        "ENC_EMB_DIM = 150\n",
        "DEC_EMB_DIM = 150\n",
        "ENC_HID_DIM = 18\n",
        "DEC_HID_DIM = 18\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "LEARNING_RATE = 0.001\n",
        "SRC_PAD_IDX = PAD_TOKEN\n",
        "TRG_PAD_IDX = PAD_TOKEN\n",
        "PREFIX = \"tr\"\n",
        "N_EPOCHS = 1000\n",
        "CLIP = 1\n",
        "STEP_SIZE_EVALUATION = 10"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpwRQ7xLvnWR"
      },
      "source": [
        "## Plotting & Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6vb8He6vnWR"
      },
      "source": [
        "def plotTransfer(data, title):\n",
        "    # data = [n_methods, n_tasks, combinedepochs]\n",
        "    n_methods, n_tasks, n_combinedepochs = data.shape\n",
        "    fig, axs = plt.subplots(n_tasks, 1)\n",
        "    colors = ['blue','green','orange','red','yellow','violett']\n",
        "    \n",
        "    xvals = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
        "    \n",
        "    for task_idx in range(n_tasks):\n",
        "        for method_idx in range(n_methods):\n",
        "            axs[task_idx].plot(\n",
        "                xvals,\n",
        "                data[method_idx, task_idx],\n",
        "                color=colors[method_idx]\n",
        "            )\n",
        "            axs[task_idx].set_ylim(0,1.1)\n",
        "            if task_idx != n_tasks - 1:\n",
        "                axs[task_idx].tick_params(\n",
        "                    axis='x',\n",
        "                    which='both',\n",
        "                    labelbottom=False\n",
        "                )\n",
        "        x_lines = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, N_EPOCHS)\n",
        "        for xpos in x_lines:\n",
        "            axs[task_idx].axvline(xpos, color=\"grey\")\n",
        "    fig.suptitle(title)\n",
        "    fig.tight_layout()\n",
        "    \n",
        "def plotResults(hist_loss, hist_hits):\n",
        "    plotTransfer( hist_loss[:,0,:].unsqueeze(0), \"Train Loss\")\n",
        "    plotTransfer( hist_loss[:,1,:].unsqueeze(0), \"Test Gr Loss\")\n",
        "    plotTransfer( hist_loss[:,2,:].unsqueeze(0), \"Test Ugr Loss\")\n",
        "    plotTransfer( hist_hits[:,0,:].unsqueeze(0), \"Train Hits\")\n",
        "    plotTransfer( hist_hits[:,1,:].unsqueeze(0), \"Test Gr Hits\")\n",
        "    plotTransfer( hist_hits[:,2,:].unsqueeze(0), \"Test Ugr Hits\")"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQWqbao6vnWR"
      },
      "source": [
        "def visual_eval(model, test_dl, ggen=None):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    errors = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(test_dl):\n",
        "\n",
        "            src, src_len = batch\n",
        "            trg = src\n",
        "\n",
        "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
        "            show_batch(output, trg)\n",
        "\n",
        "\n",
        "def show_batch(output, trg):\n",
        "    bs = output.shape[1]\n",
        "    pred = output.argmax(-1)[1:]\n",
        "    trg = trg[1:]\n",
        "    for b in range(bs):\n",
        "        p = cutEndToken(pred[:,b].tolist())\n",
        "        t = cutEndToken(trg[:,b].tolist())\n",
        "        status = \"same\" if p == t else \"different\"\n",
        "        print(f\"pred = {p} - {status} \\ntrg  = {t}\\n-\")"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPn1QguwvnWR"
      },
      "source": [
        "def accuracy(model):\n",
        "    for task_id in range(N_TASKS + 1):\n",
        "        gr_not_hits = evaluate_extra(model, test_dls[task_id], allOrNoneLoss)\n",
        "        ugr_not_hits = evaluate_extra(model, test_ugr_dls[task_id], allOrNoneLoss)\n",
        "        gr_hits = 1 - gr_not_hits\n",
        "        ugr_hits = 1 - ugr_not_hits\n",
        "        total_acc = (gr_hits + ugr_not_hits) / 2\n",
        "        print(f\"Task {task_id}: Acc {total_acc:2.2}% | Gr acc {gr_hits:2.2} | Ugr acc {ugr_not_hits:2.2}\")\n",
        "        \n",
        "def accuracyAll(models):\n",
        "    for model_id in range(len(models)):\n",
        "        print(f\"\\nModel {model_id}\")\n",
        "        accuracy(models[model_id])"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VCOcUbMvnWS"
      },
      "source": [
        "## Baseline A: Individual Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9vjyK31vnWT"
      },
      "source": [
        "SEED = 54321\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS5ogr-8vnWT",
        "outputId": "14a1c972-638e-4393-c971-f128a896b298"
      },
      "source": [
        "models_A = []\n",
        "hist_losses_A = []\n",
        "hist_hitsss_A = []\n",
        "for n_task in range(N_TASKS + 1):\n",
        "    SUFFIX = f\"A{n_task}\"\n",
        "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
        "    LOADNAME = \"models/autosave/\" + title + \".pt\"\n",
        "    SAVENAME = \"models/autosave/\" + title + \".pt\"\n",
        "    PLOTSAVE = \"plots/autosave/\" + title + \".png\"\n",
        "\n",
        "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
        "    \n",
        "    print(title)\n",
        "    print(model.apply(init_weights))\n",
        "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
        "    \n",
        "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
        "    hist_losses_A.append(hist_loss_temp)\n",
        "    hist_hitsss_A.append(hist_hits_temp)\n",
        "    models_A.append(model)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 341 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
            "Epoch: 342 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
            "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
            "Epoch: 343 | Time: 0m 0s\n",
            "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
            "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
            "Epoch: 344 | Time: 0m 0s\n",
            "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
            "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
            "Epoch: 345 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
            "\t Val. Loss: 0.130 |  Val. PPL:   1.138\n",
            "Epoch: 346 | Time: 0m 0s\n",
            "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
            "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
            "Epoch: 347 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
            "Epoch: 348 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
            "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
            "Epoch: 349 | Time: 0m 0s\n",
            "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
            "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
            "Epoch: 350 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
            "Epoch: 351 | Time: 0m 0s\n",
            "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
            "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
            "Epoch: 352 | Time: 0m 0s\n",
            "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
            "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
            "Epoch: 353 | Time: 0m 0s\n",
            "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
            "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
            "Epoch: 354 | Time: 0m 0s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
            "Epoch: 355 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
            "Epoch: 356 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
            "Epoch: 357 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
            "Epoch: 358 | Time: 0m 0s\n",
            "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
            "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
            "Epoch: 359 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
            "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
            "Epoch: 360 | Time: 0m 0s\n",
            "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
            "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
            "Epoch: 361 | Time: 0m 0s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
            "Epoch: 362 | Time: 0m 0s\n",
            "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
            "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
            "Epoch: 363 | Time: 0m 0s\n",
            "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
            "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
            "Epoch: 364 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
            "Epoch: 365 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.145 |  Val. PPL:   1.155\n",
            "Epoch: 366 | Time: 0m 0s\n",
            "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
            "Epoch: 367 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
            "Epoch: 368 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
            "Epoch: 369 | Time: 0m 0s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
            "Epoch: 370 | Time: 0m 0s\n",
            "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
            "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
            "Epoch: 371 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
            "Epoch: 372 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
            "Epoch: 373 | Time: 0m 0s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
            "Epoch: 374 | Time: 0m 0s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
            "Epoch: 375 | Time: 0m 0s\n",
            "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
            "Epoch: 376 | Time: 0m 0s\n",
            "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
            "Epoch: 377 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
            "Epoch: 378 | Time: 0m 0s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
            "Epoch: 379 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
            "Epoch: 380 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
            "Epoch: 381 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
            "Epoch: 382 | Time: 0m 0s\n",
            "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
            "Epoch: 383 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
            "Epoch: 384 | Time: 0m 0s\n",
            "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
            "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
            "Epoch: 385 | Time: 0m 0s\n",
            "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
            "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
            "Epoch: 386 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
            "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
            "Epoch: 387 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
            "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
            "Epoch: 388 | Time: 0m 0s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
            "Epoch: 389 | Time: 0m 0s\n",
            "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
            "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
            "Epoch: 390 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
            "Epoch: 391 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
            "Epoch: 392 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
            "Epoch: 393 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
            "Epoch: 394 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
            "Epoch: 395 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
            "Epoch: 396 | Time: 0m 0s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
            "Epoch: 397 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
            "Epoch: 398 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
            "Epoch: 399 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
            "Epoch: 400 | Time: 0m 0s\n",
            "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
            "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
            "Epoch: 401 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
            "Epoch: 402 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
            "Epoch: 403 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
            "Epoch: 404 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
            "Epoch: 405 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
            "\t Val. Loss: 0.131 |  Val. PPL:   1.139\n",
            "Epoch: 406 | Time: 0m 0s\n",
            "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
            "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
            "Epoch: 407 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
            "Epoch: 408 | Time: 0m 0s\n",
            "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
            "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
            "Epoch: 409 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
            "Epoch: 410 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
            "Epoch: 411 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
            "Epoch: 412 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
            "Epoch: 413 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
            "Epoch: 414 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
            "Epoch: 415 | Time: 0m 0s\n",
            "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
            "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
            "Epoch: 416 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
            "Epoch: 417 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
            "Epoch: 418 | Time: 0m 0s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
            "Epoch: 419 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
            "Epoch: 420 | Time: 0m 0s\n",
            "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
            "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
            "Epoch: 421 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
            "Epoch: 422 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
            "Epoch: 423 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
            "Epoch: 424 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
            "Epoch: 425 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
            "Epoch: 426 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
            "Epoch: 427 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
            "Epoch: 428 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 429 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 430 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
            "Epoch: 431 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "Epoch: 432 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 433 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
            "Epoch: 434 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "Epoch: 435 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
            "Epoch: 436 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
            "Epoch: 437 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "Epoch: 438 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "Epoch: 439 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 440 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
            "Epoch: 441 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 442 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 443 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 444 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 445 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
            "Epoch: 446 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 447 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 448 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 449 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "Epoch: 450 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
            "Epoch: 451 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 452 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 453 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 454 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 455 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "Epoch: 456 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
            "Epoch: 457 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 458 | Time: 0m 0s\n",
            "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
            "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
            "Epoch: 459 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 460 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 461 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "Epoch: 462 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 463 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 464 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 465 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
            "Epoch: 466 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 467 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 468 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 469 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 470 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 471 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 472 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 473 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 474 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 475 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 476 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 477 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 478 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 479 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 480 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 481 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 482 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 483 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
            "Epoch: 484 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "Epoch: 485 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 486 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 487 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 488 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 489 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 490 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 491 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 492 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
            "Epoch: 493 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 494 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 495 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 496 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 497 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 498 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 499 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
            "Epoch: 500 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 501 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 502 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 503 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 504 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 505 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 506 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 507 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 508 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Epoch: 509 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 510 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Epoch: 511 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 512 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 513 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 514 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 515 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 516 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "Epoch: 517 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 518 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 519 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 520 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 521 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 522 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 523 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 524 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 525 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 526 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 527 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 528 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 529 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 530 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 531 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 532 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 533 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 534 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 535 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 536 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 537 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 538 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 539 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 540 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 541 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 542 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 543 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 544 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 545 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 546 | Time: 0m 0s\n",
            "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 547 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 548 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 549 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 550 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 551 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 552 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 553 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 554 | Time: 0m 0s\n",
            "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 555 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 556 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 557 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 558 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 559 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 560 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 561 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 562 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 563 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 564 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 565 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 566 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 567 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 568 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 569 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 570 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 571 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 572 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 573 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 574 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 575 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 576 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 577 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 578 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 579 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 580 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 581 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 582 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 583 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 584 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 585 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 586 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 587 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 588 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 589 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 590 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 591 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 592 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 593 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 594 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 595 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 596 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 597 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 598 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 599 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 600 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 601 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 602 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 603 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 604 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 605 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 606 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 607 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 608 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 609 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 610 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 611 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 612 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 613 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 614 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 615 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 616 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 617 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 618 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 619 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 620 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 621 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 622 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 623 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 624 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 625 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 626 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 627 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 628 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 629 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 630 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 631 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 632 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 633 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 634 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 635 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 636 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 637 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 638 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 639 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 640 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 641 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 642 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 643 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 644 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 645 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 646 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 647 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 648 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 649 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 650 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 651 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 652 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 653 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 654 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 655 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 656 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 657 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 658 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 659 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 660 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 661 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 662 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 663 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 664 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 665 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 666 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 667 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 668 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 669 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 670 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 671 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 672 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 673 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 674 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 675 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 676 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 677 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 678 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 679 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 680 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 681 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 682 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 683 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 684 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 685 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 686 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 687 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 688 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 689 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 690 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 691 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 692 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 693 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 694 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 695 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 696 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 697 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 698 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 699 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 700 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 701 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 702 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 703 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 704 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 705 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 706 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 707 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 708 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 709 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 710 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 711 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 712 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 713 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 714 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 715 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 716 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 717 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 718 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 719 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 720 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 721 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 722 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 723 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 724 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 725 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 726 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 727 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 728 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 729 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 730 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 731 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 732 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 733 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 734 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 735 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 736 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 737 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 738 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 739 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 740 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 741 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 742 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 743 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 744 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 745 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 746 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 747 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 748 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 749 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 750 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 751 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 752 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 753 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 754 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 755 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 756 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 757 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 758 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 759 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 760 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 761 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 762 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 763 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 764 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 765 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 766 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 767 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 768 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 769 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 770 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 771 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 772 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 773 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 774 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 775 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 776 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 777 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 778 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 779 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 780 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 781 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 782 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 783 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 784 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 785 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 786 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 787 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 788 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 789 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 790 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 791 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 792 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 793 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 794 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 795 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 796 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 797 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 798 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 799 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 800 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 801 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 802 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 803 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 804 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 805 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 806 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 807 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 808 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 809 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 810 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 811 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 812 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 813 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 814 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 815 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 816 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 817 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 818 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 819 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 820 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 821 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 822 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 823 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 824 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 825 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 826 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 827 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 828 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 829 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 830 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 831 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 832 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 833 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 834 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 835 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 836 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 837 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 838 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 839 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 840 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 841 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 842 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 843 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 844 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 845 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 846 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 847 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 848 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 849 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 850 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 851 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 852 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 853 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 854 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 855 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 856 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 857 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 858 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 859 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 860 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 861 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 862 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 863 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 864 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 865 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 866 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 867 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 868 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 869 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 870 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 871 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 872 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 873 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 874 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 875 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 876 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 877 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 878 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 879 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 880 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 881 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 882 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 883 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 884 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 885 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 886 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 887 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 888 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 889 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 890 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 891 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 892 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 893 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 894 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 895 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 896 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 897 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 898 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 899 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 900 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 901 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 902 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 903 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 904 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 905 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 906 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 907 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 908 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 909 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 910 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 911 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 912 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 913 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 914 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 915 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 916 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 917 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 918 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 919 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 920 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 921 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 922 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 923 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 924 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 925 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 926 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 927 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 928 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 929 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 930 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 931 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 932 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 933 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 934 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 935 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 936 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 937 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 938 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 939 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 940 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 941 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 942 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 943 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 944 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 945 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 946 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 947 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 948 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 949 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 950 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 951 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 952 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 953 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 954 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 955 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 956 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 957 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 958 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 959 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 960 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 961 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 962 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 963 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 964 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 965 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 966 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 967 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 968 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 969 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 970 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 971 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 972 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 973 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 974 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 975 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 976 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 977 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 978 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 979 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 980 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 981 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 982 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 983 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 984 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 985 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 986 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 987 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 988 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 989 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 990 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 991 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 992 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 993 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 994 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 995 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 996 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 997 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 998 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 999 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 1000 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "tr-AE-150-18-0.001-A2\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Embedding(8, 150)\n",
            "    (rnn): GRU(150, 18, bidirectional=True)\n",
            "    (fc): Linear(in_features=36, out_features=18, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (attention): Attention(\n",
            "      (attn): Linear(in_features=54, out_features=18, bias=True)\n",
            "      (v): Linear(in_features=18, out_features=1, bias=False)\n",
            "    )\n",
            "    (embedding): Embedding(8, 150)\n",
            "    (rnn): GRU(186, 18)\n",
            "    (fc_out): Linear(in_features=204, out_features=8, bias=True)\n",
            "    (dropout): Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            ")\n",
            "The model has 35198 trainable parameters\n",
            "Epoch: 01 | Time: 0m 0s\n",
            "\tTrain Loss: 0.585 | Train PPL:   1.795\n",
            "\t Val. Loss: 0.599 |  Val. PPL:   1.820\n",
            "Epoch: 02 | Time: 0m 0s\n",
            "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
            "\t Val. Loss: 0.597 |  Val. PPL:   1.817\n",
            "Epoch: 03 | Time: 0m 0s\n",
            "\tTrain Loss: 0.520 | Train PPL:   1.683\n",
            "\t Val. Loss: 0.576 |  Val. PPL:   1.780\n",
            "Epoch: 04 | Time: 0m 0s\n",
            "\tTrain Loss: 0.518 | Train PPL:   1.678\n",
            "\t Val. Loss: 0.556 |  Val. PPL:   1.743\n",
            "Epoch: 05 | Time: 0m 0s\n",
            "\tTrain Loss: 0.496 | Train PPL:   1.642\n",
            "\t Val. Loss: 0.539 |  Val. PPL:   1.714\n",
            "Epoch: 06 | Time: 0m 0s\n",
            "\tTrain Loss: 0.494 | Train PPL:   1.639\n",
            "\t Val. Loss: 0.532 |  Val. PPL:   1.702\n",
            "Epoch: 07 | Time: 0m 0s\n",
            "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
            "\t Val. Loss: 0.519 |  Val. PPL:   1.681\n",
            "Epoch: 08 | Time: 0m 0s\n",
            "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
            "\t Val. Loss: 0.511 |  Val. PPL:   1.666\n",
            "Epoch: 09 | Time: 0m 0s\n",
            "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
            "\t Val. Loss: 0.499 |  Val. PPL:   1.647\n",
            "Epoch: 10 | Time: 0m 0s\n",
            "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
            "\t Val. Loss: 0.483 |  Val. PPL:   1.621\n",
            "Epoch: 11 | Time: 0m 0s\n",
            "\tTrain Loss: 0.399 | Train PPL:   1.491\n",
            "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
            "Epoch: 12 | Time: 0m 0s\n",
            "\tTrain Loss: 0.411 | Train PPL:   1.509\n",
            "\t Val. Loss: 0.476 |  Val. PPL:   1.610\n",
            "Epoch: 13 | Time: 0m 0s\n",
            "\tTrain Loss: 0.419 | Train PPL:   1.521\n",
            "\t Val. Loss: 0.477 |  Val. PPL:   1.612\n",
            "Epoch: 14 | Time: 0m 0s\n",
            "\tTrain Loss: 0.412 | Train PPL:   1.511\n",
            "\t Val. Loss: 0.480 |  Val. PPL:   1.615\n",
            "Epoch: 15 | Time: 0m 0s\n",
            "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
            "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
            "Epoch: 16 | Time: 0m 0s\n",
            "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
            "\t Val. Loss: 0.466 |  Val. PPL:   1.593\n",
            "Epoch: 17 | Time: 0m 0s\n",
            "\tTrain Loss: 0.396 | Train PPL:   1.485\n",
            "\t Val. Loss: 0.455 |  Val. PPL:   1.575\n",
            "Epoch: 18 | Time: 0m 0s\n",
            "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
            "\t Val. Loss: 0.466 |  Val. PPL:   1.594\n",
            "Epoch: 19 | Time: 0m 0s\n",
            "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
            "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
            "Epoch: 20 | Time: 0m 0s\n",
            "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
            "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
            "Epoch: 21 | Time: 0m 0s\n",
            "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
            "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
            "Epoch: 22 | Time: 0m 0s\n",
            "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
            "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
            "Epoch: 23 | Time: 0m 0s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
            "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
            "Epoch: 24 | Time: 0m 0s\n",
            "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
            "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
            "Epoch: 25 | Time: 0m 0s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
            "\t Val. Loss: 0.417 |  Val. PPL:   1.518\n",
            "Epoch: 26 | Time: 0m 0s\n",
            "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
            "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
            "Epoch: 27 | Time: 0m 0s\n",
            "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
            "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
            "Epoch: 28 | Time: 0m 0s\n",
            "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
            "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
            "Epoch: 29 | Time: 0m 0s\n",
            "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
            "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
            "Epoch: 30 | Time: 0m 0s\n",
            "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
            "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
            "Epoch: 31 | Time: 0m 0s\n",
            "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
            "\t Val. Loss: 0.403 |  Val. PPL:   1.497\n",
            "Epoch: 32 | Time: 0m 0s\n",
            "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
            "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
            "Epoch: 33 | Time: 0m 0s\n",
            "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
            "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
            "Epoch: 34 | Time: 0m 0s\n",
            "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
            "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
            "Epoch: 35 | Time: 0m 0s\n",
            "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
            "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
            "Epoch: 36 | Time: 0m 0s\n",
            "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
            "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
            "Epoch: 37 | Time: 0m 0s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
            "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
            "Epoch: 38 | Time: 0m 0s\n",
            "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
            "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
            "Epoch: 39 | Time: 0m 0s\n",
            "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
            "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
            "Epoch: 40 | Time: 0m 0s\n",
            "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
            "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
            "Epoch: 41 | Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
            "\t Val. Loss: 0.386 |  Val. PPL:   1.470\n",
            "Epoch: 42 | Time: 0m 0s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
            "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
            "Epoch: 43 | Time: 0m 0s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
            "Epoch: 44 | Time: 0m 0s\n",
            "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
            "Epoch: 45 | Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train PPL:   1.333\n",
            "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
            "Epoch: 46 | Time: 0m 0s\n",
            "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
            "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
            "Epoch: 47 | Time: 0m 0s\n",
            "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
            "Epoch: 48 | Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
            "Epoch: 49 | Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train PPL:   1.333\n",
            "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
            "Epoch: 50 | Time: 0m 0s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
            "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
            "Epoch: 51 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
            "Epoch: 52 | Time: 0m 0s\n",
            "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
            "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
            "Epoch: 53 | Time: 0m 0s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
            "Epoch: 54 | Time: 0m 0s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
            "Epoch: 55 | Time: 0m 0s\n",
            "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
            "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
            "Epoch: 56 | Time: 0m 0s\n",
            "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
            "Epoch: 57 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
            "Epoch: 58 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
            "Epoch: 59 | Time: 0m 0s\n",
            "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.378\n",
            "Epoch: 60 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
            "Epoch: 61 | Time: 0m 0s\n",
            "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
            "Epoch: 62 | Time: 0m 0s\n",
            "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 63 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
            "Epoch: 64 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
            "Epoch: 65 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 66 | Time: 0m 0s\n",
            "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 67 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 68 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 69 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 70 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 71 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
            "Epoch: 72 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 73 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 74 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
            "Epoch: 75 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 76 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
            "Epoch: 77 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 78 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
            "Epoch: 79 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 80 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 81 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 82 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
            "Epoch: 83 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 84 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.330\n",
            "Epoch: 85 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 86 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
            "Epoch: 87 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 88 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
            "Epoch: 89 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
            "Epoch: 90 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
            "Epoch: 91 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
            "Epoch: 92 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
            "Epoch: 93 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
            "Epoch: 94 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 95 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 96 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 97 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 98 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 99 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 100 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
            "Epoch: 101 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 102 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 103 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 104 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 105 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 106 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 107 | Time: 0m 0s\n",
            "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
            "Epoch: 108 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 109 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 110 | Time: 0m 0s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 111 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 112 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 113 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
            "Epoch: 114 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 115 | Time: 0m 0s\n",
            "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 116 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 117 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
            "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
            "Epoch: 118 | Time: 0m 0s\n",
            "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 119 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 120 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
            "Epoch: 121 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 122 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 123 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 124 | Time: 0m 0s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 125 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
            "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
            "Epoch: 126 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
            "Epoch: 127 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
            "Epoch: 128 | Time: 0m 0s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
            "Epoch: 129 | Time: 0m 0s\n",
            "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
            "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
            "Epoch: 130 | Time: 0m 0s\n",
            "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
            "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
            "Epoch: 131 | Time: 0m 0s\n",
            "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
            "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
            "Epoch: 132 | Time: 0m 0s\n",
            "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
            "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
            "Epoch: 133 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
            "Epoch: 134 | Time: 0m 0s\n",
            "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
            "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
            "Epoch: 135 | Time: 0m 0s\n",
            "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
            "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
            "Epoch: 136 | Time: 0m 0s\n",
            "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
            "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
            "Epoch: 137 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
            "Epoch: 138 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
            "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
            "Epoch: 139 | Time: 0m 0s\n",
            "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
            "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
            "Epoch: 140 | Time: 0m 0s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
            "Epoch: 141 | Time: 0m 0s\n",
            "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
            "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
            "Epoch: 142 | Time: 0m 0s\n",
            "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
            "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
            "Epoch: 143 | Time: 0m 0s\n",
            "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
            "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
            "Epoch: 144 | Time: 0m 0s\n",
            "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
            "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
            "Epoch: 145 | Time: 0m 0s\n",
            "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
            "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
            "Epoch: 146 | Time: 0m 0s\n",
            "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
            "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
            "Epoch: 147 | Time: 0m 0s\n",
            "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
            "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
            "Epoch: 148 | Time: 0m 0s\n",
            "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
            "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
            "Epoch: 149 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
            "Epoch: 150 | Time: 0m 0s\n",
            "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
            "\t Val. Loss: 0.198 |  Val. PPL:   1.220\n",
            "Epoch: 151 | Time: 0m 0s\n",
            "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
            "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
            "Epoch: 152 | Time: 0m 0s\n",
            "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
            "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
            "Epoch: 153 | Time: 0m 0s\n",
            "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
            "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
            "Epoch: 154 | Time: 0m 0s\n",
            "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
            "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
            "Epoch: 155 | Time: 0m 0s\n",
            "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
            "Epoch: 156 | Time: 0m 0s\n",
            "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
            "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
            "Epoch: 157 | Time: 0m 0s\n",
            "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
            "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
            "Epoch: 158 | Time: 0m 0s\n",
            "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
            "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
            "Epoch: 159 | Time: 0m 0s\n",
            "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
            "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
            "Epoch: 160 | Time: 0m 0s\n",
            "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
            "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
            "Epoch: 161 | Time: 0m 0s\n",
            "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
            "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
            "Epoch: 162 | Time: 0m 0s\n",
            "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
            "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
            "Epoch: 163 | Time: 0m 0s\n",
            "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
            "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
            "Epoch: 164 | Time: 0m 0s\n",
            "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
            "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
            "Epoch: 165 | Time: 0m 0s\n",
            "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
            "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
            "Epoch: 166 | Time: 0m 0s\n",
            "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
            "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
            "Epoch: 167 | Time: 0m 0s\n",
            "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
            "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
            "Epoch: 168 | Time: 0m 0s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
            "Epoch: 169 | Time: 0m 0s\n",
            "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
            "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
            "Epoch: 170 | Time: 0m 0s\n",
            "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
            "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
            "Epoch: 171 | Time: 0m 0s\n",
            "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
            "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
            "Epoch: 172 | Time: 0m 0s\n",
            "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
            "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
            "Epoch: 173 | Time: 0m 0s\n",
            "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
            "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
            "Epoch: 174 | Time: 0m 0s\n",
            "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
            "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
            "Epoch: 175 | Time: 0m 0s\n",
            "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
            "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
            "Epoch: 176 | Time: 0m 0s\n",
            "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
            "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
            "Epoch: 177 | Time: 0m 0s\n",
            "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
            "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
            "Epoch: 178 | Time: 0m 0s\n",
            "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
            "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
            "Epoch: 179 | Time: 0m 0s\n",
            "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
            "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
            "Epoch: 180 | Time: 0m 0s\n",
            "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
            "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
            "Epoch: 181 | Time: 0m 0s\n",
            "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
            "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
            "Epoch: 182 | Time: 0m 0s\n",
            "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
            "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
            "Epoch: 183 | Time: 0m 0s\n",
            "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
            "\t Val. Loss: 0.168 |  Val. PPL:   1.184\n",
            "Epoch: 184 | Time: 0m 0s\n",
            "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
            "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
            "Epoch: 185 | Time: 0m 0s\n",
            "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
            "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
            "Epoch: 186 | Time: 0m 0s\n",
            "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
            "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
            "Epoch: 187 | Time: 0m 0s\n",
            "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
            "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
            "Epoch: 188 | Time: 0m 0s\n",
            "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
            "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
            "Epoch: 189 | Time: 0m 0s\n",
            "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
            "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
            "Epoch: 190 | Time: 0m 0s\n",
            "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
            "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
            "Epoch: 191 | Time: 0m 0s\n",
            "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
            "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
            "Epoch: 192 | Time: 0m 0s\n",
            "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
            "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
            "Epoch: 193 | Time: 0m 0s\n",
            "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
            "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
            "Epoch: 194 | Time: 0m 0s\n",
            "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
            "Epoch: 195 | Time: 0m 0s\n",
            "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
            "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
            "Epoch: 196 | Time: 0m 0s\n",
            "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
            "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
            "Epoch: 197 | Time: 0m 0s\n",
            "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
            "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
            "Epoch: 198 | Time: 0m 0s\n",
            "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
            "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
            "Epoch: 199 | Time: 0m 0s\n",
            "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
            "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
            "Epoch: 200 | Time: 0m 0s\n",
            "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
            "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
            "Epoch: 201 | Time: 0m 0s\n",
            "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
            "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
            "Epoch: 202 | Time: 0m 0s\n",
            "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
            "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
            "Epoch: 203 | Time: 0m 0s\n",
            "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
            "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
            "Epoch: 204 | Time: 0m 0s\n",
            "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
            "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
            "Epoch: 205 | Time: 0m 0s\n",
            "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
            "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
            "Epoch: 206 | Time: 0m 0s\n",
            "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
            "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
            "Epoch: 207 | Time: 0m 0s\n",
            "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
            "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
            "Epoch: 208 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
            "Epoch: 209 | Time: 0m 0s\n",
            "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
            "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
            "Epoch: 210 | Time: 0m 0s\n",
            "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
            "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
            "Epoch: 211 | Time: 0m 0s\n",
            "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
            "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
            "Epoch: 212 | Time: 0m 0s\n",
            "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
            "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
            "Epoch: 213 | Time: 0m 0s\n",
            "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
            "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
            "Epoch: 214 | Time: 0m 0s\n",
            "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
            "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
            "Epoch: 215 | Time: 0m 0s\n",
            "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
            "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
            "Epoch: 216 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
            "Epoch: 217 | Time: 0m 0s\n",
            "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
            "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
            "Epoch: 218 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
            "Epoch: 219 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
            "Epoch: 220 | Time: 0m 0s\n",
            "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
            "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
            "Epoch: 221 | Time: 0m 0s\n",
            "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
            "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
            "Epoch: 222 | Time: 0m 0s\n",
            "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
            "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
            "Epoch: 223 | Time: 0m 0s\n",
            "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
            "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
            "Epoch: 224 | Time: 0m 0s\n",
            "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
            "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
            "Epoch: 225 | Time: 0m 0s\n",
            "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
            "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
            "Epoch: 226 | Time: 0m 0s\n",
            "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
            "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
            "Epoch: 227 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
            "Epoch: 228 | Time: 0m 0s\n",
            "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
            "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
            "Epoch: 229 | Time: 0m 0s\n",
            "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
            "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
            "Epoch: 230 | Time: 0m 0s\n",
            "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
            "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
            "Epoch: 231 | Time: 0m 0s\n",
            "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
            "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
            "Epoch: 232 | Time: 0m 0s\n",
            "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
            "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
            "Epoch: 233 | Time: 0m 0s\n",
            "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
            "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
            "Epoch: 234 | Time: 0m 0s\n",
            "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
            "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
            "Epoch: 235 | Time: 0m 0s\n",
            "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
            "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
            "Epoch: 236 | Time: 0m 0s\n",
            "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
            "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
            "Epoch: 237 | Time: 0m 0s\n",
            "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
            "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
            "Epoch: 238 | Time: 0m 0s\n",
            "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
            "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
            "Epoch: 239 | Time: 0m 0s\n",
            "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
            "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
            "Epoch: 240 | Time: 0m 0s\n",
            "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
            "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
            "Epoch: 241 | Time: 0m 0s\n",
            "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
            "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
            "Epoch: 242 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
            "Epoch: 243 | Time: 0m 0s\n",
            "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
            "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
            "Epoch: 244 | Time: 0m 0s\n",
            "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
            "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
            "Epoch: 245 | Time: 0m 0s\n",
            "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
            "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
            "Epoch: 246 | Time: 0m 0s\n",
            "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
            "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
            "Epoch: 247 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
            "Epoch: 248 | Time: 0m 0s\n",
            "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
            "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
            "Epoch: 249 | Time: 0m 0s\n",
            "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
            "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
            "Epoch: 250 | Time: 0m 0s\n",
            "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
            "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
            "Epoch: 251 | Time: 0m 0s\n",
            "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
            "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
            "Epoch: 252 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
            "Epoch: 253 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
            "Epoch: 254 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
            "Epoch: 255 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
            "Epoch: 256 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
            "Epoch: 257 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
            "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
            "Epoch: 258 | Time: 0m 0s\n",
            "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
            "Epoch: 259 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
            "Epoch: 260 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
            "Epoch: 261 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
            "Epoch: 262 | Time: 0m 0s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
            "Epoch: 263 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
            "Epoch: 264 | Time: 0m 0s\n",
            "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
            "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
            "Epoch: 265 | Time: 0m 0s\n",
            "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
            "Epoch: 266 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
            "Epoch: 267 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
            "Epoch: 268 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 269 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 270 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
            "Epoch: 271 | Time: 0m 0s\n",
            "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
            "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
            "Epoch: 272 | Time: 0m 0s\n",
            "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
            "Epoch: 273 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 274 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 275 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
            "Epoch: 276 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 277 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
            "Epoch: 278 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
            "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
            "Epoch: 279 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
            "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
            "Epoch: 280 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
            "Epoch: 281 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
            "Epoch: 282 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
            "Epoch: 283 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
            "Epoch: 284 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 285 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "Epoch: 286 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "Epoch: 287 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 288 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
            "Epoch: 289 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 290 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 291 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "Epoch: 292 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 293 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 294 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 295 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 296 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "Epoch: 297 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 298 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 299 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 300 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 301 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 302 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 303 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 304 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 305 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 306 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 307 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 308 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 309 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 310 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 311 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 312 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 313 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 314 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 315 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 316 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 317 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 318 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Epoch: 319 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 320 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Epoch: 321 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 322 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 323 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Epoch: 324 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 325 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 326 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 327 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 328 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 329 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 330 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 331 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
            "Epoch: 332 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 333 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 334 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 335 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 336 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 337 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 338 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 339 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 340 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
            "Epoch: 341 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Epoch: 342 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 343 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
            "Epoch: 344 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 345 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 346 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 347 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 348 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 349 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "Epoch: 350 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 351 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 352 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 353 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 354 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 355 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 356 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 357 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 358 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 359 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 360 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 361 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 362 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 363 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 364 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 365 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 366 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 367 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 368 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 369 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 370 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 371 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 372 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 373 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 374 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 375 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 376 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 377 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 378 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 379 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 380 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 381 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 382 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 383 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 384 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 385 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 386 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 387 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 388 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 389 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 390 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 391 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 392 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 393 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 394 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 395 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 396 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 397 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 398 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 399 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 400 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 401 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 402 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 403 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 404 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 405 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 406 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 407 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 408 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 409 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 410 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 411 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 412 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 413 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 414 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 415 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 416 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 417 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 418 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 419 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 420 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 421 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 422 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 423 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 424 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 425 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 426 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 427 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 428 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 429 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 430 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 431 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 432 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 433 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 434 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 435 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 436 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 437 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 438 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 439 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 440 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 441 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 442 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 443 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 444 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 445 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 446 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 447 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 448 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 449 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 450 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 451 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 452 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 453 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 454 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 455 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 456 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 457 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 458 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 459 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 460 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 461 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 462 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 463 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 464 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 465 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 466 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 467 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 468 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 469 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 470 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 471 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 472 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 473 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 474 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 475 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 476 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 477 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 478 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 479 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 480 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 481 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 482 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 483 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 484 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 485 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 486 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 487 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 488 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 489 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 490 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 491 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 492 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 493 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 494 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "Epoch: 495 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 496 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 497 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 498 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 499 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 500 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 501 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 502 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 503 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 504 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 505 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 506 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 507 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 508 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 509 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 510 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 511 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 512 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 513 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 514 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 515 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 516 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 517 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 518 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 519 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 520 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 521 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 522 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 523 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 524 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 525 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 526 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 527 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 528 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 529 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 530 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 531 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 532 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 533 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 534 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 535 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 536 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 537 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 538 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 539 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 540 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 541 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 542 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 543 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 544 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 545 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 546 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 547 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 548 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 549 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 550 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 551 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 552 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 553 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 554 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 555 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 556 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 557 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 558 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 559 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 560 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 561 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 562 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 563 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 564 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 565 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 566 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 567 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 568 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 569 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 570 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 571 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 572 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 573 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 574 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 575 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 576 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 577 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 578 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 579 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 580 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 581 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 582 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 583 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 584 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 585 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 586 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 587 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 588 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 589 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 590 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 591 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 592 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 593 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 594 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 595 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 596 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 597 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 598 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 599 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 600 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 601 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 602 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 603 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 604 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 605 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 606 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 607 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 608 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 609 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 610 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 611 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 612 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 613 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 614 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 615 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 616 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 617 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 618 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 619 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 620 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 621 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 622 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 623 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 624 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 625 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 626 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 627 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 628 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 629 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 630 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 631 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 632 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 633 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 634 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 635 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 636 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 637 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 638 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 639 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 640 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 641 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 642 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 643 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 644 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 645 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 646 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 647 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 648 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 649 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 650 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 651 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 652 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 653 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 654 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 655 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 656 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 657 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 658 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 659 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 660 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 661 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 662 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 663 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 664 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 665 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 666 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 667 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 668 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 669 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
            "Epoch: 670 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 671 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 672 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 673 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 674 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
            "Epoch: 675 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 676 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 677 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 678 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 679 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 680 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 681 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 682 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 683 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 684 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 685 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 686 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 687 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 688 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 689 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 690 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 691 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 692 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 693 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 694 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 695 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 696 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 697 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 698 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 699 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 700 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 701 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 702 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 703 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 704 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 705 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 706 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 707 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 708 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 709 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 710 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 711 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 712 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 713 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 714 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 715 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 716 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 717 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 718 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 719 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 720 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 721 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 722 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 723 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 724 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 725 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 726 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 727 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 728 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 729 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 730 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 731 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 732 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 733 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 734 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 735 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 736 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 737 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 738 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 739 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 740 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 741 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 742 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 743 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 744 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 745 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 746 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 747 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 748 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 749 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 750 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 751 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 752 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 753 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 754 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 755 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 756 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 757 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 758 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 759 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 760 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 761 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 762 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 763 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 764 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 765 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 766 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 767 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 768 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 769 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 770 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 771 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 772 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 773 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 774 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 775 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 776 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 777 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 778 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 779 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 780 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 781 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 782 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 783 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 784 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 785 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 786 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 787 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 788 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 789 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 790 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 791 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 792 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 793 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 794 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 795 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 796 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 797 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 798 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 799 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 800 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 801 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 802 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 803 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 804 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 805 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 806 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 807 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 808 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 809 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 810 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 811 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 812 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 813 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 814 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 815 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 816 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 817 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 818 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 819 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 820 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 821 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 822 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 823 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 824 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 825 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 826 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 827 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 828 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 829 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 830 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 831 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 832 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 833 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 834 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 835 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 836 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 837 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 838 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 839 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 840 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 841 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 842 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 843 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 844 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 845 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 846 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 847 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 848 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 849 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 850 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 851 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 852 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 853 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 854 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 855 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 856 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 857 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 858 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 859 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 860 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 861 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 862 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 863 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 864 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 865 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 866 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 867 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 868 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 869 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 870 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 871 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 872 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 873 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 874 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 875 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 876 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 877 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 878 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 879 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 880 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 881 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 882 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 883 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 884 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 885 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 886 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 887 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 888 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 889 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 890 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 891 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 892 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 893 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 894 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 895 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 896 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 897 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 898 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 899 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 900 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 901 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 902 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 903 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 904 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 905 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 906 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 907 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 908 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 909 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 910 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 911 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 912 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 913 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 914 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 915 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 916 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 917 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 918 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 919 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 920 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 921 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 922 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 923 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 924 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 925 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 926 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 927 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 928 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 929 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 930 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 931 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 932 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 933 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 934 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 935 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 936 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 937 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 938 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 939 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 940 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 941 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 942 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 943 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 944 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 945 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 946 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 947 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 948 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 949 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 950 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 951 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 952 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 953 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 954 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 955 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 956 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 957 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 958 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 959 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 960 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 961 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 962 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 963 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 964 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 965 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 966 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 967 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 968 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 969 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 970 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 971 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 972 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 973 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 974 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 975 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 976 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 977 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 978 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 979 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 980 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 981 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 982 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 983 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 984 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 985 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 986 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 987 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 988 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 989 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 990 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 991 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 992 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 993 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 994 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 995 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 996 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 997 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 998 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 999 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 1000 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAVeSfB_vnWU"
      },
      "source": [
        "Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WBCWzONDvnWU",
        "outputId": "a35cb977-3189-479b-f355-e077d9878914"
      },
      "source": [
        "hist_loss_A = torch.cat(hist_losses_A, dim=2)\n",
        "hist_hits_A = torch.cat(hist_hitsss_A, dim=2)\n",
        "\n",
        "plotResults(hist_loss_A, hist_hits_A)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq_UPpKlvnWV"
      },
      "source": [
        "In numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlY2ObLqvnWV",
        "outputId": "2f18b434-7432-4e00-c290-86fe0e30d6ff"
      },
      "source": [
        "accuracyAll(models_A)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model 0\n",
            "Task 0: Acc 0.97% | Gr acc 0.95 | Ugr acc 1.0\n",
            "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
            "Task 2: Acc 0.72% | Gr acc 0.44 | Ugr acc 1.0\n",
            "\n",
            "Model 1\n",
            "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
            "Task 1: Acc 0.8% | Gr acc 0.6 | Ugr acc 1.0\n",
            "Task 2: Acc 0.67% | Gr acc 0.33 | Ugr acc 1.0\n",
            "\n",
            "Model 2\n",
            "Task 0: Acc 0.68% | Gr acc 0.8 | Ugr acc 0.55\n",
            "Task 1: Acc 0.82% | Gr acc 1.0 | Ugr acc 0.65\n",
            "Task 2: Acc 0.75% | Gr acc 0.89 | Ugr acc 0.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD_rpqtSvnWV"
      },
      "source": [
        "## Baseline B: Keep Training same model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRGPK1CjvnWV"
      },
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9zXm7oSvnWW",
        "outputId": "b3bdff53-8da6-4838-c35f-bcdb5252ec1e"
      },
      "source": [
        "models_B = []\n",
        "hist_losses_B = []\n",
        "hist_hitsss_B = []\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "print(model.apply(init_weights))\n",
        "\n",
        "for n_task in range(N_TASKS + 1):\n",
        "    SUFFIX = f\"B{n_task}\"\n",
        "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
        "    LOADNAME = \"models/autosave/\" + title + \".pt\"\n",
        "    SAVENAME = \"models/autosave/\" + title + \".pt\"\n",
        "    PLOTSAVE = \"plots/autosave/\" + title + \".png\"\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
        "    \n",
        "    print(title)\n",
        "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
        "    \n",
        "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
        "    hist_losses_B.append(hist_loss_temp)\n",
        "    hist_hitsss_B.append(hist_hits_temp)\n",
        "    models_B.append(copy.deepcopy(model))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 335 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 336 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 337 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 338 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 339 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 340 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 341 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 342 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 343 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 344 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 345 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 346 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 347 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 348 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 349 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 350 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 351 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 352 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 353 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 354 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 355 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 356 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 357 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 358 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 359 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 360 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 361 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 362 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 363 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 364 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 365 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 366 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 367 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 368 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 369 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 370 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 371 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 372 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 373 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 374 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 375 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 376 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 377 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 378 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 379 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 380 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 381 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 382 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 383 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 384 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 385 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 386 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 387 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 388 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 389 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 390 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 391 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 392 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 393 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 394 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 395 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 396 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 397 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 398 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 399 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 400 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 401 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 402 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 403 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 404 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 405 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 406 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 407 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 408 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 409 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 410 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 411 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 412 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 413 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 414 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 415 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 416 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 417 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 418 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 419 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 420 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
            "Epoch: 421 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 422 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 423 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 424 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 425 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 426 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 427 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 428 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 429 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 430 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 431 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 432 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 433 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 434 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 435 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 436 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 437 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 438 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 439 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 440 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 441 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 442 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 443 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 444 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 445 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 446 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 447 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 448 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 449 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 450 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 451 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 452 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 453 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 454 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 455 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 456 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 457 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 458 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 459 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 460 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 461 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 462 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 463 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 464 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 465 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 466 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 467 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 468 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 469 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 470 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 471 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 472 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 473 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 474 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 475 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 476 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 477 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 478 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 479 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 480 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 481 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 482 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 483 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 484 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 485 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 486 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 487 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 488 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 489 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 490 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 491 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 492 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 493 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 494 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 495 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 496 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 497 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 498 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 499 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 500 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 501 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 502 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 503 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 504 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 505 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 506 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 507 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 508 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 509 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 510 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 511 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 512 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 513 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 514 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 515 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 516 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 517 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 518 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 519 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 520 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 521 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 522 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 523 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 524 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 525 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 526 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 527 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 528 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 529 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 530 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 531 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 532 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 533 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 534 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 535 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 536 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 537 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 538 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 539 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 540 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 541 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 542 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 543 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 544 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 545 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 546 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 547 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 548 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 549 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 550 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 551 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 552 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 553 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 554 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 555 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 556 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 557 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 558 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 559 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 560 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 561 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 562 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 563 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 564 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 565 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 566 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 567 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 568 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 569 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 570 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 571 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 572 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 573 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 574 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 575 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 576 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 577 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 578 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 579 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 580 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 581 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 582 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 583 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 584 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 585 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 586 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 587 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 588 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 589 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 590 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 591 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 592 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 593 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 594 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 595 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 596 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 597 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 598 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 599 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 600 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 601 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 602 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 603 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 604 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 605 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 606 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 607 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 608 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 609 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 610 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 611 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 612 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 613 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 614 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 615 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 616 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 617 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 618 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 619 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 620 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 621 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 622 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 623 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 624 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 625 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 626 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 627 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 628 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 629 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 630 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 631 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 632 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 633 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 634 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 635 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 636 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 637 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 638 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 639 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 640 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 641 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 642 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 643 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 644 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 645 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 646 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 647 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 648 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 649 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 650 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 651 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 652 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 653 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 654 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 655 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 656 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 657 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 658 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 659 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 660 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 661 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 662 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 663 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 664 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 665 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 666 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 667 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 668 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 669 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 670 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 671 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 672 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 673 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 674 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 675 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 676 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 677 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 678 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 679 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 680 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 681 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 682 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 683 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 684 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 685 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 686 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 687 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 688 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 689 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 690 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 691 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 692 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 693 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 694 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 695 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 696 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 697 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 698 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 699 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 700 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 701 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 702 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 703 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 704 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 705 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 706 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 707 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 708 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 709 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 710 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 711 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 712 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 713 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 714 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 715 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 716 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 717 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 718 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 719 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 720 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 721 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 722 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 723 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 724 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 725 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 726 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 727 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 728 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 729 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 730 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 731 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 732 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 733 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 734 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 735 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 736 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 737 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 738 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 739 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 740 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 741 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 742 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 743 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 744 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 745 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 746 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 747 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 748 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 749 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 750 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 751 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 752 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 753 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 754 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 755 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 756 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 757 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 758 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 759 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 760 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 761 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 762 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 763 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 764 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 765 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 766 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 767 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 768 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 769 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 770 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 771 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 772 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 773 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 774 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 775 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 776 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 777 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 778 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 779 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 780 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 781 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 782 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 783 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 784 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 785 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 786 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 787 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 788 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 789 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 790 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 791 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 792 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 793 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 794 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 795 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 796 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 797 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 798 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 799 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 800 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 801 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 802 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 803 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 804 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 805 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 806 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 807 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 808 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 809 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 810 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 811 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 812 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 813 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 814 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 815 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 816 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 817 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 818 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 819 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 820 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 821 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 822 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 823 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 824 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 825 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 826 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 827 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 828 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 829 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 830 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 831 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 832 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 833 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 834 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 835 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 836 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 837 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 838 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 839 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 840 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 841 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 842 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 843 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 844 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 845 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 846 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 847 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 848 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 849 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 850 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 851 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 852 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 853 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 854 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 855 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 856 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 857 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 858 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 859 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 860 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 861 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 862 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 863 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 864 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 865 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 866 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 867 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 868 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 869 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 870 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 871 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 872 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 873 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 874 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 875 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 876 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 877 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 878 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 879 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 880 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 881 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 882 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 883 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 884 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 885 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 886 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 887 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 888 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 889 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 890 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 891 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 892 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 893 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 894 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 895 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 896 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 897 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 898 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 899 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 900 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 901 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 902 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 903 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 904 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 905 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 906 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 907 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 908 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 909 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 910 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 911 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 912 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 913 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 914 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 915 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 916 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 917 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 918 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 919 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 920 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 921 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 922 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 923 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 924 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 925 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 926 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 927 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 928 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 929 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 930 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 931 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 932 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 933 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 934 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 935 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 936 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 937 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 938 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 939 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 940 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 941 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 942 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 943 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 944 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 945 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 946 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 947 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 948 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 949 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 950 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 951 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 952 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 953 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 954 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 955 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 956 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 957 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 958 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 959 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 960 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 961 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 962 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 963 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 964 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 965 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 966 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 967 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 968 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 969 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 970 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 971 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 972 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 973 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 974 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 975 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 976 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 977 | Time: 0m 0s\n",
            "\tTrain Loss: 0.052 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 978 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 979 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 980 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 981 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 982 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 983 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 984 | Time: 0m 0s\n",
            "\tTrain Loss: 0.052 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 985 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 986 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 987 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 988 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 989 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 990 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 991 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 992 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 993 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 994 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 995 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 996 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 997 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 998 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 999 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 1000 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "tr-AE-150-18-0.001-B2\n",
            "The model has 35198 trainable parameters\n",
            "Epoch: 01 | Time: 0m 0s\n",
            "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
            "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
            "Epoch: 02 | Time: 0m 0s\n",
            "\tTrain Loss: 0.297 | Train PPL:   1.345\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
            "Epoch: 03 | Time: 0m 0s\n",
            "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
            "Epoch: 04 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
            "Epoch: 05 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.271\n",
            "Epoch: 06 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 07 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
            "Epoch: 08 | Time: 0m 0s\n",
            "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
            "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
            "Epoch: 09 | Time: 0m 0s\n",
            "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
            "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
            "Epoch: 10 | Time: 0m 0s\n",
            "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
            "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
            "Epoch: 11 | Time: 0m 0s\n",
            "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
            "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
            "Epoch: 12 | Time: 0m 0s\n",
            "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
            "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
            "Epoch: 13 | Time: 0m 0s\n",
            "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
            "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
            "Epoch: 14 | Time: 0m 0s\n",
            "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
            "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
            "Epoch: 15 | Time: 0m 0s\n",
            "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
            "Epoch: 16 | Time: 0m 0s\n",
            "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
            "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
            "Epoch: 17 | Time: 0m 0s\n",
            "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
            "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
            "Epoch: 18 | Time: 0m 0s\n",
            "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
            "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
            "Epoch: 19 | Time: 0m 0s\n",
            "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
            "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
            "Epoch: 20 | Time: 0m 0s\n",
            "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
            "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
            "Epoch: 21 | Time: 0m 0s\n",
            "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
            "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
            "Epoch: 22 | Time: 0m 0s\n",
            "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
            "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
            "Epoch: 23 | Time: 0m 0s\n",
            "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
            "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
            "Epoch: 24 | Time: 0m 0s\n",
            "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
            "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
            "Epoch: 25 | Time: 0m 0s\n",
            "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
            "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
            "Epoch: 26 | Time: 0m 0s\n",
            "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
            "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
            "Epoch: 27 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
            "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
            "Epoch: 28 | Time: 0m 0s\n",
            "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
            "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
            "Epoch: 29 | Time: 0m 0s\n",
            "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
            "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
            "Epoch: 30 | Time: 0m 0s\n",
            "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
            "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
            "Epoch: 31 | Time: 0m 0s\n",
            "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
            "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
            "Epoch: 32 | Time: 0m 0s\n",
            "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
            "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
            "Epoch: 33 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
            "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
            "Epoch: 34 | Time: 0m 0s\n",
            "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
            "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
            "Epoch: 35 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
            "Epoch: 36 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
            "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
            "Epoch: 37 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
            "Epoch: 38 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
            "Epoch: 39 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
            "Epoch: 40 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
            "Epoch: 41 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
            "Epoch: 42 | Time: 0m 0s\n",
            "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 43 | Time: 0m 0s\n",
            "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
            "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
            "Epoch: 44 | Time: 0m 0s\n",
            "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
            "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
            "Epoch: 45 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
            "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
            "Epoch: 46 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
            "Epoch: 47 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
            "Epoch: 48 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
            "Epoch: 49 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
            "Epoch: 50 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
            "Epoch: 51 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
            "Epoch: 52 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
            "Epoch: 53 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
            "Epoch: 54 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
            "Epoch: 55 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
            "Epoch: 56 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
            "Epoch: 57 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
            "Epoch: 58 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "Epoch: 59 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
            "Epoch: 60 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "Epoch: 61 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "Epoch: 62 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 63 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 64 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 65 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 66 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "Epoch: 67 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "Epoch: 68 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 69 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
            "Epoch: 70 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
            "Epoch: 71 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
            "Epoch: 72 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 73 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "Epoch: 74 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 75 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 76 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 77 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 78 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 79 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 80 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 81 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 82 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "Epoch: 83 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 84 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 85 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 86 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 87 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "Epoch: 88 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 89 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 90 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "Epoch: 91 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 92 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 93 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "Epoch: 94 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 95 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 96 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 97 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 98 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 99 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 100 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 101 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 102 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 103 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 104 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 105 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 106 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 107 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 108 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 109 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 110 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 111 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 112 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 113 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 114 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 115 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 116 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 117 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 118 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 119 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 120 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 121 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 122 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 123 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 124 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 125 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 126 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 127 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 128 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 129 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
            "Epoch: 130 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 131 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 132 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 133 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 134 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 135 | Time: 0m 0s\n",
            "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 136 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
            "Epoch: 137 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
            "Epoch: 138 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
            "Epoch: 139 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 140 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 141 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 142 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
            "Epoch: 143 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 144 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 145 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 146 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 147 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 148 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 149 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 150 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 151 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
            "Epoch: 152 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 153 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 154 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 155 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 156 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 157 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 158 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 159 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 160 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 161 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 162 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 163 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 164 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 165 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
            "Epoch: 166 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 167 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 168 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 169 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 170 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 171 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 172 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 173 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 174 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 175 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
            "Epoch: 176 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 177 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 178 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 179 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 180 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 181 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 182 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 183 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 184 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 185 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 186 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 187 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 188 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 189 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 190 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 191 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 192 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 193 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 194 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 195 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 196 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 197 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 198 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 199 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 200 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 201 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 202 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 203 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 204 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 205 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 206 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 207 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 208 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 209 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 210 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 211 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 212 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 213 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 214 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 215 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 216 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 217 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 218 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
            "Epoch: 219 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 220 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 221 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 222 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 223 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 224 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 225 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 226 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 227 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 228 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 229 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 230 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 231 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 232 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 233 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 234 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 235 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 236 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 237 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 238 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 239 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 240 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 241 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 242 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 243 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 244 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 245 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 246 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 247 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 248 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 249 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 250 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 251 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 252 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 253 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 254 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
            "Epoch: 255 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 256 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 257 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 258 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 259 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 260 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 261 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 262 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 263 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 264 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 265 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 266 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
            "Epoch: 267 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 268 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 269 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
            "Epoch: 270 | Time: 0m 0s\n",
            "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
            "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
            "Epoch: 271 | Time: 0m 0s\n",
            "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
            "Epoch: 272 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
            "Epoch: 273 | Time: 0m 0s\n",
            "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 274 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 275 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 276 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
            "Epoch: 277 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 278 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 279 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 280 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 281 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 282 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 283 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 284 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 285 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 286 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 287 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
            "Epoch: 288 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 289 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 290 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 291 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 292 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 293 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 294 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 295 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 296 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 297 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 298 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 299 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 300 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 301 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 302 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 303 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 304 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 305 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 306 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 307 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 308 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 309 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 310 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 311 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 312 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 313 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 314 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 315 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 316 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 317 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 318 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 319 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 320 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 321 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 322 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 323 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 324 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 325 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 326 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 327 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 328 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 329 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 330 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 331 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 332 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 333 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 334 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 335 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 336 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 337 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 338 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 339 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 340 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 341 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 342 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 343 | Time: 0m 0s\n",
            "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 344 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 345 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 346 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 347 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 348 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 349 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 350 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 351 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 352 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 353 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 354 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 355 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 356 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 357 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 358 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 359 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 360 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 361 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 362 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 363 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 364 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 365 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 366 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 367 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 368 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 369 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 370 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 371 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 372 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 373 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
            "Epoch: 374 | Time: 0m 0s\n",
            "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
            "Epoch: 375 | Time: 0m 0s\n",
            "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 376 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
            "Epoch: 377 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 378 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 379 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 380 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 381 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 382 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 383 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 384 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 385 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 386 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 387 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 388 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 389 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 390 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 391 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 392 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 393 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 394 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 395 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 396 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 397 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 398 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 399 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 400 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 401 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 402 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 403 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 404 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 405 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 406 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 407 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 408 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 409 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 410 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 411 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 412 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 413 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 414 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 415 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 416 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 417 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 418 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 419 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 420 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 421 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 422 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 423 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 424 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 425 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 426 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 427 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 428 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 429 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 430 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 431 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 432 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 433 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 434 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 435 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 436 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 437 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 438 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 439 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 440 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 441 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 442 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 443 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 444 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 445 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 446 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 447 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 448 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 449 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 450 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 451 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 452 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 453 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 454 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
            "Epoch: 455 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 456 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 457 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 458 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 459 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 460 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 461 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 462 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 463 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 464 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 465 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 466 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 467 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 468 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 469 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 470 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 471 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 472 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 473 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 474 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 475 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 476 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 477 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 478 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 479 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 480 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 481 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 482 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 483 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 484 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 485 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 486 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 487 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 488 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
            "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
            "Epoch: 489 | Time: 0m 0s\n",
            "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
            "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
            "Epoch: 490 | Time: 0m 0s\n",
            "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
            "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
            "Epoch: 491 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
            "Epoch: 492 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
            "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
            "Epoch: 493 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
            "Epoch: 494 | Time: 0m 0s\n",
            "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
            "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
            "Epoch: 495 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 496 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 497 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 498 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 499 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 500 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 501 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 502 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 503 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 504 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 505 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 506 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 507 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 508 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 509 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 510 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 511 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 512 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 513 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 514 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 515 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 516 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 517 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 518 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 519 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 520 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 521 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 522 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 523 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 524 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 525 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 526 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 527 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 528 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 529 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 530 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 531 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 532 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 533 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 534 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 535 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 536 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 537 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 538 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 539 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 540 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 541 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 542 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 543 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 544 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 545 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 546 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 547 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 548 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 549 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 550 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 551 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 552 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 553 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 554 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 555 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 556 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 557 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 558 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 559 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 560 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 561 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 562 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 563 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 564 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 565 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 566 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 567 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 568 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 569 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 570 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 571 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 572 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 573 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 574 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 575 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 576 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 577 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 578 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 579 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 580 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 581 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 582 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 583 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 584 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 585 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 586 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 587 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 588 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 589 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 590 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 591 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 592 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 593 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 594 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 595 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 596 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 597 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 598 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 599 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 600 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 601 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 602 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 603 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 604 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 605 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 606 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 607 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 608 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 609 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 610 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 611 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 612 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 613 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 614 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 615 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 616 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 617 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 618 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 619 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 620 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 621 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 622 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 623 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 624 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 625 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 626 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 627 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 628 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 629 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 630 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 631 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 632 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 633 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 634 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 635 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 636 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 637 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 638 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 639 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 640 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 641 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 642 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 643 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 644 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 645 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 646 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 647 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 648 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 649 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 650 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 651 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 652 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 653 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 654 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 655 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 656 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 657 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 658 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 659 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 660 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 661 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 662 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 663 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 664 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 665 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 666 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 667 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 668 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 669 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 670 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 671 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 672 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 673 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 674 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 675 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 676 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 677 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 678 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 679 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 680 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 681 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 682 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 683 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 684 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 685 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 686 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 687 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 688 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 689 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 690 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 691 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 692 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 693 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 694 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 695 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 696 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 697 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 698 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 699 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 700 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 701 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 702 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 703 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 704 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 705 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 706 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 707 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 708 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 709 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 710 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 711 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 712 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 713 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 714 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 715 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 716 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 717 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 718 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 719 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 720 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 721 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 722 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 723 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 724 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 725 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 726 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 727 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 728 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 729 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 730 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 731 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 732 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 733 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 734 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 735 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 736 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 737 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 738 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 739 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 740 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 741 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 742 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 743 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 744 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 745 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 746 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 747 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 748 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 749 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 750 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 751 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 752 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 753 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 754 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 755 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 756 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 757 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 758 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 759 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 760 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 761 | Time: 0m 0s\n",
            "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 762 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
            "Epoch: 763 | Time: 0m 0s\n",
            "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
            "Epoch: 764 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
            "Epoch: 765 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
            "Epoch: 766 | Time: 0m 0s\n",
            "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
            "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
            "Epoch: 767 | Time: 0m 0s\n",
            "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "Epoch: 768 | Time: 0m 0s\n",
            "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "Epoch: 769 | Time: 0m 0s\n",
            "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
            "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
            "Epoch: 770 | Time: 0m 0s\n",
            "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
            "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
            "Epoch: 771 | Time: 0m 0s\n",
            "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
            "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
            "Epoch: 772 | Time: 0m 0s\n",
            "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
            "Epoch: 773 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
            "Epoch: 774 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 775 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
            "Epoch: 776 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 777 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 778 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 779 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 780 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 781 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 782 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 783 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 784 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 785 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 786 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 787 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 788 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 789 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 790 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 791 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 792 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 793 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 794 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 795 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 796 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 797 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 798 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 799 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 800 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 801 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 802 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 803 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 804 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 805 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 806 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 807 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 808 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 809 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 810 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 811 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 812 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 813 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 814 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 815 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 816 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 817 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 818 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 819 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 820 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 821 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 822 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 823 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 824 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 825 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 826 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 827 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 828 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 829 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 830 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 831 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 832 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 833 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 834 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 835 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 836 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 837 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 838 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 839 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 840 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 841 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 842 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 843 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 844 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 845 | Time: 0m 0s\n",
            "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 846 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 847 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 848 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 849 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 850 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 851 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 852 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 853 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 854 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 855 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 856 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 857 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 858 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 859 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 860 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 861 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 862 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 863 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 864 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 865 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 866 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 867 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 868 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 869 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 870 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 871 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 872 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 873 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 874 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 875 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 876 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 877 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 878 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 879 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 880 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 881 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 882 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 883 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 884 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 885 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 886 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 887 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 888 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 889 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 890 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 891 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 892 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 893 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 894 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 895 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 896 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 897 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 898 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 899 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 900 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 901 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 902 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 903 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 904 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 905 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 906 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 907 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 908 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 909 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 910 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 911 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 912 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 913 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 914 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 915 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 916 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 917 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 918 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 919 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 920 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 921 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 922 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 923 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 924 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 925 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 926 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 927 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 928 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 929 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 930 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 931 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 932 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 933 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 934 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 935 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 936 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 937 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 938 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 939 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 940 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 941 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 942 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 943 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 944 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 945 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 946 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 947 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 948 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 949 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 950 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 951 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 952 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 953 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 954 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 955 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 956 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 957 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 958 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 959 | Time: 0m 0s\n",
            "\tTrain Loss: 0.052 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 960 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 961 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 962 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 963 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 964 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 965 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 966 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 967 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 968 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 969 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 970 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 971 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 972 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 973 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 974 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 975 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 976 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 977 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 978 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 979 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 980 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 981 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 982 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 983 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 984 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 985 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 986 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 987 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 988 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 989 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 990 | Time: 0m 0s\n",
            "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 991 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
            "Epoch: 992 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 993 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
            "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
            "Epoch: 994 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 995 | Time: 0m 0s\n",
            "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 996 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 997 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 998 | Time: 0m 0s\n",
            "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 999 | Time: 0m 0s\n",
            "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
            "Epoch: 1000 | Time: 0m 0s\n",
            "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
            "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xVWE0hVcvnWW",
        "outputId": "c8d32e28-272e-4bc6-a3dc-48484884396c"
      },
      "source": [
        "hist_loss_B = torch.cat(hist_losses_B, dim=2)\n",
        "hist_hits_B = torch.cat(hist_hitsss_B, dim=2)\n",
        "\n",
        "plotResults(hist_loss_B, hist_hits_B)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZxcVZn/8c/Te5bOQrqzdXYSRDYhCZssIkIM/FBkwCGgSBRE0SjMb/QHuAAiiqggMMMIUZIgOMCIOoZFMQPOMMqIaZSdCQkJ2SGdhJC1u9Pdz++PpypV6XTS3Ul16nb19/163Vfd/Z5Tt24995x77r3m7oiIiCRNUb4TICIi0hYFKBERSSQFKBERSSQFKBERSSQFKBERSSQFKBERSSQFKJEOMLPfmtnF+U6HSE9iug9KCpWZbc4a7A00AM2p4c+5+8/3UzreBC519//YH9sTKRQl+U6ASFdx977p/j0FCTMrcfem/Zk2EWmfqvikxzGzU8xshZldZWZvAbPNbKCZPWpmdWb2Tqp/RNYy/2lml6b6p5vZH83sh6l5l5jZGXuRjnIzu83MVqW628ysPDWtKpWGDWa23sz+28yKUtOuMrOVZrbJzBaY2Ydy9NWIJEreqviqqqp8zJgx+7SOdevWATBo0KAcpEiSoiv260svvcTo0aPp168fmzZt4vXXX2fIkCEMHz4cgJaWFjZt2kT//v1xd958803cnfHjxwOwYMECBg0aRFVVFWvXrmXp0qWMGjVqx/Dq1as5/PDDMbM9bjvbqlWr2LhxIwceeCAAb7zxBpWVldTU1LBy5UqampoYNWoUAJs3b6Zv3740NDTw+uuvc/DBB1NWVkZDQwMA5eXlOfuuuoKOVdmT5557bq27V+8ywd3z0k2aNMn31ezZs3327Nn7vB5Jlq7Yr6NHj/Z58+a5u/sf/vAHLy0t9W3btu12/r/97W8+YMCAHcMf+MAH/Cc/+cmO9B144IE7pm3ZssUBX716dbvbzjZu3Dh/7LHHdgz/7ne/89GjR7u7+ze/+U3/6Ec/6gsXLtxpmYULF3p1dbXPmzfPGxsb28l1cuhYlT0Bar2NOKEqPumRqqurqaio2DG8detWPve5z+0o6Zx88sls2LCB5ubmNpcfOnTojv7evXsDUcrpjFWrVjF69Ogdw6NHj2bVqlUAfPWrX2X8+PFMmTKFcePG8b3vfQ+A8ePHc9ttt3H99dczePBgpk2btmMZkUKjACU9UuuquFtuuYUFCxbw7LPPsnHjRp5++mkgahi6yvDhw1m6dOmO4WXLlu2ocqysrOSWW25h8eLFzJ07l1tvvZUnn3wSgAsvvJA//vGPLF26FDPjqquu6rI0iuRTuwHKzGaZ2Roze3k3083M7jCzRWb2oplNzH0yRbrWpk2b6NWrFwMGDGD9+vV861vfyun6t2/fTn19/Y6uqamJCy64gBtvvJG6ujrWrl3LDTfcwCc/+UkAHn30URYtWoS7079/f4qLiykqKmLBggU89dRTNDQ0UFFRQa9evSgq0nmmFKaO/LLnAFP3MP0MYEKquwz48b4nS2T/uvLKK9m2bRtVVVUcd9xxTJ26p59855155pn06tVrR3f99dfzjW98g8mTJ3PEEUdw+OGHM3HiRL7xjW8AsHDhQk477TT69u3L8ccfzxe+8AU++MEP0tDQwNVXX01VVRVDhw5lzZo13HTTTTlNq0hSdKgVn5mNAR5198PamHY38J/u/kBqeAFwiruv3tM6J0+e7LW1tXuT5h3mzJkDwPTp0/dpPZIs2q+FR/tU9sTMnnP3ya3H56JuoAZYnjW8IjWurURcZma1ZlZbV1eXg02LiEih2q+V1+4+090nu/vk6updm7yLiIik5SJArQRGZg2PSI0TERHZa7kIUHOBT6Va8x0HvNve9ScREZH2tPuwWDN7ADgFqDKzFcB1QCmAu98FPA6cCSwCtgKf7qrEiohIz9FugHL3C9qZ7sAXc5YiERER9CQJERFJKAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoEemUdevgzjuhpWXP8/3+97Bs2f5JkxQmBSiRHHrrLejAS6r32quvwqc/HUEi7Ze/hBtv7Pg66uvhxRdhddY7B155BS68EH73u8y49esjEDU07Lz8D34AM2bAn/60+20sWQJnnBFpXbYM3n234+kTSVOAEsmR+fNhxAj4zGf2LUht3AjPPLPreHe49FKYMwcuvjhKMFu3wuc/D9/8Jjz4IDQ2wre/DQsXtr3uhgaYNAne9z4YMwYuvzy6970PHngAPvvZWOdbb8FZZ0UgeuihWPaFFyJd990Xw088sev606WqH/0o+p96CkaPhuef3/vvQ3ouBSiRTti6Fa64Alat2nm8O/zDP0BRUQSQ738/xj//PFx5JSxaFCWXbE8/DQcfDEOGwG23xToWL4bjj4cTToDf/jbm27Ytpv385/A//wNTpsBjj8G//Avcey+sXRtB4AtfgO9+F669Nkov//u/sH17rGPZMvje9+Cqq6IUdsstcMEFMGsW3H13BL5f/hJWrIDx42HYMHj2WaishH//9yhNnX56pGvVKujdO9Lw1a/C4YdHYPv856G6Gr7zHbjnHvj4x2Hw4Ex+m5u7ZJdIIXP3vHSTJk3yfTV79myfPXv2Pq9HkiUp+3XhQvfm5p3HzZnjDu4zZmTGvfSS+9FHx/i773Y/5xz3Xr3cZ850r6yM8enu1FPd77/f/b773AcOdD/wwBgH7mPHupeUuPfrF/3DhrlPnOhuFvMMGuR+zDHuTU3uU6a49+nj3r9/jFuwILYJ7hMmuJeWRv+AAe4f/KB7376ZNEydmkl7U5P7xo2Z4SuvdD/9dPebbnJ/8UX3L3wh1vuJT7gXFbm///3uo0a5f/ObmfWdfHKmf8SI+DzsMPclS9xfeMH9qqvcp0+f7f/yL7O7cndJNwbUehtxQgFKEicX+7WlJbq99eCDcXTcfLP7ww+7p5MzZUqM79vXfcOGCGCTJrlXV7v/+Mcx/OabmWBx+OHuf/mL+403ul9zTcyX/jMfPtz9jTdimbvucj/3XPcrrnBftsz9mWfcBw92/8AH3C+/PIJDWZn7K69EOpYsieB0wgnuixfHuH/6pwhmv/99zHfPPe6f+pT7cce5X3SR+3PPud92W6y/o+bNy6T3qqviO62vj+BVWup+/fUx32OPuc+aFdPmzXNvbMys46mnIkD96Eez936HSEHbXYBq94WFIkn15z/D0qVw/vkxvGULPPccHHpoVJk98AC8/DJUVLS/rk2bonquT5+oRrv44hj/r/8a12M2boTjjoMnn4zqs9/+Nqrxampim/ffD5/4RCwzenQs9/bbcT2qtBSOPjqmXXddVPe1tMB73gNlZTH+c5+LLm3kyFg+7WMfg6YmOOSQGB4zBlaujKo2sxg3Y0Z8F9XVMXzIIbH9bBMnduir3eEDH4gqzZNOgr/7u9hWeXlU69XVQf/+Md+ZZ2aWOe20ndcxYkR8tm5sIdIeBSjpllpaooXYG2/AMcfAzTfH9Zj6+hh+9VXYvBl+9jO47LJdl1+/PqZdemm0aDvnHBg4MK7H/P3fx5/quedmriVBXPtpboYf/jCurXz3uzH+tNOiBVy2j32s7XSXl0cA7awpU3Yd16fPruPSwSlXSksj2LclHZzaU1MTnwpQ0lkKUNIt/cd/RCMAiLP7VavgkkuiwcF3vhPjR4+OwPWhD8GBB8a4N96IJtB33AGPPBINDRYvjhLLkiVwyilQVRUNAIqKIkANGRLLP/MMfOtbUTKZOTMaL/TvH+tKl2JkV717Q0mJApR0ngKUdDvumcAxdmxU9X35y3D77TFtyZI487/wwmgqfdBB8Otfw0c+EtVUL74Y67ngAvjNb2K+22+P+3r+9Cf4yldg0KCY59xzo+Xa6afDvHlR3QVRNZdufi3tKy9XgJLOU4CSbufuu+Na0B13xHWcW27J3KhqFs2x05Yuhf/zf6Iq7wc/iOD02c/CuHHw//5fVBWWpI6Cs86KLtvDD2f6Dzusa/NVyBSgZG8oQEm3sn59lHCmTIEvfjGq4dq6PpM2bFg0YJg8GaZPj2tHd9yRaThRpDsB94vy8miIItIZ3TpAbd+uuv+eZvbsaK33/e93PLgcckg8BeHWW6NVWkda9UlulZfH8drQEP0iHdGhQ9zMpprZAjNbZGZXtzF9upnVmdnzqe7S3Cd1Z6tWxUXr7Ka4UrhaWuA//zOeDXfSSfFons6YMAF+/GOYNq1LkiftSDenr6vLbzqke2m3BGVmxcCdwOnACmC+mc1191dbzfqQu8/ogjS2adiw+NGr2qBnuO++qKIrKoJ/+qd8p0Y6q7g4PnW8Smd0pAR1DLDI3Re7eyPwIHB21yarfWbxnLCNG/OdEtkfHnoobk5dtiwaPUj3km6IoqeaS2d0JEDVAMuzhlekxrV2rpm9aGYPm9nInKSuHf36xb0oGzbsj61JvmzYEPc9nXtu5qZP6V7SJSidUEpn5KoN0yPAGHc/ApgH3NvWTGZ2mZnVmlltXQ4qoysr47O2dp9XJQl1xx1w4olxgf3cc/OdGtlb6RKUApR0RkcC1Eogu0Q0IjVuB3df5+7puxx+Ckxqa0XuPtPdJ7v75OocPJMlHaD+8pd9XpUk1G23xYv1PvxhOPbYfKdG9pZKULI3OhKg5gMTzGysmZUB04C52TOY2bCswY8Cr+UuibtXUgK9esXDOqXwLF8eT4W49tp406vuWeq+VIKSvdFuKz53bzKzGcATQDEwy91fMbMbiEekzwW+bGYfBZqA9cD0LkzzTvr0iddVS+F5+un4PPnk/KZD9p1KULI3OnSjrrs/Djzeaty1Wf3XANfkNmkd06dPvN66vl43YBaS5cujYUT//nDEEflOjewrsygBK0BJZ3T7SpM+feImzgUL8p0SyZXt2+MJ408/HQ0k0mff0r2VlChASecURICCeDGdFIbm5vicMiUe8CqFobhY90FJ53T7ANWrV7xaQQGqcDQ1xefll8N735vftEjuqAQlndXtA5RZvHJh1qx4vbUe6d/9pUtQ/frlNx2SW8XFClDSOd0+QAF84hPxquvZs+HKKzNn4NI9pfefAlRhUQlKOqsgAtTVV0cV31e+AnfdFQ+S/e//zneqZG+lS1DpG7GlMChASWcVRIBKu/lm+NWvYMCAeK3C7Nlxk+e9bT54SZJKJajCpCo+6axu/cLC1oqK4Jxz4qnXxx8f16QgrlMdfLAeldNd6BpUYUqXoNz1olHpmIIqQaUddRS8+Sa8/jqsXQvDh8e7hJ58Mg4OSbZ0gOrdO7/pkNwqLo57FrduzXdKpLsoyAAFMHRovEV10KCo6qurg9NOg+uuU5BKuqamONvWWXZh0eOOpLMKNkBlO/10WLECLrkEvv1t+Od/zkx7+OGo+lu9On/pk501N+vpEYVILy2Uziqoa1B7UlEBM2fC22/DV78az3rbvj1a/dXXwze+Affck+9UCmRKUFJYVIKSzupRfwNFRRGEjjoKbr01nkAxblyUoGbPhokT4fOf19l7vqkEVZhKS+Pz7bfzmw7pPnpUgAIYPDiefl5SkjlgNm6EpUthxgx46qm4j6qlBe68U9dB8kElqMLUp0+ceDz7LHzkI/lOjXQHPfJvoHXrsP7949UOP/oR/OM/ZsYPHx6fJ5wAJ50Uf5ruUTX4l7/EPVZjxsA110SDjDSVAPZNc7NenVKIiouj9uKPf8x3SqS76BGNJDrCDP7v/4VHHomnUBx7LHzzm9GdeioMGQIXXgiHHhpngqeeGq/4ePjheCXECy/Een7603iA7XnnxSsjpPOamhTgC9WJJ0YJqrEx3ymR7kABqpWzzoqD6N/+DX7yE1i1Kp5OcdZZUf1XUgJf/jJcemkEpfnzoawMTjklnmQxYwYcdBDMmweTJ8N//deu22hqUlP3PWluVhVfoTrppGiU9Ne/5jsl0h3ob2A3Ro2KIATxdIpzzml7vqqqKHGddlo8E/DAA+EPf4BNmyKofeQjUdp6550olT3zTJxBFhXFPVlf+9r+y1N30NKiKtJCduKJcfJx8cUwZ0488UVkd1SCyoExY+CVV6KhxYIF8WT1ceOiFDVoUNS519XBLbfE9at/+Af40Ifg61+H88+Hiy6KwHX77bpHZPPm+FQJqjANHgyPPhpVfGefrfsPZc/0N5Aj5eVR6spWUxOBq7g4qgEbGjIX/7dtg/e/Hx57LKr77r8/xt9/fzwK5vjj4eMfjyB3002ZFoeFLn2PjEpQhevDH47f/eTJcV33d7+L40ekNQWoLpbdYjC7ZVqvXvA//xPBacUK+P3vI4h9/vMwcGDcr5W+cbilBX74w6gWLHSbNsWnAlRhO+SQuHH+oosiYB17bNQ2nHYaHHkkvPVWXOMtLY0q8sbGCGK67aNnUYDKo3TAmjAh00z9tNPiPqwrrogH3o4cGc3fH3ggmrNfdFF8LlwY4wYPzlvyu8Rbb8WnqvgK3yc/Gddmv/vdOFlLt+yrqIiGFGnDhkVV4PHHRwva0tKoHhw3LoLYX/8ayxx3XDRWKi+PeUpLo7+8PE7+FNy6H/0NJMzYsfF5993x2dQUB92990bQuuKKGF9WFveUHHccbNmSeThu375xvauhIapQBg2KJ7pv3Rr9w4fDsmVRahs/Pg7sioq4F2zJkggQRx4JBxwQDUD69YvrQu++G9sZNmznFwlu3RrN6Zcti2txY8ZEqbGlJdZVVxetGrdti+tvVVU7B5/ly+P9XZWV8Qf1ta/BYYfpZYU9xZe+FB3E7/TRR+Gll+I67kknxW/yF7+I3+ojj0QJe+vWqGlIGzUqfu+zZ+95W8OHZwKYe6YbOhTe974Iilu2xDZeey3mP/bYePLFqFHRbd0Ka9bEcdmvX/xO+/SJ8e++G1X65eVRPV9SAn/7WxwPw4bFrSpbtsSJ5+DBsd7Bg2M96UAKcZwUF0fQbmmJW1vKyzPHcFFR5n7MoqKdjyf3aGSUvlWjtDSGi4oyAbq+PvIwcmQc2336xPRt22K5Xr1g3brYB0VFMf/GjXHsusfJ8bBh8Z/R1RSgEq6kJM40P/GJqAasrY3A068ffOc78OqrEZRefjnzYsb0jzYX95oUFcVB0tllzDKvzWi9jv79I81mEShLSyNIrV0bB8A55/Sca26SUVUVr8XJdsIJ8fuHaGQE8Rt5441olHTEEfFH6h7B4JVX4o873TU2xh9sQ0McK08/Heswy3SrVsW82YYPjwCS/g131uWXd36ZysoIDmvWxPGbfnFnttLSSHP2sV1REcdXU9Oux2rfvhGEzGK+Xr0iAG/fHv3btmVKlw0Nuy47ZEgcow0NEfAqKiLIQix36qnw2992Pq8d1aEAZWZTgduBYuCn7v69VtPLgZ8Bk4B1wPnu/mZuk9qzmUVd/Yc/nBn36KM7z7N5c/yQBg6MA+vll+PHWFUVP7bVq6MbPToOwNdfjx/0li1x9jduXMz717/GD/ett2L5/v0jIPbuHQfztm2ZbZaVxXIjR8KiRZll3GHEiCiJvfRSpKm8PEpU69fHNhsaYNKkWGbDhjgre9/71JJR9swsSlTjx+88buLE6Dpr69YIdr17R2miT5/48161Kv6ca2qipL9sWUwbPDgCyKZN0W3ZEsv26xe3maxaFfdMukctx/btmWOvpCRqFOrq4s//7bdj+/X1MW7Lljhutm6Nmoy+feO4amyMNKVbPZaVRdfUFGkoKdm1a2iIKtQBA+I437YtusrK2Ea6JLR+faR14MBYbsuWOG4XLYqTxo99LP4z0sd2+hrhpk2R367UboAys2LgTuB0YAUw38zmuvurWbNdArzj7uPNbBpwM3B+VyRYdq9v3+ggSi1HHbXz9NatDIcMaXs9e/uje//72x7/93/fufXMmbN32xfZG717w3vfu+v44cMzjzurqYnq9I6oqYGjj85d+nqyjrQLOwZY5O6L3b0ReBA4u9U8ZwOpCiYeBj5kpkuSIiKy98zbeeaOmZ0HTHX3S1PDFwHHuvuMrHleTs2zIjX8Rmqeta3WdRlwWWrwPcCCHOShCljb7lyFQXktPD0ln6C8Fqpc5HW0u1e3HrlfG0m4+0xgZi7XaWa17j45l+tMKuW18PSUfILyWqi6Mq8dqeJbCYzMGh6RGtfmPGZWAvQnGkuIiIjslY4EqPnABDMba2ZlwDRgbqt55gIXp/rPA57y9uoORURE9qDdKj53bzKzGcATRDPzWe7+ipndANS6+1zgHuA+M1sErCeC2P6S0yrDhFNeC09PyScor4Wqy/LabiMJERGRfOgBjx8VEZHuSAFKREQSSQFKREQSSQFKREQSSQFKREQSSQFKREQSSQFKREQSSQFKREQSSQFKREQSSQFKREQSSQFKREQSab++DypbVVWVjxkzZp/WsW5dvNFj0KBBOUiRJIX2a+HRPpU9ee6559bm/YWF2caMGUNtbe0+rWPOnDkATJ8+fd8TJImh/Vp4tE9lT8xsaVvjVcUnIiKJpAAlIiKJ1G6AMrNZZrbGzF7ezXQzszvMbJGZvWhmE3OfTBER6Wk6UoKaA0zdw/QzgAmp7jLgx/ueLBER6enaDVDu/jTxGvfdORv4mYc/AwPMbFiuEigiIj1TLq5B1QDLs4ZXpMbtwswuM7NaM6utq6vLwaZFRKRQ7ddGEu4+090nu/vk6updmryLiIjskIsAtRIYmTU8IjVORERkr+UiQM0FPpVqzXcc8K67r87BekVEpAdr90kSZvYAcApQZWYrgOuAUgB3vwt4HDgTWARsBT7dVYkVEZGeo90A5e4XtDPdgS/mLEUiIiLoSRIiIpJQClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIClAiIpJIHQpQZjbVzBaY2SIzu7qN6dPNrM7Mnk91l+Y+qdITrFoFtbVQX5/vlIhIvpW0N4OZFQN3AqcDK4D5ZjbX3V9tNetD7j6jC9IoPchrr8GWLbBxY75TIiL51pES1DHAIndf7O6NwIPA2V2bLOmptm2Lz4aG/KZDRPKvIwGqBlieNbwiNa61c83sRTN72MxGtrUiM7vMzGrNrLaurm4vkiuFLl21pwAlIrlqJPEIMMbdjwDmAfe2NZO7z3T3ye4+ubq6OkeblkKiEpSIpHUkQK0EsktEI1LjdnD3de6e/kv5KTApN8mTnkYlKBFJ60iAmg9MMLOxZlYGTAPmZs9gZsOyBj8KvJa7JEpPohKUiKS124rP3ZvMbAbwBFAMzHL3V8zsBqDW3ecCXzazjwJNwHpgehemWQpYugTV2BhdWVl+0yMi+dNugAJw98eBx1uNuzar/xrgmtwmTXqidAkK4p6oMWPylhQRyTM9SUISJTtALV+++/lEpPApQEmiZD9BYsWK/KVDRPJPAUoSZds2KC4GM/jzn/OdGhHJJwUoSZT6eigpgepquPde2Lw53ykSkXxRgJJESZegamrg3XfhjjvAPd+pEpF8UICSRKmvh6Ii6NcPTj8dvv51+Mxn8p0qEckHBShJlG3bIkABPP44fOUrMGcO/OY3eU2WiOSBApQkSnaAKimB734XDjsMvvQlXY8S6WkUoCRR0lV8aaWlcNddcU/UDTfkL10isv8pQEmiZJeg0k44AS69FH7wAzj8cHjppfykTUT2LwUoSZTWJai0H/0oqvtWr47qPrXsEyl8ClCSKOlm5q317QvXXAPf/jb8139Fy74nnlCgEilkClCSKLsrQaVdeimcdx788pcwdSqce65ezSFSqBSgJFHaugaVrbQUfvELWLsWbroJfv1rOOMM+Na31MpPpNB06HUbIvuDe/slqLSyMrj6aujTB66/Hv7wB1i4EO67L57j19QEmzbBwIFdnmwR6SIqQUlipKvqOhKg0r70JVi3Lpqg//zncN11sGQJvP/9MG4cvP5616RVRLqeApQkRvpdUJ0JUGlf+xpMnx6NKMaNg5dfjpLU3/0drFmT02SKyH6iKj5JjPS7oPYmQBUXw6xZ0XBizRo47bRokn7WWTBxYnTHHgsnngg/+QnMnw+1tVBZmds8iEjuKEBJYuxLCQqixHT++Znh974Xnn4a/vEf4Y034JFHMutvaYErr4Tf/Q4uvBBuvBHKy/ct/SKSWwpQkhj7UoLancmT474pgJUr4ZVXYMQI+OIXo8TVty/88IdQVxfBbcGCCFwikn8KUJIY6RJUWzfq5kJNTXQQjSouvhj+9V9h7txosv7zn0frv5NPjipBEckvBShJjH2t4uuMk06CxYuj/8gj434qgLfegssvh0MPjQYWU6bESxO3bYtm7aWlXZ+2QrRuXXzfv/pVlFA/+MGokhXZEwUoSYyuqOLriIqKaDRRUhIPpL322ngg7ezZMa6pKeabNQuGD4fevaPr0yeGBwyIt//26wcbN8L69XE9q7o6ul69YNUqGDw4Atz27dDYGJ9mMHZs5LlPn8y0vn1hw4ZYb2Vl3NO1ZEmk5eijY5tFRfHOrLIyOOqoWO6AA2DQoEwgXbw4utJSOOigSHdRUXRmmf62xpWWRmn2uefiml1lZaSrsjLmW7UKxo+P7W7YEPOXlsLbb8OQIbB0aaSztBT+93/ju/zrX+FDH4r+MWPi4b99+sQ9bMceG99FY2N8h+7x/bW0RL6bmqC5OfLlHl1jY2ynVy8YOhSGDYvvIPs7Hjo0vse33460r1gR+238+MiHe6Rh2zbYsiW6rVuhf/+oDi4ujmkvvggjR8KyZbHc5Mmx/vLyeNr+oEGx/m3boquvj+kHHRRpXrEi0l9dHWnp2zeWqaiI6fX1ke9evaJbtizWV1MTN6anx2/cGPP37h3rKC2NE6vVq+NWjfR306tXfMctLbHukpJYZvPm+M6am6PF69q1sX8qKuK3VVkZ311DQ3ymu+3bY7sjRsT2iori+zj00K47NhWgJFHwHIAAABBZSURBVDH2Zwmqtb594/NrX4trUWPHwr//O/zpT3FPVVFR3ASc/gOrq4vP5cvjQK6oiD+Bior4g2xoiFJDLqUDRzpgQvw5NTfHn1CSXXIJHHFEPPT3/vuj0crChXEisHEjHHgg3H135l64srL4bGzc/TrN4k931KhY7q23dv5udif7pEP2zamnwpNPdt36OxSgzGwqcDtQDPzU3b/Xano58DNgErAOON/d38xtUqXQ5asEla24OHPG+/GPR5d23nm7zt/cHGeWFRXxZ5r+Y4X4E1y3Ls7Ghw+PM9WmppgnXdpoaoqSEUTAKymJM/LNm+MpGP37xx94375x5trcHA09Nm+O72vSpPijXr48llu/PrbZ1BRn0cOGxQsf6+vjpuXGxghm6c595+HscQ0NsdxRR8WZ96ZN0W3eHOsfMiSCTO/eURJIn3UPHhwlhJEj4Z134sTjrbcifRUV8TzFtrS0xPwVFbFOiG0VF8f3Uly85+uTLS2R9w0b4jsuK4vlli2L9Y0aFekfMiT2xaJF8d2ZxXb69Ml0vXvHPG+/Hd95cXGUFFasiOWbm+P7LC+P76imJrOv0yWddGn11VdjX9fUxHBdXeyXzZtjmcbG+M7LyuJ7rK+P30J6nWvXxjbr6+O7rKyM9WzdGvlpbMyUHisqMnnatClKSqWlMb6pKZYvL48TMLP4Dqqr4eCD43f20kux3rKymC/9PaZ/s83N8VsbOjSWz/69d4V2A5SZFQN3AqcDK4D5ZjbX3V/Nmu0S4B13H29m04CbgfN3XZvI7uWzBLW3sv80Wx+sJSXxx5KWbqDR2qBBndvm5Mm7jquq2vMy/fvvnJZcOfHEjs03Z0778xQV7fpddOY+taKiTLVqtuzhPn0y41rP19rQoRHcW49LGzOmY+nqyiqw9hx99J6nZ6ftgAM6lqdjjtmnJHVKR0pQxwCL3H0xgJk9CJwNZAeos4HrU/0PA/9sZubedS9D2LIlziQBzjmnq7Yi+9Obb8ZndwpQItJ1rL0YYmbnAVPd/dLU8EXAse4+I2uel1PzrEgNv5GaZ22rdV0GXJYafA+wIAd5qALWtjtXYVBeC09PyScor4UqF3kd7e67lGn3ayMJd58JzMzlOs2s1t3bqPQoPMpr4ekp+QTltVB1ZV47UpmyEhiZNTwiNa7NecysBOhPNJYQERHZKx0JUPOBCWY21szKgGnA3FbzzAUuTvWfBzzVldefRESk8LVbxefuTWY2A3iCaGY+y91fMbMbgFp3nwvcA9xnZouA9UQQ219yWmWYcMpr4ekp+QTltVB1WV7bbSQhIiKSD2rQKyIiiaQAJSIiiaQAJSIiiaQAJSIiiaQAJSIiiaQAJSIiiaQAJSIiiaQAJSIiiaQAJSIiiaQAJSIiiaQAJSIiibRf3weVraqqysd09J3Ju7FuXbzRY1Bn35ktiab9Wni0T2VPnnvuubV5f2FhtjFjxlBbW7tP65gzZw4A06dP3/cESWJovxYe7VPZEzNb2tZ4VfGJiEgitRugzGyWma0xs5d3M93M7A4zW2RmL5rZxNwnU0REepqOlKDmAFP3MP0MYEKquwz48b4nS0REerp2A5S7P028JXd3zgZ+5uHPwAAzG5arBIqISM+Ui2tQNcDyrOEVqXG7MLPLzKzWzGrr6upysGkRESlU+7WRhLvPdPfJ7j65unqXFoUiIiI75CJArQRGZg2PSI0TERHZa7kIUHOBT6Va8x0HvOvuq3OwXhER6cHavVHXzB4ATgGqzGwFcB1QCuDudwGPA2cCi4CtwKe7KrEiItJztBug3P2CdqY78MWcpUhERAQ9SUJERBJKAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBJJAUpERBKpQwHKzKaa2QIzW2RmV7cxfbqZ1ZnZ86nu0twnVUREepKS9mYws2LgTuB0YAUw38zmuvurrWZ9yN1ndEEaRUSkB+pICeoYYJG7L3b3RuBB4OyuTZaIiPR0HQlQNcDyrOEVqXGtnWtmL5rZw2Y2sq0VmdllZlZrZrV1dXV7kVwREekpctVI4hFgjLsfAcwD7m1rJnef6e6T3X1ydXV1jjYtIiKFqCMBaiWQXSIakRq3g7uvc/eG1OBPgUm5SZ6IiPRUHQlQ84EJZjbWzMqAacDc7BnMbFjW4EeB13KXRBER6YnabcXn7k1mNgN4AigGZrn7K2Z2A1Dr7nOBL5vZR4EmYD0wvQvTLCIiPUC7AQrA3R8HHm817tqs/muAa3KbNBER6cn0JAkREUkkBSgREUkkBSgREUkkBSgREUkkBSgREUmkDrXiExHZF5s2wZtvQkMDlJfnOzXSXagEJSJdbsMGWL8elizJd0qkO1GAEpEu19wcnytX7nk+kWwKUCLS5Zqa4nPVqvymQ7oXBSgR6XIqQcneUIASkS6nEpTsDQUoEelyKkHJ3lCAEpEupxKU7A0FKBHpcukApRKUdIYClIh0uXQV3+rV0NKS37RI96EAJSJdrqkJioris64u36mR7kIBSkS6VFNTlJp6945hXYeSjlKAEpEutXFjfPbrF5+/+lX+0iLdiwKUiHSpdICqrITzz4dbb4XFi/ObJukeFKBEpEulA1RxMdx4YzSYGD8errwS3PObNkk2BSgR6VLvvhufJSURmF56CS65BG6/HT77WVi2LL/pk+TS+6BEpEulS1AlqX+bCRNg5sy4JnXbbXDPPTBuHGzeDIcdlllu2DAYOhQGD4aysiiBFRdDr14wYEBUGZaURAOM2lro0wcOOggaG6GmBg49FEpLo5HG66/HOg85ZP/mXfaNApSIdKl0Caq4ODPODG65Bb78ZXjggQgw/frBCy9EMCoqgmeeifum6uv3brsVFbGdbdsy4448EkaMiKrFpqZMFaNZpmtrOB0Ye/fOdPX10WS+pibed9XcHNfWtm+PQFtSsvN60usqLYUDDoiblltaIp0VFTG+pSU6s9hmUdHOHcS23OPFjyUlkY90t307rFkT4ydMiOHGRhg1Ktb5+uuxP0aNivQ2NsbLJHv3hv79M2lMc48S7qZNMHJknCxs3BhpKSuLEvG0aXu3fzpCAUpEulTrElS20aPh6qt3v6w7bNkSf7TNzdFt2xZ/0hs3xnBLSwSEzZujCXtpaby9d/78+COtrIw/5A0b4JFHIjAUFWUCgHumS2+z9XBzcwSkrVszXXExVFVFED3ggBgePjz+uH/960hXW+tqaIh1DRgQaU0Pb98e6TGL+dLLt5YuTTY0xDzFxfHdlpRk0lRfD/ffH+srKYlABJlAtHp1TCstjRODzZt3DuTZqqpg4ECYOzfWW1SUudn69NMTEKDMbCpwO1AM/NTdv9dqejnwM2ASsA44393fzG1SRaQ7ym4k0Vlm0Ldvx+YdMgQOPDD6jzuu7T/OK6/sfBpyzT2CQfq+sPbmTQerdFdeninpuO9a6knbvj1zUrB6dcw3eHDsh5aWTIksvZ7dPeEjO2hu3pzZH+kThq7UboAys2LgTuB0YAUw38zmuvurWbNdArzj7uPNbBpwM3B+VyRYRLqXtqr4ejKzjgWn9LxmOweT1tN3p7Q00z98+M7TWq8vXaXYXloqKzPD6VJbV+rI6o8BFrn7YgAzexA4G8gOUGcD16f6Hwb+2czMvesakb77Lvzxj9H/pS911VYkH85PndpovxaG+nqYPj3fqZDuyNqLIWZ2HjDV3S9NDV8EHOvuM7LmeTk1z4rU8Bupeda2WtdlwGWpwfcAC3KQhypgbbtzFQbltfD0lHyC8lqocpHX0e5e3Xrkfm0k4e4zgZm5XKeZ1br75FyuM6mU18LTU/IJymuh6sq8duRG3ZXAyKzhEalxbc5jZiVAf6KxhIiIyF7pSICaD0wws7FmVgZMA+a2mmcucHGq/zzgqa68/iQiIoWv3So+d28ysxnAE0Qz81nu/oqZ3QDUuvtc4B7gPjNbBKwngtj+ktMqw4RTXgtPT8knKK+Fqsvy2m4jCRERkXzQw2JFRCSRFKBERCSRum2AMrOpZrbAzBaZ2R6e5tV9mNmbZvaSmT1vZrWpcQeY2TwzW5j6HJgab2Z2Ryr/L5rZxPymfs/MbJaZrUndM5ce1+m8mdnFqfkXmtnFbW0r33aT1+vNbGVq3z5vZmdmTbsmldcFZvbhrPGJ/42b2Ugz+4OZvWpmr5jZFanxBbVv95DPgtuvZlZhZn8xsxdSef1WavxYM3s2le6HUo3mMLPy1PCi1PQxWetq8zvoMHfvdh3RWOMNYBxQBrwAHJLvdOUgX28CVa3GfR+4OtV/NXBzqv9M4LeAAccBz+Y7/e3k7WRgIvDy3uYNOABYnPocmOofmO+8dTCv1wNfaWPeQ1K/33JgbOp3XdxdfuPAMGBiqr8SeD2Vp4Lat3vIZ8Ht19S+6ZvqLwWeTe2rfwOmpcbfBVye6v8CcFeqfxrw0J6+g86kpbuWoHY8fsndG4H045cK0dnAvan+e4GPZY3/mYc/AwPMbFg+EtgR7v400cIzW2fz9mFgnruvd/d3gHnA1K5PfefsJq+7czbwoLs3uPsSYBHx++4Wv3F3X+3uf031bwJeA2oosH27h3zuTrfdr6l9szk1WJrqHDiVeJQd7LpP0/v6YeBDZmbs/jvosO4aoGqA5VnDK9jzj6W7cOD3ZvacxWOhAIa4++pU/1vAkFR/IXwHnc1bd8/zjFS11qx0lRcFlNdU1c5RxBl3we7bVvmEAtyvZlZsZs8Da4iThTeADe7elJolO9078pSa/i4wiBzktbsGqEJ1ortPBM4AvmhmJ2dP9Cg3F+R9AYWct5QfAwcCRwKrgVvym5zcMrO+wC+BK919Y/a0Qtq3beSzIPeruze7+5HEk4OOAQ7ORzq6a4DqyOOXuh13X5n6XAP8mvhhvJ2uukt9rknNXgjfQWfz1m3z7O5vpw76FuAnZKo6un1ezayU+NP+ubv/KjW64PZtW/ks5P0K4O4bgD8AxxPVsemHO2Sne3ePutvnvHbXANWRxy91K2bWx8wq0/3AFOBldn6M1MXAb1L9c4FPpVpFHQe8m1Wl0l10Nm9PAFPMbGCqKmVKalzitbo+eA6xbyHyOi3VEmosMAH4C93kN5661nAP8Jq735o1qaD27e7yWYj71cyqzWxAqr8X8S7A14hAdV5qttb7tK1H3e3uO+i4fLcY2duOaA30OlE3+vV8pycH+RlHtHh5AXglnSeiLvdJYCHwH8ABnmlpc2cq/y8Bk/Odh3by9wBRBbKdqIu+ZG/yBnyGuNi6CPh0vvPVibzel8rLi6kDd1jW/F9P5XUBcEbW+MT/xoETieq7F4HnU92ZhbZv95DPgtuvwBHA31J5ehm4NjV+HBFgFgG/AMpT4ytSw4tS08e19x10tNOjjkREJJG6axWfiIgUOAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoERFJJAUoERFJpP8POU22bZFaZboAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9fLNObvnWW"
      },
      "source": [
        "In numbers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZD9r3RykvnWX",
        "outputId": "ea845ccc-bc98-4d00-e4d9-49ed5e90ef7e"
      },
      "source": [
        "accuracyAll(models_B)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model 0\n",
            "Task 0: Acc 0.97% | Gr acc 0.95 | Ugr acc 1.0\n",
            "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:825: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.num_layers, self.dropout, self.training, self.bidirectional)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:822: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Task 2: Acc 0.72% | Gr acc 0.44 | Ugr acc 1.0\n",
            "\n",
            "Model 1\n",
            "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
            "Task 1: Acc 0.72% | Gr acc 0.5 | Ugr acc 0.95\n",
            "Task 2: Acc 0.64% | Gr acc 0.31 | Ugr acc 0.97\n",
            "\n",
            "Model 2\n",
            "Task 0: Acc 0.9% | Gr acc 0.8 | Ugr acc 1.0\n",
            "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
            "Task 2: Acc 0.83% | Gr acc 0.67 | Ugr acc 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUleAoTVvnWY"
      },
      "source": [
        "## Baseline C: Freeze Parameters\n",
        "\n",
        "1. Define functions\n",
        "2. Train model, freeze core weights in between tasks\n",
        "3. Look at performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiFSSGphvnWY"
      },
      "source": [
        "def applyOnParameters(model, conditions, apply_function):\n",
        "    for name, param in model.named_parameters():\n",
        "        # Check every condition\n",
        "        for condition in conditions:\n",
        "            # check every keyword\n",
        "            allincluded = True\n",
        "            for keyword in condition:\n",
        "                if keyword not in name:\n",
        "                    allincluded = False\n",
        "                    break\n",
        "            if allincluded:\n",
        "                apply_function(param)\n",
        "\n",
        "def freezeParameters(model, conditions):\n",
        "    def freeze(param):\n",
        "        param.requires_grad = False\n",
        "    applyOnParameters(model, conditions, freeze)\n",
        "\n",
        "def unfreezeParameters(model, conditions):\n",
        "    def unfreeze(param):\n",
        "        param.requires_grad = True\n",
        "    applyOnParameters(model, conditions, unfreeze)\n",
        "\n",
        "def showModelParameters(model, requires_grad=False):\n",
        "    for name, param in model.named_parameters():\n",
        "        if requires_grad:\n",
        "            if param.requires_grad:\n",
        "                print(name)\n",
        "        else:\n",
        "            print(name)\n",
        "            \n",
        "def onTaskUpdate(model):\n",
        "    # Freeze core weights\n",
        "    freezeParameters(model, ((\"\"),))    # Freeze everything\n",
        "    unfreezeParameters(model, ((\"encoder\",\"embedding\"), (\"decoder\",\"fc_out\"), (\"attention\",))) # Unfreeze relevant stuff\n",
        "    \n",
        "    # Reinitialize\n",
        "    to_constant = lambda param: nn.init.constant_(param.data, 0)\n",
        "    applyOnParameters(model, ((\"decoder\",\"fc_out\",\"bias\"),(\"attn\",\"bias\")), to_constant)\n",
        "    to_normal = lambda param: nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "    applyOnParameters(model, ((\"encoder\",\"embedding\"),(\"decoder\",\"fc_out\",\"weight\"),(\"attention\",\"weight\")), to_normal)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJlVK1CCvnWZ"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_UJzijZvnWZ"
      },
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H83P-2BXvnWZ",
        "outputId": "7a5ed6d3-68d2-4745-8c61-95dd5affbfec"
      },
      "source": [
        "models_C = []\n",
        "hist_losses_C = []\n",
        "hist_hitsss_C = []\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "print(model.apply(init_weights))\n",
        "\n",
        "for n_task in range(N_TASKS + 1):\n",
        "    SUFFIX = f\"C{n_task}\"\n",
        "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
        "    LOADNAME = \"models/autosave/\" + title + \".pt\"\n",
        "    SAVENAME = \"models/autosave/\" + title + \".pt\"\n",
        "    PLOTSAVE = \"plots/autosave/\" + title + \".png\"\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
        "    \n",
        "    print(title)\n",
        "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
        "    \n",
        "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
        "    hist_losses_C.append(hist_loss_temp)\n",
        "    hist_hitsss_C.append(hist_hits_temp)\n",
        "    models_C.append(copy.deepcopy(model))\n",
        "    \n",
        "    # Freeze, reinitialize\n",
        "    onTaskUpdate(model)\n",
        "    "
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 335 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
            "Epoch: 336 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
            "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
            "Epoch: 337 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
            "Epoch: 338 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
            "Epoch: 339 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
            "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
            "Epoch: 340 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
            "Epoch: 341 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
            "Epoch: 342 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
            "Epoch: 343 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
            "Epoch: 344 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
            "Epoch: 345 | Time: 0m 0s\n",
            "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
            "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
            "Epoch: 346 | Time: 0m 0s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
            "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
            "Epoch: 347 | Time: 0m 0s\n",
            "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 348 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
            "Epoch: 349 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
            "Epoch: 350 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
            "Epoch: 351 | Time: 0m 0s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
            "Epoch: 352 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 353 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
            "Epoch: 354 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
            "Epoch: 355 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
            "Epoch: 356 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
            "Epoch: 357 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
            "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
            "Epoch: 358 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
            "Epoch: 359 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
            "Epoch: 360 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 361 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
            "Epoch: 362 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
            "Epoch: 363 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
            "Epoch: 364 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 365 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
            "Epoch: 366 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
            "Epoch: 367 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
            "Epoch: 368 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 369 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
            "Epoch: 370 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 371 | Time: 0m 0s\n",
            "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
            "Epoch: 372 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.378\n",
            "Epoch: 373 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.379\n",
            "Epoch: 374 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
            "Epoch: 375 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "Epoch: 376 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
            "Epoch: 377 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
            "Epoch: 378 | Time: 0m 0s\n",
            "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
            "Epoch: 379 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
            "Epoch: 380 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
            "Epoch: 381 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
            "Epoch: 382 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 383 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 384 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
            "Epoch: 385 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 386 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
            "Epoch: 387 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
            "Epoch: 388 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 389 | Time: 0m 0s\n",
            "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
            "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
            "Epoch: 390 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
            "Epoch: 391 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 392 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 393 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
            "Epoch: 394 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
            "Epoch: 395 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
            "Epoch: 396 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
            "Epoch: 397 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
            "Epoch: 398 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 399 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 400 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 401 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.351\n",
            "Epoch: 402 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 403 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
            "Epoch: 404 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
            "Epoch: 405 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 406 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 407 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
            "Epoch: 408 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "Epoch: 409 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 410 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 411 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
            "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
            "Epoch: 412 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
            "Epoch: 413 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
            "Epoch: 414 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 415 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 416 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
            "Epoch: 417 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
            "Epoch: 418 | Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.330\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 419 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 420 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
            "Epoch: 421 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
            "Epoch: 422 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.378\n",
            "Epoch: 423 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
            "Epoch: 424 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
            "Epoch: 425 | Time: 0m 0s\n",
            "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
            "Epoch: 426 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 427 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 428 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
            "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
            "Epoch: 429 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
            "Epoch: 430 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.330 |  Val. PPL:   1.390\n",
            "Epoch: 431 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
            "Epoch: 432 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
            "Epoch: 433 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
            "Epoch: 434 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 435 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
            "Epoch: 436 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 437 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
            "Epoch: 438 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "Epoch: 439 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 440 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
            "Epoch: 441 | Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
            "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
            "Epoch: 442 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
            "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
            "Epoch: 443 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.372\n",
            "Epoch: 444 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
            "Epoch: 445 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
            "Epoch: 446 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
            "Epoch: 447 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
            "Epoch: 448 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 449 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 450 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 451 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.330 |  Val. PPL:   1.392\n",
            "Epoch: 452 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
            "Epoch: 453 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
            "Epoch: 454 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 455 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
            "Epoch: 456 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
            "Epoch: 457 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.379\n",
            "Epoch: 458 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 459 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 460 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 461 | Time: 0m 0s\n",
            "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 462 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "Epoch: 463 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 464 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 465 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
            "Epoch: 466 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
            "Epoch: 467 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 468 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
            "Epoch: 469 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
            "Epoch: 470 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
            "Epoch: 471 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
            "Epoch: 472 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
            "Epoch: 473 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 474 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
            "Epoch: 475 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
            "Epoch: 476 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 477 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 478 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 479 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 480 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
            "Epoch: 481 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.361\n",
            "Epoch: 482 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 483 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 484 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 485 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 486 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
            "Epoch: 487 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
            "Epoch: 488 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 489 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 490 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 491 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
            "Epoch: 492 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
            "Epoch: 493 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 494 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 495 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
            "Epoch: 496 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 497 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 498 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
            "Epoch: 499 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 500 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 501 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 502 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
            "Epoch: 503 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
            "Epoch: 504 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 505 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 506 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 507 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 508 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
            "Epoch: 509 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 510 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
            "Epoch: 511 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 512 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 513 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
            "Epoch: 514 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 515 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
            "Epoch: 516 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
            "Epoch: 517 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
            "Epoch: 518 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
            "Epoch: 519 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
            "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
            "Epoch: 520 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
            "Epoch: 521 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 522 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.367\n",
            "Epoch: 523 | Time: 0m 0s\n",
            "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
            "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
            "Epoch: 524 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 525 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 526 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
            "Epoch: 527 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
            "Epoch: 528 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 529 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
            "Epoch: 530 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
            "Epoch: 531 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 532 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
            "Epoch: 533 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 534 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
            "Epoch: 535 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 536 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 537 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
            "Epoch: 538 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 539 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
            "Epoch: 540 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 541 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.371\n",
            "Epoch: 542 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
            "Epoch: 543 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 544 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
            "Epoch: 545 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
            "Epoch: 546 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 547 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 548 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
            "Epoch: 549 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 550 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
            "Epoch: 551 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
            "Epoch: 552 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
            "Epoch: 553 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 554 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 555 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
            "Epoch: 556 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
            "Epoch: 557 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 558 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 559 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
            "Epoch: 560 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 561 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 562 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 563 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
            "Epoch: 564 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 565 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.357\n",
            "Epoch: 566 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
            "Epoch: 567 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 568 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 569 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 570 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
            "Epoch: 571 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.357\n",
            "Epoch: 572 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
            "Epoch: 573 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
            "Epoch: 574 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
            "Epoch: 575 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
            "Epoch: 576 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
            "Epoch: 577 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
            "Epoch: 578 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
            "Epoch: 579 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
            "Epoch: 580 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 581 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
            "Epoch: 582 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
            "Epoch: 583 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 584 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 585 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "Epoch: 586 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
            "Epoch: 587 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 588 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
            "Epoch: 589 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 590 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
            "Epoch: 591 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
            "Epoch: 592 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
            "Epoch: 593 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
            "Epoch: 594 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
            "Epoch: 595 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 596 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 597 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
            "Epoch: 598 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
            "Epoch: 599 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
            "Epoch: 600 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 601 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
            "Epoch: 602 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 603 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 604 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 605 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 606 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
            "Epoch: 607 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 608 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 609 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
            "Epoch: 610 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
            "Epoch: 611 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
            "Epoch: 612 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
            "Epoch: 613 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
            "Epoch: 614 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 615 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
            "Epoch: 616 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 617 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 618 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
            "Epoch: 619 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 620 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
            "Epoch: 621 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
            "Epoch: 622 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 623 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
            "Epoch: 624 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
            "Epoch: 625 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
            "Epoch: 626 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
            "Epoch: 627 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
            "Epoch: 628 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
            "Epoch: 629 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
            "Epoch: 630 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 631 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 632 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 633 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
            "Epoch: 634 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
            "Epoch: 635 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 636 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
            "Epoch: 637 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
            "Epoch: 638 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 639 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 640 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 641 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 642 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
            "Epoch: 643 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.355\n",
            "Epoch: 644 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
            "Epoch: 645 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
            "Epoch: 646 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.309\n",
            "Epoch: 647 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 648 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 649 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
            "Epoch: 650 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
            "Epoch: 651 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 652 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
            "Epoch: 653 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
            "Epoch: 654 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 655 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 656 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
            "Epoch: 657 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
            "Epoch: 658 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 659 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
            "Epoch: 660 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
            "Epoch: 661 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
            "Epoch: 662 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 663 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 664 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 665 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
            "Epoch: 666 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
            "Epoch: 667 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 668 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 669 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
            "Epoch: 670 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
            "Epoch: 671 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
            "Epoch: 672 | Time: 0m 0s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
            "Epoch: 673 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 674 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
            "Epoch: 675 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 676 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
            "Epoch: 677 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
            "Epoch: 678 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 679 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 680 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
            "Epoch: 681 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
            "Epoch: 682 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 683 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
            "Epoch: 684 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 685 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
            "Epoch: 686 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 687 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 688 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 689 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
            "Epoch: 690 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 691 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
            "Epoch: 692 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 693 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 694 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 695 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 696 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
            "Epoch: 697 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
            "Epoch: 698 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.305\n",
            "Epoch: 699 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.309\n",
            "Epoch: 700 | Time: 0m 0s\n",
            "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
            "Epoch: 701 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 702 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 703 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
            "Epoch: 704 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 705 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
            "Epoch: 706 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
            "Epoch: 707 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
            "Epoch: 708 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 709 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 710 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 711 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 712 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
            "Epoch: 713 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 714 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 715 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 716 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 717 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
            "Epoch: 718 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
            "Epoch: 719 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
            "Epoch: 720 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 721 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
            "Epoch: 722 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
            "Epoch: 723 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
            "Epoch: 724 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 725 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
            "Epoch: 726 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 727 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 728 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
            "Epoch: 729 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
            "Epoch: 730 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 731 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 732 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 733 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
            "Epoch: 734 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 735 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 736 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 737 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 738 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 739 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 740 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
            "Epoch: 741 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
            "Epoch: 742 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 743 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 744 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
            "Epoch: 745 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 746 | Time: 0m 0s\n",
            "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 747 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 748 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
            "Epoch: 749 | Time: 0m 0s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 750 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
            "Epoch: 751 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
            "Epoch: 752 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
            "Epoch: 753 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
            "Epoch: 754 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 755 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 756 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 757 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
            "Epoch: 758 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 759 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 760 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 761 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 762 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
            "Epoch: 763 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
            "Epoch: 764 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 765 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
            "Epoch: 766 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 767 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 768 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 769 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
            "Epoch: 770 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 771 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
            "Epoch: 772 | Time: 0m 0s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 773 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 774 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 775 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
            "Epoch: 776 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 777 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 778 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 779 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.319\n",
            "Epoch: 780 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
            "Epoch: 781 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
            "Epoch: 782 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 783 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 784 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 785 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 786 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 787 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
            "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
            "Epoch: 788 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 789 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 790 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
            "Epoch: 791 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
            "Epoch: 792 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
            "Epoch: 793 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
            "Epoch: 794 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 795 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
            "Epoch: 796 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 797 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 798 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
            "Epoch: 799 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 800 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 801 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
            "Epoch: 802 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 803 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
            "Epoch: 804 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
            "Epoch: 805 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 806 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
            "Epoch: 807 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 808 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 809 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 810 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
            "Epoch: 811 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
            "Epoch: 812 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 813 | Time: 0m 0s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 814 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 815 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 816 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 817 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 818 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 819 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
            "Epoch: 820 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
            "Epoch: 821 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 822 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 823 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 824 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 825 | Time: 0m 0s\n",
            "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 826 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
            "Epoch: 827 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 828 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 829 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 830 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
            "Epoch: 831 | Time: 0m 0s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 832 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 833 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 834 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 835 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 836 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
            "Epoch: 837 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 838 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 839 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 840 | Time: 0m 0s\n",
            "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 841 | Time: 0m 0s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 842 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 843 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 844 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 845 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 846 | Time: 0m 0s\n",
            "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 847 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 848 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 849 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 850 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.309\n",
            "Epoch: 851 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 852 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 853 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 854 | Time: 0m 0s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 855 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
            "Epoch: 856 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 857 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 858 | Time: 0m 0s\n",
            "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 859 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
            "Epoch: 860 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 861 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 862 | Time: 0m 0s\n",
            "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
            "Epoch: 863 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
            "Epoch: 864 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 865 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 866 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 867 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
            "Epoch: 868 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 869 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
            "Epoch: 870 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 871 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
            "Epoch: 872 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 873 | Time: 0m 0s\n",
            "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
            "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
            "Epoch: 874 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 875 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
            "Epoch: 876 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
            "Epoch: 877 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
            "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
            "Epoch: 878 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 879 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 880 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 881 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 882 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.305\n",
            "Epoch: 883 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
            "Epoch: 884 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 885 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 886 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 887 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
            "Epoch: 888 | Time: 0m 0s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 889 | Time: 0m 0s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 890 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 891 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 892 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
            "Epoch: 893 | Time: 0m 0s\n",
            "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 894 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
            "Epoch: 895 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 896 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 897 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
            "Epoch: 898 | Time: 0m 0s\n",
            "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
            "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
            "Epoch: 899 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.271\n",
            "Epoch: 900 | Time: 0m 0s\n",
            "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 901 | Time: 0m 0s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 902 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 903 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 904 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 905 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 906 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 907 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 908 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 909 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 910 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 911 | Time: 0m 0s\n",
            "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 912 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
            "Epoch: 913 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 914 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.201\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 915 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 916 | Time: 0m 0s\n",
            "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 917 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 918 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
            "Epoch: 919 | Time: 0m 0s\n",
            "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 920 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
            "Epoch: 921 | Time: 0m 0s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
            "Epoch: 922 | Time: 0m 0s\n",
            "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
            "Epoch: 923 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
            "Epoch: 924 | Time: 0m 0s\n",
            "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 925 | Time: 0m 0s\n",
            "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 926 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 927 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
            "Epoch: 928 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 929 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 930 | Time: 0m 0s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
            "Epoch: 931 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
            "Epoch: 932 | Time: 0m 0s\n",
            "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
            "Epoch: 933 | Time: 0m 0s\n",
            "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 934 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 935 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 936 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
            "Epoch: 937 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 938 | Time: 0m 0s\n",
            "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 939 | Time: 0m 0s\n",
            "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 940 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
            "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
            "Epoch: 941 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
            "Epoch: 942 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 943 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
            "Epoch: 944 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 945 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 946 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 947 | Time: 0m 0s\n",
            "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 948 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 949 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
            "Epoch: 950 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
            "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
            "Epoch: 951 | Time: 0m 0s\n",
            "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
            "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
            "Epoch: 952 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 953 | Time: 0m 0s\n",
            "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
            "Epoch: 954 | Time: 0m 0s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 955 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 956 | Time: 0m 0s\n",
            "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
            "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
            "Epoch: 957 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
            "Epoch: 958 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 959 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 960 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 961 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 962 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 963 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 964 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 965 | Time: 0m 0s\n",
            "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
            "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
            "Epoch: 966 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
            "Epoch: 967 | Time: 0m 0s\n",
            "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
            "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
            "Epoch: 968 | Time: 0m 0s\n",
            "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
            "Epoch: 969 | Time: 0m 0s\n",
            "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
            "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
            "Epoch: 970 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
            "Epoch: 971 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
            "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
            "Epoch: 972 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 973 | Time: 0m 0s\n",
            "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 974 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 975 | Time: 0m 0s\n",
            "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
            "Epoch: 976 | Time: 0m 0s\n",
            "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
            "Epoch: 977 | Time: 0m 0s\n",
            "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 978 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 979 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 980 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 981 | Time: 0m 0s\n",
            "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
            "Epoch: 982 | Time: 0m 0s\n",
            "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
            "Epoch: 983 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
            "Epoch: 984 | Time: 0m 0s\n",
            "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
            "Epoch: 985 | Time: 0m 0s\n",
            "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 986 | Time: 0m 0s\n",
            "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 987 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 988 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
            "Epoch: 989 | Time: 0m 0s\n",
            "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
            "Epoch: 990 | Time: 0m 0s\n",
            "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
            "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
            "Epoch: 991 | Time: 0m 0s\n",
            "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 992 | Time: 0m 0s\n",
            "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
            "Epoch: 993 | Time: 0m 0s\n",
            "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
            "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
            "Epoch: 994 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
            "Epoch: 995 | Time: 0m 0s\n",
            "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 996 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
            "Epoch: 997 | Time: 0m 0s\n",
            "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 998 | Time: 0m 0s\n",
            "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
            "Epoch: 999 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
            "Epoch: 1000 | Time: 0m 0s\n",
            "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "tr-AE-150-18-0.001-C2\n",
            "The model has 3848 trainable parameters\n",
            "Epoch: 01 | Time: 0m 0s\n",
            "\tTrain Loss: 0.550 | Train PPL:   1.734\n",
            "\t Val. Loss: 0.536 |  Val. PPL:   1.709\n",
            "Epoch: 02 | Time: 0m 0s\n",
            "\tTrain Loss: 0.458 | Train PPL:   1.580\n",
            "\t Val. Loss: 0.522 |  Val. PPL:   1.685\n",
            "Epoch: 03 | Time: 0m 0s\n",
            "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
            "\t Val. Loss: 0.504 |  Val. PPL:   1.656\n",
            "Epoch: 04 | Time: 0m 0s\n",
            "\tTrain Loss: 0.436 | Train PPL:   1.547\n",
            "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
            "Epoch: 05 | Time: 0m 0s\n",
            "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
            "\t Val. Loss: 0.474 |  Val. PPL:   1.607\n",
            "Epoch: 06 | Time: 0m 0s\n",
            "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
            "\t Val. Loss: 0.475 |  Val. PPL:   1.609\n",
            "Epoch: 07 | Time: 0m 0s\n",
            "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
            "\t Val. Loss: 0.468 |  Val. PPL:   1.598\n",
            "Epoch: 08 | Time: 0m 0s\n",
            "\tTrain Loss: 0.421 | Train PPL:   1.524\n",
            "\t Val. Loss: 0.459 |  Val. PPL:   1.582\n",
            "Epoch: 09 | Time: 0m 0s\n",
            "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
            "\t Val. Loss: 0.454 |  Val. PPL:   1.574\n",
            "Epoch: 10 | Time: 0m 0s\n",
            "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
            "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
            "Epoch: 11 | Time: 0m 0s\n",
            "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
            "\t Val. Loss: 0.458 |  Val. PPL:   1.581\n",
            "Epoch: 12 | Time: 0m 0s\n",
            "\tTrain Loss: 0.386 | Train PPL:   1.472\n",
            "\t Val. Loss: 0.435 |  Val. PPL:   1.544\n",
            "Epoch: 13 | Time: 0m 0s\n",
            "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
            "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
            "Epoch: 14 | Time: 0m 0s\n",
            "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
            "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
            "Epoch: 15 | Time: 0m 0s\n",
            "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
            "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
            "Epoch: 16 | Time: 0m 0s\n",
            "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
            "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
            "Epoch: 17 | Time: 0m 0s\n",
            "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
            "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
            "Epoch: 18 | Time: 0m 0s\n",
            "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
            "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
            "Epoch: 19 | Time: 0m 0s\n",
            "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
            "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
            "Epoch: 20 | Time: 0m 0s\n",
            "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
            "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
            "Epoch: 21 | Time: 0m 0s\n",
            "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
            "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
            "Epoch: 22 | Time: 0m 0s\n",
            "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
            "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
            "Epoch: 23 | Time: 0m 0s\n",
            "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
            "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
            "Epoch: 24 | Time: 0m 0s\n",
            "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
            "\t Val. Loss: 0.403 |  Val. PPL:   1.496\n",
            "Epoch: 25 | Time: 0m 0s\n",
            "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
            "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
            "Epoch: 26 | Time: 0m 0s\n",
            "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
            "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
            "Epoch: 27 | Time: 0m 0s\n",
            "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
            "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
            "Epoch: 28 | Time: 0m 0s\n",
            "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
            "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
            "Epoch: 29 | Time: 0m 0s\n",
            "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
            "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
            "Epoch: 30 | Time: 0m 0s\n",
            "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
            "\t Val. Loss: 0.392 |  Val. PPL:   1.479\n",
            "Epoch: 31 | Time: 0m 0s\n",
            "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
            "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
            "Epoch: 32 | Time: 0m 0s\n",
            "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
            "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
            "Epoch: 33 | Time: 0m 0s\n",
            "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
            "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
            "Epoch: 34 | Time: 0m 0s\n",
            "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
            "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
            "Epoch: 35 | Time: 0m 0s\n",
            "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
            "\t Val. Loss: 0.386 |  Val. PPL:   1.470\n",
            "Epoch: 36 | Time: 0m 0s\n",
            "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
            "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
            "Epoch: 37 | Time: 0m 0s\n",
            "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
            "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
            "Epoch: 38 | Time: 0m 0s\n",
            "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
            "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
            "Epoch: 39 | Time: 0m 0s\n",
            "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
            "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
            "Epoch: 40 | Time: 0m 0s\n",
            "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
            "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
            "Epoch: 41 | Time: 0m 0s\n",
            "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
            "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
            "Epoch: 42 | Time: 0m 0s\n",
            "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
            "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
            "Epoch: 43 | Time: 0m 0s\n",
            "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
            "\t Val. Loss: 0.376 |  Val. PPL:   1.457\n",
            "Epoch: 44 | Time: 0m 0s\n",
            "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
            "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
            "Epoch: 45 | Time: 0m 0s\n",
            "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
            "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
            "Epoch: 46 | Time: 0m 0s\n",
            "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
            "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
            "Epoch: 47 | Time: 0m 0s\n",
            "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
            "\t Val. Loss: 0.377 |  Val. PPL:   1.457\n",
            "Epoch: 48 | Time: 0m 0s\n",
            "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
            "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
            "Epoch: 49 | Time: 0m 0s\n",
            "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
            "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
            "Epoch: 50 | Time: 0m 0s\n",
            "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
            "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
            "Epoch: 51 | Time: 0m 0s\n",
            "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
            "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
            "Epoch: 52 | Time: 0m 0s\n",
            "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
            "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
            "Epoch: 53 | Time: 0m 0s\n",
            "\tTrain Loss: 0.345 | Train PPL:   1.411\n",
            "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
            "Epoch: 54 | Time: 0m 0s\n",
            "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
            "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
            "Epoch: 55 | Time: 0m 0s\n",
            "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
            "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
            "Epoch: 56 | Time: 0m 0s\n",
            "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
            "Epoch: 57 | Time: 0m 0s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
            "Epoch: 58 | Time: 0m 0s\n",
            "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
            "\t Val. Loss: 0.368 |  Val. PPL:   1.444\n",
            "Epoch: 59 | Time: 0m 0s\n",
            "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
            "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n",
            "Epoch: 60 | Time: 0m 0s\n",
            "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
            "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
            "Epoch: 61 | Time: 0m 0s\n",
            "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
            "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
            "Epoch: 62 | Time: 0m 0s\n",
            "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
            "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
            "Epoch: 63 | Time: 0m 0s\n",
            "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
            "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
            "Epoch: 64 | Time: 0m 0s\n",
            "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
            "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
            "Epoch: 65 | Time: 0m 0s\n",
            "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
            "Epoch: 66 | Time: 0m 0s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.363\n",
            "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
            "Epoch: 67 | Time: 0m 0s\n",
            "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
            "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
            "Epoch: 68 | Time: 0m 0s\n",
            "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
            "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
            "Epoch: 69 | Time: 0m 0s\n",
            "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
            "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
            "Epoch: 70 | Time: 0m 0s\n",
            "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
            "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n",
            "Epoch: 71 | Time: 0m 0s\n",
            "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
            "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
            "Epoch: 72 | Time: 0m 0s\n",
            "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
            "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
            "Epoch: 73 | Time: 0m 0s\n",
            "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
            "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
            "Epoch: 74 | Time: 0m 0s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
            "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
            "Epoch: 75 | Time: 0m 0s\n",
            "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
            "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
            "Epoch: 76 | Time: 0m 0s\n",
            "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
            "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
            "Epoch: 77 | Time: 0m 0s\n",
            "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
            "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
            "Epoch: 78 | Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
            "Epoch: 79 | Time: 0m 0s\n",
            "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
            "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
            "Epoch: 80 | Time: 0m 0s\n",
            "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
            "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
            "Epoch: 81 | Time: 0m 0s\n",
            "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
            "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
            "Epoch: 82 | Time: 0m 0s\n",
            "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
            "Epoch: 83 | Time: 0m 0s\n",
            "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
            "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
            "Epoch: 84 | Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
            "Epoch: 85 | Time: 0m 0s\n",
            "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
            "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
            "Epoch: 86 | Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
            "Epoch: 87 | Time: 0m 0s\n",
            "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
            "Epoch: 88 | Time: 0m 0s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
            "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
            "Epoch: 89 | Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
            "Epoch: 90 | Time: 0m 0s\n",
            "\tTrain Loss: 0.294 | Train PPL:   1.341\n",
            "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
            "Epoch: 91 | Time: 0m 0s\n",
            "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
            "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
            "Epoch: 92 | Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
            "Epoch: 93 | Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
            "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
            "Epoch: 94 | Time: 0m 0s\n",
            "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
            "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
            "Epoch: 95 | Time: 0m 0s\n",
            "\tTrain Loss: 0.294 | Train PPL:   1.341\n",
            "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
            "Epoch: 96 | Time: 0m 0s\n",
            "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
            "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
            "Epoch: 97 | Time: 0m 0s\n",
            "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
            "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
            "Epoch: 98 | Time: 0m 0s\n",
            "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
            "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
            "Epoch: 99 | Time: 0m 0s\n",
            "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
            "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
            "Epoch: 100 | Time: 0m 0s\n",
            "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
            "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
            "Epoch: 101 | Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.332\n",
            "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
            "Epoch: 102 | Time: 0m 0s\n",
            "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
            "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
            "Epoch: 103 | Time: 0m 0s\n",
            "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
            "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
            "Epoch: 104 | Time: 0m 0s\n",
            "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
            "Epoch: 105 | Time: 0m 0s\n",
            "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
            "Epoch: 106 | Time: 0m 0s\n",
            "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
            "Epoch: 107 | Time: 0m 0s\n",
            "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
            "Epoch: 108 | Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
            "Epoch: 109 | Time: 0m 0s\n",
            "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
            "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
            "Epoch: 110 | Time: 0m 0s\n",
            "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
            "\t Val. Loss: 0.350 |  Val. PPL:   1.418\n",
            "Epoch: 111 | Time: 0m 0s\n",
            "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
            "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
            "Epoch: 112 | Time: 0m 0s\n",
            "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
            "Epoch: 113 | Time: 0m 0s\n",
            "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
            "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
            "Epoch: 114 | Time: 0m 0s\n",
            "\tTrain Loss: 0.273 | Train PPL:   1.315\n",
            "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
            "Epoch: 115 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
            "Epoch: 116 | Time: 0m 0s\n",
            "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
            "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
            "Epoch: 117 | Time: 0m 0s\n",
            "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
            "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
            "Epoch: 118 | Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
            "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
            "Epoch: 119 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
            "Epoch: 120 | Time: 0m 0s\n",
            "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 121 | Time: 0m 0s\n",
            "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 122 | Time: 0m 0s\n",
            "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
            "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
            "Epoch: 123 | Time: 0m 0s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
            "Epoch: 124 | Time: 0m 0s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
            "Epoch: 125 | Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
            "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
            "Epoch: 126 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
            "Epoch: 127 | Time: 0m 0s\n",
            "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
            "Epoch: 128 | Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 129 | Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
            "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
            "Epoch: 130 | Time: 0m 0s\n",
            "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
            "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
            "Epoch: 131 | Time: 0m 0s\n",
            "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
            "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
            "Epoch: 132 | Time: 0m 0s\n",
            "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
            "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
            "Epoch: 133 | Time: 0m 0s\n",
            "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
            "Epoch: 134 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
            "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
            "Epoch: 135 | Time: 0m 0s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
            "Epoch: 136 | Time: 0m 0s\n",
            "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
            "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
            "Epoch: 137 | Time: 0m 0s\n",
            "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
            "\t Val. Loss: 0.330 |  Val. PPL:   1.392\n",
            "Epoch: 138 | Time: 0m 0s\n",
            "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
            "Epoch: 139 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
            "Epoch: 140 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
            "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
            "Epoch: 141 | Time: 0m 0s\n",
            "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
            "Epoch: 142 | Time: 0m 0s\n",
            "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 143 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
            "Epoch: 144 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
            "Epoch: 145 | Time: 0m 0s\n",
            "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
            "Epoch: 146 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 147 | Time: 0m 0s\n",
            "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
            "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
            "Epoch: 148 | Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
            "Epoch: 149 | Time: 0m 0s\n",
            "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
            "Epoch: 150 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 151 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
            "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
            "Epoch: 152 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 153 | Time: 0m 0s\n",
            "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 154 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 155 | Time: 0m 0s\n",
            "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
            "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
            "Epoch: 156 | Time: 0m 0s\n",
            "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
            "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
            "Epoch: 157 | Time: 0m 0s\n",
            "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
            "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
            "Epoch: 158 | Time: 0m 0s\n",
            "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 159 | Time: 0m 0s\n",
            "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
            "Epoch: 160 | Time: 0m 0s\n",
            "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
            "Epoch: 161 | Time: 0m 0s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
            "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
            "Epoch: 162 | Time: 0m 0s\n",
            "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
            "Epoch: 163 | Time: 0m 0s\n",
            "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 164 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
            "Epoch: 165 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
            "Epoch: 166 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
            "Epoch: 167 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 168 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
            "Epoch: 169 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 170 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 171 | Time: 0m 0s\n",
            "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
            "Epoch: 172 | Time: 0m 0s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
            "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
            "Epoch: 173 | Time: 0m 0s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
            "Epoch: 174 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 175 | Time: 0m 0s\n",
            "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
            "Epoch: 176 | Time: 0m 0s\n",
            "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
            "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
            "Epoch: 177 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
            "Epoch: 178 | Time: 0m 0s\n",
            "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.378\n",
            "Epoch: 179 | Time: 0m 0s\n",
            "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 180 | Time: 0m 0s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 181 | Time: 0m 0s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 182 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
            "Epoch: 183 | Time: 0m 0s\n",
            "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
            "Epoch: 184 | Time: 0m 0s\n",
            "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 185 | Time: 0m 0s\n",
            "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
            "Epoch: 186 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
            "Epoch: 187 | Time: 0m 0s\n",
            "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 188 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.375\n",
            "Epoch: 189 | Time: 0m 0s\n",
            "\tTrain Loss: 0.289 | Train PPL:   1.336\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 190 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 191 | Time: 0m 0s\n",
            "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 192 | Time: 0m 0s\n",
            "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
            "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
            "Epoch: 193 | Time: 0m 0s\n",
            "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 194 | Time: 0m 0s\n",
            "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 195 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 196 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.385\n",
            "Epoch: 197 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "Epoch: 198 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 199 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
            "Epoch: 200 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
            "Epoch: 201 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
            "Epoch: 202 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 203 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 204 | Time: 0m 0s\n",
            "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 205 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 206 | Time: 0m 0s\n",
            "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
            "Epoch: 207 | Time: 0m 0s\n",
            "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.383\n",
            "Epoch: 208 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 209 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 210 | Time: 0m 0s\n",
            "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.385\n",
            "Epoch: 211 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.371\n",
            "Epoch: 212 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
            "Epoch: 213 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
            "Epoch: 214 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 215 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 216 | Time: 0m 0s\n",
            "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.379\n",
            "Epoch: 217 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 218 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
            "Epoch: 219 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 220 | Time: 0m 0s\n",
            "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
            "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
            "Epoch: 221 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.375\n",
            "Epoch: 222 | Time: 0m 0s\n",
            "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 223 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 224 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
            "Epoch: 225 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
            "Epoch: 226 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 227 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
            "Epoch: 228 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
            "Epoch: 229 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 230 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
            "Epoch: 231 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 232 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.368\n",
            "Epoch: 233 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 234 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 235 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
            "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
            "Epoch: 236 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 237 | Time: 0m 0s\n",
            "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 238 | Time: 0m 0s\n",
            "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 239 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 240 | Time: 0m 0s\n",
            "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
            "Epoch: 241 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.379\n",
            "Epoch: 242 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
            "Epoch: 243 | Time: 0m 0s\n",
            "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
            "Epoch: 244 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
            "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
            "Epoch: 245 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 246 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 247 | Time: 0m 0s\n",
            "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
            "Epoch: 248 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 249 | Time: 0m 0s\n",
            "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
            "Epoch: 250 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.368\n",
            "Epoch: 251 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 252 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
            "Epoch: 253 | Time: 0m 0s\n",
            "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 254 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 255 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 256 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 257 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
            "Epoch: 258 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
            "Epoch: 259 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.367\n",
            "Epoch: 260 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 261 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 262 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
            "Epoch: 263 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 264 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 265 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
            "Epoch: 266 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 267 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 268 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 269 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 270 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 271 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 272 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.371\n",
            "Epoch: 273 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 274 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 275 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
            "Epoch: 276 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 277 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
            "Epoch: 278 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 279 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
            "Epoch: 280 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 281 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
            "Epoch: 282 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 283 | Time: 0m 0s\n",
            "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
            "Epoch: 284 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 285 | Time: 0m 0s\n",
            "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
            "Epoch: 286 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
            "Epoch: 287 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
            "Epoch: 288 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 289 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
            "Epoch: 290 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 291 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 292 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 293 | Time: 0m 0s\n",
            "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 294 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 295 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 296 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 297 | Time: 0m 0s\n",
            "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
            "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
            "Epoch: 298 | Time: 0m 0s\n",
            "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
            "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
            "Epoch: 299 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.371\n",
            "Epoch: 300 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
            "Epoch: 301 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 302 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
            "Epoch: 303 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 304 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
            "Epoch: 305 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
            "Epoch: 306 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
            "Epoch: 307 | Time: 0m 0s\n",
            "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 308 | Time: 0m 0s\n",
            "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 309 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.361\n",
            "Epoch: 310 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 311 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
            "Epoch: 312 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
            "Epoch: 313 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 314 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
            "Epoch: 315 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
            "Epoch: 316 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.363\n",
            "Epoch: 317 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 318 | Time: 0m 0s\n",
            "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 319 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 320 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 321 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 322 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 323 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
            "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
            "Epoch: 324 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
            "Epoch: 325 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 326 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
            "Epoch: 327 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 328 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 329 | Time: 0m 0s\n",
            "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
            "Epoch: 330 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
            "Epoch: 331 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 332 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
            "Epoch: 333 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
            "Epoch: 334 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 335 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.318 |  Val. PPL:   1.375\n",
            "Epoch: 336 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.351\n",
            "Epoch: 337 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
            "\t Val. Loss: 0.293 |  Val. PPL:   1.341\n",
            "Epoch: 338 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 339 | Time: 0m 0s\n",
            "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 340 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
            "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
            "Epoch: 341 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 342 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 343 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 344 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
            "Epoch: 345 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 346 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.289\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 347 | Time: 0m 0s\n",
            "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
            "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
            "Epoch: 348 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 349 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 350 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
            "Epoch: 351 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 352 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
            "Epoch: 353 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 354 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
            "Epoch: 355 | Time: 0m 0s\n",
            "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 356 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
            "Epoch: 357 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 358 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 359 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
            "Epoch: 360 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
            "Epoch: 361 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 362 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
            "Epoch: 363 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
            "Epoch: 364 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 365 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
            "Epoch: 366 | Time: 0m 0s\n",
            "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
            "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
            "Epoch: 367 | Time: 0m 0s\n",
            "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
            "Epoch: 368 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
            "Epoch: 369 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 370 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 371 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
            "Epoch: 372 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 373 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
            "Epoch: 374 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
            "Epoch: 375 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
            "Epoch: 376 | Time: 0m 0s\n",
            "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
            "\t Val. Loss: 0.293 |  Val. PPL:   1.341\n",
            "Epoch: 377 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 378 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\t Val. Loss: 0.293 |  Val. PPL:   1.341\n",
            "Epoch: 379 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
            "Epoch: 380 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
            "Epoch: 381 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
            "Epoch: 382 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
            "Epoch: 383 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
            "Epoch: 384 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
            "Epoch: 385 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
            "Epoch: 386 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 387 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 388 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
            "Epoch: 389 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
            "Epoch: 390 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
            "Epoch: 391 | Time: 0m 0s\n",
            "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 392 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
            "Epoch: 393 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
            "Epoch: 394 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
            "Epoch: 395 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
            "Epoch: 396 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.303 |  Val. PPL:   1.353\n",
            "Epoch: 397 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
            "Epoch: 398 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
            "Epoch: 399 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
            "Epoch: 400 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
            "Epoch: 401 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
            "Epoch: 402 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
            "Epoch: 403 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
            "Epoch: 404 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
            "Epoch: 405 | Time: 0m 0s\n",
            "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
            "Epoch: 406 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 407 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
            "Epoch: 408 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 409 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
            "Epoch: 410 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 411 | Time: 0m 0s\n",
            "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
            "Epoch: 412 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 413 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
            "Epoch: 414 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
            "Epoch: 415 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 416 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
            "Epoch: 417 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
            "Epoch: 418 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
            "Epoch: 419 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
            "Epoch: 420 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
            "Epoch: 421 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 422 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
            "Epoch: 423 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
            "Epoch: 424 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
            "Epoch: 425 | Time: 0m 0s\n",
            "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
            "Epoch: 426 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
            "Epoch: 427 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
            "Epoch: 428 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
            "Epoch: 429 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
            "Epoch: 430 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
            "Epoch: 431 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
            "Epoch: 432 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
            "Epoch: 433 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
            "Epoch: 434 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
            "Epoch: 435 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
            "Epoch: 436 | Time: 0m 0s\n",
            "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
            "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
            "Epoch: 437 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
            "Epoch: 438 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
            "Epoch: 439 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
            "Epoch: 440 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
            "Epoch: 441 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
            "Epoch: 442 | Time: 0m 0s\n",
            "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 443 | Time: 0m 0s\n",
            "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
            "Epoch: 444 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 445 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
            "Epoch: 446 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
            "Epoch: 447 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
            "Epoch: 448 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
            "Epoch: 449 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
            "Epoch: 450 | Time: 0m 0s\n",
            "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
            "Epoch: 451 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
            "Epoch: 452 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
            "Epoch: 453 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
            "Epoch: 454 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
            "Epoch: 455 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
            "Epoch: 456 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
            "Epoch: 457 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
            "Epoch: 458 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
            "Epoch: 459 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
            "Epoch: 460 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
            "Epoch: 461 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
            "Epoch: 462 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 463 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
            "Epoch: 464 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
            "Epoch: 465 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
            "Epoch: 466 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
            "Epoch: 467 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
            "Epoch: 468 | Time: 0m 0s\n",
            "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 469 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
            "Epoch: 470 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
            "Epoch: 471 | Time: 0m 0s\n",
            "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 472 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 473 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
            "Epoch: 474 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 475 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
            "Epoch: 476 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 477 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
            "Epoch: 478 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 479 | Time: 0m 0s\n",
            "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
            "Epoch: 480 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
            "Epoch: 481 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 482 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 483 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 484 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
            "Epoch: 485 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 486 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
            "Epoch: 487 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 488 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 489 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
            "Epoch: 490 | Time: 0m 0s\n",
            "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 491 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
            "Epoch: 492 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 493 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 494 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
            "Epoch: 495 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 496 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
            "Epoch: 497 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 498 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
            "Epoch: 499 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 500 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 501 | Time: 0m 0s\n",
            "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
            "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
            "Epoch: 502 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 503 | Time: 0m 0s\n",
            "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.309\n",
            "Epoch: 504 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
            "Epoch: 505 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 506 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 507 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 508 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
            "Epoch: 509 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 510 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 511 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 512 | Time: 0m 0s\n",
            "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
            "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
            "Epoch: 513 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
            "Epoch: 514 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 515 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 516 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
            "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
            "Epoch: 517 | Time: 0m 0s\n",
            "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 518 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
            "Epoch: 519 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 520 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 521 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
            "Epoch: 522 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
            "Epoch: 523 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 524 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
            "Epoch: 525 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 526 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
            "Epoch: 527 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 528 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
            "Epoch: 529 | Time: 0m 0s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
            "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
            "Epoch: 530 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
            "Epoch: 531 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
            "Epoch: 532 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
            "Epoch: 533 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 534 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 535 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
            "Epoch: 536 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
            "Epoch: 537 | Time: 0m 0s\n",
            "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
            "\t Val. Loss: 0.270 |  Val. PPL:   1.309\n",
            "Epoch: 538 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 539 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
            "Epoch: 540 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 541 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 542 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 543 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
            "Epoch: 544 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 545 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 546 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
            "Epoch: 547 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 548 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 549 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 550 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 551 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
            "Epoch: 552 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 553 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 554 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
            "Epoch: 555 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 556 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
            "Epoch: 557 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 558 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 559 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 560 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 561 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 562 | Time: 0m 0s\n",
            "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 563 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 564 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 565 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 566 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 567 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
            "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
            "Epoch: 568 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
            "Epoch: 569 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 570 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 571 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 572 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 573 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 574 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
            "Epoch: 575 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 576 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 577 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 578 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 579 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 580 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 581 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 582 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
            "Epoch: 583 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 584 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 585 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 586 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
            "Epoch: 587 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 588 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 589 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 590 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 591 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 592 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 593 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 594 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 595 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 596 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
            "Epoch: 597 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
            "Epoch: 598 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.294\n",
            "Epoch: 599 | Time: 0m 0s\n",
            "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
            "Epoch: 600 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
            "Epoch: 601 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 602 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 603 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 604 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 605 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 606 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 607 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
            "Epoch: 608 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 609 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 610 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.289\n",
            "Epoch: 611 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 612 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 613 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
            "Epoch: 614 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 615 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 616 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 617 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 618 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 619 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 620 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 621 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 622 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 623 | Time: 0m 0s\n",
            "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 624 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 625 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 626 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
            "Epoch: 627 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 628 | Time: 0m 0s\n",
            "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 629 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 630 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 631 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 632 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 633 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 634 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 635 | Time: 0m 0s\n",
            "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 636 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
            "Epoch: 637 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
            "Epoch: 638 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 639 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
            "Epoch: 640 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
            "Epoch: 641 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 642 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
            "Epoch: 643 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
            "Epoch: 644 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 645 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
            "Epoch: 646 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 647 | Time: 0m 0s\n",
            "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 648 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 649 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
            "Epoch: 650 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 651 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 652 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 653 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 654 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
            "Epoch: 655 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 656 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 657 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 658 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 659 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 660 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 661 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
            "Epoch: 662 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
            "Epoch: 663 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 664 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 665 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 666 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 667 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 668 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
            "Epoch: 669 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
            "Epoch: 670 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 671 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 672 | Time: 0m 0s\n",
            "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 673 | Time: 0m 0s\n",
            "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 674 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 675 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 676 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 677 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.294\n",
            "Epoch: 678 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 679 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 680 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 681 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
            "Epoch: 682 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 683 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 684 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 685 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
            "Epoch: 686 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 687 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 688 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 689 | Time: 0m 0s\n",
            "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 690 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 691 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 692 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 693 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 694 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 695 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 696 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.260 |  Val. PPL:   1.298\n",
            "Epoch: 697 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 698 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
            "Epoch: 699 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
            "Epoch: 700 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 701 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 702 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
            "Epoch: 703 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 704 | Time: 0m 0s\n",
            "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 705 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 706 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 707 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 708 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 709 | Time: 0m 0s\n",
            "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 710 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 711 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
            "Epoch: 712 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
            "Epoch: 713 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 714 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 715 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 716 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 717 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 718 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 719 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
            "Epoch: 720 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 721 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
            "Epoch: 722 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 723 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.294\n",
            "Epoch: 724 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 725 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 726 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 727 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 728 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 729 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 730 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
            "Epoch: 731 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 732 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
            "Epoch: 733 | Time: 0m 0s\n",
            "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 734 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 735 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 736 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
            "Epoch: 737 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 738 | Time: 0m 0s\n",
            "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
            "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
            "Epoch: 739 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 740 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 741 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 742 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 743 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 744 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 745 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
            "Epoch: 746 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 747 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 748 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 749 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 750 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 751 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 752 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 753 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 754 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 755 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 756 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 757 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 758 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 759 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
            "Epoch: 760 | Time: 0m 0s\n",
            "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
            "Epoch: 761 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
            "Epoch: 762 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 763 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
            "Epoch: 764 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 765 | Time: 0m 0s\n",
            "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 766 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
            "Epoch: 767 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 768 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
            "Epoch: 769 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 770 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
            "Epoch: 771 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
            "Epoch: 772 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 773 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
            "Epoch: 774 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
            "Epoch: 775 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
            "Epoch: 776 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 777 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
            "Epoch: 778 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
            "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
            "Epoch: 779 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 780 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 781 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
            "Epoch: 782 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 783 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 784 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 785 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 786 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 787 | Time: 0m 0s\n",
            "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
            "Epoch: 788 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 789 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 790 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 791 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 792 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 793 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
            "Epoch: 794 | Time: 0m 0s\n",
            "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
            "Epoch: 795 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 796 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 797 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 798 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
            "Epoch: 799 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 800 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 801 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
            "Epoch: 802 | Time: 0m 0s\n",
            "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 803 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 804 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 805 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 806 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
            "Epoch: 807 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
            "Epoch: 808 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 809 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 810 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
            "Epoch: 811 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
            "Epoch: 812 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
            "Epoch: 813 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 814 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 815 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 816 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
            "Epoch: 817 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 818 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 819 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 820 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 821 | Time: 0m 0s\n",
            "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 822 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 823 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 824 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 825 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
            "Epoch: 826 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 827 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 828 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 829 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
            "Epoch: 830 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
            "Epoch: 831 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 832 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 833 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 834 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 835 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
            "Epoch: 836 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
            "Epoch: 837 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 838 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 839 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 840 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
            "Epoch: 841 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 842 | Time: 0m 0s\n",
            "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 843 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
            "Epoch: 844 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
            "Epoch: 845 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 846 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 847 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 848 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 849 | Time: 0m 0s\n",
            "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 850 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 851 | Time: 0m 0s\n",
            "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
            "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
            "Epoch: 852 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 853 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 854 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 855 | Time: 0m 0s\n",
            "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
            "Epoch: 856 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
            "Epoch: 857 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 858 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 859 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 860 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 861 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 862 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 863 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
            "Epoch: 864 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 865 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
            "Epoch: 866 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 867 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
            "Epoch: 868 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 869 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 870 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 871 | Time: 0m 0s\n",
            "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
            "Epoch: 872 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
            "Epoch: 873 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
            "Epoch: 874 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 875 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 876 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
            "Epoch: 877 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 878 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 879 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 880 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 881 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 882 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
            "Epoch: 883 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 884 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 885 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
            "Epoch: 886 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 887 | Time: 0m 0s\n",
            "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 888 | Time: 0m 0s\n",
            "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 889 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 890 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 891 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 892 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 893 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 894 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 895 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 896 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 897 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 898 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
            "Epoch: 899 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 900 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 901 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.254\n",
            "Epoch: 902 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 903 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
            "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
            "Epoch: 904 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 905 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
            "Epoch: 906 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
            "Epoch: 907 | Time: 0m 0s\n",
            "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 908 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 909 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 910 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
            "Epoch: 911 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
            "Epoch: 912 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
            "Epoch: 913 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 914 | Time: 0m 0s\n",
            "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
            "Epoch: 915 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 916 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 917 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
            "Epoch: 918 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
            "Epoch: 919 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
            "Epoch: 920 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
            "Epoch: 921 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
            "Epoch: 922 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
            "Epoch: 923 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
            "Epoch: 924 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 925 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
            "Epoch: 926 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 927 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
            "Epoch: 928 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
            "Epoch: 929 | Time: 0m 0s\n",
            "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 930 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 931 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
            "Epoch: 932 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
            "Epoch: 933 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
            "Epoch: 934 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 935 | Time: 0m 0s\n",
            "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 936 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
            "Epoch: 937 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 938 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
            "Epoch: 939 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
            "Epoch: 940 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
            "Epoch: 941 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
            "Epoch: 942 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 943 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 944 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
            "Epoch: 945 | Time: 0m 0s\n",
            "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.254\n",
            "Epoch: 946 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 947 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
            "Epoch: 948 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 949 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
            "Epoch: 950 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 951 | Time: 0m 0s\n",
            "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
            "Epoch: 952 | Time: 0m 0s\n",
            "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
            "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
            "Epoch: 953 | Time: 0m 0s\n",
            "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 954 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
            "Epoch: 955 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 956 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
            "Epoch: 957 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
            "Epoch: 958 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
            "Epoch: 959 | Time: 0m 0s\n",
            "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 960 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 961 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
            "Epoch: 962 | Time: 0m 0s\n",
            "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 963 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
            "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
            "Epoch: 964 | Time: 0m 0s\n",
            "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
            "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
            "Epoch: 965 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
            "Epoch: 966 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
            "Epoch: 967 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 968 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 969 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.249\n",
            "Epoch: 970 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
            "Epoch: 971 | Time: 0m 0s\n",
            "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
            "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
            "Epoch: 972 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
            "Epoch: 973 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
            "Epoch: 974 | Time: 0m 0s\n",
            "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
            "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
            "Epoch: 975 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
            "Epoch: 976 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
            "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
            "Epoch: 977 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
            "Epoch: 978 | Time: 0m 0s\n",
            "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
            "Epoch: 979 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
            "Epoch: 980 | Time: 0m 0s\n",
            "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.249\n",
            "Epoch: 981 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
            "Epoch: 982 | Time: 0m 0s\n",
            "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.254\n",
            "Epoch: 983 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
            "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
            "Epoch: 984 | Time: 0m 0s\n",
            "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
            "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
            "Epoch: 985 | Time: 0m 0s\n",
            "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
            "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
            "Epoch: 986 | Time: 0m 0s\n",
            "\tTrain Loss: 0.189 | Train PPL:   1.207\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 987 | Time: 0m 0s\n",
            "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
            "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
            "Epoch: 988 | Time: 0m 0s\n",
            "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
            "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
            "Epoch: 989 | Time: 0m 0s\n",
            "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
            "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
            "Epoch: 990 | Time: 0m 0s\n",
            "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
            "\t Val. Loss: 0.232 |  Val. PPL:   1.260\n",
            "Epoch: 991 | Time: 0m 0s\n",
            "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
            "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
            "Epoch: 992 | Time: 0m 0s\n",
            "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 993 | Time: 0m 0s\n",
            "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
            "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
            "Epoch: 994 | Time: 0m 0s\n",
            "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
            "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
            "Epoch: 995 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 996 | Time: 0m 0s\n",
            "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
            "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
            "Epoch: 997 | Time: 0m 0s\n",
            "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
            "Epoch: 998 | Time: 0m 0s\n",
            "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
            "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
            "Epoch: 999 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
            "Epoch: 1000 | Time: 0m 0s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
            "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x_07dWu8vnWa",
        "outputId": "e12121e3-1816-46d9-b106-0cf1d6b1ee85"
      },
      "source": [
        "hist_loss_C = torch.cat(hist_losses_C, dim=2)\n",
        "hist_hits_C = torch.cat(hist_hitsss_C, dim=2)\n",
        "\n",
        "plotResults(hist_loss_C, hist_hits_C)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXwUVdb+n0MgLAJBVpElrLIIQRBQBNxlQEdwcHnhFYQRBRfG1/m5jIy7jjPqjDruggoKyOKK6KCI466ABoWwGUU2WWQVZBMScn9/PF1WJySkSbrT1Z3n+/n0p7bb1femKvXUOffcc805ByGEECJoVIh3BYQQQojCkEAJIYQIJBIoIYQQgUQCJYQQIpBIoIQQQgQSCZQQQohAIoESIkLM7B0zGxbD8y81s9NjdX4hEg3TOCiRzJjZ7rDNagD2AzgY2h7lnHupjOqxGsAVzrn3w/YND+3rVUj5uwC0cs4NKYv6CRFEKsa7AkLEEudcdW+9MJEIO1bROZdblnUTQhweufhEucTMTjezdWb2FzP7CcAEMzvazN42sy1m9nNovXHYdz4ysytC68PN7DMz+1eo7Coz61fKOq02s7PNrC+AvwL4HzPbbWaLwn5zpZntCv3epaX5PSGCTtxcfHXr1nXNmjUr1Tm2bdsGAKhTp04UaiSCQqyu6+LFi5Geno6aNWti165d+O6779CgQQMce+yxAIC8vDzs2rULaWlpcM5h9erVcM6hVatWAIDs7GzUqVMHdevWxdatW7FmzRo0bdr0t+2NGzeiY8eOMLPD/rbH1q1bsXXrVrRt2/aQMhs2bMD+/fvRvHlzAMDBgweRlZWFdu3aoUqVKsjJyUFubi6qVq0a1b9RrND/qjgcCxYs2Oqcq3fIAedcXD4nnniiKy0TJkxwEyZMKPV5RLCI1XVNT093c+bMcc459+GHH7pKlSq5ffv2FVn+m2++cbVq1fpt+7TTTnPPPvvsb3Vs2bLlb8f27NnjALiNGzcW+dtHHXWUS0tL++1TtWpV17Nnz0Lrd+edd7pLL730t2O7d+92aWlp7tVXX3V79+4tQevji/5XxeEAkOkK0Qm5+ES5pV69eqhSpcpv23v37sWoUaN+s2JOPfVU7NixAwcPHiz0+8ccc8xv69WqVQMA7N69u9CyADBjxgzs2LHjt89TTz0VcV2POuooTJ8+Hc888wwaNmyI8847D99++23E3xciEZFAiXJLQVfcQw89hOzsbMyfPx+//PILPvnkEwD0MsS7bgDwu9/9DnPmzMHGjRvRtm1bXHnllWVeLyHKkmIFyszGm9lmM1tSxHEzs8fMbIWZZZlZl+hXU4jYs2vXLlStWhW1atXC9u3bcffdd8etLg0aNMDq1auRl5cHANi0aRPefPNN7NmzB5UrV0b16tVRoYLeL0VyE8kd/gKAvoc53g9A69BnJICnS18tIcqe66+/Hvv27UPdunVx8skno2/fw932seXiiy8GwKCCLl26IC8vDw8//DCOPfZY1K5dGx9//DGeflr/aiK5iSiKz8yaAXjbOdehkGNjAXzknJsa2s4GcLpzbuPhztm1a1eXmZlZkjoDAHbtAsaMeSF0ruEYPrzEpxIBYssW4L77XkBuLvDZZ8PjXR0RJXr1egEAr2nFisBjjwGnnBLfOongYGYLnHNdC+6PxkDdRgB+DNteF9p3iECZ2UjQykLTpk1L9aMVKgBVqgC7dwP33AMMGwYU4rYXCcayZcDOnUDNmkApRyGIAOHFojRrBrz1FvCf/0igRPGUaSYJ59w4AOMAWlClOddRRwEdOgA//QSsWgXMnasbPhnIyeGyRQvg0UfjWxcRPV54gct//YsitWZNPGsjEoVoCNR6AE3CthuH9pUJ9eoBVasCN9/sC1TlysANNwC1apVVLUS08ARK1nDykp4OrF3L9RUr+HI5dCi3n30W+P574Pzzgd6941dHEQyiIVAzAYw2s2kATgKws7j+p2iSkgKMGgWMHQt8/TXgHPDrr0DTpoCicBOP3FA2PAlU8tK0KRCK4Mdf/gK8/jpfLvPygJEjuX/mTGD5ct0H5Z1IwsynApgLoE0od9kIM7vKzK4KFZkFYCWAFQCeBXBNzGpbBI88Auzdy8+ePUD16kBWVlnXQkQDz4JSBHXykp4OrF/PgJi33+a+l17ixwy4804gOxtYsCC+9RTxp1gLyjk3uJjjDsC1UatRKalQAejYkQL1009AaipQu3a8ayUiRS6+5Cc9HTh4kH2MBw4ATZoAkybRgjrjDOD664F//AN4+GHgj39kf3OPHsCPPwJHH817Y948/l930ajLpCYp31M7daJAnXoqcE2Z23OiNMjFl/ykp3P5xBNA27bAvfeyL2rlSmD4cPYdX3ABMHUq0KcP0LMnI/86dwZuvRW44w7gnHOAE08ElhSaPkAkC0kpUBkZwI4d7GxdvjzetRFHgiyo5McbYbJzJzBkCAMkvvwS+Oor4NLQBCLPPQd89hnw6ae0mq66Cti+nWUyMylsKSnA5Mnxa4eIPUkrUB4KZ00sZEElP+FDIC+9lG75bt2Arl39vscaNWg59eoFXHIJsDEUdrV4Mb0jZ5wB9O3LfqtQNiiRhCSlQHXsyAdcxYp8S9u5M941EpEiCyr5qVYNaNCAYeSRDMYeEpr0vlEjBkHt3MmX0CFDgHXraEmZ0fU3fTpwwgnA5s1A8+a0wgD2Z/XpE7MmiRiRlFO+16wJvPoqB/DeeCPHXHTsGO9aiUiQQJUPJk+m4ERCz54c6HvMMbSaAApUt24c+LtrFzBxIkPXt2wBFi2iZbV6NTB7Nq2wt94CPvoI+OUXPh9EYpCUAgUAAwcy0gegm08ClRh4Lj6FmSc3Z58deVkzpjLbu5f3RV4es8hUqsQB+QD7m+fOZcQf4PdNZWVxbKQ37GTJEmWcSSSS+jHgRQutWUMf9rBh7GgVwUUWlCiKatWA1q3puitoBaWnMwx91Spuf/01l1lZwIYN/v+9xkcmFklrQQH0c6em0sU3dizdACecAPz5z/GumSgKCZQ4HDfc4FvZ4TRtyv0FJxlevdrPWgHQ/ScSh6S2oCpU4CDA1at9k19hqcFGUXzicFx5JXD11Yfu97wl4bMHnXACly+9xGVGhiyoRCOpLSiAN+6777JztFs3jqNYtgxo3z7eNROF4VlQQhwJnkABDFfPzGSU38KFwKxZPN67N4MtHnyQEb6XXcYowPfe4/dq1wYuv1z9n0Ei6QWqRw/ggw+A+vWZTqVtW+CddyRQQSU3Vw8IceSEj626/HJ6TS65hNnRs7MZlHHmmcBTTzFBLcCgi/fe42Dg8PMoHD04JP2j4G9/49iJ9euBNm2Ahg1l5geZnBy598SRU726n3PzwgsZbt6kCbB0Kf//n32Wkb1eQukWLdgftWgRM6j//DOQlqYugKCR9BYUwOgfj6L80Js2MagC4E2cm6vxEvFAAiVKSno6sG8f54jzSEnJ///vzezbqRMwZw5d/126MP/fxRcz/9+f/kQXoHdOT/jWrPGjAVu21POhLEh6C6ognTqxDyq8r2PePFpW3qjzyy4D+vWLT/3KO7m5EihRMtq2BY47LrL7JyPDzzDjpUa77DJaV927U7S6dGFKJYAvsK1a+fvPOy82bRD5KRcWVDgZGUzxn53NwX4AMGECo38WL6YbcMYMvnU5p4dlWZOTw7deIY6Uxx6jBRUJ4fk6vedA797MNrFjB7dfegl4800ODP76a748Pfggx1o9/TQHB7duHdUmiAKUOwvKuzE9N9/+/cDLL3N97Vrm8jp4kOlTlMOv7JGLT5SUunXZ7xQJ3nOgRQsmpvU47TRgwAB+Tj+dL7ObNvnPiyuvBP76V96jXvi6iB3lTqDatGGKlCuv5A3doAHfmMzoY5461X9ArlkDbN1Kk37uXGD0aODuu+Nb/2RHUXyiLGjRgmmRwi2pgoRnolm0iBF+tWoBjRvT9Td1auHfmzGD81Upy3rpKXcuvtRUmufffOPvq1ePoeg//MBxE6edRlN/zRrg449Z9rbbuO+EEzgltYgNsqBEWVChAsdEtWxZdBkvdH3NGlpQnTr5x/r0AW65hdF/Rx+d/3uvvQa8/z5D3Vu0iHbNyxflTqAAYMSIQ/etXs1USHl5QP/+vkBNmsTjH3zApeaXii0KkhBlxUUXHf64Z0F99x1TKF1wgX/ME6vFizlzdzieOzArSwJVWuRMCdG0qW+Sn3UWw1Hfe48j0sNv5G3beOPdcQf7qkR0kQUlgkLNmnTpzZ7N//Vwd2DBvmyPAwf8Wby/+oqDgtetK5v6FkZuLvvMNm+OXx1KQ7m0oArDe1uqVInhqk2bAm+/TVfAo49yXERODs33u+8GXn+db05HMm2AKB4JlAgS6enA559z/aST/P0NGwJ16hyafDY72x/C8uSTDLQ6eJDzVsWDb74B/vEPtmPUqPjUoTTIggrhCVS7duyn8rbPPhs49lh2iHpZ0GfP5lKjzqOPXHwiSHjPgV698uf7Myt80L+33aaNHwU8ZUr8vC1el0Sidk1IoEJ4N59nunvb3nTT4fv27OHytdeYdUJED1lQIkh4//NDhx56LCODfVBTpvC+XbCAQ1ZSU5mVAmDOz40b/T7sgixaBPz0E7sOMjO5b86cQ6cUycnh74wfz3Gbmzb5x/LygFdeYR96waExnjCtXXtk7Q4KEQmUmfU1s2wzW2FmtxRyfLiZbTGzhaHPFdGvamxp0oTRfKefzu0uXZhgNrxjtGFDPwXKaacBu3cz/FxEj5wchZmL4NClC1MdFRZQ0asXBwZfeimFo08fYOZMugLPPptCNWlS0Tn+nGO5v/wFuPdePlO+/prnefbZ/GUnTuTvjBjBZLjhkcSffMLEuMOGHfq9pLegzCwFwJMA+gFoD2CwmRWWC3y6c+6E0Oe5KNcz5lSuzM7Myy/n9lVX8a0jfBBfSoo/EHDAAC43bizbeiY7cvGJIDFsGGfk9fLxhXPRRTzWujVFZvt2Dt597z2Kzc6dFLiLL6a3xfO8eKxbx3GW33xDYdq71+/v8qKHPSZP5u+sWQP07MnhMB7hQ2YKBmR4llPSChSA7gBWOOdWOucOAJgGYEBsqxUfUlP9h6MZRasg3tiIc87hctMmPlQ1KC86yMUngkRRzwGPhg3ZDbBzJz0ul1ziJ6T1lkOGUJwmTvS7BPbt8/urli/3gy28qT/mzuW4TIAi89FHdDM2bQqceCJdi94zJyuLCQdateLzKC+P0YSAL0zr15d8rrWDB32Xo3fesnrmRSJQjQD8GLa9LrSvIBeaWZaZvWpmESYcSTxat6Yr8PjjKWibNgEnnwzcdVe8a5YcyIISicall3I5eLDfBRBO794Ulmuu4fipL76g22/cOB7PzWVWdYDuOu9F2UulNGVK/t/JyKDQrVzJ7aws7mvQgM+jq67i3FcABapaNYrJhg0la9/119MV+eWX9CitWsXnX1lEJkbL2/8WgGbOuQwAcwC8WFghMxtpZplmlrlly5Yo/XTZcu+97MQ04w2xfj1N7GXL4l2z5EAWlEg0WraksNxzT+HHK1TgkJU//xlYsYLzT+XksL8qNTV/2U2b+BJ8xhl08znHZc+e/qBfb5BwVhbFbelS7vME6uuv6SqcO5eZLnr0YPmSuvk++ogBHAsW0IL67DMOXvaCOmJJJAK1HkC4RdQ4tO83nHPbnHP7Q5vPATixsBM558Y557o657rWC5+0JYE45hj/BmnQgDdDXp4/T4woHRIokYj07n34+aE6duTkqdWrU1C8e9wLpgD8eavS0+kWXLGCVtayZfmjidu3p+hlZTGj+v79+S0oT4juu49LL9NFSQRq/35m0dizx+/r8tyQZdGvFYlAfQWgtZk1N7NUAIMAzAwvYGYNwzb7A1gevSoGl2OO4cA8gGGiovTIxSeSlWrVONsv4E8736UL3WXNm/tTdzRtytl/q1QBrruOyQO8sHXvPK1bU6C8fixPoLZtY+AFAPznP1z26sXlDTcwCYEXCBbOQw8VnmN0+XK//+mTT/Ivy0Kgis0k4ZzLNbPRAGYDSAEw3jm31MzuAZDpnJsJ4Doz6w8gF8B2AMNjWOfA0KABTXBAFlS0UJi5SGb++legUSP2WVeqxEkSu3dnn9KUKQyWSE9nH9W//w18+CEFpk6d/OfJyKD3pl079nu1bevPCA7QnfjTTzxPz550Py5dyoCLCROY6Pa44/zy48YxH+n11+dPfhs+ENl7GfeWmzYBv/7qB4PEgohSHTnnZgGYVWDfHWHrYwCMiW7Vgk/4DSELKjrIxSeSmeOO811vXp9Vq1ZceiHm3uDgUaOKTk+UkcHBuZ9/TnGqXDn/82jgQN9yAoDbb+dy/XoOlXnpJX/qoL176Sp0Dnj1VU5F5JGVxRfGoiL2fvwxtpM2KhdfKQi/Ifbt4+eXXzjrphkwZsyhbz7i8MjFJ8ornjCFp1QqCi/jzSefMHoQyP88KuocjRoxGfbkybTizGhZOUchevDB/FbTrFmcYmj58sJnK16zJrYCJWdKKQi/IQBGzDz6KPDww/TpPv10fOqVyMiCEuWVM88EunXzp6A/HJ5AOeeve8+jihWZP7QohgxhiPq8edz2BOnmm/kMmzLF/+zYwQHJnuB582d5EYWx7oeSQJUC74aoVInLrVtpOvfty5Hkkyf7fVQiMmRBifJK584ca3S4aECP9HS/XHhUMcAZf1NSiv7uwIFA1ap++qWsLM4ufN99fIZt25b/M2aML1C9e3N5yim0uCRQAca7Ibw3njffZCfk0KF8S8nO9scK7Nvnj5VauPDQZJCCyIISoni8bOqAv6xenRF+XraboqhRgzlGp03jM+vTTxkGf7jgJO+cnkC1bEkrTQIVYBo35pvHGWdwe/x4RrQMGECzuGJF3gAA8NhjfNN55x2+KT36aPzqHVTy8vhRFJ8QxdOzJy2bhmGDfNq2zT81fVEMH87I4wsu4Pim7t0PX75TJ07e6I3b6tSJohXrLOkKkigFNWowNHPPHvY7rV7NC33UUTzetq2fYyszk1bT8OHcnjiR4xKEj6xKISLn7rs5nirc4/Dhh4dmpyiMPn0Y+LB3L7/fvrD032GMGgUMGsSgrx9/ZLq3447zn3WxQgJVSurWpT/Xo+C00J99xnWvI3LzZpb3BtmFly/veMksZUEJUTyVKx+ayDaS/iuPtm0jL1uxoh+RXL8+l8cfH/n3S4oeBVGgWjX/RikoUGvXMknj99/7ftx//YsX/JVXyr6uQcYTKPVBCSEAWVBRwYzzxWzcmN//661Pncpovgcf5EN48GD2SX37bXzqG1Q8F58ESggBSKCiRp06FKiOHf19njXlTT7WrZs/fiA9PXEnEYsVsqCEEOHIxRclatdmCpHwPFYNG1K4Fi1iCGizZv6xsoiASTRkQQkhwpEFFSWuvtqfdMzDjEkZ332Xo8TDO//T08sm2WIiIQtKCBGOBCpKDBpU+P4bb+SnIN7I7LVr82cVLs9IoIQQ4cjFFyc8gfrvfznbZk4O8NxztKjKK56LT2HmQghAFlTc8ELOr7uOEX7/+hfncNm/H7j22vjWLV7IghJChKN31TjRqBEthdxc4OBB9lUBfgLH8ogESggRjgQqTlSqRJECGO23fz+X8+YBM2cCu3bFt37xQFF8QohwJFBxpF07jo266SZaU5MmMcPEgAHAiBHxrl3ZIwtKCBGO+qDiiOfOq12bc0i1awd8/TVw//2cevnnn/OPq0p2ZEEJIcKRBRVH6tXjJyWF4gQwE8UNNwAHDpS/XH1KFiuECEePggDSuTMF65prgBNO8B/cyY5cfEKIcCRQAcQMGDeOSWUXLQLeey/eNSob5OITQoQjgQoovXpxht46dfxks8mOLCghRDgSqABTqRJTKM2YAVxyCbBqFffPns0ZfJMNCZQQIpyIBMrM+ppZtpmtMLNbCjle2cymh47PN7Nm0a5oeeXaazmv1BtvAI8/zn033cT8fhs2xLdu0UYuPiFEOMUKlJmlAHgSQD8A7QEMNrOCM9iPAPCzc64VgEcAPBDtipZX2rUD5s8Hzj8fmDKFYeiLFzM90pQp8a5ddJEFJYQIJ5JxUN0BrHDOrQQAM5sGYACAZWFlBgC4K7T+KoAnzMyccy6KdS3XDBlCK2rECA7mbd2agRRVq8a7ZtHjs8+4VJi5EAIArDgNMbOLAPR1zl0R2h4K4CTn3OiwMktCZdaFtn8Ildla4FwjAYwMbbYBkB2FNtQFsLXYUsmB2pp8lJd2AmprshKNtqY75+oV3FmmmSScc+MAjIvmOc0s0znXNZrnDCpqa/JRXtoJqK3JSizbGokzZT2AJmHbjUP7Ci1jZhUBpAHYFo0KCiGEKJ9EIlBfAWhtZs3NLBXAIAAzC5SZCWBYaP0iAB+o/0kIIURpKNbF55zLNbPRAGYDSAEw3jm31MzuAZDpnJsJ4HkAk8xsBYDtoIiVFVF1GQYctTX5KC/tBNTWZCVmbS02SEIIIYSIBwroFUIIEUgkUEIIIQKJBEoIIUQgkUAJIYQIJBIoIYQQgUQCJYQQIpBIoIQQQgQSCZQQQohAIoESQggRSCRQQgghAokESgghRCAp0/mgwqlbt65r1qxZqc6xbRtn9KhTp04UaiSCgq5r8qFrKg7HggULtsZ9wsJwmjVrhszMzFKd44UXXgAADB8+vPQVEoFB1zX50DUVh8PM1hS2Xy4+IYQQgUQCJYQQIpAUK1BmNt7MNpvZkiKOm5k9ZmYrzCzLzLpEv5pCCCHKG5H0Qb0A4AkAE4s43g9A69DnJABPh5ZClJq8PGD79qKPp6UBlSqV/NwVQq9oubnAjh1AzZpAamrJzicOT24usHWrv12nDmCWv0z4NRGi2FvBOfcJOI17UQwAMNGReQBqmVnDaFVQlC9++AH49FNg1y5uX3EFUK9e0Z+uXYGSTAr9wQdAjRrAjz9y+9xzeb5OnfiQFNFl61bg88/zX7u//CV/mX37gGbNgGeeiUsVRQCJRhRfIwA/hm2vC+3bWLCgmY0EMBIAmjZtGoWfFsnG2rUUiA0buP3xx0D37sDQoYeWzcoCnn0W+PJL4KQjtNmfeQbYuxdYvBjIyQHmzAFOOAFYuJACedpppW+L8Nm3j8t//QuoXBl4+mle23DefpsvDAsXln39RDAp0zBz59w4AOMAoGvXriV47xXJTm4ul1u28LNyJXD55cDo0YeW3bkTmDgRmDz5yARq505g5kyur1kDLFjA9alTaZFNniyBijaeVXr99UBKCrBiBV8uDh7kNsC/OwBs2hSfOorgEQ1v73oATcK2G4f2CXHE5ORwefAgcP/9XM/IKLxsWhrQvz/w/PNA585At260pgDgjTeAUaOAzZuBU0+l8Hhv7K+/Duzfz/U1a4CXXgJOPx1o2xYYOBCYNAno2dN3/z3+OM/vfU4+Gfj225g0P2k5eJB9S54YZWTQgn3ySWD4cLoAZ83iMQmU8IiGBTUTwGgzmwYGR+x0zh3i3hMiEjyBAoAnnuCyKIECgDFjaHUdPAh8+CHwyCO0hO6+G1i0CNizhy67ypWBl1+mZfTFF+ygT0sDvvkGyM4GRozg+W6+md954w3gued4nnHjgJ9/Bk48kWVmz+aD9fHHY/M3SEbCLSXAv6Y33QQcOMBrmJsLdOgggRI+kYSZTwUwF0AbM1tnZiPM7CozuypUZBaAlQBWAHgWwDUxq61IejwX39FH88GVlgYcrruyc2daRG++CQwZAsyYQQFatIjHX3qJFk/37v6+RYsYDJGeDnz0Efd16sRlhw7Aa68BZ55Jl9P+/bSWhg3jb7z5Jq22adPyi6k4PAUF6vjjaVEdOMDtl14COnYEzj5bAiV8IoniG+yca+icq+Sca+yce94594xz7pnQceecu9Y519I519E5V7r8RaJc4z30jzmGy4yMQ0ORi2LIEODXX7msWBHo1cvfn5HBoIrcXGDJEl+gvAekJ1Dh51q5Epgwgd8Jt+KGDKFLasQIRqYVZMYM4LvvIm9zeaCgQFWtChx3HK+t1983ZAjQoAEt2Llz/X7CgmRn0xoWyY9GHIhA4VlQNWoAv/89cMEFkX+3Rw+K0k8/0eK56y72Kw0aRIHZtYvh5fv2cTs9nd+rX58PxnAGDuRD9K67uB0uUH378m1/yhRg5Mj8Ye55ecDgwcCDDx5py5Mbrw8qnAsvZHTm3/8OtG7Nde86jBwJXHIJA1oK8o9/8G/sRXqK5CVuyWKFKAzPgjID3nrryL5rxv6mcJYv59ITGC9SLCPDjywrrI+rZk1gwAC68ipX5gPUIzWV1tgzzwBXX82w6M6deWzLFlpxawpNfVl+KWhBAcDf/uavexanJ1BLQnlrXnuNUZzhZGXx2k2dCtxwQ2zqK4KBLCgRKMIFKpp06MDlq6/yTb59e9+CKioIY8gQLo8/ni7Dglx8MbNYPPQQx1HNm+cL05o1dFWtW8ftb78t2YDiZKEwgSqMgpbsuHF093l/u9xcYOlSrk+YwBeSvDz+vefM4Ti6gmzYwGM//MDtZcu4XLmS+9ev5zm8l5mVK/mSAXD5wQesA8Dw+DlzaKV7LF9evq9tLJFAiUDhufiine6menUKjefeq1IFaNOGD82TTy78O336AI0aFT3Gqk4dBky89BLL9uhBAQT4oLztNn534UKgXTsGc5RX8vKOXKB69gTmzwdOOYWiANDSOnCAx5Yu5RCCadOA3r15DU47LX8mEOfoku3Th9dixgzeB6+/zuvVpw8DM8aP50vLnDk8fttt/P4TTwBnncU6vPUWhzL06UMXI8DIUe97IvpIoESgiJUFBQDvvw989hnw7rvcbtyYb8sXXVR4+UqVgK+/Bv75z6LPOWECz/nmm9x+7TUu9+9nJ/+GDXzIAkBoSqRySaQWVP36/vqkSYzIPPpoDsgG6N4DGOI/fz4jPG+8kWPW+vcHVq/O7+ZduJDZQvr3B7ZtA64JxRiPHs0xcv3707q94w7uHzaMVtPkyXxZyszkS0qtWhxXt2MHrfEFCyiE3jX96qvS/HVEUUigRKCIpUAdcwzfvMPf0ps2Pfxv1a8PHHVU0cdr1OA5zz+fD7GVK/1j3rr3wOhZOn0AACAASURBVHz3XfZRlUciFajUVKB2bYpSs2a0ci65hOPSdu+mQFWqRCune3fg0kuBjRvZZzhhAi3lSZP8802ezHM+9xyv+8aN7FPcuJH5AJ9/3t8OX27axBearCwO8r74Yh5r3Bi49loG3Cxb5lvFnnCK6CKBEoHCc/HFQqBiiZnfl1UwG/qnn7IPKzfXf6A9/zzfxFet4oO4sHD1ZMG5yAUKABo2ZNi/dw8MGcKsEzVqMIKvXTv/b+z1E158MYVt4ED+bc34efhh4LzzKEaeW+6hh7gcNAioW5cvF4BvKd96K182nnuOIe0ZGf7vXHopczYCDPLYvRs49lgK1O9+B/z1rzzmDQ5PTZX7rzQoik8EilhaULEmIwP45BP2dYS7mdau5UNt/Xq6gkaNAh59lH0oQ4f6Hfw9e8av7rHESxQbab/i2LG0hDx69mRyWS8w4ayz/GPt2zOa79RTuX3vvUDLln4/lBlFBaDwdO7Mv3nNmhQUgMI0eDCjNqtWpXCtW8cADYDXtXdvWmj9+1N0zIDp04EmTZiq6d576SqcP5/uwrFjgWrVaO2NHQucc84R/9kEJFAiYCSqBQX4FlTHjnyj/uUXWk45OYwYrFOH+xctYr8I4FtOXpaLZGTPHi4jtaAKCrUZcNVVhZcFKCgeTZv6/UkFqVsXuOwyrodnx2/WjB+A07sAtJjCBcqMQuTRsiUj+sItKoDjtqZPp0ty6FAK3lNPMVXW0UcX3QZROHLxiUCRk5OY4gT4AtW0KT8tW/qh7OnpPL5kCfDiixQuz21UpQqF64knGAywaBEfyJ5Ye3z0Ea2Aw3HgAAe5BimTxe7dXEYqUEGgZ0+KVtWqvI4F8a710KF8IQEYLXjMMcCf/kSX5JAh/Bw4QKvvnHMYHbhlC4MxiuuPfOKJ8h35CciCEgEjkQWqc2c+ePr359tyXh7DzlesoEDVrk1319ixQL9+TES7fz/F7KGHmDi1Rg26q157jS6nfv38899+OyMGb76ZOQoL45tvOI2Fc1wGgSO1oIJAhQrMcPH994XX+4orKGDt2/M6jxjBz9KldAW2bMnQdDPuX76crtz332c04cSJ/G7BSRs9nAPuvJP31MCBMW1qoJFAiUCRm5u4ApWa6ocdt2vHpTf9R9OmQIsWXPferlu0YE65mTMpUL/+yo8Xqj5pki9Qq1ZRnAC6B708gwXxosleeYWh2FWqRLWJJSIRLSjAD6oojH79/GtToQIDKgBGHXpuQg/v2KpVvObejMGTJvFlo7D7fcMGYPt2ZSSRi08EikS2oAoj3MXXvj0fZjVr+pFjgJ+otnlzWlkAQ5tnzGAU2tatHAzsER7S/MUXDHfevJlC5x3buRP4z3/4gPvgg+i1Z/36Iz9fIlpQsaB5c79/rWtXWlu33164O9a7jt4M0x5euH1RfPgh80AWjAp9/32GyScasqBEoEg2gerZkwM927ShNXPmmey/qFrVL9O0KS2uK69kv8ScOYxa69GDueY2bOB06KeeSuvJC6jYv58JdVu04ANv7Fie66ST6JqaNYvZD6ZMYQScJ36l4YEH+Du7dh0aTl8UiWpBxYJrr6XLd+pUXqf77mPkX8FQdE+gDhzgmKyGDekmHDiQQRdXX134+YcO5UtEu3Z+Sqe9e2nt3Xgjw/QTCQmUCBS5uQzNTRbOPtvPxwcUPibGzH+YAOz7AGh5XHQRxWrvXj5gJk/2H17vvMPosAUL/O+vXQucey7DtLOyKPg5OXT5jRpV+vasXMmHZna2HxxQHJ4FFe30VYnI4MG+63DTJvYz3X8/X0KOPdYvF24lr11LgfJeTFatKvzc+/dTnABm1HDOv7dyc2llJxq6ZUSgSDYLqjSkpjIseu9erl90Ea2vxYv9YIjatfng37ePg1EBlvEiBj3h8rK4b9niu4xyc+k+BCg8CxfyU9CFtHmznwzV6xOJJCz+5595PWVBFU7Firy+XmZ25+j2W7iQLx2ee3jlSl4DT7S8a7B/P6+Dt/3jj1x26cL7wYsS9K7V9u1cHjzIe2jhQt4j4S7EoCGBEoEikYMkYsHvf8+IwAEDmN3gxBNpkXTpQhfeZZdxwGnDhn4mhK5d2a/1668UiA4dGGDx/ffsB3n4YZa7+26gVSv2Y7VqxYixzp2ZWshj61Y+KJ9+mttetvDiUvvs20e35t//rj6ow9GmDRPQTp7MyL4OHXgNsrP96L2bbqIb1+v7867Bn//MMVitWtGq8vb37p2/nHettm3j8t//5gtM5860goOcI1ICJQKFLKj8VKnCSMCxY7k9eDCF6Y03mKD2nns4rurzzylWmZl84IVPIeINcp00iWLx/PN8i54wgcEUQ4ZQPKZPZ5j87Nn+ZIDffEOhe/55Jkr95RfuL06g3nqLb/BLlsiCKo6hQ2nN3H03w9PfeIMBL/fcw5eS9et53bykw2vW8JpMmcJ+ydxcBtF4lpQnUN62d622b6eVNn48xemNN/jy4503iEigRKCQQB1Kq1Z+FoLUVHZ4X3ABx1vVqEHXXvPm/LudeCLLtWtHQahUyc/W7rn5vv2WVtT69SyzahX7rS65BBgzhi6fadNY1nu4ff21nwU+La14gfJ+y5sXy8uNJw7lf/7Hvw7DhvHann8++xE9N58n7mlpjMZ7/XW+XNx2G6cYmTyZf2szP1JwzRoKUrgFtWgR3b5XXsnf6dQp2IluJVAiUMjFFx28+a7atWMW78aN+QCsXp0id8stDHf/f/+P5b2sFp7LacwYuo/mzuVbfEqKH7zRrx8fknXrUhwL+7z9Nq/jmjWM+JP1VDT16/t5Ab28gR7p6byWt9zCba/c/fcza8WZZ/LaZWdTtBo25PWuXp3j6Ro3puWUlsbl1Kl8afHcuF6f5s03MwNG0FAUnwgUsqCix7//7QtDRgajCbt0YWaD+fOB009nCp7atdnH5fHoo8CTT9JttHgxJ+gDfAtqzBg+HA8cKPq3K1XitXzqKb6hh+erE4fywAPAH/7gD+b2uPVWugD79KHodO7Mwd2LF/PlIiWFFvK113Jfjx78/0lPZ99ijRrsq6pYkX2U8+dzqpI6dXj+jAxauA89xO/dcYcfbBMEJFAiUOTkKBw5WoRn0M7IYN9VRgb7qrykqYD/du7RowfH6Hz8MUXNiwp8912+zXfsCDzySPG/P3kyBeqLL4rOfCFIhw78FKR7d34AXidv2nrAt3pr1aJL8LXXOA4O4HLpUk5D8vDD7EME2Kd4xhn+Oby+Si+Sb/p0TuYYFPQoEIFCLr7Y4GWrCA+eOBwVKvjupowM9lccdVTxEzyG4z0s8/IOP+mjiJwmTfj3P/74/Fapl509PHMJ4IuYN0j7l1/8YwDPU6EC+zkzMvxAmquu4ncXLoxte4ojIgvKzPoCeBRACoDnnHP3Fzg+HMA/AYSGieEJ59xzUaynKCfIxRcbzjyT7jyvDyMSrrqKEYRnnUWBue02Rv9FSviDMHx+J1FyUlOByy/nAPDw/5N+/eimPe88bg8YwKAIb54sz6UH5L8u1aoxYOKUUzjW6qabOLfV2LF00+7b5+eGjAfFCpSZpQB4EsA5ANYB+MrMZjrnlhUoOt05FyDjUCQisqBiQ/36zMd2JDRrlj/vXkFXYHE0asQ+koMHZUFFk+cKefVPTWXuRo++ffnxCE9z5Vm2Hl7y2vXrGSzx4INA69YUu3jPZRWJi687gBXOuZXOuQMApgEYUMx3hCgRsqCSh4oVKVINGyZX+qpEpCgLKpxGjWhpO0eX4dChDIS59Vb2XR08yFRdXlaRt9468peeIyUSgWoE4Mew7XWhfQW50MyyzOxVM2tS2InMbKSZZZpZ5pbiZusS5RIJVHJx4omMFhTxJdyCKkqgAOZrrFKF/U+dO7Of6+mnGUU4bhyX8+ax7J13+tlLYkW0giTeAtDMOZcBYA6AFwsr5Jwb55zr6pzrWi9IsYwiMOTmKoovmXj5ZabwEfGlcmW6WatUOXwY+cUXs+/KG/j9xRe8flu3cngBQGsqN5dRgpEG3ZSUSB4F6wGEW0SN4QdDAACcc9ucc/tDm88BODE61RPlDVlQyUXFivyI+FO7dmRRmNWq+etVqwKDBtFFuHMn92VlcQ6rAweCIVBfAWhtZs3NLBXAIAAzwwuYWcOwzf4AlkeviqI8IYESIjYceyzDyY+USpWYA9KM38/K8tMjxVqgin23cc7lmtloALPBMPPxzrmlZnYPgEzn3EwA15lZfwC5ALYDGB7DOoskRlF8QsSGF1+kq68k3Hsv0yO98gqTDC9cSMu4bdvo1rEgERnfzrlZAGYV2HdH2PoYAGOiWzVRHpEFJURsaNOm5N+tVYtZ0rOzmZ1+5kyKU0kFL1LUHS0ChSwoIYKL59Jbvjz27j1AufhEwJAFJURw6dKFufq2bGGC2lgjgRKBQslihQguFSsCjz9edr+nR4EIFHLxCSE8JFAiMBw8yDQqEighBCCBEgEiJ4dLCZQQApBAiQCRm8ulBEoIAUigRICQBSWECEcCJQKDLCghRDgSKBEYZEEJIcKRQInA4AmUxkEJIQAJlAgQcvEJIcKRQInAIBefECIcCZQIDBIoIUQ4EigRGOTiE0KEI4ESgUEWlBAiHAmUCAyeBaUoPiEEIIESAUIWlBAiHAmUCAwSKCFEOBIoERgUJCGECEcCJQKDLCghRDgSKBEYJFBCiHAkUCIwyMUnhAhHAiUCg5LFCiHCiehRYGZ9zSzbzFaY2S2FHK9sZtNDx+ebWbNoV1QkP7KghBDhFCtQZpYC4EkA/QC0BzDYzNoXKDYCwM/OuVYAHgHwQLQrKpIf9UEJIcKpGEGZ7gBWOOdWAoCZTQMwAMCysDIDANwVWn8VwBNmZs45F8W65mPPHmDpUq7/4Q+x+hVRlqxaxaUESggBAFachpjZRQD6OueuCG0PBXCSc250WJkloTLrQts/hMpsLXCukQBGhjbbAMiOQhvqAthabKnkQG1NPspLOwG1NVmJRlvTnXP1Cu6MxIKKGs65cQDGRfOcZpbpnOsazXMGFbU1+Sgv7QTU1mQllm2NJEhiPYAmYduNQ/sKLWNmFQGkAdgWjQoKIYQon0QiUF8BaG1mzc0sFcAgADMLlJkJYFho/SIAH8Sy/0kIIUTyU6yLzzmXa2ajAcwGkAJgvHNuqZndAyDTOTcTwPMAJpnZCgDbQRErK6LqMgw4amvyUV7aCaityUrM2lpskIQQQggRDzRmXwghRCCRQAkhhAgkEighhBCBRAIlhBAikEighBBCBBIJlBBCiEAigRJCCBFIJFBCCCECiQRKCCFEIJFACSGECCQSKCGEEIGkTOeDCqdu3bquWbNmpTrHtm2c0aNOnTpRqJEICrquyYeuqTgcCxYs2Br3CQvDadasGTIzM0t1jhdeeAEAMHz48NJXSAQGXdfkQ9dUHA4zW1PYfrn4hBBCBJJiBcrMxpvZZjNbUsRxM7PHzGyFmWWZWZfoV1MIIUR5IxIL6gUAfQ9zvB+A1qHPSABPl75aQgghyjvFCpRz7hNwltyiGABgoiPzANQys4bRqqAQIvHJywO2bAGmTQNeeQXYvTveNRKJQDT6oBoB+DFse11o3yGY2UgzyzSzzC1btkThp4UQicDmzcCyZcDgwcAllwAPPBDvGolEoEyDJJxz45xzXZ1zXevVOySiUAiRpOTmcjlvHtCrF/Dqq4WXmzQJWLmy7Oolgk00BGo9gCZh241D+4QQAgDgHJft2wODBgHffgssX56/zBtvAJddBvzzn2VfPxFMoiFQMwFcFormOxnATufcxiicVwiRJHgCVakS8Ic/cH36dOCzz4AxY4Dt24Frr+X++fPjU0cRPIodqGtmUwGcDqCuma0DcCeASgDgnHsGwCwA5wJYAWAvgD/GqrJCiMQkXKCOPRb4/e+Bhx4CnnkG2LSJLr9Nm7j/nXeAvXuBatXiW2cRf4oVKOfc4GKOOwDXRq1GQoikwxOoCiGfzeOPAx06AFu3Al26AF9/Ddx8M9C7N/D228CCBVwX5Zu4pToSQpQfnAPM+AGAZs2AGTOAHTuAnj2BF18E/u///PDzDz8EevQAKhbyhHKOLsGqVWVlJTtKdSSEiDmeQIVz9tnARRcBDRsCt9xCwalXD2jZErjzTqBzZ2DPnkPPdffdQN26LLthQ9nUX8QHCZQQIuYUJlBFMXUqcM89wJIltKoWLuT+DRuAX3+ltdWmDfupXnstdnUW8UcCJYSIOXl5kQtUt27A7bcD11wDPP88Lal77gHatmV/1erVwE03MWRdApXcSKCEEDHnSCwoj8cf9wf23nkn+6eWLwdSUoABA4ALLwQ+/RR48EHg4Yc5tmrTJgZZ5OUBM2f6fVqrVrFfSyQWEighRMxxzo/gi5QKFYCTTqIVdcwxDEnv0wfo3599UP/7vwyi+MtfgBtuAEaNopCdfz4wdChF7Oqrea4//Qk45xzfXSgSAwmUECLmlMSC8jjuOGD9emDkSI6R8tx6bdsCO3fy89e/0pp6+WUemzIFSEsDJk9mgto5c4CDB4HLL/fTLoXz44/ArbcCBw6UrI4iNkighBAxpzQCBfjWV4UK+c9TpQpQsybTJzkH/Pwz+6fOOosZKdq3pzV14ACtrG++4QDhgnUbNgz4+9+BWbNKXkcRfTQOSggRc0orUMXRoQPQujUtoTvuAKpX5/7nnwdOOYXZKx58kAEWd9zBSEGP/fvZf2VGy2v6dPZ1XXEFMHp07OosikcCJYSIObEWKDMmmd2wwRcnADj5ZODpp2llVagAPPUU17cXmOHuwguBdesYwg4Axx/PEPeTTmJUoYgPEighRMw5kjDzkjJgQOH7R43y1+vXB8aPL7zc229ToIYMAZ54gu7BESOAzExg1y6K1qZNLDt0KDBxYnTrLw5FfVBCiJhTkii+sqZfP2DsWIpTWhqjBhcv5uSKM2ZQnK6/nv1b06YxTZOILbKghBAxJ9YuvmiQksJIQY/zz2fwxb33Au3aAc2bc7zVvHns13r7bVpbInYE/J1GCJEMJIJAFcajj7LPKisLGDiQbTjpJAZdjB3LGYBfeSV/6PrOncBXX3F91iyGum/dGp/6JzoSKCFEzElUgapfny6/lBQODAboqvzf/+Vki5ddBlxyCV1+HrffzkzsM2cC553H/qpbbolP/RMdCZQQIuYkqkABdPNt3848gB733w/88APw/fdAo0accBFgMMirr3JQ8DXXcILGc89lH1ZhA4TF4ZFACSFiTllE8cWSmjXzb6ekAC1aAK1aMUR99mzm/Zs3D9i4kW1dv55TiowYAWzbBnz8cXzqnshIoIQQMSeRLajiuPBCTgNy5pnAH/8IpKb6oe0XXgj07cuJFadPP/S7f/sb0zeJwpFACSFiTiKEmZeUnj2BwYPpzqtbl3kBb76ZfVMXXURxuvRSjr8KT1a7YQP7qx55JH51DzoKMxdCxJxktqBSUpgiqSDhFtP99zNo4vLLgXHjmEKpUyce+/JLukCTVcBLgwRKCBFzklmgIqF2beDJJ2lRnX46p7KfP5/Hdu4EsrM51gpgbsDKlRlUYUYB3LKF26mpQJ06hf/G/v08npfHv3fF0NM9J4fWXSIizRZCxJzyLlAA+6MGDqQ4XXkl9w0cyKUnVp99xoCM998HTj0V+MMf2E9Vvz7HXtWrxwwXBVm5EmjQgNbZiBHMdgEwv2BaWuJmaZcFJYSIORIo8uKLnDzx9NMpWCefTDGaMYMCc/31nBrkrruAuXP5nbfeAn7/e36mTuW0IfXrA6edRmvKOQrezp0cPPztt8C+fRStuXO5/uKLDHdPNCRQQoiYk+hh5tGienWKEwD87ndc9u4NvPkmPxUqAB07Ap9/zmOdOgGbN1NgatemyHTsSHFr3py5AqdNAz74AMjI4HxXHq+/Dqxdy/X//IdCVbVqmTU1KkTk4jOzvmaWbWYrzOyQMdFmNtzMtpjZwtDniuhXVQiRqMiCKpqXXqKlM3cuB/7edx/3d+tG19/SpRQnAGjSBPjuO35n1SrmArzhBlpTXqBG3boUq9de47ismjXpVnzvPf83f/2V1ta//81JHoNKsRaUmaUAeBLAOQDWAfjKzGY655YVKDrdOafpvYQQh5DMYealJS2Nrj6PY4/lIOARIxgsUbly/vL16zPV0qJFnISxfn3g2Wc5YWPv3kyzVLcuQ90rVKDb8LnnON7Km5Lkttv8mYXff59uxCC+QETi4usOYIVzbiUAmNk0AAMAFBQoIYQoFFlQkVOlCtMoFccDDwB3381oPS9i75NPuPz1V467+vZboFcvJrudPx/4738pZq+8wn6rNm2AG28E+vcHatSg5VUwb+B//8tgi2HDotvOSIjknaYRgB/DtteF9hXkQjPLMrNXzaxJYScys5FmlmlmmVu2bClBdYUQiUZeHpcSqOhTpYovTgX3T5zI/q7TT2cG9sWLgT//mf1RZ5/NGYivv55uwuxsCtGYMYe6/O68E7juOv86liXRMrrfAtDMOZcBYA6AFwsr5Jwb55zr6pzrWq9evSj9tBAiyOTkcCmBKlu6dQM+/BA4+mi6EA8epEjdeitzB6alcYzVpEl+vxZAV2CPHgzGePttYMEC4JdfGKrety8T515+OacRiTWRuPjWAwi3iBqH9v2Gc25b2OZzAB4sfdWEEMmABCr+nHSSv37hhYWX6daN12jMGEb/paUB115LdyHA9bVrKWATJjBTRq9eQLNmsat3JBbUVwBam1lzM0sFMAjAzPACZtYwbLM/gOXRq6IQIpHxppmQQMWPevUYeNGxI4MpCiMtjdks1q7l8sor/TD1SpX89ddf53LvXs5A7Fzs6l2sQDnncgGMBjAbFJ6XnXNLzeweM+sfKnadmS01s0UArgMwPFYVFkIkFrKggsHkyeyXOhyepXXhhb6lVb++P3YL8LOv3347Bx3H8rpGNFDXOTcLwKwC++4IWx8DYEx0qyaESAZkQQWDHj2KL9O7N913F18MdOgApKcDXbtywPBHH9Gyyspizr877ig8QCOaKJOEECKmeBaUxkEFn8su48zBGRnc/uQTThdy1FEUraeeokC1aRN7cQIkUEKIGCMXX+KQkuJPAwIATZv6623b+hnXvWWs0TuNECKmyMWXPLRvn38ZayRQQoiYIgsqeejcmS7Avn3L5vfk4hNCxBRZUMlDzZocuFtWyIISQsQUWVCipEighBAxRVF8oqTolhFCxBS5+ERJkUAJIWKKXHyipEighBAxRQIlSooESggRU+TiEyVFAiWEiCmyoERJkUAJIWKKLChRUiRQQoiYojBzUVJ0ywghYopcfKKkSKCEEDFFLj5RUiRQQoiYIgtKlBQJlBAipsiCEiVFAiWEiCmyoERJkUAJIWKKBEqUFAmUECKmeC4+hZmLI0W3jBAipsiCEiVFAiWEiCmeBSXEkSKBEkLElJwcWU+iZEQkUGbW18yyzWyFmd1SyPHKZjY9dHy+mTWLdkWFEIlJTo76n0TJKPa2MbMUAE8C6AegPYDBZta+QLERAH52zrUC8AiAB6JdUSFEYpKbKwtKlIyKEZTpDmCFc24lAJjZNAADACwLKzMAwF2h9VcBPGFm5pxzUaxrPnbuBD77jOt/+lOsfkXEg//5Hy51XZODX38F/vjHeNdCJCJWnIaY2UUA+jrnrghtDwVwknNudFiZJaEy60LbP4TKbC1wrpEARoY22wDIjkIb6gLYWmyp5EBtTT7KSzsBtTVZiUZb051z9QrujMSCihrOuXEAxkXznGaW6ZzrGs1zBhW1NfkoL+0E1NZkJZZtjaTrcj2AJmHbjUP7Ci1jZhUBpAHYFo0KCiGEKJ9EIlBfAWhtZs3NLBXAIAAzC5SZCWBYaP0iAB/Esv9JCCFE8lOsi885l2tmowHMBpACYLxzbqmZ3QMg0zk3E8DzACaZ2QoA20ERKyui6jIMOGpr8lFe2gmorclKzNpabJCEEEIIEQ80fE4IIUQgkUAJIYQIJAkrUMWlX0pEzGy1mS02s4VmlhnaV9vM5pjZ96Hl0aH9ZmaPhdqfZWZd4lv7w2Nm481sc2jMnLfviNtmZsNC5b83s2GF/Va8KaKtd5nZ+tC1XWhm54YdGxNqa7aZ/S5sf+DvcTNrYmYfmtkyM1tqZv8X2p9U1/Yw7Uy662pmVczsSzNbFGrr3aH9zY2p7FYYU9ulhvYXmequqL9BxDjnEu4DBmv8AKAFgFQAiwC0j3e9otCu1QDqFtj3IIBbQuu3AHggtH4ugHcAGICTAcyPd/2LadupALoAWFLStgGoDWBlaHl0aP3oeLctwrbeBeDGQsq2D92/lQE0D93XKYlyjwNoCKBLaL0GgO9CbUqqa3uYdibddQ1dm+qh9UoA5oeu1csABoX2PwPg6tD6NQCeCa0PAjD9cH+DI6lLolpQv6Vfcs4dAOClX0pGBgB4MbT+IoALwvZPdGQegFpm1jAeFYwE59wnYIRnOEfatt8BmOOc2+6c+xnAHAB9Y1/7I6OIthbFAADTnHP7nXOrAKwA7++EuMedcxudc1+H1ncBWA6gEZLs2h6mnUWRsNc1dG12hzYrhT4OwJlgKjvg0GvqXetXAZxlZoai/wYRk6gC1QjAj2Hb63D4myVRcADeM7MFxrRQANDAObcxtP4TgAah9WT4Gxxp2xK9zaNDbq3xnssLSdTWkGunM/jGnbTXtkA7gSS8rmaWYmYLAWwGXxZ+ALDDOefN7hVe79/aFDq+E0AdRKGtiSpQyUov51wXMHP8tWZ2avhBR7s5KccFJHPbQjwNoCWAEwBsBPBQfKsTXcysOoDXAFzvnPsl/FgyXdtC2pmU19U5d9A5dwKYOag7gLbxqEeiClQk6ZcSDufc+tByM4A3wBtjk+e6Cy03h4onw9/gSNuWsG12zm0K/dPnAXgWvqsj4dtqZpXAh/ZLzrnXQ7uT7toW1s5k6oaWTAAAAXpJREFUvq4A4JzbAeBDAD1Ad6yX3CG83kWluit1WxNVoCJJv5RQmNlRZlbDWwfQB8AS5E8jNQzAm6H1mQAuC0VFnQxgZ5hLJVE40rbNBtDHzI4OuVL6hPYFngL9g38Ary3Atg4KRUI1B9AawJdIkHs81NfwPIDlzrmHww4l1bUtqp3JeF3NrJ6Z1QqtVwVwDtjn9iGYyg449JoWluquqL9B5MQ7YqSkHzAa6DvQN3prvOsThfa0ACNeFgFY6rUJ9OX+F8D3AN4HUNv5kTZPhtq/GEDXeLehmPZNBV0gOaAvekRJ2gbgcrCzdQWAP8a7XUfQ1kmhtmSF/nEbhpW/NdTWbAD9wvYH/h4H0At032UBWBj6nJts1/Yw7Uy66wogA8A3oTYtAXBHaH8LUGBWAHgFQOXQ/iqh7RWh4y2K+xtE+lGqIyGEEIEkUV18QgghkhwJlBBCiEAigRJCCBFIJFBCCCECiQRKCCFEIJFACSGECCQSKCGEEIHk/wOgyCeBjYbhhgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K7g1tH3vnWa",
        "outputId": "d9692024-94d3-4acc-c740-0eabff4efa20"
      },
      "source": [
        "accuracyAll(models_C)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model 0\n",
            "Task 0: Acc 0.97% | Gr acc 0.95 | Ugr acc 1.0\n",
            "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:825: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.num_layers, self.dropout, self.training, self.bidirectional)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:822: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Task 2: Acc 0.72% | Gr acc 0.44 | Ugr acc 1.0\n",
            "\n",
            "Model 1\n",
            "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
            "Task 1: Acc 0.6% | Gr acc 0.2 | Ugr acc 1.0\n",
            "Task 2: Acc 0.56% | Gr acc 0.11 | Ugr acc 1.0\n",
            "\n",
            "Model 2\n",
            "Task 0: Acc 0.62% | Gr acc 0.3 | Ugr acc 0.95\n",
            "Task 1: Acc 0.55% | Gr acc 0.1 | Ugr acc 1.0\n",
            "Task 2: Acc 0.6% | Gr acc 0.22 | Ugr acc 0.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkGxECJqvnWb"
      },
      "source": [
        "## Transfer D: EWC\n",
        "\n",
        "1. Create Fisher functions\n",
        "2. Train model on tasks\n",
        "3. Compare results\n",
        "\n",
        "Based on: https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNS4W04AvnWc"
      },
      "source": [
        "Compute optimal parameters and fisher information after training on tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI4QzSeavnWc"
      },
      "source": [
        "def onTaskUpdate_ewc(model, task_id, train_dl, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #accumulate Gradient\n",
        "    for it in range(100):\n",
        "        for seq, seq_len in train_dl:\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(seq, seq_len, seq, 0)\n",
        "\n",
        "            if criterion == F.cross_entropy:\n",
        "              output_dim = output.shape[-1]\n",
        "                \n",
        "              output = output[1:].view(-1, output_dim)\n",
        "\n",
        "              trg = seq[1:].view(-1)\n",
        "\n",
        "              loss = criterion(output, trg)\n",
        "            else:\n",
        "              loss = criterion(output, seq)\n",
        "            #print(loss)\n",
        "\n",
        "            loss.backward()\n",
        "        \n",
        "    fishers.append({})\n",
        "    optParams.append({})\n",
        "    \n",
        "    for name, param in model.named_parameters():\n",
        "        fishers[task_id][name] = param.grad.data.clone().pow(2)\n",
        "        optParams[task_id][name] = param.data.clone()"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJu-KM7mvnWc"
      },
      "source": [
        "Adapt evaluation and training function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRW-TlhwvnWd"
      },
      "source": [
        "def train_ewc(model, task_id, dataloader, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for seq, seq_len in dataloader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(seq, seq_len, seq)\n",
        "        loss = criterion(output, seq)\n",
        "        \n",
        "        if task_id > 0:\n",
        "            print(\"-\\n\", loss)\n",
        "        \n",
        "        # EWC Training penalty\n",
        "        for other_task_id in range(task_id):\n",
        "            for name, param in model.named_parameters():\n",
        "                fisher = fishers[other_task_id][name]\n",
        "                optParam = optParams[other_task_id][name]\n",
        "                #print(ewc_lambda)\n",
        "                loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
        "                #print((fisher * (optParam - param).pow(2)).sum())\n",
        "                #print((optParam - param).pow(2).sum())\n",
        "                #loss += ewc_lambda * (optParam - param).pow(2).sum()\n",
        "        \n",
        "        if task_id > 0:\n",
        "            print(loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(dataloader)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnQbtl1yvnWe"
      },
      "source": [
        "def evaluate_ewc(model, task_id, dataloader, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "        for seq, seq_len in dataloader:\n",
        "\n",
        "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
        "\n",
        "            loss = criterion(output, seq).type(torch.float)\n",
        "            \n",
        "            # EWC Training penalty\n",
        "            for other_task_id in range(task_id):\n",
        "                for name, param in model.named_parameters():\n",
        "                    fisher = fishers[other_task_id][name]\n",
        "                    optParam = optParams[other_task_id][name]\n",
        "                    loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
        "                    \n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(dataloader)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgGMwiHcvnWe"
      },
      "source": [
        "def fit_ewc(model, task_id, epochs, step_size_evaluation, clip ):\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
        "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
        "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
        "    # [task_id, dataset, evaluations]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
        "        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), SAVENAME)\n",
        "\n",
        "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
        "            idx = epoch//STEP_SIZE_EVALUATION\n",
        "            for other_id in range(task_id + 1):\n",
        "                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)\n",
        "                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)\n",
        "                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)\n",
        "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
        "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
        "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
        "\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        \n",
        "    return total_loss, total_hits"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NI--0ZFvnWe"
      },
      "source": [
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB0VOa5lvnWf",
        "outputId": "73f80b88-a4e7-4e23-bd7f-671ed1dbfe3b"
      },
      "source": [
        "fishers = []\n",
        "optParams = []\n",
        "ewc_lambda = 10\n",
        "\n",
        "models_D = []\n",
        "hist_losses_D = []\n",
        "hist_hitsss_D = []\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "print(model.apply(init_weights))\n",
        "\n",
        "for task_id in range(N_TASKS + 1):\n",
        "    SUFFIX = f\"D{task_id}\"\n",
        "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
        "    LOADNAME = \"models/autosave/\" + title + \".pt\"\n",
        "    SAVENAME = \"models/autosave/\" + title + \".pt\"\n",
        "    PLOTSAVE = \"plots/autosave/\" + title + \".png\"\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
        "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
        "    \n",
        "    hist_loss_temp, hist_hits_temp = fit_ewc(model, task_id, 500, STEP_SIZE_EVALUATION, CLIP)\n",
        "    hist_losses_D.append(hist_loss_temp)\n",
        "    hist_hitsss_D.append(hist_hits_temp)\n",
        "    models_D.append(copy.deepcopy(model))\n",
        "    onTaskUpdate_ewc(model, task_id, train_dls[task_id], F.cross_entropy)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 334 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 335 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0941, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 336 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 337 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 338 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 339 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 340 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 341 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 342 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "-\n",
            " tensor(0.0801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0927, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0988, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 343 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0768, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 344 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0880, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 345 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 346 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 347 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 348 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0899, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0925, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 349 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0887, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0934, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 350 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 351 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 352 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 353 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0972, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 354 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 355 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 356 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0889, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 357 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 358 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 359 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 360 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 361 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 362 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0893, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 363 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0794, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 364 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 365 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 366 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 367 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 368 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 369 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0935, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 370 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 371 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 372 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0862, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 373 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 374 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1312, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 375 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 376 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 377 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 378 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 379 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0958, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 380 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0917, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 381 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0843, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 382 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 383 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 384 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 385 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 386 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 387 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 388 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 389 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0863, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1162, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 390 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 391 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 392 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0929, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 393 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 394 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0916, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 395 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0922, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 396 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 397 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1032, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 398 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0894, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 399 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 400 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 401 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 402 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0847, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1012, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 403 | Time: 0m 0s\n",
            "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 404 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 405 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 406 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0913, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 407 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0969, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 408 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 409 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 410 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 411 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 412 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0720, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 413 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0701, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 414 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 415 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 416 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0704, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 417 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 418 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1322, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1444, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0861, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 419 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 420 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0895, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0694, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 421 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0902, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 422 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0930, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1148, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 423 | Time: 0m 0s\n",
            "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0923, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 424 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
            "-\n",
            " tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0883, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0879, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0850, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 425 | Time: 0m 0s\n",
            "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
            "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1275, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0834, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0761, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0886, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 426 | Time: 0m 0s\n",
            "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
            "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
            "-\n",
            " tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0867, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1034, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1142, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1401, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1022, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0871, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 427 | Time: 0m 0s\n",
            "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
            "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
            "-\n",
            " tensor(0.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1030, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1381, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 428 | Time: 0m 0s\n",
            "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
            "-\n",
            " tensor(0.0795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0921, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 429 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0836, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 430 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 431 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 432 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0708, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0768, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 433 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 434 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 435 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 436 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0855, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 437 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0866, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 438 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 439 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0744, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0840, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0857, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 440 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0984, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0831, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 441 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0758, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0881, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 442 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 443 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 444 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 445 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0833, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0960, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 446 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0818, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0732, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 447 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 448 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0919, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 449 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 450 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0859, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0846, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 451 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0722, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 452 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 453 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0845, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 454 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 455 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0824, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0856, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0892, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 456 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0754, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0875, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 457 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0901, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0826, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 458 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0888, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0869, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0912, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 459 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0805, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 460 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 461 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0852, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0751, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 462 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0849, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0882, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 463 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0717, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 464 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0719, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0844, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0791, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 465 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0838, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0864, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0764, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 466 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0819, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0738, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 467 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0841, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0825, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 468 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0713, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 469 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0858, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 470 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0853, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0649, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 471 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0692, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0757, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 472 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0813, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0762, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 473 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0785, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0629, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 474 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0731, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 475 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1008, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 476 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0797, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0772, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 477 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0827, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0802, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0746, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 478 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0748, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0638, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0808, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0761, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 479 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0705, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0752, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 480 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0714, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0716, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0817, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0670, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 481 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0865, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0774, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0789, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0910, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 482 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0765, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0724, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 483 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0816, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0715, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0750, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0787, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0854, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 484 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0795, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0832, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0733, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 485 | Time: 0m 0s\n",
            "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0726, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0725, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 486 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0699, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0837, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 487 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0710, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0728, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0789, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0784, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 488 | Time: 0m 0s\n",
            "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0782, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0753, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0701, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0820, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0736, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0868, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0727, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 489 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0742, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0776, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0786, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0812, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0936, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0771, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 490 | Time: 0m 0s\n",
            "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0835, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0763, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0811, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0739, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 491 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
            "-\n",
            " tensor(0.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0804, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1149, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0807, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0828, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 492 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
            "-\n",
            " tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.1084, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0799, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0775, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0815, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0770, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0842, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0732, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 493 | Time: 0m 0s\n",
            "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
            "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0656, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0777, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0783, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0758, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 494 | Time: 0m 0s\n",
            "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0780, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0747, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0754, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0794, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0891, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0756, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0790, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 495 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0809, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0806, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0667, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0788, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0792, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0878, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0778, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0773, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 496 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0735, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0743, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0709, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0830, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0800, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0860, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0796, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0745, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 497 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0741, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0801, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0779, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0823, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0730, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0755, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0721, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 498 | Time: 0m 0s\n",
            "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
            "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
            "-\n",
            " tensor(0.0592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0713, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0645, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0729, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0821, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0798, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0723, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0737, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0870, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 499 | Time: 0m 0s\n",
            "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
            "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
            "-\n",
            " tensor(0.0598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0718, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0814, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0872, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0749, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0848, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0740, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0810, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0781, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "-\n",
            " tensor(0.0648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "tensor(0.0769, device='cuda:0', grad_fn=<AddBackward0>)\n",
            "Epoch: 500 | Time: 0m 0s\n",
            "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IYEBTL-bo_Tr",
        "outputId": "f9c416ec-e726-497e-d0ed-33a923c1ecc1"
      },
      "source": [
        "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
        "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
        "\n",
        "plotResults(hist_loss_D, hist_hits_D)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxcdb3/8denWdp0X5JutDSlrWwWlFZZf+xgWS5cEC+tV6AIFsTixlUpAgKXqyAqiKBsUpYrm6BYAcUicNUrYoNCWbylpXYvNN23NM3y+f3xmWmmadok7SRzJnk/H4/zmLPNmc85yZzPfL/ne77H3B0REZGk6ZLrAERERJqiBCUiIomkBCUiIomkBCUiIomkBCUiIomkBCUiIomkBCXSAmb2GzO7INdxiHQmpvugpKMys40Zk92BaqAuNX2Ju/+sneJYAFzs7i+0x+eJdBSFuQ5ApK24e8/0+K6ShJkVuntte8YmIs1TFZ90OmZ2rJktMbNvmNn7wHQz62dmz5hZpZmtSY0Py3jPy2Z2cWp8spn9ycy+l1r3n2Z2ym7E0dXMbjOzZanhNjPrmlpWmophrZmtNrM/mlmX1LJvmNlSM9tgZnPM7IQsHRqRRMlZFV9paamXl5fv0TZWrVoFwIABA7IQkXRkb775JiNGjKB3795s2LCBd999l0GDBjF06FAA6uvr2bBhA3369MHdWbBgAe7O6NGjAZgzZw4DBgygtLSUlStXsnDhQvbee+9t08uXL2fs2LGY2S4/O9OyZctYv349o0aNAuC9996jV69e7LXXXixdupTa2lr23ntvADZu3EjPnj2prq7m3XffZb/99qO4uJjq6moAunbt2mbHLlv0fZWdee2111a6e9kOC9w9J8O4ceN8T02fPt2nT5++x9uRjm/EiBE+c+ZMd3d/6aWXvKioyKuqqna6/t///nfv27fvtuljjjnG7733XneP/7tRo0ZtW7Zp0yYHfPny5c1+dqZ99tnHn3322W3Tv/3tb33EiBHu7n7NNdf4GWec4XPnzt3uPXPnzvWysjKfOXOmb926tZm9ThZ9X2VngApvIk+oik86pbKyMrp167ZtevPmzVxyySXbSjpHH300a9eupa6ursn3Dx48eNt49+7dgSjltMayZcsYMWLEtukRI0awbNkyAL72ta8xevRoTj75ZPbZZx9uuukmAEaPHs1tt93Gddddx8CBA5k4ceK294h0NEpQ0ik1ror7/ve/z5w5c3j11VdZv349f/jDH4CoYWgrQ4cOZeHChdumFy1atK3KsVevXnz/+99n/vz5zJgxgx/84Af8/ve/B+DTn/40f/rTn1i4cCFmxje+8Y02i1Ekl5pNUGZ2v5mtMLO3drLczOx2M5tnZrPN7JDshynStjZs2EBJSQl9+/Zl9erVXH/99Vndfk1NDVu2bNk21NbWMmnSJG688UYqKytZuXIlN9xwA5/5zGcAeOaZZ5g3bx7uTp8+fSgoKKBLly7MmTOHF198kerqarp160ZJSQlduuh3pnRMLfnPfgCYsIvlpwBjUsMU4Cd7HpZI+/ryl79MVVUVpaWlHHbYYUyYsKt/+dY79dRTKSkp2TZcd911XH311YwfP56DDjqIsWPHcsghh3D11VcDMHfuXE488UR69uzJ4YcfzmWXXcZxxx1HdXU1V155JaWlpQwePJgVK1bwne98J6uxiiRFi1rxmVk58Iy7f7iJZXcDL7v7o6npOcCx7r58V9scP368V1RU7E7M2zzwwAMATJ48eY+2IyJtT99X2Rkze83dxzeen426gb2AxRnTS1LzmgpiiplVmFlFZWVlFj5aREQ6qnatvHb3e9x9vLuPLyvbscm7iIhIWjYS1FJgeMb0sNQ8ERGR3ZaNBDUDOD/Vmu8wYF1z159ERESa02xnsWb2KHAsUGpmS4BvAUUA7n4X8BxwKjAP2Axc2FbBiohI59FsgnL3Sc0sd+ALWYtIREQE9SQhIiIJpQQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQlIiKJpAQl0sktXw6zZ+c6CpEdKUGJdAIrV8L77+84f/NmOPZYOPRQmDu3Yf6778Lata3/nKoq2LBht8MU2Y4SlEgr1dTEiXhX6uvhzTdj3Z354AM47TT4938H9+zGmGnVKjjkEPjYx2DTpu2XfeMbkYwKCuDiiyPuF16AsWPhuONgy5aWf051NRx9NAwcCJ/9LLz6amxPZHcpQUmH4Z69E2JVFWzd2vRnfOpTcbLfWVJ56SX4+MfhoINg//3hscd2jOsvf4Fx4+C55+CRR+DZZ3fcztSpcMAB8Pjju79f9fVw3nlRelqyBL797YZlM2fCHXfAl74Et98Of/gDXH45/Ou/wpAh8Prr8JWvxLpbtsDTT8Nf/7p90so8BlddBRUVcOqp8MQTcNhhUFYGZ50VyVqk1dw9J8O4ceN8T02fPt2nT5++x9uR/Ld2rfvxx7sPG+b+y1/ufL133nE/7DD3yy5zf+kl9//+b/ejjnLv3t39yCPdp051P/ZY9+Ji9yOOcK+r2/79Tz3lHqdl9z/+cftltbXul14ay4YPd//Od9zHjo3p8nL3K65wv/vuiNPMfZ993GfNct9vP/fRo92rqxu29fOfx/tKS+P1wx92P/NM93PPdX/iiZYdk6oq9299K97/k5+4n3de7NfcubHvgwbFZ2/e7F5f737SSbHuPvu4L1vm/rWvxfSll7oPHdqw34WF7v36uRcVuffu7f71r7s/9FAs+8IXGv4eDz3k/tnPunfr5v65z+n7KjsHVHgTeUIJSvLe8uXuBx8cJ859943/6tNPd7/9dvfnnnNfvz7WW7HCfeRI9z593EtKGk64o0e7X3JJJK5u3WJbZ58dyx57rOFz1q9332uvSBY9e7pfdFHDsq1bI3mA+3/8R5z03SNpPfqo+ymnxAkd3EeNcr/uOvfVq2Od3/wm5t9yS0wvWRIJYPx49y1b4kR/1FHuBx0Unw/uF1zgPm9e7OPpp7uff7779de7X3ml+xlnuI8Z496lS6w7aVIkoGXL3Hv1ch8xIhLkmDHub73VsA+LFrlfeKH7e+817NMRR8Q2jjrK/dlnI0FfdZX75Ze7T5vm/qlPNXzO2LEN+53pxBPdDzlE31fZOSUo6XA2bnS/444orfTo4f7b38ZJ9eab3fv2bUhA/frFyfvIIyMBvfqq+4YNcbJ98cUdS0nukVgOOihKE+mSzZe/HCf2V16JE3mvXhFDTU0kBXD/7nd3Hu+aNe6zZ0eyaOzUUyO2o46KJNu9u/ucOTuuV1Pjfs01DUkBItEMG9ZQujngAPdzznG/9toobWWWzG69Nda77LKIvTlr18bxairmtLlz3b/5zXhtypVXRnL+6U/1fZWmdbgEtXat+/XXT/cf/Wj6Hm1H8tMrr0TiAfdDD3X/61+3X15f7/7+++4zZzYkD2h59Zh7JDyIhDBpUoxfckks+5//iemHH44SE7j/6Ee7vz+LFkWp6JhjooSWWXJryv/+byTDt99umFdVFQl6V+rro8TZntLVlbffrgQlTdtZgirM7RWwPbNgwa5bSUnH5A5XXAElJfDrX8MRR4DZ9uuYwaBBMZx4IrzxRrSaO/nkln/OySfHe//zP6G4GK67DqZNi2VHHQX77BOt4JYtg8sui0YNu2v4cHjggZavf8QRMWTq1q3595nB4MGtCm2PjR8frxs2QK9e7fvZkt9a1IrPzCaY2Rwzm2dmVzaxfLKZVZrZ66nh4uyHur0+faB/f6isVFPWzmbmTPjzn+Hqq+HII3dMTk05+ODWJSeI7d55ZzS/fv11+Na3IlEBdOkCF1wQyenoo+G221q/H53FiBEwYIDuj5LWa7YEZWYFwJ3AScASYJaZzXD3dxqt+ri778FvyNYrK4t7PF59FQ4/vD0/WXLFPUoyw4fHvTZt7UMfgnvvbXrZF74Q9xVdcQUUFbV9LPnKLJrUb9yY60gk37SkBPVxYJ67z3f3rcBjwJltG1bLDBgQ//w//3muI5H2MnMmvPJK3HPTtWtuYxkwAG6+OW5MlV0bPz6SuWo7pDVakqD2AhZnTC9JzWvsk2Y228yeNLPhWYmuGYWFUc335JP6x+8M6uriGlB7lZ4ke8aPj9KvSlHSGtnqSeLXQLm7HwTMBB5saiUzm2JmFWZWUVlZmZUPLiuDxYvjDnfp2O68E/72N/judxuuBUl+GDcuXnUdSlqjJQlqKZBZIhqWmreNu69y9+rU5H3AuKY25O73uPt4dx9fVla2O/HuYMCAqP9XNV/HtnRpNIo4+WQ499xcRyOtNXx4fE9VgpLWaEmCmgWMMbORZlYMTARmZK5gZkMyJs8A/pG9EHetsBBOOQUefnjHjjClY3CP/uJqauDHP25Zqz1JFrNoYq4SlLRGswnK3WuBqcDzROJ5wt3fNrMbzOyM1GpfNLO3zewN4IvA5LYKuCnf+EY0N7/zzvb8VGkPVVUweTI89RRccw2MGpXriGR39egRj/eoq8t1JJIvWnSjrrs/BzzXaN61GePTgGnZDa3ljjgCJkyIaxOf/7xuBuwoPvggesb+29+iafmVO9yBJ/mkpCRKw0uXwt575zoayQcd5nEbN9wQ90TdfnuuI5Fsuftu+PvfYcaMuEm2S4f5b+2cSkridd683MYh+aPDfOU/9jH4l3+B730P1qzJdTSSDf/8JwwdGn9XyX9KUNJaHSZBAdx4I6xfH7+2Jf8tXBjd5EjH0LVrNJZQgpKW6lAJ6qCD4hrUnXfC7Nlx8+7VV8d1jHffzXV00lqLFulaRUdTUqIEJS2X172ZN+WGG+IR2V/4QpzcHnkkenk++GD42tdg+XJ44QU45BC466640Xd3VFU1VFlI9tXXxw3Yn/xkriORbFKCktboUCUoiK6PvvMd+NOfIjl9+9vw3ntw0knx2IQnnoADD4Rnn4WxY+F3v9v+/bff3vxjDx59FPr2hXcad5crWfPBB7B1q6r4Opp0gnLPdSSSDzpcgoLop+3yyyNBTZsWF9p/9av4YqxcCc88A7NmQWlpVP+98EK871e/ihtCL7kkklpT1q2Dr3wlTp733dd++9TZLFoUr6ri61hKSqL2YfnyXEci+aBDJqguXaIkNGlSwzyzuMkz/ViEsWOjV+z994dPfQpeeikS29ixsc7Xv970tq+/HlasiCrDhx+ORCXZpwTVMakln7RGh0xQLdWrVzyRtagIjj8eqqujZ/Rp0+AXv4CXX95+/bffjsT3uc/Bf/1XlMaefTYnoXd4CxfGq6r4OhYlKGmNTp2gAMrL4Ze/jMYSd90VD6j76lfjl/vUqVH9t3kz3HNPXMfq3TuS0yc+AUOGwPTpud6DjmnRojjWffrkOhLJpq5do/9MJShpiQ7Xim93HHlkXJRPd0JaUhKdkn7qU5GUCgqi/7AjjoAf/jCuXQGcf37cGDx3Lrz/flyfOuCASHrq9WDPqIl5x2QG++yjBCUtowSV0riH7NNOiyq8F16I61PHHgtnnLH9ehdeGE9U/dCHtn9v9+7RUvDDH47rWkcd1bBs2TIYPFgJrDm6SbfjGj1aCUpaRqfJXejePZLSrbfCmWfumMT23Rduuy16sHj2Wfjzn+Hee+MaVe/e0SrwhBPg6afjvp5rr4W99oKPfjTm/e530YqwXz845xx47DE1ukhTCarjSicoNTWX5qgEtYe+9KXtpw8/vGF8zZp4VtU558Qjr199Fc4+G958E846K9YZNAhOPz1Kak89FY01fvUr6Nmz/fYhaTZuhNWrlaA6qtGj47lQK1bE/7/IzqgE1Yb69YOZM+GYY+KRET/+cbQSfOeduNn3kUeiKuvhh2HJkriv6uWX46mxL78MU6ZEVeGNN8Latbnem/aTbmKuKr6OKV0l3riVrEhjKkG1sV69oipv5cqGX4uFhTBx4vbrFRTARRdFUps4EY47LqoYDz44HtR3yy1xw/GKFXH96mMfi1LZ6NFxIh8/Ph4IB7BlCzz0UGxjzJj23d9s0D1QHdvxx8f/9Ze+BCeeCAMG5DoiSSolqHZQUNDyqoyzz4bf/z5aBp5zTlzLev31aD24aRMMHBh34s+aBc8/H9e2IJLXrbfGF//cc+GNNyKRnXde9Kpx8MGRGNPmz48qxbVr4eijo4ViUh70qATVsRUVwYMPxo+qdI8vIk1Rgkqg//f/Ykj7yEeavt9qy5Y4mc+ZE48YOffcSIZ9+8aXvqIiqhUffDBKVwccEAlt/fpIgBDrf/vbMd67dzShv/BCuOqq3LU0XLgw4ho6NDefL23v4IOj0dC110bvLZ//fPzfimTSNag81q1b1Of/y7/AX/8avVz8279FiWvSJPj+92HBgkhWn/1snAAGDYoTwi23xAMB162LKsgbb4zENHp0VCmedVYs213z5sGll+7edYZFi2DYsEhS0nFdeWW0cr3qqrjp/YQT4paMQw+Nm+Zra3MdoeSaSlAdRGFhVJdcfvn28wcNimSV2S9hYyedFANE09877ogOcfffPxLemWfGdrp2jdedtTB0j5LZffdF8/uamrgW9pvfREORllqwQNV7nUFRUTQieu01uP/+eO3RI6qwP//5KP1fcEHM69MnfoyNGROtPBctinn77bfj7R/ScShByXbMIsl99KPw3e/GL9kf/nD7dYYMiQRSUBDVgGYxzJ3b0Ev15MmR5CZNipueH300Lo6nG3I0paYGrrgiHpVyxRVttouSIGZxLWr8+IZ57tEX5te+Bv/xH7t+/+jRcStH//7RA0xJSdQsrFsXNQnz58eDTI85Jtbt3h2Ki+Mz6usbhr594x7FxtXamzfHa/fu2d3vfFdVFV2+FRdHNW1bUYKSJh11VAwbNsAf/xiv1dWwdGk8nXjp0vhiZ37RjzkmWg6ecEL0HA/R4OOYYxp64Rg5Mq51de8e15hGj46Ty+rV8OKLUVX5la/ATTfldv8ld8ziQZVnnRXXS6uq4v9jzpyoOu7VC4YPj1sznn46SuxVVTtuZ/jw6HbssceiL83mdO0aSaqgIP6vV65suL2jrCyqnYuLG36YFRRsP15cHMOWLfG+TZsa9qe0NH7Ybd0aNQRr1sSPvJEjo0Yiva10t2pLl8b+dekS35f+/SO2gQMj+VZWxncunZC7dYv4q6qihFlQEK0j+/SJqtKamvjszNfM8QED4lgNGtSw/6tXR6vhzZsjpsLCOAZm8aTy996LlsfubVeKVYKSXerVK3q72F2DB0eLw9//PloWzpkTX6DNm2H27LgpuaYmqnuGDIGf/Qw+/ensxS/5q0uXKNn07Rv/GwceuOM6l14ar3V18QOqqiqG7t3jpJ5e9sYb0V/m5s2xXpcuDYNZJKP33ouuyNI9XAwYED+i3KPhztKlcbKvq4uhvj5O8HV1Mb+2NrbdrVvE3L9/bLuuLrY/Z078n5eXR7JZvDg6qq6q2n67ZvG5w4dHHB98EO//4IOG2Hr0iISxZUt8ZqbCwthOcz11FBZGPIWF8QO0NT70ofhOH398697XWkpQ0uZ6945fw+neMzLV1sZJo1cvXUuQ3VdQEEmpqaq4ggI45JD2jynbampg1aooFaUfWwKRKKurI1mVlERJqr4+SnEbNkQCKi6OZJR+LSra/vtWVRXX9SorI7G5R4ItK4sSXmFhfMaSJbHOoYdGIm5rLUpQZjYB+CFQANzn7jc1Wt4VeAgYB6wCznX3BdkNVTqiwsJIYCKya0VFUSPRWJcuDdff0tJVfC29CbqkJPoW3Xffna/TtWvcqtKemm1mbmYFwJ3AKcABwCQzaxzmRcAadx8N3ArcnO1ARUSkc2nJfVAfB+a5+3x33wo8BpzZaJ0zgQdT408CJ5ipwkZERHafeTNX0szsHGCCu1+cmj4PONTdp2as81ZqnSWp6fdS66xstK0pwJTU5L7AnCzsQymwstm1kkmx504+x6/YcyOfY4dkxz/C3csaz2zXRhLufg/QggafLWdmFe4+vvk1k0ex504+x6/YcyOfY4f8jL8lVXxLgeEZ08NS85pcx8wKgT5EYwkREZHd0pIENQsYY2YjzawYmAjMaLTODOCC1Pg5wIveXN2hiIjILjRbxefutWY2FXieaGZ+v7u/bWY3ABXuPgP4KfCwmc0DVhNJrL1ktcqwnSn23Mnn+BV7buRz7JCH8TfbSEJERCQX9LgNERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJpHZ9HlSm0tJSLy8v36NtrFoVT/QYMGBAFiISkbak76vszGuvvbYy5w8szFReXk5FRcUebeOBBx4AYPLkyXsekIi0KX1fZWfMbGFT81XFJyIiiaQEJSIiidRsgjKz+81shZm9tZPlZma3m9k8M5ttZodkP0wREelsWlKCegCYsIvlpwBjUsMU4Cd7HpaIiHR2zSYod/8D8Rj3nTkTeMjDX4C+ZjYkWwGKiEjnlI1rUHsBizOml6Tm7cDMpphZhZlVVFZWZuGjRUSko2rXRhLufo+7j3f38WVlOzR5FxER2SYbCWopMDxjelhqnoiIyG7LRoKaAZyfas13GLDO3ZdnYbsiItKJNduThJk9ChwLlJrZEuBbQBGAu98FPAecCswDNgMXtlWwIiLSeTSboNx9UjPLHfhC1iISERFBPUmIiEhCKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgiKUGJiEgitShBmdkEM5tjZvPM7Momlk82s0ozez01XJz9UKWzePpp+POfcx2FiORaYXMrmFkBcCdwErAEmGVmM9z9nUarPu7uU9sgRulkvvpVGDgQ/vKXXEciIrnUkhLUx4F57j7f3bcCjwFntm1Y0pmtXg1//SusWpXrSEQkl1qSoPYCFmdML0nNa+yTZjbbzJ40s+FNbcjMpphZhZlVVFZW7ka40tHV1cG6deAOM2fmOhoRyaVsNZL4NVDu7gcBM4EHm1rJ3e9x9/HuPr6srCxLHy0dydq1DePPP5+7OEQk91qSoJYCmSWiYal527j7KnevTk3eB4zLTnjS2axZE69du8JvfxslKRHpnFqSoGYBY8xspJkVAxOBGZkrmNmQjMkzgH9kL0TpTNIJ6rTT4P33Yfbs3MYjIrnTbIJy91pgKvA8kXiecPe3zewGMzsjtdoXzextM3sD+CIwua0Clo5t9ep4nTgxXlXNJ9J5NdvMHMDdnwOeazTv2ozxacC07IYmnVG6BHXggXDQQfDMMzB1KnTvntu4RKT9qScJSZR0gurXD045Bf74R+jRA/r0gQkT4Ac/gGXLchujiLSPFpWgRNpLuoqvXz+YNg323x+WL4dFi+Dll+GKK+BHP4K33orEJSIdlxKUJMqaNVBSAt26xXDBBdsvf/FFOOEEuOaaKE2JSMelKj5JlDVroH//nS8//nj4/Ofhhz+M3iZEpONSgpJEWb06qvd25aabYOhQ+Oxn1R2SSEemBCWJsmZN8wmqd2+491549134yEeiIYWIdDxKUJIozVXxpU2YAK+8Ej1OHHssXHYZLFjQ1tGJSHtSgpJEaUkVX9q4cfC3v8Ell8B998Ho0XDqqXHf1A9+APPnt22sItK2lKAkUVpSxZepd2/48Y8jGV1+OSxdCo88Es3RR42CY45RFaBIvlKCksTYuhU2bWpZFV9jw4bBrbfCG29EKWzhQviv/4rXk06CX/86+/GKSNtSgpLEyOxFYk/tvTdcdRW89lp0mXT22dGwYvPmPd+2iLQPJShJjGwmqLQBA+CFF+DII2HKlJg+8UT49Kdh8mT47/9ueKRHRQUcfTRce21DLPX1UFWVvXhEpOWUoCQx0klhd6r4dqV373g67/PPw6WXRhVgRUU8b+q886JF4M03RxJ76y34z/+EkSPhqKOgb994/w03QE1NduMSkV1TV0eSGJn98GVbURGcfHIMafX1cNdd8PWvw+9+B6efDg8+CEuWxPWrpUvh/PNhxQr41rdgxoxIZMcdB126xLOqfvOb6JppwIBIcOXl2Y9dpLNSgpLEaIsqvl3p0iXunzrttChRnXVWzOvfHx5/fPt1f/GL6GLpxBMjCfXvH03cM5nFtqZMiYYZ3bq1z36IdFSq4pPEaKsqvuaMGAGf/GQkp505++y4EfiRR+J+q4KC6A/wgw+gshLefBO++U2YNQvOOAPKyuDcc+NmYhHZPSpBSWKkq/j69s1tHDtTUgKTJsXQWGkpfPjD0cv6iy/C00/Dk0/CE0/EzcMnnhj717s3fPGL0QNGfX1UJQJcfXWUwESkgRKUJMaaNdCrFxTm8X9lcXE0upgwAb73PbjjDrjlFnjuuSih1dfDU0/Bz34Wz7v6+c8b3nvNNbmLWySJVMUnidHSfvjyRc+ecOWV8QTgNWuiFeBTT0VLwTFjIjndcks88+raa+Huu3MdsUiy5PFvVeloWtMPXz7p2jUGiGtZ++4LX/0qXHQR/Nu/ReKqrIwm8K+8At/5TpTEnnkG6uqiSrGkJO7XevVVGDQomsHnm82bYz8/97l4TMrBB0dT/iFD4hgUFcV4376q7pSgBCWJ0dp++PLVgQfGPVlpRUVxveq66+C22+K61datkZwgGl9MnBjVhO++29Ba8Pzzo0Xh0KHRzD3JrQZnzYK//x1qa6Npfp8+cZ0ufZN0ppKS2KehQ2O9goJI8KWlMHBgNEApK4uk9s9/RrIbMwbGjo33bt7cMGzdGgl92DDo0aP5OGtqYO1aWL8+kmV5eXx+S9TWRtdaBQXx3vSPEtl9SlCSGGvWwP775zqK3CgpiXuspkyB7343TsT/+q+wYQPceGMkriOOiOtW8+dHdeAzz2y/ja5do/TRr9/2r+nx2lpYvjxKqoWFkRizNRQWRj+K69ZFYu3ePa4nDhwY+zBpUtS2ENMAABCqSURBVPTeMW4cTJ8e8a5bB3/5S7wWFkYyWb487j9btixely6N7VVVwcqVDS09M3XrBlu2tM3fpbg49sGsYejSZcfx2lpYtChe03r3jr9rSUkkyFGjGqqw04nZPfZt06YYNm6E6urYp+7d473duzcMxcWRPFevjuO1s5jcYzDb/v3du8e2i4pivY0bY3vFxfF/UlgYf481ayJRr10b+zFqFOy1V2wvve36+vgRMWFC2xx7UIKSBOmoVXytMWrUjteijj02TiQ9ezbM++Y3o2n7smUN17jWrt3+xFJZCXPnNswvKIDBg6O0VV8fpYWamjjRpccbD02VcHZ3vz7yke1LFX36wCc+0brt1NREiamyMvanvDxO4osXw9tvR4LIPBkXFsatAIsXtyyJFRTEibpXr0iO//d/kRjTJ+X0ibnxdEFB3FYwZkxsZ+nSeN+WLZF4Fi2KLrfWr2/4rHQ1ZklJlO569Ii/cdeu8fdesSJKgVVVDSXC6upIGP36xXpNxVVf35Cs6usjhs2bI476+h33Od14J1NJSRyHPn0iYS1f3vTxOuEEJSjpJDpLFd/uyExOECen8eNb/v50omnttZ26up0nr8yhtjZOsOkquU2bouS0YkX88Dj+eHj22dZ9dlOKiiLJDh68/fy9946hKQceuOefmxTpUtHuvremJhJeTU38bXv1imRUVxfJs6YmElPj6smNGyPRw/altbauVm5RgjKzCcAPgQLgPne/qdHyrsBDwDhgFXCuuy/IbqjSkVVVxa/DjtSKL0l296RWUBBDkq9vdSZ70njELKryiot3XFZYuOvvXs+eO/5Iag/NNjM3swLgTuAU4ABgkpkd0Gi1i4A17j4auBW4OduBSsfWlv3wiUh+akkJ6uPAPHefD2BmjwFnAu9krHMmcF1q/EngDjMz92zVYO9o06aoc4boQ03y28aN8aoEJSJp1lwOMbNzgAnufnFq+jzgUHefmrHOW6l1lqSm30uts7LRtqYAU1KT+wJzsrAPpcDKZtdKJsWeO/kcv2LPjXyOHZId/wh3L2s8s10bSbj7PcA92dymmVW4eysuFyeHYs+dfI5fsedGPscO+Rl/S7o6WgoMz5gelprX5DpmVgj0IRpLiIiI7JaWJKhZwBgzG2lmxcBEYEajdWYAF6TGzwFebMvrTyIi0vE1W8Xn7rVmNhV4nmhmfr+7v21mNwAV7j4D+CnwsJnNA1YTSay9ZLXKsJ0p9tzJ5/gVe27kc+yQh/E320hCREQkF/S4DRERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSaR2fR5UptLSUi8vL9+jbaxaFU/0GDBgQBYiEpG2pO+r7Mxrr722MucPLMxUXl5ORUXFHm3jgQceAGDy5Ml7HpCItCl9X2VnzGxhU/NVxSciIonUbIIys/vNbIWZvbWT5WZmt5vZPDObbWaHZD9MERHpbFpSgnoAmLCL5acAY1LDFOAnex6WiIh0ds0mKHf/A/GU3J05E3jIw1+AvmY2JFsBiohI55SNa1B7AYszppek5u3AzKaYWYWZVVRWVmbho0VEpKNq10YS7n6Pu4939/FlZTu0KBQREdkmGwlqKTA8Y3pYap6IiMhuy0aCmgGcn2rNdxiwzt2XZ2G7IiLSiTV7o66ZPQocC5Sa2RLgW0ARgLvfBTwHnArMAzYDF7ZVsCIi0nk0m6DcfVIzyx34QtYiEhERQT1JiIhIQilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIilBiYhIIrUoQZnZBDObY2bzzOzKJpZPNrNKM3s9NVyc/VBFRKQzKWxuBTMrAO4ETgKWALPMbIa7v9No1cfdfWobxCgiIp1QS0pQHwfmuft8d98KPAac2bZhiYhIZ9eSBLUXsDhjeklqXmOfNLPZZvakmQ1vakNmNsXMKsysorKycjfCFRGRziJbjSR+DZS7+0HATODBplZy93vcfby7jy8rK8vSR4uISEfUkgS1FMgsEQ1LzdvG3Ve5e3Vq8j5gXHbCExGRzqolCWoWMMbMRppZMTARmJG5gpkNyZg8A/hH9kIUEZHOqNlWfO5ea2ZTgeeBAuB+d3/bzG4AKtx9BvBFMzsDqAVWA5PbMGYREekEmk1QAO7+HPBco3nXZoxPA6ZlNzQREenM1JOEiIgkkhKUiIgkkhKUiIgkkhKUiIgkkhKUiIgkkhKUiIgkUouamYuI7Kn6+hhEWkoJSkTaxfz5sHo1uINZrqORfKAqPhFpF5s2QVUVzJuX60gkXyhBiUi72LIlXl98MbdxSP5QghKRNldXB9Wp5x289FJuY5H8oQQlIm1u2bKGa08vvRTjIs1RghKRNrdgQbyWlsKKFfDOOzkNR/KEEpSItLmFC+N18OB4VTWftIQSlIi0uXSC6tsXysvVUEJaRglKRNrcggVQVARdusBxx8HLL+umXWmeEpSItLmFC6Fbtxg//nhYswYefzy3MUnyKUGJSJtbsKAhQZ11Fhx2GHzmM3D33TkNSxJOXR2JSJuqr4dFixoSVI8e8MILcO65cOml8MwzcMIJMHZs9DRRUwNHHgkDB+Y2bsk9JSgRaVMffBA36Xbt2jCvRw/45S/hmmvgiSciSWUqKICTToKPfSzGe/aEAw+MJFZaCsXF6s+vM1CCEpE2lW7Bly5BpRUVwU03xbB4Mbz3XiSuujqYMQMeewx++9umt9mlC5SUxNCjB/TuDX36RBI0iwTWsyf06hVDz57xnq1b4/09e8b76usjeRYWxrySEqitjaGmJl67d4e994YhQ+K96Xl9+8bnVVdHN07poUuXmJ85dOsWr4WFDYnVPeIpLIwkXFcXnemuWxfzunaFAQNiXxqrq4vPyVWSXrwYnn02Yrzwwrb7HCUoEWlT6Zt0GyeoTMOHx5B22GHw7W/HeH09rF0Lb70Vw9q1URVYVQWbN0cntOvXx/wtW2L9NWsi4W3YEMPGjZEQiopimzU1bbKrzTKLk3pBQcSfbslYWBhJp6keNgYOhP79Y1lNTezbmjWRuAYPjkRZXR3b27IlXgsLI2H37BnbqK+P96dfG4/X1ESyLCqKbZaVxTY3boxl6fUgjnm6w9+TT1aCEpE8trMSVEt16RIn6KOPjmF3pE+u6RLH1q1x8i0sjJNyXV1Mb9kS89JDUVEkuEWLYPnyiCWdXNaujZN4t24NQ9eu8VlbtsSyzCFzXl1dlMLSJbb055aVRcJJz6ushCVL4rPSn92/fwzV1RHTunUNn19SEq+1tTF/48bYZ7OGElfj8XSJs7g4tvn++/G53bptX4JLH7vCQrjkEjjtNNhvv937e7SUEpSItKkFC+KEWlCQuxgaV4UVF0dMmdKljcb69IFhw9omLtm1FjUzN7MJZjbHzOaZ2ZVNLO9qZo+nlr9qZuXZDlRE8tPChdF7hEhrNZugzKwAuBM4BTgAmGRmBzRa7SJgjbuPBm4Fbs52oCKSnxYuhBEjch2F5KOWVPF9HJjn7vMBzOwx4Ewgsz/iM4HrUuNPAneYmbm3Xaf669bBn/4U45df3lafIiJ7auNG+MQnch2F5CNrLoeY2TnABHe/ODV9HnCou0/NWOet1DpLUtPvpdZZ2WhbU4Apqcl9gTlZ2IdSYGWzayWTYs+dfI5fsedGPscOyY5/hLuXNZ7Zro0k3P0e4J5sbtPMKtx9fDa32V4Ue+7kc/yKPTfyOXbIz/hb0khiKZBxhwLDUvOaXMfMCoE+wKpsBCgiIp1TSxLULGCMmY00s2JgIjCj0TozgAtS4+cAL7bl9ScREen4mq3ic/daM5sKPA8UAPe7+9tmdgNQ4e4zgJ8CD5vZPGA1kcTaS1arDNuZYs+dfI5fsedGPscOeRh/s40kREREckHPgxIRkURSghIRkUTK2wTVXPdLuWZmw83sJTN7x8zeNrMvpeb3N7OZZjY39dovNd/M7PbU/sw2s0NyuwfRi4iZ/d3MnklNj0x1ZTUv1bVVcWp+4rq6MrO+Zvakmf2fmf3DzA7Pl2NvZl9J/c+8ZWaPmlm3JB97M7vfzFak7odMz2v1sTazC1LrzzWzC5r6rHaK/ZbU/81sM/ulmfXNWDYtFfscM/tExvx2Px81FXvGsivMzM2sNDWdqOPeYu6edwPRWOM9YB+gGHgDOCDXcTWKcQhwSGq8F/Au0VXUd4ErU/OvBG5OjZ8K/AYw4DDg1QTsw1eBR4BnUtNPABNT43cBn0+NXwbclRqfCDyegNgfBC5OjRcDffPh2AN7Af8ESjKO+eQkH3vgaOAQ4K2Mea061kB/YH7qtV9qvF+OYj8ZKEyN35wR+wGpc01XYGTqHFSQq/NRU7Gn5g8nGrUtBEqTeNxbvI+5DmA3/zCHA89nTE8DpuU6rmZi/hVwEtF7xpDUvCHAnNT43cCkjPW3rZejeIcBvweOB55J/WOvzPjibvsbpL4Mh6fGC1PrWQ5j75M6yVuj+Yk/9kSCWpw6YRSmjv0nkn7sgfJGJ/lWHWtgEnB3xvzt1mvP2BstOwv4WWp8u/NM+tjn8nzUVOxEd3MHAwtoSFCJO+4tGfK1ii/9JU5bkpqXSKlql48CrwKD3H15atH7wKDUeNL26Tbg60DqkWoMANa6e21qOjO+bbGnlq9LrZ8rI4FKYHqqivI+M+tBHhx7d18KfA9YBCwnjuVr5M+xT2vtsU7M36CRzxIlD8iD2M3sTGCpu7/RaFHiY29KviaovGFmPYGngC+7+/rMZR4/WRLXzt/MTgdWuPtruY5lNxUSVR8/cfePApuIaqZtEnzs+xGdL48EhgI9gAk5DWoPJfVYN8fMvgnUAj/LdSwtYWbdgauAa3MdS7bka4JqSfdLOWdmRURy+pm7/yI1+wMzG5JaPgRYkZqfpH06EjjDzBYAjxHVfD8E+lp0ZQXbx5e0rq6WAEvc/dXU9JNEwsqHY38i8E93r3T3GuAXxN8jX459WmuPdZL+BpjZZOB04N9TCRaSH/so4ofNG6nv7jDgb2Y2mOTH3qR8TVAt6X4pp8zMiB42/uHuP8hYlNkt1AXEtan0/PNTrW0OA9ZlVJG0K3ef5u7D3L2cOLYvuvu/Ay8RXVnBjrEnpqsrd38fWGxm+6ZmnUA8Hibxx56o2jvMzLqn/ofSsefFsc/Q2mP9PHCymfVLlSJPTs1rd2Y2gajePsPdN2csmgFMTLWcHAmMAf5KQs5H7v6muw909/LUd3cJ0VDrffLguDcp1xfBdncgWqW8S7Se+Wau42kivqOIao3ZwOup4VTi+sDvgbnAC0D/1PpGPBjyPeBNYHyu9yEV17E0tOLbh/hCzgN+DnRNze+Wmp6XWr5PAuL+CFCROv5PEy2U8uLYA9cD/we8BTxMtBpL7LEHHiWul9UQJ8WLdudYE9d75qWGC3MY+zziukz6e3tXxvrfTMU+BzglY367n4+air3R8gU0NJJI1HFv6aCujkREJJHytYpPREQ6OCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJJCUoERFJpP8P56GoPDXCwDEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV5dn/8c9Fwo4sElQEyiK4gHWNiNZWBcUdtFoVV9yothZbf/VRax+1tLZ1qdZqn1qsgltdqxX3XbuoSKgbYpGIsokQUIIEgQSu3x/XOSSEQA7kJGeSfN+v17xyZjkz10zOmevc99xzj7k7IiIiSdMi1wGIiIjURAlKREQSSQlKREQSSQlKREQSSQlKREQSSQlKREQSSQlKREQSSQlKGjUzW15lWGtmX1cZP3UL1veqmZ1byzKtzOxKM5thZmVmNt/MnjGz4Zu5LTez/psbo0hzkZ/rAETqwt07pF+b2afAue7+Yj1v9hGgB3AG8HZq2lDgKOD56gubWb67V9RzTCJNjkpQ0iSZWQszu8zMPjazJWb2kJltnZrXxszuTU1famZTzGxbM7sG+DZwa6oEdmsN6z0EOBQY6e6T3X11anjW3S+qstynZnapmb0HlJlZxj8GzayTmd1tZiVmNtvMfm5mLVLz+pvZa2ZWamaLzezB1HQzs5vMbJGZLTOz981s1zodRJEcs1x1dVRQUOB9+vSp0zqWLFkCQNeuXbMQkTR277//Pr1796Zjx44sXLiQL7/8kn79+pGfn8/cuXNZs2YN/fr1o6SkhNLSUvr164eZsWLFCtq0aUNeXh4zZsyga9euFBQU1LiNefPmUVZWxk477VRrLHl5efTv35/8/HxatNjwt+DUqVMZNGgQbdq0WW/6J598wpo1a+jbty8VFRXMnDmT7bbbjoKCAmbNmkXbtm3ZbrvtcHdWrFhBhw4dKC0t5bPPPmPAgAHk5eWxcuVK8vPzadmy5ZYf0CzT91U2ZurUqYvdvdsGM9w9J8Pee+/tdTVhwgSfMGFCndcjTUPv3r39hRdecHf3nXfe2V988cV18z777DPPz8/38vJyv+OOO3y//fbzd999d4N1HHjggX777bdvdBvnnHOOn3TSSevGlyxZ4p06dfKOHTt669at14vljjvu2GS8gM+cOXO9aRUVFd6yZUv/4IMP1k277bbb/MADD3R399NPP93PO+88nzt37nrve+mll3zAgAH+xhtv+Jo1aza53VzR91U2BijyGvKEqvikSZo9ezbHHXccnTt3pnPnzuyyyy7k5eWxcOFCTj/9dA477DBOPvlktt9+e/7nf/6H8vLyjNbbtWtXFixYsG586623ZunSpUydOpVVq1att2yvXr02O+7FixdTXl5O7969103r3bs38+fPB+C6667D3Rk8eDCDBg3izjvvBGDo0KFceOGF/PCHP2SbbbZhzJgxLFu2bLO3L5IkSlDSJPXq1YtnnnmGpUuXrhtWrlxJjx49aNmyJVdddRXTp0/n9ddf58knn+Tuu+8GwMw2ud5hw4YxZcoU5s2bV2sMta2rJgUFBbRs2ZLZs2evmzZnzhx69OgBwHbbbcftt9/OZ599xp///Gd+8IMfUFxcDMDYsWOZOnUq06dP56OPPuL666/f7O2LJEmtCcrM7kxdeJ22kflmZn8ws2Ize8/M9sp+mCKb5/zzz+eKK65Yd6IvKSnh8ccfB+CVV17h/fffZ82aNXTs2JGWLVuuu0a07bbbMmvWrI2ud/jw4Rx88MEce+yxTJ48mdWrV1NeXs6bb765RXGuXr2alStXrhsATjzxRK644gq++uorZs+ezY033shpp50GwMMPP7wuOXbp0gUzo0WLFkyZMoXJkydTXl5O+/btadOmTY3XvUQak0w+wROBwzcx/whgQGoYA/yp7mGJ1M1FF13EiBEjGD58OFtttRVDhgxh8uTJAHz++eeccMIJdOzYkV122YUDDzyQ008/fd37HnnkEbp06cLYsWNrXPdjjz3G0UcfzWmnnUbnzp3p27cv9913H88999xmxzlo0CDatm27bpgwYQK33HIL7du3p1+/fhxwwAGccsopnH322QBMmTKFfffdlw4dOjBixAhuvvlm+vXrx7JlyzjvvPPo0qULvXv3pmvXrlxyySVbePREkiGjVnxm1gd40t03aLZqZn8GXnX3+1PjM4CD3H1B9WWrKiws9KKioi2JeZ2JEycCMHr06DqtR0Tqn76vsjFmNtXdC6tPz0YdQA9gbpXxealpNQUxxsyKzKyopKQkC5sWEZGmqkErqd19vLsXuntht24bNnkXERFJy0aCmg9UbU/bMzVNRERki2UjQU0Czki15hsClNZ2/UlERKQ2tfYPZmb3AwcBBWY2D7gKaAng7rcBTwNHAsXACuCs+gpWRESaj1oTlLuPqmW+Az/MWkQiIiKoJwkREUkoJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJShp0ioqYMUKyODB0SKSMEpQ0mTNmQP9+0P79pCXB3vtBQsXbtm63nwTRo6E/faD/feHM86A6dNjnju8/TZccw1861swZAh8+WX29qMxWLYMiothyhT4/PNcRyNNRa29mYs0Bu7w+9/Dxx/Dz38ObdrAkUfC0qWROMrKYv5hh8Grr0LnzpXve+mlGMrLwQyOPhq+8514PXUq/PrX8OijsM02sPvu8Z7HHoN774VDDoFp02BB6gloe+8N770Ho0bBk09CfhP7hi1YAOPGwaxZUFAQ06ZMgZkzK5dp0wZ++lO49FJo1SoSVo8euYlXGrcm9vWR5qisDM45Bx58MJLK3XdD374wYwY89xwMHRrLfec7cMwxkbiOPTaS12OPwX//G4mkVauoErzhhkg0rVvD669Dhw7wi1/AxRfHa4DFi+H66+Hhh6PUdNRRcMQRsO22cMcdcO65cYL+3e+2bJ/WroWioigFdu0KHTtCaSmUlERJZdq0KKXtuCMMHAjHHRfbBvjPf2DSpNi/sjLo1Qu++U3Yd1/Yfvv1t7N0aaxrxQrYeWfo2TMSSnFxbG/16sph1iy48cZ4vfvu8WNg9eoomY4eHe/t1Akeegh+9avY95UrI6H/8pcxX2RzmOeocr6wsNCLiorqtI6JEycCMHr06LoHJI3S6tWRIKZOhd/+NqrhLr4YnnkGJkyAM89cf/mHH4bTTov3mUFhIVx4IZx4Yvzy//pruOceuPlmWLMGzj8fzjorTrybY+xYuOUW2GknOPjgSIDvvRcn9a++isTRuTN07x5VkOXlcSLv2DHGp0yJZLQxffpAly5Rclm+PJLriSdGFeYLL8S+bbUVtGsX09Jf8z33jGrKTz6B99+HefPWX2+LFpEcN2bkyEjg/ftvev9ffx3++lfo1i0SVrt28MMfTgT0fZUNmdlUdy+sPl0lKGnU7r47Shr33QennBLTnnoqShddumy4/Pe+FyUogLZt44RcVdu2MGZMDHVx440wYECU4O67LxLEN78Jhx4aya5du4hxwYJIii1bxvuWLYP582H48Ihz0KBYrrQ0ElrXrpGc0iU5d/jwQ7jttkjI7dvDtdfC979fmVTLyqKU9OqrUe14112RYA48MGLadddY33//GyW2Hj1ifteukfjSQ7t2laW02uy/fwwQx/iqqyIJp/dTJBMqQUmjVV4eJZSuXeGtt6LUkETpEkn1ZJhtq1ZFY5CkXfeaPDkajlx77US22UbfV9mQSlDS5Nx7b1RV/eEPyU1OUP+JKa1164bZzuYqLIzS7JdfRkMTkUw16mbmy5dvur5cmq6Kimidt9de0UBBkisvL1o7fvFFriORxqbRJqiPP44L43Pm5DoSyYXrr4/PwJVXJrv0JOGww6JhSllZriORxqTRJqgddogLtnPmxMVdaT7uuQd+9jM46SQYMSLX0Ugmhg+Pv83tBmapm0aboAD69YvqgwsuUFc2zcWzz8LZZ8OwYdEaTaWnxqFXr2gFqGo+2RwZJSgzO9zMZphZsZldVsP80WZWYmbvpIZzsx/qhlq1ihsyX301LphL0/bWW3D88dE0+tFHk9soQGrWqVPcAyaSqVoTlJnlAX8EjgAGAqPMbGANiz7o7nukhr9kOc6N2n77aCV0zTUNtUXJhRkzojHEttvC00/HDa3SuLRtG41bli7NdSTSWGRSghoMFLv7LHdfDTwAjKzfsDbPqFFxAqt+V7w0DaWlcPjhUZ33/POw3Xa5jki2RNu28XfWrNzGIY1HJgmqBzC3yvi81LTqjjez98zsETPrlZXoMpTua+2llxpyq9JQfvMb+PRTePzx2rvYkeRq0yb+fvxxbuOQxiNbjSSeAPq4+27AC8BdNS1kZmPMrMjMiko21dHYZtptt+hZ+eWXs7ZKSYhPP41eyE8/PfqQk8ZLJSjZXJkkqPlA1RJRz9S0ddx9ibuvSo3+Bdi7phW5+3h3L3T3wm7dum1JvDVq0SI65HzpJbXma2quuCKq9nSNsfHLy4u++FSCkkxlkqCmAAPMrK+ZtQJOBiZVXcDMulcZHQF8mL0QMzN0aHSy+dFHDb1lqS9TpkSP2BdfHM2UpfFr21YlKMlcrQnK3SuAC4HniMTzkLt/YGbjzCx9m+RYM/vAzN4FxgKj6yvgjRk2LP6qmq/p+Pvf41f3pZfmOhLJljZtVIKSzGXUWay7Pw08XW3alVVeXw5cnt3QNk///vEr+6WX4sZdafzmzYvbCNSkvOlo2zZ6f9GjNyQTjboniarMoprvlVfW70B2+nT405/gX/+K+y/mzImqo0WLcherZGbuXD2Ftalp0ya+n7Nn5zoSaQyaTIKC6JDyiy/iwrp7JKL99oMf/AC+/e3o8r93bxg8OJ5kOmxYPO1TkmnePF17amrUkk82R5N6HtSJJ0YJ6re/jUdhv/RSPMzu1VejAcWHH8ZTSbfZJnpCv//+6HC0d2/Yd9/N29batQ33nJ/myD1KUMcck+tIJJt0L5RsjiaVoPLy4M9/jp4GfvnLeDT2K69EAtpzTzj66MplR46ESy6Jx3JffHFUAWba8eizz0afcO+8E++X7PviC1i5UiWopqZ16xhUgpJMNLkygBmMGwcvvgivvx7JaWO22ioS2euvw9/+Fr/an3kG/vOfTW/jN7+BFSvUQW19mpvqu0TXoJqefv1UgpLMNLkElTZsWFxnqs3ZZ8Ouu8JPfxrVfEceGdeyliypefn//Af+8Y/oSf3++3VjcH1J96uoElTT06+fSlCSmSaboDKVlwe/+120Klq4EH71q3io2sbuvfn976FDB/j1r+M6V22lLdkyKkE1XekSlH7cSW2a1DWoLTV8OLz/flxPat06mqPfcAOceSZ84xtR7bfTTjH/gQfiPquzzoLLL49S1N41duwkdTFvXvx4UM/lTc8OO8Dy5bB4MWSxxzNpgpp9CSpt110rH4B39dWRmI48MhpaXHBB3GPVv388z+ZHP4Ktt46qwAcfXP++K8mOuXPjJt28vFxHItm2ww7xd/Lk3MYhyacEVYP27WHChGj59+tfwwcfxP1SxxwTVX/pRz6MGhW/9B9/PO6Ml+zRPVBN19ChkaQuvBCWLct1NJJkquLbiKFDK58zBTBwIHzve+svM2JE3Ff13e9Cfn6Utrp3r6y2qKiI+vbhw+ORIAsXRg8W++wT92dtibIy+OyzSJKZNovfmMsvh08+iWrKuq4r2+bOhb32ynUUUh/atYN77oEDDoAf/xjuvDPXEUlSKUHVQYcOcS/Ua6/Bf/8bLZM+/zxet2gRw/PPR8OKqvLzo3pwl12iHj79CGyzSHA77hh/W7aMpvBDhsS2Xnoprn3NnVvZE8auu8byBxywefX5r70WNzRDlARHJugZye5RgkpSTJJd++0Hl10WNRSHHx432YtUpwRVR717wxlnbHz+11/HTcAzZ1Z2fPrss3Ht6sUXI6l07hzJbM2aSELVqz1atYrqxsmTIxndfHPcu/Xii5X3YrVqBaeeGrG0axfr69275qS1ahWcf36U+Fq3jhPFUUdF4kyCJUviJl214GvarroqPu+nnAKlpXDeebmOSJImIaekpqttWzj00BjShg6F666LkkL1qjV3KCmJobw8/j73XPSIcdFF8YuzXTsYOzaWLy2NLpzuvhvuuiuunVXVtWtlkmrfPra9fHmU8p5+OpLVccfF+848M5Jg797RSCRXdA9U89CqFbzwQpSexoyJa72XXprZ/YuSPbNmwbbbxvkhaZSgcqim6z5m0VfgNttUTqua3Krr1CmqAIcMiXu43norWhWWl8f1pQ8/rKxCXLQIbropro1973twxBGREPffP7p9uuSSSHhmMe+CC6IlY0P3OZhOUCpBNX1bbQVPPBE/uG6+GW69NUrzl18en+lce+qp+E6cckquI6kfn38O3/xm9EmaxGuBSlBNyNZbR33+pixbBv/+d1yzgkhGv/99VA3uv38kpHffhTvuiFaLu+4aiatv32jyXVISN1muWhWNP9INGZYujWtmHTrE+MyZ8cDBvfaKUtvmNMJI36SrElTzkJ8P//d/8JOfxEnyjjviGtWxx8K550bVeFlZNOZ56qno8eWSS6CwsH7j+uILOO206NbsW9/adLdpjdX118f+3Xdf1M4k7r5Dd8/JsPfee3tdTZgwwSdMmFDn9ciGVq92v+ce94ED3aOcVfPQrZt7x47x2sx9553dBw9ef5nCQvfnn695O2vXupeVrT/tZz9zz893r6io//2UhpPp9/Wrr9zHjXPv0GH9z1GbNu5HHln5eTvoIPennorP0Oefuz/xhPvChdmL95JL4jPdqpX72Wdnb71JsXChe9u2cRzN3K+8MnexAEVeQ57QfVBSo5Yt49fj++/Dm29Gg4xnnolrVIsXR3XhxIlRFXjGGdHzxtVXR28b7tGh7qxZMH58dB115JHw8svrb2Pt2ugVvlev9W/a1E26zVuHDvC//xufg3RHzg8+GLdpPPVUTL/hBigujurAbt3il/8xx0RjorffrlxXpt0puce6r746buOYNw9uuSW+Az/4QXzWZ8zIfB8qKuJzX5OysrjetmbNptfx1VcRy+LF8d05+eR4pt1xx0W16KJFUSNSUbHxfZozJ4ZFizY8FjfcEDUhf/5zPOnhT3+KxkmZcq//+9jMc9QhVmFhoRcVFdVpHRMnTgRg9OjRdQ9I6k1paVSRzJ8Pb7wBO+8c0y+7DK69NqomV6+OE8Suu0bSy8+PqkhpOrL9fV29OhLXs8/CHntEC9cf/Siq5r7//fjRM2UK9OgRt3Skh1694r0rV8YJuqwsEtAbb8R6O3SI5d59N5JSu3ZxP+Mhh0RL2XfeicZPffvG9L59o5HB0qXRp+cjj0RV5YIF8aDU730vll+yJD7TL7wQ2y4oiGrygw6KKs1ddokfZWVl0Yjq+uujFXBaly5xe8rLL6//RPD8/PhhuOuu8YPvuOOiGv7886Nj67T9948fjoMHx20m3/1uLHvvvbHOYcMiQS9cGF269eoVCX///eM6eN++sZ6VK6O69aab4n7MRx+t+//SzKa6+waVtroGJfWuUyd48sm4dnDYYfGrFCI5XXBBPAH5kEPg4IMru406++zcxSuNQ6tWcPrpMaQNHhz3z910U1yjuuCCaAjw4YfRpH3VqprX1bNnlCS+8514ssFTT8VNxH36xPyf/CQaIT3+eCSR6qWfFi0qP7stWsSPrD33jGSVbnEL0Tp2zJi4cf+116KF7l//GvPy86MF49dfR6npxBPj+u3q1dFoasSISHTl5XF/5aefxv4sXgzTp0dp8+GHo0S5dGkk2muvjZa8JSVRIjzwwLi1ZNWquL3lf/83tn3wwRHT1VfHcf3ud2MdzzwTrYMhEmpeXrQCLiuLxhX1fa+iSlDSYKZMgXPOiS/TmjWRlJ5+OqoTFy2KKoeCgvhiH3BA5ePBpWloqO+re5xA0w120tasiZatCxbEk33TQ+vWlTfGp733XpRo0tNWroxGP+mSytq1kSA++SSqsj/7LD67PXpEiSPdwMc9lsnPj5qC9u3XbzDkHlWVb7wRt3589llsa+zYWM/mWLs2Etedd8a2xo1bvzXw11/DbbdFld+hh0ayqtq0/M03I4mffXZlU3/3iOuFF2DatEi+LVtGYho2LHs90GysBKUEJQ1u9er4kqRbBkrzoO+rbIyq+CQxWrWq7HBXRGRj1IpPREQSKaMEZWaHm9kMMys2s8tqmN/azB5MzZ9sZn2yHaiIiDQvtSYoM8sD/ggcAQwERpnZwGqLnQN86e79gZuAa7MdqIiINC+ZlKAGA8XuPsvdVwMPANUbF44EUo0ReQQYZpa0JwyJiEhjUmsrPjM7ATjc3c9NjZ8O7OvuF1ZZZlpqmXmp8Y9Tyyyutq4xwJjU6E7AZtybvVEFwOJal0omxZ47jTl+xZ4bjTl2SHb8vd19g4cDNWgrPncfD4zP5jrNrKim5omNgWLPncYcv2LPjcYcOzTO+DOp4psPVO1XumdqWo3LmFk+0AlYko0ARUSkecokQU0BBphZXzNrBZwMTKq2zCTgzNTrE4CXPVd3AIuISJNQaxWfu1eY2YXAc0AecKe7f2Bm44gu0icBdwD3mFkx8AWRxBpKVqsMG5hiz53GHL9iz43GHDs0wvhz1tWRiIjIpqgnCRERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSSQlKBERSaQGfR5UVQUFBd6nT586rWPJkniiR9euXbMQkYjUJ31fZWOmTp26OOcPLKyqT58+FBUV1WkdEydOBGD06NF1D0hE6pW+r7IxZja7pumq4hMRkURSghIRkUSqNUGZ2Z1mtsjMpm1kvpnZH8ys2MzeM7O9sh+miIg0N5mUoCYCh29i/hHAgNQwBvhT3cMSEZHmrtYE5e7/IB7jvjEjgbs9vAl0NrPu2QpQRESap2xcg+oBzK0yPi81bQNmNsbMisysqKSkJAubFhGRpqpBG0m4+3h3L3T3wm7dNmjyLiIisk42EtR8oFeV8Z6paSIiIlssGwlqEnBGqjXfEKDU3RdkYb0iItKM1dqThJndDxwEFJjZPOAqoCWAu98GPA0cCRQDK4Cz6itYERFpPmpNUO4+qpb5DvwwaxGJiIigniRERCShlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKBERCSRlKAkca67Du6/P9dRiEiu1fpEXZGG9n//Bx07wqhNPstZRJo6laAkcZYtg/ffh0WLch2JiOSSEpQkinskKIBXX81pKCKSY0pQkigrVsCaNfH6pZdyG4uI5JYSlCRKuvQE8PLLuYtDRHJPCUoSpbQ0/u6zDxQXw5w5uY1HRHInowRlZoeb2QwzKzazy2qYP9rMSszsndRwbvZDleYgXYI67rj4q1KUSPNVa4Iyszzgj8ARwEBglJkNrGHRB919j9TwlyzHKc1EOkHtvz9066YEJdKcZVKCGgwUu/ssd18NPACMrN+wpLlKV/F17gwHHwxPPAGjR8OFF8LMmTkNTUQaWCYJqgcwt8r4vNS06o43s/fM7BEz61XTisxsjJkVmVlRSUnJFoQrTV26BNWpE5x1FmyzDbzyCtx5Jxx4IHz0UW7jE5GGk61GEk8Afdx9N+AF4K6aFnL38e5e6O6F3bp1y9KmpSlJl6A6doTDD4cZM2D2bHjrLaioiFKVSlIizUMmCWo+ULVE1DM1bR13X+Luq1KjfwH2zk540tykS1BbbbX+9F13jfuiVq+GvfeGW26pvF9KRJqmTBLUFGCAmfU1s1bAycCkqguYWfcqoyOAD7MXojQny5ZBu3bQsuWG8775TZg8GfbbD8aOhSFDQDXFIk1XrQnK3SuAC4HniMTzkLt/YGbjzGxEarGxZvaBmb0LjAVG11fA0rSVlkb13sb06wfPPhu9nU+bBkcfDWVlDRefiDScjHozd/engaerTbuyyuvLgcuzG5o0R8uWRQOJTTGDk0+GNm3g+OPhpJPg73+HfPXNL9KkqCcJSZRlyzZdgqrq2GPhj3+Ep56CM86A8vL6jS3XvvoKrrkGnn669mVFmgL95pREqa2Kr7rzz4/3XHYZrFwZVX+tW9dffNlWXAwdOsB22218meXL4W9/i338/POYdsUV8ItfQF5ew8QpkgsqQUmiZFLFV92ll8LNN8Njj8W9UvffH72iz5wZJ/YXX4QvvqifeDclHcOrr8Knn1ZOLyuD8eOjkceAAbDjjnFdrbqHH46GIZ06xc3KvXrBa6/BuedGSeqII2DhwgbaGZEcUAlKEmVzS1BpY8fC1lvD//4vnHJKXKdyX3+Z3XeHG26AQw7JTqxVzZoFH3wQ92oVF0cPGP/+N6xdW7nM0KEwaBDccw8sXRpN56+7Dv7612jsceutUSIEmDcPzj4beveOfdpvPzj0UGjRAr7zHdh3X/jRjyKBTZgARx21YUxffhnrPv74TZfQMrFmTST7ww7b/B8QaRUVMcimrVwZt1Q891xUYw8dmuuIckcJShJlS0pQaaedFsnp1Vej1LTDDpGUSkth6tQotRx6aCSDggL47LNIFCtWxPWrDh3i/qt+/SKRDBwYQ69ekfA2Zto0GDwYvv66ctruu0fJbuedoXt3ePPN6A3jtdfgu9+Fiy6K/gbNIimNGgUXXBDVeT/9Kfz4x3EynzQp4qnu3HPj/aecEvtz2GHw//5f3CM2b15cp7r22ti/Bx+M3jjqUh14660R0z77wAsvbP7/6N//hilT4j62KVOierJnz02/Z80a+PBDKCqC+fPjWO6+e/xfN/X/yNSnn8LHH8eTm3fZBfbYI6YvWwZ33BFJf8cd676dzTFxYvzwWL48xm+/Pa6xNtckZV79Z2YDKSws9KKiojqtY+LEiQCMHj267gFJzq1dGy3xfv5zGDcu++tfuRJ+9zu46SZo2zYSx9Zbx31X+flR9VZaGtVyVR83v/320WntTsom09UAABZMSURBVDttuM7ly+OkvXRpVMl16BDdM22/fc37t2JFLFNdRUUk2AcfjL/33gu/+lVca9qUVatif26+ufL6VNrRR0fivPJK+OUv47iWlETprrQ0jscxx0RJLh3fyy/H8OabcOSRkfQWLIjk0Lt39Oyx115RCly5MlpSDhiw/n5Mnx5J5aOPoFWraNxxyy1w3nkT2XpruOGG0eTlwW9/G30stqjhQsPbb0fSnjFjw3kHHRT7nE4oEMsdf3zE2qVL7NNVV8Gee1bGlZdXmdj+8If4kVDVCSdEIhg3Lo5lp05RXXzEEZv+H6StXg133x3//2OO2bwkunZtlJR//evYv0svjdLxYYfBJ5/E56Ffv7i+2qsXtG9f+d6KCnjooUhu3bvH53G77So/ayNHbvzHSXl5bHP27Phx1qVLfHa33TbeX1oa7+3SJfZr0CDo2jWqlt94I+Ydc0zm+7kxZjbV3Qs3mOHuORn23ntvr6sJEyb4hAkT6rweSYbSUndwv/76XEfiXlLi/o9/uP/pT+4FBe6DBrkvX77+MmvXup92mnuLFu4vv1z3ba5e7X788XEMdtrJfeXKzN+7cqX7vfe633ij+8MPu0+bVhnjKae45+W5jxnj3r59rD89tGzp/otfuD//vPuee8a0vDz3/v3j9U9+4n7SSe6tW7vPnOn+2GPu+fnrr+O449zfecf91lvdv/GNyulVlzvhBPfbb4/v6yefuB91VEw/9NBY59//HsPjj7v/9rfurVq5b7+9+x13uE+f7v7VV+5TprjfcIN7167uZnHsX3/d/a234n+0zTbuF1zgfvLJ7l26xPqPOMJ98ODYz513dr/zzthfcB850v3VV+NYXXWVe4cOMX2ffSKO3XeP/+1PfxrbWLOm5mNfXu7+9NPuO+5Yub/f/nZM++c/3d94w/3rryuXr6hwnz8//jfu7kVFcRzA/dxz43OQ9vnn8VmoerzBfdtt3ffYw33oUPc+fWJav34xvfqy++wT2/jb32L5YcPc338/PjPHHhvL9Ojh3qlTHNfq768+dO5c+Xq//TbnE75xQJHXkCeUoCQx5s6NT+T48bmOZH3PP195Qly6NJLRpZfGCQHcx43L3rZWrYoT6DvvZG+dS5fGScwsktXbb7t/8YX7ggXuo0ZVnmy+8Q33u++OZLBmjfuPflQ57+qrK9f31lvut98eifAXv6g8sYP7/vvHOmbMiHWsXRv75L7+93XtWvfbbnNv167mk+BRR8WPhJp88YX7xRdXbtfMvW/fSKBpX37pfsUV7j17uh94YCy/++6V6z/jjEgsVS1cGAkrnYiWL3c/9dRIUhAn8H793L/5TfchQ+JEP3iwe5s2MX/AAPcnn4z96tZt/f3p2NH9zDPdR4+OZJo+0e+1V7zu0sX9llsqk1ZVX37p/sQT7o8+Gj9CrrnG/Zxz3I8+2v1b34rk9vjjlcd7zhz3d991Ly52v+++SNzpOHr3ju23bOm+224x7Q9/qNzW6tXx/qIi9w8/dP/ssxh/7z33Z591/93v3M8/P35E/utf6yfeuthYglIVnyTGBx9E1cwDD8TNt0kyblxUGaXl58OwYVENdfrpNVdTJcmCBXGNrKbrWU8+GdVI550XVXZp7lHN+M9/xrWwqvOqWrQI7rorqpYOPHDjVVs1fV8XLYrrS+ntQVQLDhpUexXZ8uXxWXn99WjV2L37ppd3j4YHs2bFdb9M/2eLF0cry3//O6ory8oqhzZtoqqxsDCuLaZvcVi2LDo4do/lJk2CRx+NeUcdFcfqww+jOnTYMPjJT7b82mttvvgirr/utBOMGBGNZ3784zh248dHY5xc21gVnxKUJMYbb8SF/6efzrzev6GsXRsnwRYtoiHC4MFx/Uoy19y/r+kWjEnp8eTrr+NabBJsLEEl5FCJrP8sqKRp0SIuYotsqaQkprSkJKdNSXjFhDQnVZ8FJSKiBCWJkeQSlIg0PCUoSYx0glIJSkRACUoSJF3FV9ONrCLS/ChBSWIsWxZ3s6uHbhEBJShJkC3tKFZEmiYlKEmMunQUKyJNjxKUJMbmPE1XRJo+JShJDFXxiUhVSlCSGKriE5GqlKAkMVTFJyJVKUFJYpSWqgQlIpWUoCQR1qyJxxKoBCUiaRklKDM73MxmmFmxmV1Ww/zWZvZgav5kM+uT7UClaVM3RyJSXa0JyszygD8CRwADgVFmNrDaYucAX7p7f+Am4NpsBypNmzqKFZHqMnlCyWCg2N1nAZjZA8BIYHqVZUYCV6dePwLcambm9fg0xLKyeAIrwHHH1ddWpKF89VX8VQlKRNIySVA9gLlVxucB+25sGXevMLNSoCuwuOpCZjYGGJMaXW5mM7Yk6GoK4KzFtS+WSAVUO0aNSL3E/r3vZXuNG6VjnxsFZ52l72uOJDn+3jVNbNBnPLr7eGB8NtdpZkU1PSq4MVDsudOY41fsudGYY4fGGX8mjSTmA72qjPdMTatxGTPLBzoBS7IRoIiINE+ZJKgpwAAz62tmrYCTgUnVlpkEnJl6fQLwcn1efxIRkaav1iq+1DWlC4HngDzgTnf/wMzGAUXuPgm4A7jHzIqBL4gk1lCyWmXYwBR77jTm+BV7bjTm2KERxm8q6IiISBKpJwkREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUmkBn0eVFUFBQXep0+fOq1jyZJ4okfXrl2zEJGI1Cd9X2Vjpk6dutjdu1WfnrME1adPH4qKiuq0jokTJwIwevTougckIvVK31fZGDObXdN0VfGJiEgi1ZqgzOxOM1tkZtM2Mt/M7A9mVmxm75nZXtkPU0REmptMSlATgcM3Mf8IYEBqGAP8qe5hiYhIc1drgnL3fxBPyd2YkcDdHt4EOptZ92wFKCIizVM2rkH1AOZWGZ+XmrYBMxtjZkVmVlRSUpKFTYuISFPVoI0k3H28uxe6e2G3bhu0KBQREVknGwlqPtCrynjP1DQREZEtlo0ENQk4I9WabwhQ6u4LsrBeERFpxmq9UdfM7gcOAgrMbB5wFdASwN1vA54GjgSKgRXAWfUVrIiINB+1Jih3H1XLfAd+mLWIREREUE8SIiKSUEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSEpQIiKSSBklKDM73MxmmFmxmV1Ww/zRZlZiZu+khnOzH6qIiDQn+bUtYGZ5wB+BQ4F5wBQzm+Tu06st+qC7X1gPMYqISDOUSQlqMFDs7rPcfTXwADCyfsMSEZHmLpME1QOYW2V8Xmpadceb2Xtm9oiZ9appRWY2xsyKzKyopKRkC8IVEZHmIluNJJ4A+rj7bsALwF01LeTu49290N0Lu3XrlqVNi4hIU5RJgpoPVC0R9UxNW8fdl7j7qtToX4C9sxOeiIg0V5kkqCnAADPra2atgJOBSVUXMLPuVUZHAB9mL0QRaSrccx2BNCa1Jih3rwAuBJ4jEs9D7v6BmY0zsxGpxcaa2Qdm9i4wFhhdXwGLSOP0ySfwn//kOgppTGptZg7g7k8DT1ebdmWV15cDl2c3NBFpSsrKYPly+Phj2GGHXEcjjYF6khCRBrF6dfx9/vncxiGNhxKUiDSI8vL4qwQlmVKCEpEGkU5QL79c+VpkU5SgRKTeff01rFkDHTrAsmUweXKuI5LGQAlKROpduuOYbbaBFi1UzSeZUYISkXqXTlDt2sG++ypBSWaUoESk3i1aFH9btoThw2HKFFiyJLcxSfIpQYlIvUuXoFq2hJEjo0eJM89UYwnZNCUoEal36RJUq1aw555w223w1FMwejSsXZvT0CTBMupJQkSkLkpKwAzy8mJ8zJio4vvZz+Dzz+GnP4XDDosGFFWVlcHChZHgFi6EpUvh0ENh++0bfh+k4SlBiUi9W7QIWrdef9pll0Wjid/+Fo48Enr2hMGDYZddYPp0eP31SErVtWsHl14aSa1du5j29dfw61/DHnvAd78bybAxmz0bfvc76N8fvv/9DY9dNr38MvTpA/361d82tpQSlIjUu5IS6Nt3/WlmcNFFcMEF8Le/wd//Hp3JPvponCyHD4eBA2HbbWPYZpsogV1zDVx1Fdx+O/zmN/Dtb0dSSndEe/jhscyOO8Z9VzVZsiTe/9FHMGdO3KNVUABbbRX9BS5fHu8tKID994dTT80s6aV7a9/SBLloEdx6K1x/PVRUxPD738Nxx8E778B//wsHHhjxDBkCHTvCV1/BP/8Z+7/PPnHc1qyJKtT33oNjjoHCwppj+utfY11t28Yx++EP4f33Y1udOkH37vG/2G679d9fUQGPPBLbOfXULdvXTJjnqP/7wsJCLyoqqtM6Jk6cCMDo0aPrHpCI1JvBg+GAAyay2261f1/Ly6Mxxab8859w8cVQVBRJq317uOuuKHn8/OeRYCCSTKtWUXW4885xsl6zBq69FkpLoUcP6NUL8vNh8eI42W+1VawvXb345ZdwyCGR0AoK4kbjVq3iBJ6fHyfruXPhL3+BO+6I+PfYI5LrdtvFsNdesNtukajvvTdaMQ4cCLvvHqW/2bPhlVfgpZfimtzJJ8N110VJ8tJL4YMPYtkddoAXXoiYNqZLl9jHZcsqp+28c/wPvvEN2HXXqE59++34u99+sS9PPFG5PzWtc9CgGLp3j2P9yScwdGjEXFdmNtXdC6tPVwlKROrdokVxUs9EbckJotQ0eTLcdx889lhU7+28c8w76aQ4ac6fDwsWxAm3vBzeeitO9gBHHx1Vi4MGbXo7a9fC+PFwySUblgAhEl+6kUeLFpEAt902SiD33htJMK19+0hGa9dGUnzkkfWfj9W/P1x+eSSnXXeNab16RYmooqLyuKxeHUnq448jCeXlwQEHROOT116Dhx6KZUeNikT56KMx7ZVX4pisXRuJKD8fBgyAxx+PBPXww/DGG3Gf2j77RIL+7DOYOROmTYsk+eCDcR1wyBC48UYYMYJ6pRKUiNS79u3h4osnssMOuf2+zpsXJ9h0AsjUnDlw992RZDt2jCRRWhp/27SJUtexx0YJpapVqyIpvPUW/Pvf0LkznHYa7LRTlPKmT4/39uq18erIbCovj9LbE09E0vnjHzeMeVPcY787dcrudT6VoEQkJ8rKYMWKzEpG9a1nzxg21ze+EVWHm6t167iG069flIyq6tAhqt0aUsuWcU1t//237P1mkWQbiu6DEpF6lb5JN9MqPpE0JSgRqVdVuzkS2RxKUCJSr6p2cySyOZSgRKReVe3mSGRzKEGJSL1SCUq2lBKUiNSrRYuip4J0P3wimVKCEpF6VVIC3brlOgppjJSgRKReLVoU/eiJbK6MEpSZHW5mM8ys2Mwuq2F+azN7MDV/spn1yXagItI4qQQlW6rWBGVmecAfgSOAgcAoMxtYbbFzgC/dvT9wE3BttgMVkcZJJSjZUpl0dTQYKHb3WQBm9gAwEpheZZmRwNWp148At5qZeT129FdaCv/6V7z+0Y/qaysiUlfLlytByZaptbNYMzsBONzdz02Nnw7s6+4XVllmWmqZeanxj1PLLK62rjHAmNToTsCMLOxDAbC41qWSSbHnTmOOX7HnRmOOHZIdf29336AiuEE7i3X38cD4bK7TzIpq6gW3MVDsudOY41fsudGYY4fGGX8mjSTmA72qjPdMTatxGTPLBzoBS7IRoIiINE+ZJKgpwAAz62tmrYCTgUnVlpkEnJl6fQLwcn1efxIRkaav1io+d68wswuB54A84E53/8DMxgFF7j4JuAO4x8yKgS+IJNZQslpl2MAUe+405vgVe2405tihEcafsyfqioiIbIp6khARkURSghIRkURqtAmqtu6Xcs3MepnZK2Y23cw+MLOLUtO3NrMXzGxm6m+X1HQzsz+k9uc9M9srt3sQvYiY2dtm9mRqvG+qK6viVNdWrVLTE9fVlZl1NrNHzOy/Zvahme3XWI69mf0k9ZmZZmb3m1mbJB97M7vTzBal7odMT9vsY21mZ6aWn2lmZ9a0rQaK/frU5+Y9M3vMzDpXmXd5KvYZZnZYlekNfj6qKfYq8/6fmbmZFaTGE3XcM+bujW4gGmt8DPQDWgHvAgNzHVe1GLsDe6VebwV8RHQVdR1wWWr6ZcC1qddHAs8ABgwBJidgHy4G/go8mRp/CDg59fo24ILU6x8At6Venww8mIDY7wLOTb1uBXRuDMce6AF8ArStcsxHJ/nYA98B9gKmVZm2Wcca2BqYlfrbJfW6S45iHw7kp15fWyX2galzTWugb+oclJer81FNsaem9yIatc0GCpJ43DPex1wHsIX/mP2A56qMXw5cnuu4aon5ceBQoveM7qlp3YEZqdd/BkZVWX7dcjmKtyfwEjAUeDL1wV5c5Yu77n+Q+jLsl3qdn1rOchh7p9RJ3qpNT/yxJxLU3NQJIz917A9L+rEH+lQ7yW/WsQZGAX+uMn295Roy9mrzjgPuS71e7zyTPva5PB/VFDvR3dzuwKdUJqjEHfdMhsZaxZf+EqfNS01LpFS1y57AZGBbd1+QmvU5sG3qddL26ffA/wBrU+NdgaXuXpEarxrfuthT80tTy+dKX6AEmJCqovyLmbWnERx7d58P3ADMARYQx3IqjefYp23usU7M/6Cas4mSBzSC2M1sJDDf3d+tNivxsdeksSaoRsPMOgB/A37s7suqzvP4yZK4dv5mdjSwyN2n5jqWLZRPVH38yd33BMqIaqZ1EnzsuxCdL/cFtgfaA4fnNKg6Suqxro2ZXQFUAPflOpZMmFk74GfAlbmOJVsaa4LKpPulnDOzlkRyus/dH01NXmhm3VPzuwOLUtOTtE/fAkaY2afAA0Q1381AZ4uurGD9+JLW1dU8YJ67T06NP0IkrMZw7A8BPnH3EncvBx4l/h+N5dinbe6xTtL/ADMbDRwNnJpKsJD82Hcgfti8m/ru9gT+Y2bbkfzYa9RYE1Qm3S/llJkZ0cPGh+5+Y5VZVbuFOpO4NpWefkaqtc0QoLRKFUmDcvfL3b2nu/chju3L7n4q8ArRlRVsGHtiurpy98+BuWa2U2rSMOLxMIk/9kTV3hAza5f6DKVjbxTHvorNPdbPAcPNrEuqFDk8Na3BmdnhRPX2CHdfUWXWJODkVMvJvsAA4C0Scj5y9/fdfRt375P67s4jGmp9TiM47jXK9UWwLR2IVikfEa1nrsh1PDXEdwBRrfEe8E5qOJK4PvASMBN4Edg6tbwRD4b8GHgfKMz1PqTiOojKVnz9iC9kMfAw0Do1vU1qvDg1v18C4t4DKEod/78TLZQaxbEHfgH8F5gG3EO0GkvssQfuJ66XlRMnxXO25FgT13uKU8NZOYy9mLguk/7e3lZl+StSsc8AjqgyvcHPRzXFXm3+p1Q2kkjUcc90UFdHIiKSSI21ik9ERJo4JSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUkkJSgREUmk/w8X4HbJ9ItxMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QY_YzfCoVkI"
      },
      "source": [
        "L2 Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9TVqno83vnWh",
        "outputId": "634464d5-ef63-451d-b819-49716c48062e"
      },
      "source": [
        "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
        "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
        "\n",
        "plotResults(hist_loss_D, hist_hits_D)"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k0Kp66cvnWh",
        "outputId": "64cfa418-991c-4cb7-c58e-d063cfd21328"
      },
      "source": [
        "accuracyAll(models_D)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Model 0\n",
            "Task 0: Acc 0.85% | Gr acc 0.7 | Ugr acc 1.0\n",
            "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:825: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.num_layers, self.dropout, self.training, self.bidirectional)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:822: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:915.)\n",
            "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Task 2: Acc 0.65% | Gr acc 0.31 | Ugr acc 1.0\n",
            "\n",
            "Model 1\n",
            "Task 0: Acc 0.85% | Gr acc 0.7 | Ugr acc 1.0\n",
            "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
            "Task 2: Acc 0.65% | Gr acc 0.31 | Ugr acc 1.0\n",
            "\n",
            "Model 2\n",
            "Task 0: Acc 0.85% | Gr acc 0.7 | Ugr acc 1.0\n",
            "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
            "Task 2: Acc 0.65% | Gr acc 0.31 | Ugr acc 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17U8oYfQTlma"
      },
      "source": [
        "torch.max(fishers[0]['encoder.embedding.weight'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atj4m87evnWi"
      },
      "source": [
        "## Transfer D: Representation ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwQE8R-QvnWj"
      },
      "source": [
        "## Transfer E: DynaMoE\n",
        "\n",
        "1. Create DynaMoe network functions:\n",
        "2. Decider Network\n",
        "3. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysf64peLvnWj",
        "outputId": "1026a89f-ad1e-47e9-9689-37651e524eb3"
      },
      "source": [
        "class DynaMoE(nn.Module):\n",
        "    def __init__(self, gating, experts):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.gating = gating\n",
        "        self.experts = experts\n",
        "        self.activatedExperts = 1\n",
        "    \n",
        "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio = 0.5):\n",
        "        #seqs = [seqs len, batch size]\n",
        "        #seqs_len = [batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        \n",
        "        # Decide which expert to use\n",
        "        gatings = self.gating(seqs, seqs_len)\n",
        "\n",
        "        # gating = [batch_size, n_experts]\n",
        "        \n",
        "        # @TODO: Probabilistic vs argmax?\n",
        "        network_ids = argmax(gating, dim=1)\n",
        "        \n",
        "        outputs = torch.empty(seqs.shape)\n",
        "\n",
        "        for b in range(len(batch_size)):\n",
        "          outputs[:,b] = experts[network_id](seqs[:,b], seqs_len[b], teacher_forcing_ratio)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "    def init_expert(self):\n",
        "        # Get new expert\n",
        "        self.experts.append(init_expert())\n",
        "        # Update gating network\n",
        "        self.gating.add_outputUn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-123-57aabb6ddb7c>, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-123-57aabb6ddb7c>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    network_id =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFB0iAdSvnWk"
      },
      "source": [
        "class Gating(nn.Module):\n",
        "    def __init__(self, input_dim, embed_dim, n_experts, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_experts = n_experts\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "        \n",
        "        self.rnn = nn.GRU(embed_dim, n_experts)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, seqs, seqs_len):\n",
        "        \n",
        "        # seqs = [seq len, batch_size]\n",
        "        # seqs_len = [batch_size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(seqs))\n",
        "        \n",
        "        # embedded = [seq len, batch_size, embed_dim]\n",
        "\n",
        "        packed_embedded = nn.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
        "\n",
        "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
        "\n",
        "        outputs, hidden = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "\n",
        "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
        "        # hidden = [n layers * num directions, batch size, n_experts]\n",
        "        # @TODO: use outputs together with hidden for expert selection\n",
        "\n",
        "        hidden = hidden.squeeze()\n",
        "\n",
        "        # hidden = [batch_size, n_experts]\n",
        "\n",
        "        return F.softmax(hidden, dim=1)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4hPup6qpRWp"
      },
      "source": [
        "def init_expert():\n",
        "  attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "  enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "  dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "  new_expert = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "  new_expert.apply(init_weights)\n",
        "  return new_expert"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ol_xM3bZVY9u"
      },
      "source": [
        "N_EXPERTS_START = 1\n",
        "\n",
        "# If error stays within ALLOWED_ERROR_VARIANCE for N_EPOCHS_UNTIL_NEW_EXPERT\n",
        "# epochs, then a new expert is initialized\n",
        "N_EPOCHS_UNTIL_NEW_EXPERT = 300\n",
        "ALLOWED_ERROR_VARIANCE = 0.1"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxpPhG4bS9QN"
      },
      "source": [
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
        "expert = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
        "\n",
        "gate = Gating(INPUT_DIM, ENC_EMB_DIM, N_EXPERTS_START)\n",
        "\n",
        "model = DynaMoE(Gating, [expert,])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPnc_OgdhM67"
      },
      "source": [
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuFUOzDwjZgI"
      },
      "source": [
        "def fit_dynamoe(model, task_id, epochs, step_size_evaluation, clip ):\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
        "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
        "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
        "    # [task_id, dataset, evaluations]\n",
        "\n",
        "    loss_counter\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
        "        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), SAVENAME)\n",
        "\n",
        "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
        "            idx = epoch//STEP_SIZE_EVALUATION\n",
        "            for other_id in range(task_id + 1):\n",
        "                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)\n",
        "                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)\n",
        "                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)\n",
        "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
        "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
        "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
        "\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "        \n",
        "    return total_loss, total_hits"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}