{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnant-urt22Y"
   },
   "source": [
    "# AGL Autoencoder\n",
    "\n",
    "Based on https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yjgT9Azct22a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVDD0NqSt22c"
   },
   "source": [
    "Seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rytihYxWt22d"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrammarGen():\n",
    "    \"\"\"\n",
    "    Generates Grammar sequences from grammars, and offers other functionalities\n",
    "    Grammars are dictionaries:\n",
    "    - always have START\n",
    "    - all paths lead eventually to END\n",
    "    - Entries starting with the same letter\n",
    "      have same output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grammar=None):\n",
    "        if grammar is None:\n",
    "            self.grammar = data.g0()\n",
    "        else:\n",
    "            self.grammar = grammar\n",
    "\n",
    "        # find how many letters in grammar\n",
    "        self.len = len(set([token[0] for token in self.grammar if (token != 'START' and token != 'END')]))\n",
    "\n",
    "        # variable to check how many sequences have been generated for the grammaticality test\n",
    "        self.grammCheckMaxLen = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"Generates n tokens\"\"\"\n",
    "        ret = []\n",
    "        count = 0\n",
    "        hashtrack = set()\n",
    "        while count < n:\n",
    "            token = []\n",
    "            current = 'START'\n",
    "            while current != 'END':\n",
    "                # Append current\n",
    "                if current != 'START':\n",
    "                    token.append(current[0])\n",
    "                # Choose next\n",
    "                r = random.randint(0, len(self.grammar[current]) - 1)\n",
    "                current = self.grammar[current][r]\n",
    "            # Check if seq is already inside\n",
    "            tokenhash = ''.join([str(x) for x in token])\n",
    "            if tokenhash not in hashtrack:\n",
    "                hashtrack.add(tokenhash)\n",
    "                ret.append((token, ))\n",
    "                count += 1\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def generateAllGrammatical(self, maxlen=float('inf')):\n",
    "        \"\"\"Generates all grammatical sequences until length maxlen\"\"\"\n",
    "        def genAllHelp(seq, current):\n",
    "            if current == 'END':\n",
    "                return [seq]\n",
    "            if len(seq) >= maxlen:\n",
    "                return []\n",
    "            # Append Current\n",
    "            if current != 'START':\n",
    "                seq.append(current[0])\n",
    "            # Generate next possibilities\n",
    "            options = range(len(self.grammar[current]))\n",
    "            ret = [(genAllHelp(copy.copy(seq), self.grammar[current][i]))\n",
    "                   for i in options]\n",
    "            return itertools.chain(*ret)\n",
    "        return set([tuple(seq) for seq in genAllHelp([], 'START')])\n",
    "\n",
    "    def isGrammatical(self, seqs):\n",
    "        \"\"\"Check for grammaticality of sequences in seqs\"\"\"\n",
    "        maxlen = max([len(seq) for seq in seqs])\n",
    "        if self.grammCheckMaxLen < maxlen:\n",
    "            self.allGrammatical = self.generateAllGrammatical(maxlen)\n",
    "            self.grammCheckMaxLen = maxlen\n",
    "\n",
    "        return [tuple(seq) in self.allGrammatical for seq in seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVj3zdDt22d"
   },
   "source": [
    "## Data\n",
    "\n",
    "First, get the training and test sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Dataset for Sequences:\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rBmNkT78t22d"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int): amount of sequences generated\n",
    "        \"\"\"\n",
    "        self.seqs = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define collate_batch for the Dataloader: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "\n",
    "Sequences are padded and their non-padded lengths are returned.\n",
    "Since pack_padded_sequences requires sequences to be sorted, they are sorted too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tOBuBfPPt22e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    seq_lens = []\n",
    "    processed_seqs = []\n",
    "    # Sort in descending order\n",
    "    batch.sort(reverse=True, key=(lambda x: len(x)))\n",
    "    # append start and end token\n",
    "    for seq in batch:\n",
    "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
    "        seq_lens.append(len(seq))\n",
    "        processed_seqs.append(torch.tensor(seq))\n",
    "    # pad\n",
    "    padded_seqs = pad_sequence(processed_seqs)\n",
    "    seq_lens = torch.tensor(seq_lens)\n",
    "    return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7eCduf-t22m"
   },
   "source": [
    "## Model\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Next up, we define the encoder.\n",
    "\n",
    "The changes here all within the `forward` method. It now accepts the lengths of the source sentences as well as the sentences themselves. \n",
    "\n",
    "After the source sentence (padded automatically within the iterator) has been embedded, we can then use `pack_padded_sequence` on it with the lengths of the sentences. Note that the tensor containing the lengths of the sequences must be a CPU tensor as of the latest version of PyTorch, which we explicitly do so with `to('cpu')`. `packed_embedded` will then be our packed padded sequence. This can be then fed to our RNN as normal which will return `packed_outputs`, a packed tensor containing all of the hidden states from the sequence, and `hidden` which is simply the final hidden state from our sequence. `hidden` is a standard tensor and not packed in any way, the only difference is that as the input was a packed sequence, this tensor is from the final **non-padded element** in the sequence.\n",
    "\n",
    "We then unpack our `packed_outputs` using `pad_packed_sequence` which returns the `outputs` and the lengths of each, which we don't need. \n",
    "\n",
    "The first dimension of `outputs` is the padded sequence lengths however due to using a packed padded sequence the values of tensors when a padding token was the input will be all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P57X-q51t22n"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le1kHQOIt22o"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P_t5icjNt22o"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v371q1Tkt22q"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ECjR4jZot22q"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toSHWJmEt22q"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
    "\n",
    "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
    "\n",
    "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
    "\n",
    "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
    "\n",
    "The attention at each time-step is stored in the `attentions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OkcTBfr-t22s"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKt9wtL6t22s"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class CosineLoss():\n",
    "    def __init__(self, vocabsize, ignore_index):\n",
    "        self.vocabsize = vocabsize\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eye = torch.eye(self.vocabsize)\n",
    "        self.cosSim = nn.CosineSimilarity(dim=1)\n",
    "\n",
    "    def __call__(self, outputs, labels):\n",
    "        maxlen = outputs.shape[0]\n",
    "        bs = outputs.shape[1]\n",
    "        ignore_positions = labels == self.ignore_index\n",
    "        outputs[ignore_positions] = self.eye[self.ignore_index]\n",
    "        labels_onehot = torch.empty((maxlen, bs, self.vocabsize))\n",
    "        for idx in range(maxlen):\n",
    "            labels_onehot[idx,:,:] = self.eye[labels[idx,:]]\n",
    "\n",
    "        batch_first_labels = labels_onehot.permute(1,0,2)\n",
    "        processed_labels = batch_first_labels.reshape(-1, maxlen * self.vocabsize)\n",
    "        batch_first_outputs = outputs.permute(1,0,2)\n",
    "        processed_outputs = batch_first_outputs.reshape(-1, maxlen * self.vocabsize)\n",
    "        return (1 - self.cosSim(processed_labels, processed_outputs)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3d2C9Rvt22w"
   },
   "source": [
    "Training and evaluation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Fut2CtQrt22w"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch\n",
    "        trg = src\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        if isinstance(criterion, CosineLoss):\n",
    "            loss = criterion(output, trg)\n",
    "        else:\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VIT6a8uqt22w"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "            \n",
    "            #seq = [seq_len, batch_size]\n",
    "            #output = [seq_len, batch_size, output_dim]\n",
    "\n",
    "            if isinstance(criterion, CosineLoss):\n",
    "                loss = criterion(output, seq)\n",
    "            else:\n",
    "                output_dim = output.shape[-1]\n",
    "                \n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = seq[1:].view(-1)\n",
    "                \n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #output = [(trg len - 1) * batch size, output dim]\n",
    "                \n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extra(model, dataloader, loss_func):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for seqs, seqs_len in dataloader:\n",
    "\n",
    "            outputs = model(seqs, seqs_len, seqs, 0)\n",
    "\n",
    "            loss = loss_func(outputs, seqs) / seqs.shape[1]\n",
    "\n",
    "            loss_total += loss.item()\n",
    "        \n",
    "#        if loss_func == allOrNoneLoss:\n",
    "#            return loss_total\n",
    "\n",
    "        return loss_total / len(dataloader)\n",
    "\n",
    "def cutEndToken(seq):\n",
    "    ret = []\n",
    "    for stim in seq:\n",
    "        if stim == END_TOKEN:\n",
    "            break\n",
    "        ret.append(stim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def allOrNoneLoss(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    ret = 0\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        ret += not p == t\n",
    "    return torch.tensor(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "lW3r-pjXt22x"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUY7o5eGt22x"
   },
   "source": [
    "During Training in addition to collecting the train/validation loss, collect the amount of entirely correct predicted sequences on the train and test gr/ugr set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "1. Load in all the data\n",
    "2. Set parameters\n",
    "3. Define plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set base tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "START_TOKEN = 1\n",
    "END_TOKEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = data.g1_train()\n",
    "valid_seqs = data.g1_train()\n",
    "test_seqs = data.g1_test_gr()\n",
    "test_ugr_seqs = data.g1_test_ugr_balanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort for better perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs.sort(key=(lambda x: len(x)))\n",
    "valid_seqs.sort(key=(lambda x: len(x)))\n",
    "test_seqs.sort(key=(lambda x: len(x)))\n",
    "test_ugr_seqs.sort(key=(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildVocab(letterset):\n",
    "    vocab = {'<pad>': PAD_TOKEN, '<sos>': START_TOKEN, '<eos>': END_TOKEN}\n",
    "    counter = len(vocab)\n",
    "    for letter in letterset:\n",
    "        vocab[letter] = counter\n",
    "        counter +=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = set()\n",
    "for seq in train_seqs:\n",
    "    [letters.add(letter) for letter in seq]\n",
    "letters = list(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all tasks, creates an additional task with all sequences from all tasks mashed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TASKS = 2\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idxs = []\n",
    "idx2lets = []\n",
    "\n",
    "usedpermutations = set()\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        temp_letters = copy.copy(letters)\n",
    "\n",
    "        while str(temp_letters) in usedpermutations:\n",
    "            random.shuffle(temp_letters)\n",
    " \n",
    "        usedpermutations.add(str(temp_letters))\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Conversion Functions\n",
    "        let2idxs.append(lambda seq: [vocabs[-1][let] for let in seq])\n",
    "        idx2lets.append(lambda seq: [rvocabs[-1][let] for let in seq])\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idxs[-1](seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idxs[-1](seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idxs[-1](seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idxs[-1](seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'W': 3, 'P': 4, 'Z': 5, 'S': 6, 'N': 7}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'P': 3, 'Z': 4, 'N': 5, 'W': 6, 'S': 7}\n",
      "\n",
      "First Batch of Task 0:\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [3, 3, 3, 7],\n",
      "        [6, 3, 6, 7],\n",
      "        [6, 6, 3, 5],\n",
      "        [3, 7, 5, 2],\n",
      "        [5, 5, 2, 0],\n",
      "        [2, 2, 0, 0]])\n",
      "\n",
      "First Batch of Task 1:\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [6, 6, 6, 5],\n",
      "        [7, 6, 7, 5],\n",
      "        [7, 7, 6, 4],\n",
      "        [6, 5, 4, 2],\n",
      "        [4, 4, 2, 0],\n",
      "        [2, 2, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(vocabs[0])\n",
    "print(vocabs[1])\n",
    "for i in range(len(valid_dls) - 1):\n",
    "    for seqs, _ in valid_dls[i]:\n",
    "        print(f\"\\nFirst Batch of Task {i}:\")\n",
    "        print(seqs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = max(vocabs[0].values()) + 1\n",
    "OUTPUT_DIM = max(vocabs[0].values()) + 1\n",
    "ENC_EMB_DIM = 150\n",
    "DEC_EMB_DIM = 150\n",
    "ENC_HID_DIM = 18\n",
    "DEC_HID_DIM = 18\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "SRC_PAD_IDX = PAD_TOKEN\n",
    "TRG_PAD_IDX = PAD_TOKEN\n",
    "PREFIX = \"tr\"\n",
    "N_EPOCHS = 1000\n",
    "CLIP = 1\n",
    "STEP_SIZE_EVALUATION = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting & Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTransfer(data, title):\n",
    "    # data = [n_methods, n_tasks, combinedepochs]\n",
    "    n_methods, n_tasks, n_combinedepochs = data.shape\n",
    "    fig, axs = plt.subplots(n_tasks, 1)\n",
    "    colors = ['blue','green','orange','red','yellow','violett']\n",
    "    \n",
    "    xvals = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    for task_idx in range(n_tasks):\n",
    "        for method_idx in range(n_methods):\n",
    "            axs[task_idx].plot(\n",
    "                xvals,\n",
    "                data[method_idx, task_idx],\n",
    "                color=colors[method_idx]\n",
    "            )\n",
    "            axs[task_idx].set_ylim(0,1.1)\n",
    "            if task_idx != n_tasks - 1:\n",
    "                axs[task_idx].tick_params(\n",
    "                    axis='x',\n",
    "                    which='both',\n",
    "                    labelbottom=False\n",
    "                )\n",
    "        x_lines = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, N_EPOCHS)\n",
    "        for xpos in x_lines:\n",
    "            axs[task_idx].axvline(xpos, color=\"grey\")\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "def plotResults(hist_loss, hist_hits):\n",
    "    plotTransfer( hist_loss[:,0,:].unsqueeze(0), \"Train Loss\")\n",
    "    plotTransfer( hist_loss[:,1,:].unsqueeze(0), \"Test Gr Loss\")\n",
    "    plotTransfer( hist_loss[:,2,:].unsqueeze(0), \"Test Ugr Loss\")\n",
    "    plotTransfer( hist_hits[:,0,:].unsqueeze(0), \"Train Hits\")\n",
    "    plotTransfer( hist_hits[:,1,:].unsqueeze(0), \"Test Gr Hits\")\n",
    "    plotTransfer( hist_hits[:,2,:].unsqueeze(0), \"Test Ugr Hits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_eval(model, test_dl, ggen=None):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    errors = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_dl):\n",
    "\n",
    "            src, src_len = batch\n",
    "            trg = src\n",
    "\n",
    "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "            show_batch(output, trg)\n",
    "\n",
    "\n",
    "def show_batch(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        status = \"same\" if p == t else \"different\"\n",
    "        print(f\"pred = {p} - {status} \\ntrg  = {t}\\n-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model):\n",
    "    for task_id in range(N_TASKS + 1):\n",
    "        gr_not_hits = evaluate_extra(model, test_dls[task_id], allOrNoneLoss)\n",
    "        ugr_not_hits = evaluate_extra(model, test_ugr_dls[task_id], allOrNoneLoss)\n",
    "        gr_hits = 1 - gr_not_hits\n",
    "        ugr_hits = 1 - ugr_not_hits\n",
    "        total_acc = (gr_hits + ugr_not_hits) / 2\n",
    "        print(f\"Task {task_id}: Acc {total_acc:2.2}% | Gr acc {gr_hits:2.2} | Ugr acc {ugr_not_hits:2.2}\")\n",
    "        \n",
    "def accuracyAll(models):\n",
    "    for model_id in range(len(models)):\n",
    "        print(f\"\\nModel {model_id}\")\n",
    "        accuracy(models[model_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline A: Individual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr-AE-150-18-0.001-A0\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(150, 18, bidirectional=True)\n",
      "    (fc): Linear(in_features=36, out_features=18, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=54, out_features=18, bias=True)\n",
      "      (v): Linear(in_features=18, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(186, 18)\n",
      "    (fc_out): Linear(in_features=204, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 35198 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
      "\t Val. Loss: 0.614 |  Val. PPL:   1.848\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.787\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.622\n",
      "\t Val. Loss: 0.570 |  Val. PPL:   1.768\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.560 |  Val. PPL:   1.751\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.536 |  Val. PPL:   1.710\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.693\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.645\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.461 |  Val. PPL:   1.586\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.572\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.546\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.488\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.440\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.451\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.427\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.141\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "tr-AE-150-18-0.001-A1\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(150, 18, bidirectional=True)\n",
      "    (fc): Linear(in_features=36, out_features=18, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=54, out_features=18, bias=True)\n",
      "      (v): Linear(in_features=18, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(186, 18)\n",
      "    (fc_out): Linear(in_features=204, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 35198 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.615 | Train PPL:   1.849\n",
      "\t Val. Loss: 0.598 |  Val. PPL:   1.818\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.576 |  Val. PPL:   1.779\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.655\n",
      "\t Val. Loss: 0.570 |  Val. PPL:   1.769\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.786\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.657\n",
      "\t Val. Loss: 0.587 |  Val. PPL:   1.799\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
      "\t Val. Loss: 0.572 |  Val. PPL:   1.771\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
      "\t Val. Loss: 0.548 |  Val. PPL:   1.730\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.530 |  Val. PPL:   1.699\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.519 |  Val. PPL:   1.680\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.515 |  Val. PPL:   1.674\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.507 |  Val. PPL:   1.660\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.497 |  Val. PPL:   1.644\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.487 |  Val. PPL:   1.627\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
      "\t Val. Loss: 0.476 |  Val. PPL:   1.610\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.463 |  Val. PPL:   1.589\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.576\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.472\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.457\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.478\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.476\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.473\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.476\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.320\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.271\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.215\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.203\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.215\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "tr-AE-150-18-0.001-A2\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(150, 18, bidirectional=True)\n",
      "    (fc): Linear(in_features=36, out_features=18, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=54, out_features=18, bias=True)\n",
      "      (v): Linear(in_features=18, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(186, 18)\n",
      "    (fc_out): Linear(in_features=204, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 35198 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.574 | Train PPL:   1.776\n",
      "\t Val. Loss: 0.613 |  Val. PPL:   1.846\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.722\n",
      "\t Val. Loss: 0.600 |  Val. PPL:   1.823\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.535 | Train PPL:   1.708\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.787\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.564 |  Val. PPL:   1.758\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.498 | Train PPL:   1.645\n",
      "\t Val. Loss: 0.533 |  Val. PPL:   1.704\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.599\n",
      "\t Val. Loss: 0.510 |  Val. PPL:   1.666\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.489 |  Val. PPL:   1.631\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.529\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.603\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.546\n",
      "\t Val. Loss: 0.467 |  Val. PPL:   1.595\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.584\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.491\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.491\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.579\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.574\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.568\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.549\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.530\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.497\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.475\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.490\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.322\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.133\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n"
     ]
    }
   ],
   "source": [
    "models_A = []\n",
    "hist_losses_A = []\n",
    "hist_hitsss_A = []\n",
    "for n_task in range(N_TASKS + 1):\n",
    "    SUFFIX = f\"A{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    SAVENAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    PLOTSAVE = \"../plots/autosave/\" + title + \".png\"\n",
    "\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    print(title)\n",
    "    print(model.apply(init_weights))\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_A.append(hist_loss_temp)\n",
    "    hist_hitsss_A.append(hist_hits_temp)\n",
    "    models_A.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_A = torch.cat(hist_losses_A, dim=2)\n",
    "hist_hits_A = torch.cat(hist_hitsss_A, dim=2)\n",
    "\n",
    "plotResults(hist_loss_A, hist_hits_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.95% | Gr acc 0.9 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.71% | Gr acc 0.42 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.97% | Gr acc 0.95 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.77% | Gr acc 0.85 | Ugr acc 0.7\n",
      "Task 1: Acc 0.93% | Gr acc 1.0 | Ugr acc 0.85\n",
      "Task 2: Acc 0.83% | Gr acc 0.92 | Ugr acc 0.75\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline B: Keep Training same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(150, 18, bidirectional=True)\n",
      "    (fc): Linear(in_features=36, out_features=18, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=54, out_features=18, bias=True)\n",
      "      (v): Linear(in_features=18, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(186, 18)\n",
      "    (fc_out): Linear(in_features=204, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-150-18-0.001-B0\n",
      "The model has 35198 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
      "\t Val. Loss: 0.614 |  Val. PPL:   1.848\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.787\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.622\n",
      "\t Val. Loss: 0.570 |  Val. PPL:   1.768\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.560 |  Val. PPL:   1.751\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.536 |  Val. PPL:   1.710\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.693\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.645\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.461 |  Val. PPL:   1.586\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.572\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.546\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.488\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.440\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.451\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.427\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.141\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "tr-AE-150-18-0.001-B1\n",
      "The model has 35198 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.631 | Train PPL:   1.880\n",
      "\t Val. Loss: 0.647 |  Val. PPL:   1.909\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.608 | Train PPL:   1.838\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.743\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.528 |  Val. PPL:   1.696\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.604\n",
      "\t Val. Loss: 0.511 |  Val. PPL:   1.666\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.490 |  Val. PPL:   1.632\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.477 |  Val. PPL:   1.611\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.541\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.524\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.500\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.487\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.381\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.345\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.195\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.146\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.139\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.138\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "tr-AE-150-18-0.001-B2\n",
      "The model has 35198 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.061\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.052 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.061\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.059 |  Val. PPL:   1.060\n"
     ]
    }
   ],
   "source": [
    "models_B = []\n",
    "hist_losses_B = []\n",
    "hist_hitsss_B = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for n_task in range(N_TASKS + 1):\n",
    "    SUFFIX = f\"B{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    SAVENAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    PLOTSAVE = \"../plots/autosave/\" + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    print(title)\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_B.append(hist_loss_temp)\n",
    "    hist_hitsss_B.append(hist_hits_temp)\n",
    "    models_B.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjfElEQVR4nO3de5wU5Z3v8c+P4RKvIMxEEFAE8W5UHFGjUXZdE9AkJFmvGzWYKOLReHLbjUk2mhM3J2ZfObkgHpFEQnB5aXI0MSyiJLqul0QjMwQRVHQkGkdQLiqoGG7zO3881Zm2aaZrZrq7qqu/79erX9VVXdPzeyhmvvM89XSVuTsiIiJp0yfpAkRERIpRQImISCopoEREJJUUUCIikkoKKBERSSUFlIiIpJICSiQmM7vXzD5TwfdfYWYTKvX+IrXG9DkoyTIzeztvdXdgC7AjWr/c3edVqY4XgUvd/f68bVOibacU2f9bwEHufmE16hNJo75JFyBSSe6+Z+55sZDIe62vu2+vZm0i0jUN8UldMrMJZtZuZl81s1eBn5nZPma2wMzWmdkb0fMReV/z32Z2afR8ipk9ambfj/b9s5lN6mVNL5rZP5jZRODrwHlm9raZPZn3PVeZ2VvR9/t0b76fSNopoKSeDQUGAwcAUwk/Dz+L1vcH3gVmdPH1JwArgUbg34Fbzcx6W5S73wf8b+AX7r6nux9tZnsA04FJ7r4X8EFgaW+/l0iaaYhP6lkHcJ27b4nW3wXuyr1oZt8BHuzi619y959E+/4c+L/AvsCru9j/bjPLH0bsDyzpZr1Hmtlf3H0NsKYbXytSc9SDknq2zt3/mlsxs93N7BYze8nMNgEPA4PMrGEXX/+3IHL3zdHTPXexL8An3H1Q7gH8j7iFuvs7wHnANGCNmd1jZofG/XqRWqSAknpWOIX1y8AhwAnuvjdwarS918N2PbDT9Fp3X+TuZwDDgGeBn1S9KpEqUkCJdNqLMMz3ppkNBq5LsJbXgFFm1gfAzPY1s49H56K2AG/TOV1eJJMUUCKdfgTsBqwHHgfuS7CW/xctN5jZEsLP6peB1cDrwGl0Y4hQpBbpg7oiIpJK6kGJiEgqKaBERCSVFFAiIpJKCigREUklBZSIiKSSAkpERFJJASUiIqmkgBIRkVRSQImISCopoEREJJUUUCIikkoKKBERSSUFlIiIpJICSkREUkkBJSIiqaSAEhGRVFJAiYhIKvVN6hs3Njb6qFGjevUeGzZsAGDIkCFlqEjSQsc1e3RMpSutra3r3b2pcHtiATVq1ChaWlp69R5z5swBYMqUKb0vSFJDxzV7dEylK2b2UrHtGuITEZFUKhlQZjbbzNaa2fJdvG5mNt3M2sxsmZmNK3+ZIiJSb+L0oOYAE7t4fRIwNnpMBW7ufVkiIlLvSp6DcveHzWxUF7tMBua6uwOPm9kgMxvm7mvKVeSuLF8OHR1wxhkwbhx873uV/o5SLS+8EI6rZMNRR0G/frBlCwwYkHQ1UivKMUliOPBy3np7tG2ngDKzqYReFvvvv3+vv3FHB+zYAe3tcP/9cPHFcMQRvX5bSYH2dnj2WSjDfxNJgS1b4I03wjE9+uikq5FaUY6AsiLbvNiO7j4LmAXQ3NxcdJ/u+MAHwvKb34Thw+GWW2D69N6+qyTNo/8Zl10G116bbC1SHtOnw5/+BKtXK6AkvnLM4msHRuatjwBWl+F9Y2tshHPOgVmzwlDCscfCo49WswIpp1xA9euXbB1SPv37h+Xqqv5mkFpXjoCaD1wczeY7EdhYjfNPhb7xDZg8GQ4+OJybWrCg2hVIuSigskcBJT1RcojPzG4HJgCNZtYOXAf0A3D3mcBC4EygDdgMXFKpYrty2GHwi1+E54ccEk6yS21SQGVPnz7Qt68CSronziy+C0q87sCVZauoDEaPhlWrdt6+bRucdRZMmwaf+lT165J4OjrCMvdXt2TDgAHw8sul9xPJyeSVJMaMCT0oL5iGsWAB/O53YcafpJd6UNnUv796UNI9mQyo0aNh48YwrTXfLbeEpX5I0k0BlU0DBuhnT7onkwE1ZkxYfvSj8MlPhuerVsGiReG5fkjSLTfEp4DKlv794bXXwmcXReJI7GrmlTR6dFg+9lhYLl8O8+ZBQwP8/d/D008nV5uUph5UNg0YEMJp3ToYOjTpaqQWZLIHlQsoCL/kfvQjmD079KiOPx5efRW2b3/vX3Lbt+98zkqSoYDKJk01l+7KZEDtsUe4ssSECeEDvLfeCmvXwuWXw377hWD67Gfhgx8M+7/+eviw7/z5iZYtEc3iyyYFlHRXJof4IIRNU1P4oTjtNBg4ECZOhLvvDq//8pfh+mBvvgl//GOYVPH44+HDvpIs9aCyKRdQa9cmW4fUjswG1Li8u1JNndr5fL/9wnLLlrBcsgRyN/Yt9tkpqT4FVDbljuf69cnWIbUjk0N8XckFVE5LS2dA6eoT6aCAyqaGBnjf+8IkCZE46i6g8mcPDRwYwqm1NayrB5UOmmaeXY2NCiiJL7NDfLvSrx+8//1gBh/6ULiyxJtvwsiR4TIszz0HgwaFfSQZ6kFlV1OThvgkvrrrQQEceiiceiqcfHIIJ4ALLwzLk06CC7q8+qBUWi6gNIsve5qa1IOS+OquBwXwm990joeffHJYusN3vxumnK9YkXSF9U1DfNnV1ATPP590FVIr6jKgBg3qfH788WH51lud2157Dd55J3yeSqpPQ3zZ1dioIT6Jry6H+IrZa69wH6ncdfw0YSI5CqjsamoKfwzmPuYh0hUFVJ6lS+E//iM815Tz5CigsqupKSx1HkriUEDled/7wi3jQT2oJOkcVHY1NoalhvkkDgVUgX32CZ+PUg8qOZrFl13qQUl3KKAKmIXzUOpBJUdDfNmlgJLuUEAVkbtlvCRDAZVduYDSEJ/EESugzGyima00szYzu6bI6xPMbKOZLY0e15a/1Oo5+ODQg3r33aQrqU+5c1B99OdT5uyzTxil2LAh6UqkFpT8HJSZNQA3AWcA7cBiM5vv7oX3pX3E3T9agRqr7rjjwj2jli2DE05Iupr64x5+iUn29OkTPtKxcWPSlUgtiPM36nigzd1XuftW4A4g03dNam4Oy9xVzqW63NV7yrK991ZASTxxfg0MB17OW2+PthU6ycyeNLN7zeyIYm9kZlPNrMXMWtal+CzpiBHhYrEKqGR0dKgHlWUDByqgJJ44AVXsV4UXrC8BDnD3o4EbgbuLvZG7z3L3ZndvbsqdLU0hs9CLUkAlQ0N82TZwIGzalHQVUgviBFQ7MDJvfQSwOn8Hd9/k7m9HzxcC/cyssWxVJqC5GZ5+Gs44o/Nx551JV1UfNMSXbepBSVxxfg0sBsaa2YFm1h84H5ifv4OZDTULf/Oa2fjofWt6ns4554RbcmzeHB6trfDjHyddVX1QDyrbFFASV8lZfO6+3cyuAhYBDcBsd19hZtOi12cCZwNXmNl24F3gfHcvHAasKUceCQ8+2Ll+9dVw661hdl9DQ3J11QOdg8o2BZTEFet2G9Gw3cKCbTPzns8AZpS3tHRpboYbb4Rnn4Ujik4BkXLREF+2aRafxKVfAzFp6nn1aIgv2wYODLfb0C03pBQFVEyHHBJuYNjamnQl2achvmwbODAsNZNPSlFAxdTQAOPGwS23wCmnhF+iO3bAxIlw++1JV5ct6kFlWy6gNMwnpSiguuH66+Hkk+H3v4dXX4X77oNFi+AXv0i6smzROahsU0BJXPo10A2nnQZf/Wp4/sILoTcFOi9VbhriyzYFlMSlgOqm0aPD8rHH4J57YOhQeOWV0KOS8tAQX7btvXdYKqCkFAVUNx1wQBh+mjcv/KX/pS+F7Zo8UT4a4ss2TZKQuPRroJv694eRI8OtOBoaYMqU8Ne+hvnKRz2obNMQn8SlgOqBMWPC8ogjwh1CDz1UPahy0jmobNMQn8SlgOqB3Hmo3Id3Dz0U2tqSqydr1IPKtn79YPfdFVBSmgKqB3I9qFxAjRkTbhGfu1V5sVtducP69dWpr9bpHFT26Xp8Eod+DfRA7lp8J50UlqNHh8u2rFkDTzwB++4Lf/rTe79m4UIYPjzsI13TEF/2DRkCL72UdBWSdgqoHjjrrHDO6ZhjwnquR/XCC/DQQ6EHUBhQK1bA1q0aCoxDQ3zZ9/GPwwMPwF/+knQlkmYKqB7o0ydc9ignP6Bys/lWrXrv16xe/d6l7JqG+LLvssvCcf7pT5OuRNJMvwbKYP/9w5TzVas6Z/O98MJ791FAxachvuwbNSrcpfqXv0y6EkkzBVQZ9OsXQqqlpTOYdhVQOgfVtdxtLhVQ2TduXPijbseOpCuRtIp1w0IpbfTocOFY6JzVB/Dii7BtW2cwqQfVtW3bwlIBlX1jxoTj3d4ertAiUkg9qDIZNy789b/77nDuubBhQ5hG+/GPwznnaIgvrlxA6RxU9uU+T1g42iCSo18DZfLd74Ze08svw3HHhW1z58JTT8GTT8Jf/xq2KaC6ph5U/chNLiqcUCSSo4Aqk4YGOPBAGDy48y/D669/7z5Dh743oJ55Bh55pHo11oKtW8NSAZV9I0aE87dp6UG5h5uPvv120pVITqyAMrOJZrbSzNrM7Joir5uZTY9eX2Zm44q9T704+OBwjb516+DCCzu3NzfDW2+FB8BFF8GZZ3aui3pQ9aShIczmS0sP6t574Z/+Cb7//aQrkZySAWVmDcBNwCTgcOACMzu8YLdJwNjoMRW4ucx11pQ99gg9pXfegdtug2HDwvbcpZHWrAkz/lpbw19rumV8J52Dqi+jR6enB5W7AelPfwrbtydbiwRxZvGNB9rcfRWAmd0BTAaezttnMjDX3R143MwGmdkwd6/bSdV9+4YHhGD6z//sPDd13XXhMi+77x6mp3/ve7ByZXK1psmbb4alelD1YcwYePRR+PKXk62jowMWLAg/o62t4TY6++6bbE214KCD4IorKvf+5rkPnuxqB7OzgYnufmm0fhFwgrtflbfPAuAGd380Wn8A+Kq7txS811RCDwvgEKAcv5YbgXq5DKvamj310k5QW7OqHG09wN2bCjfG6UEV+1u2MNXi7IO7zwJmxfiesZlZi7s3l/M900ptzZ56aSeorVlVybbGGelvB0bmrY8ACidLx9lHREQktjgBtRgYa2YHmll/4HxgfsE+84GLo9l8JwIb6/n8k4iI9F7JIT53325mVwGLgAZgtruvMLNp0eszgYXAmUAbsBm4pHIl76SsQ4Ypp7ZmT720E9TWrKpYW0tOkhAREUmCPm0iIiKppIASEZFUUkCJiEgqKaBERCSVFFAiIpJKCigREUklBZSIiKSSAkpERFJJASUiIqmkgBIRkVRSQImISCrFuR9URTQ2NvqoUaN69R4bNmwAYMiQIWWoSNJCxzV7dEylK62tret7esPCihg1ahQtLS2ld+zCnDlzAJgyZUrvC5LU0HHNHh1T6YqZvVRsu4b4REQklUoGlJnNNrO1ZrZ8F6+bmU03szYzW2Zm48pfpoiI1Js4Pag5wMQuXp8EjI0eU4Gbe1+WiIjUu5IB5e4PA693sctkYK4HjwODzGxYuQqU+rJpEyxeDG+8kXQlIpK0cpyDGg68nLfeHm3biZlNNbMWM2tZt25dGb61ZM2f/wybN8OyZUlXIiJJK0dAWZFtRe8j7+6z3L3Z3ZubmnaaUShCR0fn89Wrk6tDRJJXjoBqB0bmrY8A9KtFemTbts7nt92WXB0ikrxyBNR84OJoNt+JwEZ3X1OG95U6lB9Qf/hDcnWISPJKflDXzG4HJgCNZtYOXAf0A3D3mcBC4EygDdgMXFKpYiX7cgE1YAD08nPcIlLjSgaUu19Q4nUHrixbRVLXcgG1997hHNTq1bDffsnWJCLJ0JUkJFW2bg3LvfcOy9bW5GoRkWQpoCRV8ntQffpomE+knimgJFVyAdXQAIcfroASqWcKKEmVXED16QPNzSGgvOin6kQk6xRQkiq5gDKD446DtWuhvT3ZmkQkGQooSZXcJAmz0IMCTZQQqVcKKEmV/CG+o48O56J0HkqkPimgJFXyh/h22w2OPFIBJVKvFFCSKvkBBTBuHCxdmlg5IpIgBZSkSmFAHXQQvPYavPNOcjWJSDIUUJIquUkSfaL/mWPGhOXKlfDzn7/3YrIikm0KKEmVwgAaPTosf/hDmDJFt+AQqScKKEmVbds6e0/Q2YP6zW/CcubM6tckIslQQEmqbNvWef4JYJ99YOBAeOutsL54MSxZkkxtIlJdCihJlcKAMusc5jvvvDD1/JZbkqlNRKpLASWpUhhQ0DnMd/rpcMEFMG8ebNpU/dpEpLoUUJIqW7e+9xwUdPagmpth2rQw5XzevOrXJiLVpYCSVCnWg5o8GT71qXBVieZmGDsWFi1Kpj4RqZ6St3wXqaZiAfXBD4ZHzvjx8NBD1a1LRKpPPShJlcJp5sU0N4dbcLz6anVqEpFkxAooM5toZivNrM3Mriny+gQz22hmS6PHteUvVepBsR5UId2GQ6Q+lAwoM2sAbgImAYcDF5jZ4UV2fcTdj4ke3y5znVIntm4tHVDHHBN6WbrKuUi2xelBjQfa3H2Vu28F7gAmV7YsqVdxelB77gmHHgo/+AFceGF16hKR6osTUMOBl/PW26NthU4ysyfN7F4zO6LYG5nZVDNrMbOWdevW9aBcybo456AAvvUt2H9/uP122LGj4mWJSALiBFSxv2e9YH0JcIC7Hw3cCNxd7I3cfZa7N7t7c1NTU7cKlfoQpwcFcM45cNll0NEBb7xR+bpEpPriBFQ7MDJvfQSwOn8Hd9/k7m9HzxcC/cyssWxVSt2IG1AAjdH/MHXGRbIpTkAtBsaa2YFm1h84H5ifv4OZDTULv1bMbHz0vhvKXaxkX7ErSexKrhOugBLJppIf1HX37WZ2FbAIaABmu/sKM5sWvT4TOBu4wsy2A+8C57t74TCgSEnd6UHlAmr9+srVIyLJiXUliWjYbmHBtpl5z2cAM8pbmtQjDfGJSI6uJCGpEncWH2iITyTrFFCSKt3pQQ0YAHvtpSE+kaxSQEmqdCegIPSi1IMSySYFlKRKnEsd5WtsVECJZJUCSlKlO+eg4L09qNdeg+3bK1OXiFSfAkpSpSdDfOvXh3AaMwamT69cbSJSXQooSQ330APqyRDf7NnhVvAPP1y5+kSkuhRQkhq54bnu9qC2bIEZ0afwdAsOkexQQElqbN0alt05B3XggWG5ejWccgq88orutCuSFbGuJCFSDdu2hWV3elD/+I/w5JPha954A047Ldxp96yzKlOjiFSPelCSGj0JqD594AMfgKOOgnHjwtf2ZJjvhRfg6afDcOFvfxvOh+XbtAkeeqj77ysiPaeAktTIBVR3hvjy7blnCKpFi7r3de7wiU/Ahz8M3/8+fOQj8F//9d59vv51mDABnn22Z7WJSPcpoCQ1etKDKvSZz8Bjj8FTT8X/mj/8AZYvD+evvv3tsG3mzM7X33kHbrstPJ81q+e1iUj36ByUpEZukkRvA+rrX4cvfQn+7u92fv3002H0aPjZzzpnDd53X7im3157hckWhx0Gd98N118PDQ2wcmUY4jvsMJgzB4YNg899DgYP7nmdIlKaAkpSoxw9qCFD4NJL4aab4P77d379Jz8JEyhuuum926+5Bt7//tBDuvNOGD8err228/WTTgrDfxMmwL/8S6jxK1/peZ0iUpoCSlKjt+egcmbMgB/8YOftd94Jn/403HwzXHBB6A3l9O8fll/8Ylhu3Ag7dnS+3q9fCKXNm+Ggg/R5K5FqUEBJapSjB5WTC5x8Z58NX/hCuPLEFVcU3yenoSE8CvXtC83NnQG1ZAnMnRuuaPG1r4WvmTcPnngivD5mDFx9da+bI1KXFFCSGuUMqGL69w/np+6/P3yot6eam+Guu8Lnrq6+OkzK6OiAY4+F44+HSy4JQdanT5hgcdppcPTR5WuHSL3QLD5JjXIN8XXlC1+ABQt6F4LHHReWc+fC738fJlPsu2+Y+TdnTmjH4sXwl7+Emyrecks5KhepP+pBSWqUYxZfNeQC6pvfDL2yqVNDT+mGG8LQ3oc+BEccEfY577wQZBs27Pr9+vYN7/XUU+GDwiefHNZztx75538OH0IWqTexAsrMJgI/BhqAn7r7DQWvW/T6mcBmYIq7LylzrZJxlR7iK5fBg+HCC8N5qHPPDeefrrgiXIHi3XfhX/+1c9+vfCVcimnZsl2/36pVIZgefDCE9Mc+FiZ0jBkDL70Uprjfc0/l2yWSNiUDyswagJuAM4B2YLGZzXf3p/N2mwSMjR4nADdHS5HYaiWgoPODuzkjRoRhvUJHHQVLl3b9Xp//fOfV2AFuvx0++1m49dYw1f3f/g1efBFGjepl0SI1Jk4PajzQ5u6rAMzsDmAykB9Qk4G57u7A42Y2yMyGufuaslec56mnwlTgCRMq+V2kWnJ3xq3kOag0uvzyEFBjx8Luu4ce1+WXh9cuvRS+8x2YOBGGDk22zt445piw1M9qthx7LPzwh5V7f/PCq2IW7mB2NjDR3S+N1i8CTnD3q/L2WQDc4O6PRusPAF9195aC95oKTI1WDwFWlqENjcD6MrxPLVBbs6de2glqa1aVo60HuHtT4cY4PahiAy6FqRZnH9x9FlDWq5mZWYu7N5fzPdNKbc2eemknqK1ZVcm2xhlMaQdG5q2PAFb3YB8REZHY4gTUYmCsmR1oZv2B84H5BfvMBy624ERgY6XPP4mISLaVHOJz9+1mdhWwiDDNfLa7rzCzadHrM4GFhCnmbYRp5pdUruSd1NMNENTW7KmXdoLamlUVa2vJSRIiIiJJqLMJvSIiUisUUCIikkoKKBERSSUFlIiIpJICSkREUkkBJSIiqaSAEhGRVFJAiYhIKimgREQklRRQIiKSSgooERFJpTj3g6qIxsZGH9XLe1hv2LABgCFDhpShIkkLHdfs0TGVrrS2tq7v6Q0LK2LUqFG0tLSU3rELc+bMAWDKlCm9L0hSQ8c1e3RMpStm9lKx7SWH+MxstpmtNbPlu3jdzGy6mbWZ2TIzG9fbYkVEROKcg5oDTOzi9UnA2OgxFbi592WJiEi9KxlQ7v4w8HoXu0wG5nrwODDIzIaVq0AREalP5ZjFNxx4OW+9PdomIiLSY+UIKCuyrehtes1sqpm1mFnLunXryvCtRUQkq8oRUO3AyLz1EcDqYju6+yx3b3b35qamnWYUioiI/E05Amo+cHE0m+9EYKO7rynD+4qISB0r+TkoM7sdmAA0mlk7cB3QD8DdZwILgTOBNmAzcEmlihURkfpRMqDc/YISrztwZdkqEhERQdfiExGRlFJAiYhIKimgREQklRRQIiKSSgooERFJJQWUiIikkgJKRERSSQElIiKppIASEZFUUkCJiEgqKaBERCSVFFAiIpJKCigREUklBZSIiKSSAkpERFJJASUiIqmkgBIRkVRSQImISCopoEREJJUUUCIikkoKKBERSaVYAWVmE81spZm1mdk1RV6fYGYbzWxp9Li2/KWKiEg96VtqBzNrAG4CzgDagcVmNt/dny7Y9RF3/2gFahQRkToUpwc1Hmhz91XuvhW4A5hc2bJEJEtefx0eegheeinpSqSWxAmo4cDLeevt0bZCJ5nZk2Z2r5kdUeyNzGyqmbWYWcu6det6UK6I1KL168PyrruSrUNqS5yAsiLbvGB9CXCAux8N3AjcXeyN3H2Wuze7e3NTU1O3ChWR2rXbbmHZ2ppsHVJb4gRUOzAyb30EsDp/B3ff5O5vR88XAv3MrLFsVYpITevoCMvFi5OtQ2pLnIBaDIw1swPNrD9wPjA/fwczG2pmFj0fH73vhnIXKyK1KRdQzz8PGt2XuEoGlLtvB64CFgHPAL909xVmNs3MpkW7nQ0sN7MngenA+e5eOAwoInVqx47O548/nlwdUltKTjOHvw3bLSzYNjPv+QxgRnlLE5Gs6OiAPtGfwytXwsc+lmw9Uht0JQkRqbgdO6B/f2hsDMN8InEooESk4nbsgIYGOPhgeO65pKuRWqGAEpGK6+gIATV2rHpQEp8CSkQqbseOcA5q7Fh45RV4552kK5JaoIASkYrL9aAOPjist7UlW4/UBgWUiFRcfg8KNMwn8SigRKTicpMkDjoorCugJA4FlIhUXG6Ib889oakJXnwx6YqkFiigRKTickN8AAccoNtuSDwKKBGpqB07wD30oEABJfEpoESkojZvDsvCHpSu1imlKKBEpKJyAZXfg3r33c6bGIrsigJKRCoq96Hc/B4UaJhPSlNAiUhFFetBgWbySWkKKBGpqF0FlHpQUooCSkQqqnCIb9Ag2GsvBZSUpoASkYoq7EGZwVFHwa9+BRs2JFeXpJ8CSkQqqrAHBXDjjbB2LVx5ZTI1SW1QQIlIRRX2oADGjYMrroBf/xr++tdk6pL0U0CJSEUVCyiA00+HrVth8eLq1yS1IVZAmdlEM1tpZm1mdk2R183MpkevLzOzceUvVURqUbEhPoBTTgnLhx+ubj27smUL/OhH8OqrSVciOSUDyswagJuAScDhwAVmdnjBbpOAsdFjKnBzmesUkRpVeKmjnMGDw2SJtATU174GX/winHtuuH6gJK9vjH3GA23uvgrAzO4AJgNP5+0zGZjr7g48bmaDzGyYu68pe8V5Hnss/Ef6/Ocr+V2k2s47Lyx1XLNhyxa46KIwe6/QaafBjBlh2nnS3n4bxo+HRx6BvffeOVBlZ6eeCvfcU7n3Ny9xxUYzOxuY6O6XRusXASe4+1V5+ywAbnD3R6P1B4CvuntLwXtNJfSwAA4BVpahDY1AvVzVS23NnnppJ6itWVWOth7g7k2FG+P0oIr83UNhqsXZB3efBcyK8T1jM7MWd28u53umldqaPfXSTlBbs6qSbY3TiW0HRuatjwBW92AfERGR2OIE1GJgrJkdaGb9gfOB+QX7zAcujmbznQhsrPT5JxERybaSQ3zuvt3MrgIWAQ3AbHdfYWbTotdnAguBM4E2YDNwSeVK3klZhwxTTm3NnnppJ6itWVWxtpacJCEiIpIETaQUEZFUUkCJiEgq1WxAlbr8Ui0ysxfN7CkzW2pmLdG2wWb2OzN7Plruk7f/16L2rzSzjyRXeWlmNtvM1prZ8rxt3W6bmR0X/Ru1RZfXKvYRh0Ttoq3fMrNXomO71MzOzHutJttqZiPN7EEze8bMVpjZ/4y2Z+64dtHWLB7X95nZE2b2ZNTW/xVtr/5xdfeaexAma7wAjAb6A08ChyddVxna9SLQWLDt34FroufXAN+Lnh8etXsAcGD079GQdBu6aNupwDhgeW/aBjwBnET47N29wKSk2xazrd8CvlJk35ptKzAMGBc93wt4LmpP5o5rF23N4nE1YM/oeT/gj8CJSRzXWu1B/e3yS+6+FchdfimLJgM/j57/HPhE3vY73H2Lu/+ZMINyfPXLi8fdHwZeL9jcrbaZ2TBgb3d/zMP//rl5X5Mau2jrrtRsW919jbsviZ6/BTwDDCeDx7WLtu5KLbfV3f3taLVf9HASOK61GlDDgZfz1tvp+j9LrXDgt2bWauGyUAD7evSZsmj5/mh7Fv4Nutu24dHzwu214ioLV/ufnTc8kom2mtko4FjCX9uZPq4FbYUMHlczazCzpcBa4HfunshxrdWAinVppRp0sruPI1wd/kozO7WLfbP6bwC7blstt/lmYAxwDLAG+D/R9ppvq5ntCdwFfMHdN3W1a5Fttd7WTB5Xd9/h7scQrgo03syO7GL3irW1VgMqk5dWcvfV0XIt8GvCkN1rUVeZaLk22j0L/wbdbVt79Lxwe+q5+2vRD30H8BM6h2Nruq1m1o/wC3ueu/8q2pzJ41qsrVk9rjnu/ibw38BEEjiutRpQcS6/VFPMbA8z2yv3HPgwsJzQrs9Eu30G+E30fD5wvpkNMLMDCffieqK6Vfdat9oWDSu8ZWYnRrOBLs77mlTL/WBHPkk4tlDDbY3quhV4xt1/kPdS5o7rrtqa0ePaZGaDoue7Af8APEsSxzXpGSM9fRAurfQcYcbIN5KupwztGU2YCfMksCLXJmAI8ADwfLQcnPc134jav5KUzQQq0r7bCUMg2wh/WX2uJ20Dmgm/BF4AZhBdDSVNj1209TbgKWBZ9AM9rNbbCpxCGLJZBiyNHmdm8bh20dYsHtcPAH+K2rQcuDbaXvXjqksdiYhIKtXqEJ+IiGScAkpERFJJASUiIqmkgBIRkVRSQImISCopoEREJJUUUCIikkr/H1/sOWH/tBDdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_B = torch.cat(hist_losses_B, dim=2)\n",
    "hist_hits_B = torch.cat(hist_hitsss_B, dim=2)\n",
    "\n",
    "plotResults(hist_loss_B, hist_hits_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.95% | Gr acc 0.9 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.71% | Gr acc 0.42 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.68% | Gr acc 0.4 | Ugr acc 0.95\n",
      "Task 2: Acc 0.6% | Gr acc 0.22 | Ugr acc 0.97\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.8% | Gr acc 0.6 | Ugr acc 1.0\n",
      "Task 1: Acc 0.68% | Gr acc 0.35 | Ugr acc 1.0\n",
      "Task 2: Acc 0.76% | Gr acc 0.53 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline C: Freeze Parameters\n",
    "\n",
    "1. Define functions\n",
    "2. Train model, freeze core weights in between tasks\n",
    "3. Look at performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyOnParameters(model, conditions, apply_function):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check every condition\n",
    "        for condition in conditions:\n",
    "            # check every keyword\n",
    "            allincluded = True\n",
    "            for keyword in condition:\n",
    "                if keyword not in name:\n",
    "                    allincluded = False\n",
    "                    break\n",
    "            if allincluded:\n",
    "                apply_function(param)\n",
    "\n",
    "def freezeParameters(model, conditions):\n",
    "    def freeze(param):\n",
    "        param.requires_grad = False\n",
    "    applyOnParameters(model, conditions, freeze)\n",
    "\n",
    "def unfreezeParameters(model, conditions):\n",
    "    def unfreeze(param):\n",
    "        param.requires_grad = True\n",
    "    applyOnParameters(model, conditions, unfreeze)\n",
    "\n",
    "def showModelParameters(model, requires_grad=False):\n",
    "    for name, param in model.named_parameters():\n",
    "        if requires_grad:\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "        else:\n",
    "            print(name)\n",
    "            \n",
    "def onTaskUpdate(model):\n",
    "    # Freeze core weights\n",
    "    freezeParameters(model, ((\"\"),))    # Freeze everything\n",
    "    unfreezeParameters(model, ((\"encoder\",\"embedding\"), (\"decoder\",\"fc_out\"), (\"attention\",))) # Unfreeze relevant stuff\n",
    "    \n",
    "    # Reinitialize\n",
    "    to_constant = lambda param: nn.init.constant_(param.data, 0)\n",
    "    applyOnParameters(model, ((\"decoder\",\"fc_out\",\"bias\"),(\"attn\",\"bias\")), to_constant)\n",
    "    to_normal = lambda param: nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "    applyOnParameters(model, ((\"encoder\",\"embedding\"),(\"decoder\",\"fc_out\",\"weight\"),(\"attention\",\"weight\")), to_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(150, 18, bidirectional=True)\n",
      "    (fc): Linear(in_features=36, out_features=18, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=54, out_features=18, bias=True)\n",
      "      (v): Linear(in_features=18, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(186, 18)\n",
      "    (fc_out): Linear(in_features=204, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-150-18-0.001-C0\n",
      "The model has 35198 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
      "\t Val. Loss: 0.614 |  Val. PPL:   1.848\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.787\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.622\n",
      "\t Val. Loss: 0.570 |  Val. PPL:   1.768\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.560 |  Val. PPL:   1.751\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.536 |  Val. PPL:   1.710\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.693\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.645\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.461 |  Val. PPL:   1.586\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.572\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.546\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.488\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.440\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.451\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.427\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.141\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.055\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.053 | Train PPL:   1.054\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.054 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "tr-AE-150-18-0.001-C1\n",
      "The model has 3848 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.592 | Train PPL:   1.808\n",
      "\t Val. Loss: 0.584 |  Val. PPL:   1.793\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.638\n",
      "\t Val. Loss: 0.549 |  Val. PPL:   1.731\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.625\n",
      "\t Val. Loss: 0.528 |  Val. PPL:   1.696\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.562\n",
      "\t Val. Loss: 0.513 |  Val. PPL:   1.670\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.645\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.500 |  Val. PPL:   1.649\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.485 |  Val. PPL:   1.624\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.584\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.466\n",
      "\t Val. Loss: 0.465 |  Val. PPL:   1.592\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.588\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.523\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.446\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.399\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.378\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.363\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.378\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.353\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.334\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.320\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.320\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.309\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.283\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.203\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.195\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.189\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.171\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.162\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "tr-AE-150-18-0.001-C2\n",
      "The model has 3848 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.579 | Train PPL:   1.784\n",
      "\t Val. Loss: 0.560 |  Val. PPL:   1.751\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.616\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
      "\t Val. Loss: 0.526 |  Val. PPL:   1.692\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.512 |  Val. PPL:   1.669\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.482 |  Val. PPL:   1.620\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.603\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.465 |  Val. PPL:   1.592\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.491\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.558\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.582\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.472\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.512\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.521\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.500\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.444\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.427\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.390\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.383\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.353\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.305\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.274\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.301\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.305\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.311\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.249\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.247\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 501 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 502 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 503 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 504 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 505 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 506 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 507 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 508 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 509 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 510 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 511 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 512 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 513 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 514 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 515 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 516 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 517 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 518 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 519 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 520 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 521 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 522 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 523 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 524 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 525 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 526 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 527 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 528 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 529 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 530 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 531 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 532 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 533 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 534 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 535 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 536 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 537 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 538 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 539 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 540 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 541 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 542 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 543 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 544 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 545 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 546 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 547 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 548 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 549 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 550 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 551 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 552 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 553 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 554 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 555 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 556 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 557 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 558 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 559 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.229\n",
      "Epoch: 560 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 561 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 562 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 563 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 564 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.220\n",
      "Epoch: 565 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 566 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.236\n",
      "Epoch: 567 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 568 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 569 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 570 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 571 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 572 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 573 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 574 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 575 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 576 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 577 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 578 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 579 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 580 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 581 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.229\n",
      "Epoch: 582 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 583 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 584 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 585 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 586 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 587 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 588 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 589 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 590 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 591 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 592 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 593 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 594 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 595 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 596 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 597 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 598 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 599 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 600 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 601 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 602 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 603 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 604 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 605 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 606 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 607 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 608 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 609 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 610 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 611 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 612 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 613 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 614 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 615 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 616 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 617 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 618 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 619 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 620 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 621 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 622 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 623 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 624 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 625 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 626 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 627 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 628 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 629 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 630 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 631 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 632 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 633 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 634 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 635 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 636 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 637 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 638 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 639 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 640 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 641 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 642 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 643 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 644 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 645 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 646 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 647 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 648 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 649 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 650 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 651 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 652 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 653 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 654 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 655 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 656 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 657 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 658 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 659 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 660 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 661 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 662 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 663 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 664 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.215\n",
      "Epoch: 665 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 666 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 667 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 668 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 669 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 670 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 671 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 672 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 673 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 674 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 675 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 676 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 677 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 678 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 679 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 680 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 681 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 682 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 683 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 684 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 685 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 686 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 687 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 688 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 689 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 690 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 691 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 692 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 693 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 694 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 695 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 696 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 697 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 698 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 699 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 700 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 701 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 702 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 703 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 704 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 705 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 706 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 707 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 708 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 709 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 710 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 711 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 712 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 713 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 714 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 715 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 716 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 717 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 718 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 719 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 720 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 721 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 722 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 723 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 724 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 725 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 726 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 727 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 728 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 729 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 730 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 731 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 732 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 733 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 734 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 735 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 736 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 737 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 738 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 739 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 740 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 741 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 742 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 743 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 744 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 745 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 746 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 747 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 748 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 749 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 750 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 751 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 752 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 753 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 754 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 755 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 756 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 757 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 758 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 759 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 760 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 761 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 762 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 763 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 764 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 765 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 766 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 767 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 768 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 769 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 770 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 771 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 772 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 773 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 774 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 775 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 776 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 777 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 778 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 779 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 780 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 781 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 782 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 783 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 784 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 785 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 786 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 787 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 788 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 789 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 790 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 791 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 792 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 793 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 794 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 795 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 796 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 797 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 798 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 799 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 800 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 801 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 802 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 803 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 804 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 805 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 806 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 807 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 808 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 809 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 810 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 811 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 812 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 813 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 814 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 815 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 816 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 817 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 818 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 819 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 820 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 821 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 822 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 823 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 824 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 825 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 826 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.207\n",
      "Epoch: 827 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 828 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 829 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 830 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 831 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 832 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 833 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 834 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 835 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 836 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 837 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 838 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 839 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 840 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 841 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 842 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 843 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 844 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 845 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 846 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 847 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 848 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 849 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 850 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 851 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 852 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 853 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 854 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 855 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 856 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 857 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 858 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 859 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 860 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 861 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 862 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 863 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 864 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 865 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 866 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 867 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 868 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 869 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 870 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 871 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 872 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 873 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 874 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 875 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 876 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 877 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 878 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 879 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 880 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 881 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 882 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 883 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 884 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 885 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 886 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 887 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 888 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 889 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 890 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 891 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 892 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 893 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 894 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 895 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 896 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 897 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 898 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 899 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 900 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 901 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 902 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 903 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 904 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 905 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 906 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 907 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 908 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 909 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 910 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 911 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.201\n",
      "Epoch: 912 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 913 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 914 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 915 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 916 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 917 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 918 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 919 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 920 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 921 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 922 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 923 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 924 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 925 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 926 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.189\n",
      "Epoch: 927 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 928 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 929 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 930 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 931 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 932 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 933 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 934 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 935 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 936 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 937 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 938 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 939 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.178\n",
      "Epoch: 940 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 941 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 942 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 943 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.176\n",
      "Epoch: 944 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 945 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 946 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 947 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 948 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 949 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 950 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 951 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 952 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 953 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 954 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 955 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 956 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 957 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 958 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 959 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 960 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 961 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 962 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 963 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 964 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 965 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 966 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 967 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 968 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 969 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 970 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 971 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 972 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 973 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 974 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 975 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 976 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 977 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 978 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 979 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 980 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 981 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 982 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 983 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 984 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 985 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 986 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 987 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 988 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 989 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 990 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 991 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 992 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 993 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 994 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 995 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 996 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 997 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 998 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 999 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 1000 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n"
     ]
    }
   ],
   "source": [
    "models_C = []\n",
    "hist_losses_C = []\n",
    "hist_hitsss_C = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for n_task in range(N_TASKS + 1):\n",
    "    SUFFIX = f\"C{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    SAVENAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    PLOTSAVE = \"../plots/autosave/\" + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    print(title)\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_C.append(hist_loss_temp)\n",
    "    hist_hitsss_C.append(hist_hits_temp)\n",
    "    models_C.append(copy.deepcopy(model))\n",
    "    \n",
    "    # Freeze, reinitialize\n",
    "    onTaskUpdate(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_C = torch.cat(hist_losses_C, dim=2)\n",
    "hist_hits_C = torch.cat(hist_hitsss_C, dim=2)\n",
    "\n",
    "plotResults(hist_loss_C, hist_hits_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.95% | Gr acc 0.9 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.71% | Gr acc 0.42 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.55% | Gr acc 0.15 | Ugr acc 0.95\n",
      "Task 2: Acc 0.53% | Gr acc 0.083 | Ugr acc 0.97\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.68% | Gr acc 0.5 | Ugr acc 0.85\n",
      "Task 1: Acc 0.55% | Gr acc 0.1 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.33 | Ugr acc 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer D: EWC\n",
    "\n",
    "1. Create Fisher functions\n",
    "2. Train model on tasks\n",
    "3. Compare results\n",
    "\n",
    "Based on: https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishers = []\n",
    "optParams = []\n",
    "ewc_lambda = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute optimal parameters and fisher information after training on tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onTaskUpdate_ewc(model, task_id, train_dl, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #accumulate Gradient\n",
    "    for it in range(100):\n",
    "        for seq, seq_len in train_dl:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0)\n",
    "\n",
    "\n",
    "            #loss = F.cross_entropy(output, seq)\n",
    "            loss = criterion(output, seq)\n",
    "            print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "        \n",
    "    fishers.append({})\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        fishers[task_id][name] = param.grad.data.clone().pow(2)\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt evaluation and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ewc(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        #if task_id > 0:\n",
    "        #    print(loss)\n",
    "        \n",
    "        # EWC Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fishers[other_task_id][name]\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "        \n",
    "        #if task_id > 0:\n",
    "        #    print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ewc(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # EWC Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    fisher = fishers[other_task_id][name]\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ewc(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9111e-06)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(fishers[0]['encoder.embedding.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(150, 18, bidirectional=True)\n",
      "    (fc): Linear(in_features=36, out_features=18, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=54, out_features=18, bias=True)\n",
      "      (v): Linear(in_features=18, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 150)\n",
      "    (rnn): GRU(186, 18)\n",
      "    (fc_out): Linear(in_features=204, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
      "\t Val. Loss: 0.614 |  Val. PPL:   1.848\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.787\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.622\n",
      "\t Val. Loss: 0.570 |  Val. PPL:   1.768\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.560 |  Val. PPL:   1.751\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.536 |  Val. PPL:   1.710\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.693\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.645\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.461 |  Val. PPL:   1.586\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.572\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.546\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.488\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.440\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.451\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.427\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.141\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2386, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0616, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1662, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0662, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0609, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0660, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0605, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0654, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0655, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0648, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0654, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0656, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0683, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0655, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0656, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0655, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0696, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0662, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1175, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0581, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0675, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0677, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0677, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2371, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2283, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0675, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0655, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0655, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0682, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0682, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0715, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0605, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0675, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0702, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0663, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0654, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0684, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0656, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0573, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0580, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0676, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0621, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0656, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0660, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0676, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0757, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0687, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2321, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0677, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0609, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0621, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0678, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0719, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0675, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0568, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0686, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0702, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0616, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0621, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.678 | Train PPL:   1.971\n",
      "\t Val. Loss: 0.691 |  Val. PPL:   1.996\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.772\n",
      "\t Val. Loss: 0.611 |  Val. PPL:   1.842\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.492 | Train PPL:   1.636\n",
      "\t Val. Loss: 0.549 |  Val. PPL:   1.731\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.493 | Train PPL:   1.637\n",
      "\t Val. Loss: 0.501 |  Val. PPL:   1.650\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train PPL:   1.536\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.476 |  Val. PPL:   1.610\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.580\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.518\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.497\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.341\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.341\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.345\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.266\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.078\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0559, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0573, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0605, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0609, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0569, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0629, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0561, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0558, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0657, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0637, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0660, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0658, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0635, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0616, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0691, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0616, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0609, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0562, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0648, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0580, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0621, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0609, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0581, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0581, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0580, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0568, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0581, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0615, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0569, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0642, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0647, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0640, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0568, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0720, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0643, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0568, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0621, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0568, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0702, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0568, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0605, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0580, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0616, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0624, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0609, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0646, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0684, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0648, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0581, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0581, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0628, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0653, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0569, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0650, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0649, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0636, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0576, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0585, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0612, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0629, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0568, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0638, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0567, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0569, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0620, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0645, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0614, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0575, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0619, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0605, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0579, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0583, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0573, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0587, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0633, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0699, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0584, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0634, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0629, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0644, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0626, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.195\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.157\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.074\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.074\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.071 |  Val. PPL:   1.073\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.072\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.069\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.070\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.067 |  Val. PPL:   1.069\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.071\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.070\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.069\n",
      "\t Val. Loss: 0.068 |  Val. PPL:   1.071\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.066 | Train PPL:   1.068\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.065 | Train PPL:   1.067\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.067\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.066 |  Val. PPL:   1.068\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.064 | Train PPL:   1.066\n",
      "\t Val. Loss: 0.065 |  Val. PPL:   1.068\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.066\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.063 | Train PPL:   1.065\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.062 | Train PPL:   1.064\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.064\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.063 |  Val. PPL:   1.065\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.061 | Train PPL:   1.063\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 401 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 402 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 403 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 404 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 405 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 406 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 407 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.069 |  Val. PPL:   1.072\n",
      "Epoch: 408 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 409 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 410 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 411 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 412 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 413 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 414 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 415 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 416 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 417 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 418 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 419 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 420 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 421 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 422 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 423 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 424 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 425 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 426 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 427 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 428 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 429 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 430 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 431 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 432 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 433 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 434 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 435 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 436 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 437 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.070 |  Val. PPL:   1.073\n",
      "Epoch: 438 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 439 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 440 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.067\n",
      "Epoch: 441 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n",
      "Epoch: 442 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.064\n",
      "Epoch: 443 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.062 |  Val. PPL:   1.063\n",
      "Epoch: 444 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 445 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 446 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 447 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 448 | Time: 0m 0s\n",
      "\tTrain Loss: 0.060 | Train PPL:   1.062\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 449 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 450 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 451 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 452 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 453 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 454 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 455 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 456 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 457 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 458 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 459 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 460 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 461 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 462 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 463 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 464 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 465 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 466 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 467 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 468 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.060\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 469 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 470 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 471 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 472 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 473 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 474 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 475 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 476 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 477 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 478 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 479 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 480 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 481 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 482 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.063\n",
      "Epoch: 483 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 484 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 485 | Time: 0m 0s\n",
      "\tTrain Loss: 0.059 | Train PPL:   1.061\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 486 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 487 | Time: 0m 0s\n",
      "\tTrain Loss: 0.058 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 488 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 489 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 490 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 491 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 492 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 493 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 494 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 495 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 496 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.056\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 497 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.061 |  Val. PPL:   1.062\n",
      "Epoch: 498 | Time: 0m 0s\n",
      "\tTrain Loss: 0.056 | Train PPL:   1.058\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 499 | Time: 0m 0s\n",
      "\tTrain Loss: 0.055 | Train PPL:   1.057\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "Epoch: 500 | Time: 0m 0s\n",
      "\tTrain Loss: 0.057 | Train PPL:   1.059\n",
      "\t Val. Loss: 0.060 |  Val. PPL:   1.062\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0630, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0527, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0664, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0558, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0558, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0566, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0677, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0558, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0559, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0582, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0601, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0621, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0559, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0574, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0527, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0581, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0675, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0559, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0604, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0561, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0671, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0673, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0561, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0623, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2220, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0600, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0679, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0562, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0578, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0528, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0565, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0665, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0607, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0668, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0562, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0558, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0560, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0561, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0684, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0617, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0608, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0561, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0602, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0527, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0603, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0569, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0586, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0547, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0546, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0595, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0555, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0554, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0537, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0538, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0592, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0556, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0606, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0591, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0594, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0589, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0541, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0534, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0563, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0540, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0590, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0551, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0598, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0659, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0573, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0552, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0533, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0531, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0597, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0675, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0550, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0571, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0544, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0539, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0553, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0593, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0532, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0599, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0548, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "models_D = []\n",
    "hist_losses_D = []\n",
    "hist_hitsss_D = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + 1):\n",
    "    SUFFIX = f\"D{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    SAVENAME = \"../models/autosave/\" + title + \".pt\"\n",
    "    PLOTSAVE = \"../plots/autosave/\" + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_ewc(model, task_id, 500, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D.append(hist_loss_temp)\n",
    "    hist_hitsss_D.append(hist_hits_temp)\n",
    "    models_D.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_ewc(model, task_id, train_dls[task_id], criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtYElEQVR4nO3deXxcddn//9fVNE2bJl2T7qVlqSyCbN1QlMUbqYhUARFEoAXtryqiiIKKtyiIcit4QwHpDxEqeguoLBapG7iAItoU2Te7QiilbbrQvVmu7x/XjJlOp820nWTOJO/n43EeM2eZM9dMMvOe8zmfc465OyIiIknTrdgFiIiI5KKAEhGRRFJAiYhIIimgREQkkRRQIiKSSAooERFJJAWUSB7M7Ddmdl6x6xDpSkzHQUlnZWbrM0YrgS1Ac2r8/3P3/+ugOhYDn3D3hzvi+UQ6i+7FLkCkvbh7Vfr+zkLCzLq7e1NH1iYibVMTn3Q5ZnasmdWb2WVmtgy4w8z6m9mvzWyFma1O3R+R8Zg/m9knUvenmNlfzeza1LKLzOz9u1FHhZldb2ZLU8P1ZlaRmleTqmGNma0ys8fMrFtq3mVm9rqZrTOzl83svQV6a0QSRQElXdUQYAAwCphGfBbuSI3vBWwCbtrJ4ycALwM1wHeBH5mZ7WINlwMTgcOAQ4HxwNdS8y4B6oFaYDDwVcDNbH/gQmCcu1cDJwKLd/F5RUqCAkq6qhbgCnff4u6b3L3B3e91943uvg64GjhmJ49f4u4/dPdm4MfAUCJIdsXZwJXuvtzdVwDfBM5JzWtMrXOUuze6+2MeO4ybgQrgIDMrd/fF7r5gF59XpCQooKSrWuHum9MjZlZpZv+/mS0xs7eAR4F+Zla2g8cvS99x942pu1U7WHZHhgFLMsaXpKYBfA+YD/zezBaa2ZdTzzUf+DzwDWC5md1tZsMQ6YQUUNJVZXdfvQTYH5jg7n2A96Sm72qz3a5YSjQppu2Vmoa7r3P3S9x9H+CDwBfS+5rc/WfufnTqsQ78TzvWKFI0CiiRUE3sd1pjZgOAKwq8/nIz65kxdAfuAr5mZrVmVgN8HfgpgJmdbGb7pfZrvUU07TWb2f5mdnyqM8XmVM3NuZ9SpLQpoETC9UAvYCXwBPDbAq9/DhEm6eEbwLeAOuAZ4FngydQ0gDHAw8B64O/AD9z9z8T+p2tSdS4DBhEdKEQ6HR2oKyIiiaQtKBERSSQFlIiIJJICSkREEkkBJSIiiaSAEhGRRFJAiYhIIimgREQkkRRQIiKSSAooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJiEgiKaBERCSRFFAiIpJICigREUmk7sV64pqaGh89evQeraOhoQGAgQMHFqAiEWlP+rzKjsybN2+lu9dmTy9aQI0ePZq6uro9WsesWbMAmDJlyp4XJCLtSp9X2REzW5Jrupr4REQkkdoMKDO73cyWm9lzO5hvZjbDzOab2TNmdkThyxQRka4mny2oWcCkncx/PzAmNUwDbtnzskREpKtrM6Dc/VFg1U4WmQzc6eEJoJ+ZDS1UgSIi0jUVYh/UcOC1jPH61LTtmNk0M6szs7oVK1YU4KlFRKSzKkRAWY5pnmtBd7/V3ce6+9ja2u16FIqIiPxHIQKqHhiZMT4CWFqA9YqISBdWiICaDZyb6s03EVjr7m8UYL0iItKFtXmgrpndBRwL1JhZPXAFUA7g7jOBOcBJwHxgIzC1vYoVEZGuo82Acvez2pjvwGcKVpGIiAg6k4SIiCSUAkpERBJJASUiIomkgBIRkURSQImISCIpoEREJJEUUCIikkgKKBERSSQFlIiIJJICSkREEkkBJSIiiaSAEhGRRFJAiYhIIimgREQkkdq83IaIdD3z50NTE+y/P2zeDNdfD9ddBz16wF57tQ777AMTJsChh0J3fZtIgelfSqRIXn0VbrsNliyJIBg9Gtatg4YG2LQJGhvBDCoroaoK9t4bxoyBAw6AsrIdr7e5GRYtimV69oQhQ2I9AK+8Ag8/DKedBoMHw6pVcMEFMHdu3J52WoTRrFngDsOHx2Pr6+EDH4h1vfoqPP00PPhghBdA795w+ulw8cURVlu2xGupqWnnN1E6NQWUSA4vvwzLl8O73906bfFiGDQoAgPiC/xf/4ov5HRg/PSn8eV+3XUxPdPixRFIb7wR9//851jHkCFw553bLmsG5eUxv7Fx23njx8MDD8DQodtO/9e/Yv333gtvvtk6feRI+PCHI/juugtaWuCyy+DTn4Z77oGlS+Hoo+HKK2Po0QMuuQTe9jb4wx8ixO68E447btvnc4fXXoPHH4dHHoGf/Qx+/GOorYUVK1prPe88mDIljzddJJu7F2U48sgjfU/dcccdfscdd+zxekQyPf20e//+7uB+xhnuc+e6n3eeu5n7/vu7P/+8+/r17h/5SCxz2mnumza5P/SQe1mZe7du7uXl7t/+tvuqVbHO++5z79s35g8b5n744e5f+Yr7okUx/623Yr319bGuTI2N7g0NUcfNN7v37u0+YoT7E0/EvDVr3D/zmaivVy/30093v+029zvucJ8xw/2UU9wrKtwrK92/+EX3xx93P/XUqH2vvdz/8Y94npdecr/uOvcFC3bvfWtocP/ud90vuMD9m990/9a33A85pPV91OdVdgSo8xw5oYCSLmvZMvcf/tD9rLPc3/nO+HL++9/dBw92Hz7c/atfjS92iNvp090HDYqAOOCACKIzzoj548ZFABxxhPvixa3hlQ4BcB87dve//DM99ZT7yJGxTrOozcz9s5+NsMpl/foIwUzPPuu+evWe17MzLS3ul18etd5wgz6vktuOAqpkm/gaG6MZoWfPYlcipejhh+GjH43mqyFDornskktiXk1NNL8dcABMnQq/+lXsXxk1Cr72NTjjDHjxRZgzB048EU49Fc49F4YNg4ceivXdcw989rPwt7/BvHlwzjnw3/8NFRV7Xvuhh0JdXTTlLVsGa9fC2WfDuHE7fkzv3ttPO/jgPa+lLWbw5S9H0+PChXDYYe3/nNJ55BVQZjYJuAEoA25z92uy5h8L/ApYlJp0n7tfWbgyc3vlFRgwoL2fRYpt+XK46Sb40IfgiCN2vuzKlREcf/lL7BeqqoofMi+8AG+9BUcdFfuR/vd/4cADI6gOOyy+SJ97LoLljDMinAD22681uCA6DTz2WHQC6NUrpn30o3D44dCvX6wbYn3vfve2+7AKadAg+NSn2mfdhVZVBd/8JjzxRPx9RPLVZkCZWRlwM3ACUA/MNbPZ7v5C1qKPufvJ7VBjTuXl8Ut3xYr4sijEL1NJluZmmDEjvtzWro3eZQ89tO2X/po1sWN/3rzoabZ4cXQCGDgw/kfWrYsfMQcdFFtAv/1t/M98+MOxQ7+6unVdBx+c31ZFt26t4ZT2trcV4AV3YhdcAM88E38fkXzlswU1Hpjv7gsBzOxuYDKQHVAdrrY2mjj+8Ac4ucOiUTrK3XfDF74AkybBpZfGFsOJJ8LNN8M73gGvvx490ZYtiy2jCROiqe3kk2NLK921OpN79HAbPDj3fGkf3bvHe75oUfxoyPxhILIj+QTUcOC1jPF6YEKO5Y4ys6eBpcAX3f357AXMbBowDWCvvfba9Wqz9O8fzTj33quA6oyeey62gn796/g7/+UvcMIJcP75rcscdBDcf//O979kMot9RNLx0t3zX3op/7+XdG35nOoo1+9Mzxp/Ehjl7ocCNwIP5FqRu9/q7mPdfWxtbe0uFZqzMItmvgcegK1b93h1kjCLFsXZCtLHGA0eHAeU/vOfEUo/+1k07enLrjSkA+rFF4tbh5SOfAKqHhiZMT6C2Er6D3d/y93Xp+7PAcrNrEOOIa+tjf0Qf/pTRzybdKTFi+PsCZkqKiKQPvQhOOss9eIsJb16xY9KBZTkK5+AmguMMbO9zawHcCYwO3MBMxtiFi36ZjY+td6GQhebS//+0Uvol7/siGeTjrR4cZz+RzoHswgpBZTkq82Acvcm4ELgd8CLwM/d/Xkzm25m01OLnQ48l9oHNQM4M3XwVbvr1g1OOSW6B8+f3xHPKB1h06bozKCA6lwqKxVQkr+8joNKNdvNyZo2M+P+TcBNhS0tf1ddFd2HTzkljrXo06dYlUihpLsjK6A6l8pKWLAg9hn36FHsaiTpOsX1oPbZJ5r4XnkljqjftKnYFcmeSgdU9j4oKW2VlXF827//XexKpBR0ioCCONPyjBnRJXnIkNYDA6U0aQuqc0qfcknNfJKPThNQEAdt/vnPcW60n/8cjjwSvv51dUEvRYsWRROQjlnqXNJn4FBAST46VUABHHMM3HFHXATuYx+L/VNvfzt85Svw17/Chg3FrlDysXhxnJqoW6f7D+3aysri76qAknx02o//gAFxrrWHHoIRI+Daa+McblVVcQG3z38+zuEnyZTrGCjpHA48UAEl+em0AZV20klxEO/KlXDffbFFNWEC3HBDbG3V1xe7QslFx0B1XgceGFcsbmkpdiWSdJ0+oNL69o0zWH/ta9Hj79574fnnYcyYuITCfvtFM+Bbb8XyTz0FX/pSdL4YNAi++tX8nqepKa59s2pVu72UTm/9+jjjuAKqczrwwOhpu2RJsSuRpOsyAZXt1FPjnG7TpsVW1oEHwjXXxGUT3vWuuL7PjBmwcSMccgh85zvwgx+0vd6rr4ZPfhIuu6z9X0Nnlf7iUkB1Tm9/e9w+/nhx65Dk67IBBRFKN9wAP/whPPhgBNaBB8bWz3XXxWUc/vEP+P3v42zpF10Ed90Fzz4bx1xlnyvj8cfj2kT9+sGsWdETTXadjoHq3CZMiJaL66/f/jMkkqlkL/neHsaNy33S2bKyOHP20UdHz8C0UaPi6qsTJ8bZK6ZNi2lz5sRVWr/97Qg/2TU6BqpzKyuLqxRPnx6XUDn22GJXJEmlgMpTdTU8+mgEWHNzbGU98EBcOrypKZbp1i0uB37AARFWt9wS+660JbBrFi2Ks5QPHlzsSqS9nHsu/Pd/R+9aBZTsiAJqF/TtG5d5SPvkJ2H1anj11bjkx5AhsP/+Me+yy+DWW+H44yOg+vePfVmHHRZXhc2+ZLi0Sh8DpSvedl69esGFF8IVV8ALL8SFJ0Wydel9UIXQvz8cemh0WU+HE0TPwB/9KPZpNTXF1WGvvDJ6Eh56KPztb4V5/paWaEr88Ifj2K5Zs3K36zc2RnNK0rv2ukNdXWyFSuf26U9HUB18cByfOGECNHTIRXqkVGgLqh2dfXYMaRs2RBPhZz8bBw1PnhxBNmBA6zBwYNxu3QpPPx092j7wgejunr1F0dQUW3GzZsG++8LDD0cX7SVL4pdpposugpkz4Xvfgy9+sd1f+m6bNy/q/8Y3il2JtLeamjg28fHHYd266CX78Y/HwfU6g4iAAqpD9e4dvQGPOQYuvxx+85vYr7V69Y57M3XvDt//fnTNPe202PoaMiSufXXPPdEh45vfjPZ8gPPPjy/3gw6Cj3wkpt11V4TT4MGxT+z44+GIIzrkJe+yX/wiXvMppxS7EukIkybFALHVPH06fOtbMHUq/OEPrQfSDxwY/9tqGu9aFFBFUF0dx1iltbTA2rXRvLFqVQxm8I53RBPi3XfHr8urrto2yHr0iG7yF13UOm3mzOgCf9550aw4YgRcfHH0QLz33gims86CJ59sPbN0UrjHQdTvfW9sRUrXMm1anC/ziiu2bwEAuPHGaC2YOLHDS5Mi0YZ0AnTrFkG0334wfnz8ojzxRBg6NHqzTZkSx2itXx+3v/41vPRSjGeGE0BFBdx/f3TIuOqq+NBXVkbIDRoEP/1pXItn0iRYuDCaCa+7Ls5PeOONERKNjXFWjXHjYgd2R3nqqajp9NM77jklOcziB9ZFF0WrwXPPxf9nc3Mci7hpUxxE/7GPxT7cpqa4+OFjj0XHmubmYr8CKTRtQZWQysoIjbYMGhQHGG/eHE2Bw4dHAEJ06f3JT2IH9TveERd7fPbZOObooouiybG+Pq5MXFUFRx0Vly458cT2fGXhl7+MY2Qye0pK19K7d7QKZDvhhPg/vfLKOLbwrrvifyUzlMrL4wwx3/mODu3oLBRQnVjPntFDKtvZZ0cnjU98In6l/vznsX/r2mtjH1Xv3rF/a+JE+OAH41RQBx8cHTEGDIgvhS1bWpske/WKHd4DB7Z28qioiCbI9FBevu14jx6xpbZhQ3QIKS+P5zzuuFiXSLY+feJ/9BvfiIBatCjOSDFsWPyoevrpOA/m/ffH/3NlZbRODB4cP9KqqqKFoHv3+MHWr1/8iFu3Lm7T8wYNisf07Nn63NkdlNLjPXrE/3tZWTTVr14d9/v2jWXcY93r18f/es+esf6ddQJJf742b47PRWVlrLMrUkB1UXvtFc0mLS2tH5ZLL40w6tcv9l1B7BP4znfi6sQvvhgn0y0riw/mwIERJps2RbPhE09EaO3JBSK/8pU9fmnSyVVVRe/VXC69NDoMPfxwhENTU5x4uD0PrzCLmtavb91HXFYWP9w2btz+uXv0iJBqaYkfaZlDU1PuWnv2jB+OvXvH59WsNSS7dYsg69699Xbz5vjxuG5d1FJevvNhw4ZYftOmqK+ionXoniMlqqoihMeObe2g1R7yCigzmwTcAJQBt7n7NVnzLTX/JGAjMMXdnyxwrdIOsn/JZW9xVVfHcVb5co8P5datOx4aG+MXYnl5fODSW1Nmse9MZHeNGBEXLM3U1ARvvhlfvunx1atj6NUr/scrKuKz0NgIy5fHeTgbG2P57B62meObN8ePsjVrYgtv4MDYAmpoiM9BVVX8j6dvN26MA/vffHPbQMkOjF69oqZ0K0Pm0NLSWoN7jDc1tQZcY2M89sgj47U1N28fhNlDTU2cILtXr/iMbtnSOmTv22tpiTBevDh+zLanNgPKzMqAm4ETgHpgrpnNdvfM3efvB8akhgnALalb6WLMWn/piSRB9+7RxCelJ59efOOB+e6+0N23AncDk7OWmQzc6eEJoJ+ZDS1wrSIi0oXk08Q3HHgtY7ye7beOci0zHHgjcyEzmwZMS42uN7OXd6na3GqmTp26sgDrKYYaQLUXRynXX9K16/NaNEmuf1SuifkEVK5Tdmaf9yCfZXD3W4Fb83jOvJlZnbuPLeQ6O4pqL55Srl+1F0cp1w6lWX8+TXz1wMiM8RHA0t1YRkREJG/5BNRcYIyZ7W1mPYAzgdlZy8wGzrUwEVjr7m9kr0hERCRfbTbxuXuTmV0I/I7oZn67uz9vZtNT82cCc4gu5vOJbuZT26/k7RS0ybCDqfbiKeX6VXtxlHLtUIL1m+/oNNoiIiJFpJPFiohIIimgREQkkRRQIiKSSAooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJiEgiKaBERCSRFFAiIpJI+VwPql3U1NT46NGj92gdDQ0NAAwcOLAAFYlIe9LnVXZk3rx5K929Nnt60QJq9OjR1NXV7dE6Zs2aBcCUKVP2vCARaVf6vMqOmNmSXNPVxCciIonUZkCZ2e1mttzMntvBfDOzGWY238yeMbMjCl+miIh0NflsQc0CJu1k/vuBMalhGnDLnpclIiJdXZsB5e6PAqt2sshk4E4PTwD9zGxooQoUEZGuqRD7oIYDr2WM16embcfMpplZnZnVrVixogBPLSIinVUhAspyTMt5HXl3v9Xdx7r72Nra7XoUioiI/EchAqoeGJkxPgJYWoD1iohIF1aIgJoNnJvqzTcRWOvubxRgvSIi0oW1eaCumd0FHAvUmFk9cAVQDuDuM4E5wEnAfGAjMLW9ihURka6jzYBy97PamO/AZwpWkYiICDqThIiIJJQCSkREEkkBJSIiiaSAEhGRRFJAiYhIIimgREQkkRRQIiKSSAooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJiEgiKaBERCSRFFAiIpJICihJnH/+E+rqil2FiBRbmxcsFOloF18MW7YopES6OgWUJM7KlfDqq9DcDGVlxa5GRIpFTXySOKtXw+bNMH9+sSsRkWJSQEmiuEdAATzzTHFrEZHiyiugzGySmb1sZvPN7Ms55h9rZmvN7KnU8PXClypdwYYN0NQU9599tri1iEhxtbkPyszKgJuBE4B6YK6ZzXb3F7IWfczdT26HGqULSW89gbagRLq6fLagxgPz3X2hu28F7gYmt29Z0lWlA6pnT21BiXR1+QTUcOC1jPH61LRsR5nZ02b2GzN7e64Vmdk0M6szs7oVK1bsRrnS2a1ZE7cTJ8LChbBuXVHLEZEiyiegLMc0zxp/Ehjl7ocCNwIP5FqRu9/q7mPdfWxtbe0uFSpdQ3oL6phj4vb554tXi4gUVz4BVQ+MzBgfASzNXMDd33L39an7c4ByM6spWJXSZaQD6j3viVs184l0XfkE1FxgjJntbWY9gDOB2ZkLmNkQM7PU/fGp9TYUuljp/NIBddhhUFWljhIiXVmbvfjcvcnMLgR+B5QBt7v782Y2PTV/JnA68CkzawI2AWe6e3YzoEibVq8GM+jXDw45RFtQIl1ZXqc6SjXbzcmaNjPj/k3ATYUtTbqi1auhb1/o1i0C6he/iIN3LdeeUBHp1HQmCUmU1auhf/+4f/jhMf7oo8WtSUSKQwEliZIZUGefDXvvDeefD+vXF7cuEel4CihJlMyAqq6GH/8YFi2CL36xuHWJSMfT5TYkUVavhuEZh4G/+91wySVw7bXw5puw114wbhyceSZ013+vSKemLShJlMwtqLSrroKPfxxefBFuvx3OOQcOOADuuAPeeqs4dYpI+1NASWKkL7WRHVA9e8JPfgIvvRSB9KtfRfPf+efDwIFw/PHw2GPbPqalpePqFpH2oYCSxNi8GbZu3T6gMpnBKafAvHnwl79E89+CBXDyya2nRfrxj2Mdv/hFx9QtIu1DASWJkT6LxM4CKq1btzgd0jXXwF//CpWV8IEPRHPglCkRdFOnwgvZF4URkZKhgJLE2JWAyjRyJDz4ICxfDl//Opx6agRTVVXc134qkdKkgJLE2N2AAhg7NkLqyivhnnvi+Kl77oH586GmBkaMiH1Vv/td7OtKa2mBX/4SvvQlWLKkMK9DRApDHXUlMdIB1a/f7j3+ve+NIe2YY+C3v4VHHomtq0cegUmT4Oij4cgjo/PFgw+2NgP+4AcRcJ/7nLqwiySBPoaSGHuyBbUj//VfMQBs2QI/+hFcfz3MmgUbNsCBB8Jdd8GECXDRRXFA8D//CT/7GZSVFa4OEdl1CihJjPYIqEwVFfDpT8eQy+zZcUDwpZfCgAGxRaWT1IoUjwJKEmNPm/j2lFnsi1q1KnoHrl0Lxx0HBx8cTYI9ehSnrq5m7drYeq2qKnYlUmwKKEmM1auhT5/iN619+9vRHDhzZjT/QXxZHn88fOhD8OEPR4hu2gSvvBK9BDduhHe8A4YOLWblpe/3v4cPfjAOExgwAC64AL773WJXJcWiXnySGLnOIlEMZvD978cZ1BctgnvvjVMtPf10nL1i8OA41VJ1dVz59z3vic4X++4b3dzXrIFly+LA4a1bi/1qSsdLL8EZZ8D++8cW7LveBd/7ng647sq0BSWJkZSASuvWDUaPjuHUU6N7+ty5sVW1cCF85CNxUcUBA2Kr79Zb40Dhq65qXUefPnEA8fveFye5PeCA4m8hFlNLC1x9NcyYAcOGxTW/xoyJLc+rr45m1AcfhFGjoLExQmr69LgdNqzY1UtHU0BJYiQtoLKZwfjxMeRy3HFw8cVxrFVNTTQL/vnP0fki3VRYWRlfyocdBitWxFbZxo2w335xwPHmzdFkOGwYHHFEDIceGo+D2Drr1Ss6fJSaVaviuLRbboktTnd46KE4BAAinP74xwgngPLyOAfj4YfD6afHYQMbNsQxbhMmxHtWVhZDdXXXDv7OSgElibF6dWxhlLLsADvnHGhuhpdfjvMHzpsHdXVxvsDa2gifqqo4n+Cf/gS9e8dQVxdnbofWLbmGhtYOBAceGE2KjY3RjFhZCX37xhZb377x5f766zH06hXNkoMHw6BBcYJdiK2Z5uZth8xp1dXxmKoqWLkyAmbgwLjkSUtLHNi8cmU8X//+sSU5YEA897p1sY+uR4+o8eqro7ZevSLA3/e+1vdo40Z44414nsGDt30/998fbroptqLq6uLxOzozSJ8+sW+wf/+4zR56947aevRoHTLHy8vjvW1sjKF///ih0KdPjDc1tc5Lv+/pYfPmCNply+LvNXBgDDU1rVvYEM9RXR2vwyxCOn3geOZt9rQtW+LzsX59vEdDh8bzbNgQ01evjve8pib+Pr16tT5+w4b4v+nePd6DHj3itaT/zk1N8dhVq2L5UaNiPc3Nsd6ysnj/uuXYIeTevj1dFVCSGGvWJHsLaneVlcFBB8Vwzjn5PcYd6uvhySdjeOmlCLTRo+N9euqpaGZMf7EuXx5f3GvXxm1zc3yRDRsWX55/+UsEXLFUVsJll8UZPTLDKT1v3313/Njzz4dzz209ePqNN+JYtfr6CMrGxnjNa9bEF+qaNTEsWtR6v7Od7qp79wiGxsbc83v2jP+hdBDtqoqKCMW0bt3iB0RmqDU3R6vBI4/s3mvIR14BZWaTgBuAMuA2d78ma76l5p8EbASmuPuTBa5VOrmkN/F1JLNo8hs5EiZP3rXHuseXR/bZMBobY4unoSHWn24e69at9X56vFu3+FX95pvxq72mJv42DQ3w6qvx+FGjIjTfeiv+dqtWxW1jY+tWQnpL4+ij9+yLLPO1DB266+9JU1N84WZu9WRvBW3dGu9bjx7xfKtXw9Kl8frLy2Naeqio2HZLrKIitk4HD473P/0+p4eWlnjPtm6N9W3c2FqbWetWSOZt5v3y8nj/e/eOrbRXX43n6d+/daiujh8qixfHc6Tft379YiuwqSm2phobY3pZWettdXVs6bW0xONffz0CacCAeE8aGuL/Ib18+rE7+2FRCG0GlJmVATcDJwD1wFwzm+3umeeJfj8wJjVMAG5J3YrkZcuWaBJSQO05s9ynaiovjy/3fLvCDxq0/RfQ3nvHeQ9LTTpYevfumOerro73SvZMPltQ44H57r4QwMzuBiYDmQE1GbjT3R14wsz6mdlQd3+j4BVnePbZSPdjj23PZ5GOkG6qUECJSJp55qmdcy1gdjowyd0/kRo/B5jg7hdmLPNr4Bp3/2tq/BHgMnevy1rXNGBaanR/4OUCvIYaYGUB1lMMqr14Srl+1V4cpVw7JLv+Ue5emz0xny2oXH00slMtn2Vw91uBW/N4zryZWZ27l2Cjg2ovplKuX7UXRynXDqVZfz5nkqgHRmaMjwCW7sYyIiIiecsnoOYCY8xsbzPrAZwJzM5aZjZwroWJwNr23v8kIiKdW5tNfO7eZGYXAr8jupnf7u7Pm9n01PyZwByii/l8opv51PYreTsFbTLsYKq9eEq5ftVeHKVcO5Rg/W12khARESkGnc1cREQSSQElIiKJpIASEZFEUkCJiEgiKaBERCSRFFAiIpJICigREUkkBZSIiCSSAkpERBJJASUiIomkgBIRkUTK53pQ7aKmpsZHjx69R+toaGgAYODAgQWoSETakz6vsiPz5s1bubsXLGwXo0ePpq6uru0Fd2LWrFkATJkyZc8LEpF2pc+r7IiZLck1vc0mPjO73cyWm9lzO5hvZjbDzOab2TNmdsSeFisiIpLPPqhZwKSdzH8/MCY1TANu2fOyRESkq2szoNz9UWDVThaZDNzp4Qmgn5kNLVSBIiLSNRWiF99w4LWM8frUNBERkd1WiICyHNNyXqbXzKaZWZ2Z1a1YsaIATy0iIp1VIQKqHhiZMT4CWJprQXe/1d3HuvvY2trtehSKiIj8RyECajZwbqo330Rgrbu/UYD1iohIF9bmcVBmdhdwLFBjZvXAFUA5gLvPBOYAJwHzgY3A1PYqVkREuo42A8rdz2pjvgOfKVhFIiIi6Fx8IiKSUAooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJiEgiKaBERCSRFFAiIpJICigREUkkBZSIiCSSAkpERBJJASUiIomkgBIRkURSQImISCIpoEREJJEUUCIikkgKKBERSSQFlIiIJJICSkREEkkBJSIiiZRXQJnZJDN72czmm9mXc8w/1szWmtlTqeHrhS9VRES6ku5tLWBmZcDNwAlAPTDXzGa7+wtZiz7m7ie3Q40iItIF5bMFNR6Y7+4L3X0rcDcwuX3LEhGRri6fgBoOvJYxXp+alu0oM3vazH5jZm/PtSIzm2ZmdWZWt2LFit0oV0REuop8AspyTPOs8SeBUe5+KHAj8ECuFbn7re4+1t3H1tbW7lKhIiLSteQTUPXAyIzxEcDSzAXc/S13X5+6PwcoN7OaglUpIiJdTj4BNRcYY2Z7m1kP4ExgduYCZjbEzCx1f3xqvQ2FLlZERLqONnvxuXuTmV0I/A4oA2539+fNbHpq/kzgdOBTZtYEbALOdPfsZkAREZG8tRlQ8J9muzlZ02Zm3L8JuKmwpYmISFemM0mIiEgiKaBERCSRFFAiIpJICigREUmkvDpJiIjsqU2bYhDJlwJKRDrEq6/Cm29CQwMMHFjsaqQUqIlPRDrEpk3gDvffX+xKpFQooESkQ6Sb9+6+u7h1SOlQQIlIu9u8GbZuhbIy+NOfoqlPpC0KKBFpd4sWxe3w4dDSAvfeW9x6pDQooESk3S1cGLcDB8Lb365mPsmPAkpE2t2CBXHbqxeceSb89a+toSWyIwooEWl3CxdCt25QXg4f/zhUVsIJJyikZOcUUCLS7hYsiK0ngNGj4Y9/hDVr4Oij4YknilmZJJkCSkTa3cKFrQEFMH48PPZYbFUddRQcd1zsl3ruOVi3DpYuhbo6WLGieDVL8SmgRKRdtbREQPXsue30gw6C55+Ha6+NLayzzoJDDoE+faK337hxsM8+MGMGNDcXp3YpLp3qSETa1bJlcRxU5hZUWt++cMklcNFF8NRTEVRLlsT02lq47Tb43Ofghz+Ed74T3va2aBYcNy62vqRzU0CJSLtK9+DL3oLKVF4eoTNu3LbTTz01mv5uvDGOnWpoiOm1tbFs377R4WLLljhTxSGHwGmnRVd2s/Z5PdJxFFAi0q7SPfVybUG1xSya/s46K8ZXrICHH4aHHoIXXoBXXoENG2Ld5eVw333wjW9EgFVXx/TKyrjt3Ruqqlpv08G2bl2su7Jy26Fnz9ZQ3bQpzoRRXg4VFdHkuHlz6xnat26N6enH9u4d42VlsaVnFrc7G9paJj3fLIbNm6P2LVtaX2d6KCuDpqadDy0trbVu3Rrram6Gfv2imbWlZdvl082s6ec3ix8IBx20J/8dO6eAEpF2tWBBfLHubAsqX7W12wZWtmXL4IEH4MknYePGCI+NG2NYsSLOaLFhA6xf3xpsVVXxZZtebvPmXauprCzCaPPm+FLvSo4/Hh55pP3Wn1dAmdkk4AagDLjN3a/Jmm+p+ScBG4Ep7v5kgWsVkRK0cCGMHNkxTW5DhsD06Xu2jubmCLbNm2PrxD2CrKIitjS2bIlQ6tUrhu6pb1H3mJ8OuvTZ21taWofs8eyhrfnpZdwj8Kuro670ltzGjRG8LS1R186GdChv2BBbhtXV8brWro2tqW7dtl0+vc8v/fzuMGDAnr3XbWkzoMysDLgZOAGoB+aa2Wx3fyFjsfcDY1LDBOCW1K2IdHELFsC++xa7ivyVlcVWVVXVrj3OLMKiogL692+f2rqafLagxgPz3X0hgJndDUwGMgNqMnCnuzvwhJn1M7Oh7v5GwSvO8Pe/x6+dz362PZ9FRPbEhg1wwQXFrkJKUT4BNRx4LWO8nu23jnItMxzYJqDMbBowLTW63sxe3qVqc6uBqSsLsJ5iqAFUe3GUcv0lV/ttt8UA1Eydqs9rkSS5/lG5JuYTULlajn03lsHdbwVuzeM582Zmde4+tpDr7CiqvXhKuX7VXhylXDuUZv35HOpWD4zMGB8BLN2NZURERPKWT0DNBcaY2d5m1gM4E5idtcxs4FwLE4G17b3/SUREOrc2m/jcvcnMLgR+R3Qzv93dnzez6an5M4E5RBfz+UQ386ntV/J2Ctpk2MFUe/GUcv2qvThKuXYowfotOt6JiIgki063KCIiiaSAEhGRRCrZgDKzSWb2spnNN7MvF7uebGY20sz+ZGYvmtnzZva51PQBZvYHM/t36rZ/xmO+kno9L5vZicWr/j/1lJnZv8zs16nxUqq9n5n90sxeSv0NjiqV+s3s4tT/zHNmdpeZ9Uxy7WZ2u5ktN7PnMqbtcr1mdqSZPZuaNyN1CrVi1P691P/NM2Z2v5n1K5XaM+Z90czczGqSWHve3L3kBqKzxgJgH6AH8DRwULHryqpxKHBE6n418ApwEPBd4Mup6V8G/id1/6DU66gA9k69vrIiv4YvAD8Dfp0aL6Xafwx8InW/B9CvFOonDnBfBPRKjf8cmJLk2oH3AEcAz2VM2+V6gX8CRxHHVf4GeH+Ran8f0D11/39KqfbU9JFEp7YlQE0Sa893KNUtqP+cfsndtwLp0y8lhru/4akT5rr7OuBF4stnMvHlSer2Q6n7k4G73X2Luy8iekSO79CiM5jZCOADwG0Zk0ul9j7Eh/dHAO6+1d3XUCL1E71re5lZd6CSOKYwsbW7+6PAqqzJu1SvmQ0F+rj73z2+Ne/MeEyH1u7uv3f3ptToE8RxnSVRe8r/Apey7ckSElV7vko1oHZ0aqVEMrPRwOHAP4DBnjpGLHU7KLVY0l7T9cQ/eeYFBEql9n2AFcAdqSbK28ysNyVQv7u/DlwLvEqcKmytu/+eEqg9y67WOzx1P3t6sZ1PbFVACdRuZqcAr7v701mzEl97LqUaUHmdWikJzKwKuBf4vLu/tbNFc0wrymsys5OB5e4+L9+H5JhWzL9Hd6Lp4xZ3PxzYQDQz7Uhi6k/tq5lMNMMMA3qb2cd39pAc0xL5WUjZUb2Jex1mdjnQBPxfelKOxRJTu5lVApcDX881O8e0xNS+I6UaUCVxaiUzKyfC6f/c/b7U5DdTm9WkbpenpifpNb0LOMXMFhPNp8eb2U8pjdoh6ql393+kxn9JBFYp1P9fwCJ3X+HujcB9wDspjdoz7Wq99bQ2pWVOLwozOw84GTg71fQFya99X+KHzdOpz+4I4EkzG0Lya8+pVAMqn9MvFVWqJ8yPgBfd/fsZs2YD56Xunwf8KmP6mWZWYWZ7E9fW+mdH1ZvJ3b/i7iPcfTTx3v7R3T9OCdQO4O7LgNfMbP/UpPcSl4cphfpfBSaaWWXqf+i9xP7LUqg90y7Vm2oGXGdmE1Ov+9yMx3Qoiwu0Xgac4u4bM2YlunZ3f9bdB7n76NRnt57oqLUs6bXvULF7aezuQJxa6RWiN8rlxa4nR31HE5vKzwBPpYaTgIHAI8C/U7cDMh5zeer1vExCetIAx9Lai69kagcOA+pS7/8DQP9SqR/4JvAS8BzwE6LnVWJrB+4i9pc1El+KF+xOvcDY1GteANxE6kw3Rah9PrG/Jv25nVkqtWfNX0yqF1/Sas930KmOREQkkUq1iU9ERDo5BZSIiCSSAkpERBJJASUiIomkgBIRkURSQImISCIpoEREJJH+H1hc5vqke/R0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.95% | Gr acc 0.9 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.71% | Gr acc 0.42 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.68% | Gr acc 0.35 | Ugr acc 1.0\n",
      "Task 2: Acc 0.6% | Gr acc 0.19 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.93% | Gr acc 0.9 | Ugr acc 0.95\n",
      "Task 1: Acc 0.7% | Gr acc 0.4 | Ugr acc 1.0\n",
      "Task 2: Acc 0.81% | Gr acc 0.64 | Ugr acc 0.97\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer D: Representation ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer E: DynaMoE\n",
    "\n",
    "1. Create DynaMoe network functions:\n",
    "2. Decider Network\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-123-57aabb6ddb7c>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-123-57aabb6ddb7c>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    network_id =\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class DynaMoE(nn.Module):\n",
    "    def __init__(self, gating, experts):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gating = gating\n",
    "        self.experts = experts\n",
    "        self.activatedExperts = 1\n",
    "    \n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio = 0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gating = self.gating(seqs, seqs_len)\n",
    "        \n",
    "        # @TODO: Probabilistic vs argmax?\n",
    "        network_id = argmax(gating)\n",
    "        \n",
    "        outputs = experts[network_id](seqs, seqs_len, teacher_forcing_ratio)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_experts, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.gate = nn.GRU(embed_dim, n_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "4 - Packed Padded Sequences, Masking, Inference and BLEU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "fa0c181e15994ab92b77e0a9eeb7815659805982e17e67ed50eb685ba3cdee26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
