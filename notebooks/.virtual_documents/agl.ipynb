BASIS = "../"
MODELFOLDER = BASIS + "/models"
PLOTSFOLDER = BASIS + "/plots"
MODELAUTOSAVE = MODELFOLDER + "/autosave/"
PLOTSAUTOSAVE = PLOTSFOLDER + "/autosave/"


from google.colab import drive
drive.mount('/content/drive')


get_ipython().getoutput("cp drive/MyDrive/Colab_data/data.py .")


get_ipython().run_line_magic("matplotlib", " inline")

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

import spacy
import numpy as np

import random
import math
import time
import copy
import itertools

import data


class GrammarGen():
    """
    Generates Grammar sequences from grammars, and offers other functionalities
    Grammars are dictionaries:
    - always have START
    - all paths lead eventually to END
    - Entries starting with the same letter
      have same output
    """

    def __init__(self, grammar=None):
        if grammar is None:
            self.grammar = data.g0()
        else:
            self.grammar = grammar

        # find how many letters in grammar
        self.len = len(set([token[0] for token in self.grammar if (token != 'START' and token != 'END')]))

        # variable to check how many sequences have been generated for the grammaticality test
        self.grammCheckMaxLen = -1

    def __len__(self):
        return self.len

    def generate(self, n):
        """Generates n tokens"""
        ret = []
        count = 0
        hashtrack = set()
        while count < n:
            token = []
            current = 'START'
            while current != 'END':
                # Append current
                if current != 'START':
                    token.append(current[0])
                # Choose next
                r = random.randint(0, len(self.grammar[current]) - 1)
                current = self.grammar[current][r]
            # Check if seq is already inside
            tokenhash = ''.join([str(x) for x in token])
            if tokenhash not in hashtrack:
                hashtrack.add(tokenhash)
                ret.append((token, ))
                count += 1

        return ret

    def generateAllGrammatical(self, maxlen=float('inf')):
        """Generates all grammatical sequences until length maxlen"""
        def genAllHelp(seq, current):
            if current == 'END':
                return [seq]
            if len(seq) >= maxlen:
                return []
            # Append Current
            if current != 'START':
                seq.append(current[0])
            # Generate next possibilities
            options = range(len(self.grammar[current]))
            ret = [(genAllHelp(copy.copy(seq), self.grammar[current][i]))
                   for i in options]
            return itertools.chain(*ret)
        return set([tuple(seq) for seq in genAllHelp([], 'START')])

    def isGrammatical(self, seqs):
        """Check for grammaticality of sequences in seqs"""
        maxlen = max([len(seq) for seq in seqs])
        if self.grammCheckMaxLen < maxlen:
            self.allGrammatical = self.generateAllGrammatical(maxlen)
            self.grammCheckMaxLen = maxlen

        return [tuple(seq) in self.allGrammatical for seq in seqs]


def init_weights(m):
    for name, param in m.named_parameters():
        if 'weight' in name:
            nn.init.normal_(param.data, mean=0, std=0.01)
        else:
            nn.init.constant_(param.data, 0)


def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

class CosineLoss():
    def __init__(self, vocabsize, ignore_index, reduction="mean"):
        self.vocabsize = vocabsize
        self.ignore_index = ignore_index
        self.eye = torch.eye(self.vocabsize).to(device)
        self.cosSim = nn.CosineSimilarity(dim=1)
        self.reduction = reduction

    def __call__(self, outputs, labels):
        maxlen = outputs.shape[0]
        bs = outputs.shape[1]

        # Deal with positions which should be ignored and make them the same as label
        if self.ignore_index is not None:
            ignore_positions = (labels == self.ignore_index).to(device)
            outputs[ignore_positions] = self.eye[self.ignore_index]

        # Convert labels to onehot
        labels_onehot = torch.empty((maxlen, bs, self.vocabsize)).to(device)
        for idx in range(maxlen):
            labels_onehot[idx,:,:] = self.eye[labels[idx,:]]

        # Put labels and output in correct form for torch cosSim function
        batch_first_labels = labels_onehot.permute(1,0,2)
        processed_labels = batch_first_labels.reshape(-1, maxlen * self.vocabsize)
        batch_first_outputs = outputs.permute(1,0,2)
        processed_outputs = batch_first_outputs.reshape(-1, maxlen * self.vocabsize)

        # use cosSim function
        res = (1 - self.cosSim(processed_labels, processed_outputs))

        # Same interface as native los functions: reductions for the output
        if self.reduction == "none":
            return res
        elif self.reduction == "mean":
            return res.mean()
        elif self.reduction == "sum":
            return res.sum()
        else:
            print("error")
            return None


def train(model, iterator, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for i, batch in enumerate(iterator):
        
        src, src_len = batch
        trg = src
        
        optimizer.zero_grad()
        
        output = model(src, src_len, trg)
        
        #trg = [trg len, batch size]
        #output = [trg len, batch size, output dim]
        
        if isinstance(criterion, CosineLoss):
            loss = criterion(output, trg)
        else:
            output_dim = output.shape[-1]
            
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)
            
            #trg = [(trg len - 1) * batch size]
            #output = [(trg len - 1) * batch size, output dim]
            
            loss = criterion(output, trg)
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(iterator)


def evaluate(model, dataloader, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():
    
        for seq, seq_len in dataloader:

            output = model(seq, seq_len, seq, 0) #turn off teacher forcing
            
            #seq = [seq_len, batch_size]
            #output = [seq_len, batch_size, output_dim]

            if isinstance(criterion, CosineLoss):
                loss = criterion(output, seq)
            else:
                output_dim = output.shape[-1]
                
                output = output[1:].view(-1, output_dim)
                trg = seq[1:].view(-1)
                
                #trg = [(trg len - 1) * batch size]
                #output = [(trg len - 1) * batch size, output dim]
                
                loss = criterion(output, trg)

            epoch_loss += loss.item()
            
    return epoch_loss / len(dataloader)


def evaluate_extra(model, dataloader, loss_func):

    model.eval()

    loss_total = 0

    with torch.no_grad():
        
        for seqs, seqs_len in dataloader:

            outputs = model(seqs, seqs_len, seqs, 0)

            loss = loss_func(outputs, seqs) / seqs.shape[1]

            loss_total += loss.item()
        
#        if loss_func == allOrNoneLoss:
#            return loss_total

        return loss_total / len(dataloader)

def cutEndToken(seq):
    ret = []
    for stim in seq:
        if stim == END_TOKEN:
            break
        ret.append(stim)
    return ret


def allOrNoneLoss(output, trg):
    bs = output.shape[1]
    ret = 0
    pred = output.argmax(-1)[1:]
    trg = trg[1:]
    for b in range(bs):
        p = cutEndToken(pred[:,b].tolist())
        t = cutEndToken(trg[:,b].tolist())
        ret += not p == t
    return torch.tensor(ret)


def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs


def fit(model, task_id, epochs, step_size_evaluation, clip ):
    best_valid_loss = float('inf')

    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr
    # [task_id, dataset, evaluations]

    for epoch in range(epochs):
        
        start_time = time.time()
        
        train_loss = train(model, train_dls[task_id], optimizer, criterion, clip)
        valid_loss = evaluate(model, valid_dls[task_id], criterion)
        
        end_time = time.time()
        
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)
        
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), SAVENAME)

        if epoch % STEP_SIZE_EVALUATION == 0:
            idx = epoch//STEP_SIZE_EVALUATION
            for other_id in range(task_id + 1):
                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], criterion)
                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], criterion)
                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], criterion)
                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)
                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)
                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)

        
        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
        
    return total_loss, total_hits


class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):
        super().__init__()
        
        self.embedding = nn.Embedding(input_dim, emb_dim)
        
        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)
        
        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, src, src_len):
        
        #src = [src len, batch size]
        #src_len = [batch size]
        
        embedded = self.dropout(self.embedding(src))
        
        #embedded = [src len, batch size, emb dim]
                
        #need to explicitly put lengths on cpu!
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))
                
        packed_outputs, hidden = self.rnn(packed_embedded)
                                 
        #packed_outputs is a packed sequence containing all hidden states
        #hidden is now from the final non-padded element in the batch
            
        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) 
            
        #outputs is now a non-packed sequence, all hidden states obtained
        #  when the input is a pad token are all zeros
            
        #outputs = [src len, batch size, hid dim * num directions]
        #hidden = [n layers * num directions, batch size, hid dim]
        
        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]
        #outputs are always from the last layer
        
        #hidden [-2, :, : ] is the last of the forwards RNN 
        #hidden [-1, :, : ] is the last of the backwards RNN
        
        #initial decoder hidden is final hidden state of the forwards and backwards 
        #  encoder RNNs fed through a linear layer
        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))
        
        #outputs = [src len, batch size, enc hid dim * 2]
        #hidden = [batch size, dec hid dim]
        
        return outputs, hidden


class Attention(nn.Module):
    def __init__(self, enc_hid_dim, dec_hid_dim):
        super().__init__()
        
        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)
        self.v = nn.Linear(dec_hid_dim, 1, bias = False)
        
    def forward(self, hidden, encoder_outputs, mask):
        
        #hidden = [batch size, dec hid dim]
        #encoder_outputs = [src len, batch size, enc hid dim * 2]
        
        batch_size = encoder_outputs.shape[1]
        src_len = encoder_outputs.shape[0]
        
        #repeat decoder hidden state src_len times
        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)
  
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        
        #hidden = [batch size, src len, dec hid dim]
        #encoder_outputs = [batch size, src len, enc hid dim * 2]
        
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) 
        
        #energy = [batch size, src len, dec hid dim]

        attention = self.v(energy).squeeze(2)
        
        #attention = [batch size, src len]
        
        attention = attention.masked_fill(mask == 0, -1e10)
        
        return F.softmax(attention, dim = 1)


class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):
        super().__init__()

        self.output_dim = output_dim
        self.attention = attention
        
        self.embedding = nn.Embedding(output_dim, emb_dim)
        
        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)
        
        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input, hidden, encoder_outputs, mask):
             
        #input = [batch size]
        #hidden = [batch size, dec hid dim]
        #encoder_outputs = [src len, batch size, enc hid dim * 2]
        #mask = [batch size, src len]
        
        input = input.unsqueeze(0)
        
        #input = [1, batch size]
        
        embedded = self.dropout(self.embedding(input))
        
        #embedded = [1, batch size, emb dim]
        
        a = self.attention(hidden, encoder_outputs, mask)
                
        #a = [batch size, src len]
        
        a = a.unsqueeze(1)
        
        #a = [batch size, 1, src len]
        
        encoder_outputs = encoder_outputs.permute(1, 0, 2)
        
        #encoder_outputs = [batch size, src len, enc hid dim * 2]
        
        weighted = torch.bmm(a, encoder_outputs)
        
        #weighted = [batch size, 1, enc hid dim * 2]
        
        weighted = weighted.permute(1, 0, 2)
        
        #weighted = [1, batch size, enc hid dim * 2]
        
        rnn_input = torch.cat((embedded, weighted), dim = 2)
        
        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]
            
        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))
        
        #output = [seq len, batch size, dec hid dim * n directions]
        #hidden = [n layers * n directions, batch size, dec hid dim]
        
        #seq len, n layers and n directions will always be 1 in this decoder, therefore:
        #output = [1, batch size, dec hid dim]
        #hidden = [1, batch size, dec hid dim]
        #this also means that output == hidden
        assert (output == hidden).all()
        
        embedded = embedded.squeeze(0)
        output = output.squeeze(0)
        weighted = weighted.squeeze(0)
        
        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))
        
        #prediction = [batch size, output dim]
        
        return prediction, hidden.squeeze(0), a.squeeze(1)


class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, src_pad_idx, device):
        super().__init__()
        
        self.encoder = encoder
        self.decoder = decoder
        self.src_pad_idx = src_pad_idx
        self.device = device
        
    def create_mask(self, src):
        mask = (src != self.src_pad_idx).permute(1, 0)
        return mask
        
    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):
        
        #src = [src len, batch size]
        #src_len = [batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing
        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time
                    
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        #tensor to store decoder outputs
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        #encoder_outputs is all hidden states of the input sequence, back and forwards
        #hidden is the final forward and backward hidden states, passed through a linear layer
        encoder_outputs, hidden = self.encoder(src, src_len)
                
        #first input to the decoder is the <sos> tokens
        input = trg[0,:]
        
        mask = self.create_mask(src)

        #mask = [batch size, src len]
                
        for t in range(1, trg_len):
            
            #insert input token embedding, previous hidden state, all encoder hidden states 
            #  and mask
            #receive output tensor (predictions) and new hidden state
            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)
            
            #place predictions in a tensor holding predictions for each token
            outputs[t] = output
            
            #decide if we are going to use teacher forcing or not
            teacher_force = random.random() < teacher_forcing_ratio
            
            #get the highest predicted token from our predictions
            top1 = output.argmax(1) 
            
            #if teacher forcing, use actual next token as next input
            #if not, use predicted token
            input = trg[t] if teacher_force else top1
            
        return outputs


class SequenceDataset(Dataset):
    """
    Dataset for Sequences
    """

    def __init__(self, seqs):
        """
        Args:
            size (int): amount of sequences generated
        """
        self.seqs = seqs

    def __len__(self):
        return len(self.seqs)

    def __getitem__(self, idx):
        return self.seqs[idx]


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def collate_batch(batch):
    seq_lens = []
    processed_seqs = []
    # Sort in descending order
    batch.sort(reverse=True, key=(lambda x: len(x)))
    # append start and end token
    for seq in batch:
        seq = [START_TOKEN] + seq + [END_TOKEN]
        seq_lens.append(len(seq))
        processed_seqs.append(torch.tensor(seq))
    # pad
    padded_seqs = pad_sequence(processed_seqs)
    seq_lens = torch.tensor(seq_lens)
    return padded_seqs.to(device), seq_lens.to(device)


N_TASKS = 2
BATCH_SIZE = 1

PAD_TOKEN = 0
START_TOKEN = 1
END_TOKEN = 2


SEED = 54321

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


train_seqs = data.g0_train()
valid_seqs = data.g0_train()
test_seqs = data.g0_test_gr()
test_ugr_seqs = data.g0_test_ugr()


train_seqs.sort(key=(lambda x: len(x)))
valid_seqs.sort(key=(lambda x: len(x)))
test_seqs.sort(key=(lambda x: len(x)))
test_ugr_seqs.sort(key=(lambda x: len(x)))


def buildVocab(letterset):
    vocab = {'<pad>': PAD_TOKEN, '<sos>': START_TOKEN, '<eos>': END_TOKEN}
    counter = len(vocab)
    for letter in letterset:
        vocab[letter] = counter
        counter +=1
    return vocab


letters = set()
for seq in train_seqs:
    [letters.add(letter) for letter in seq]
letters = list(letters)


train_dls = []
valid_dls = []
test_dls = []
test_ugr_dls = []
vocabs = []
rvocabs = []
let2idxs = []
idx2lets = []

usedpermutations = set()
train_convs = []
valid_convs = []
test_convs = []
test_ugr_convs = []
for t in range(N_TASKS + 1):
    # Create normal tasks
    if t != N_TASKS:
        temp_letters = copy.copy(letters)

        while str(temp_letters) in usedpermutations:
            random.shuffle(temp_letters)
 
        usedpermutations.add(str(temp_letters))

        # Vocab
        vocabs.append(buildVocab(temp_letters))
        rvocabs.append({v: k for k, v in vocabs[-1].items()})

        # Conversion Functions
        let2idxs.append(lambda seq: [vocabs[-1][let] for let in seq])
        idx2lets.append(lambda seq: [rvocabs[-1][let] for let in seq])

        # Convert to indices
        train_conv = [let2idxs[-1](seq) for seq in train_seqs]
        valid_conv = [let2idxs[-1](seq) for seq in valid_seqs]
        test_conv = [let2idxs[-1](seq) for seq in test_seqs]
        test_ugr_conv = [let2idxs[-1](seq) for seq in test_ugr_seqs]

        # Add conv seq to sequence collection
        train_convs.extend(train_conv)
        valid_convs.extend(valid_conv)
        test_convs.extend(test_conv)
        test_ugr_convs.extend(test_ugr_conv)

    # Create joint task
    else:
        train_conv = train_convs
        valid_conv = valid_convs
        test_conv = test_convs
        test_ugr_conv = test_ugr_convs

    # Datasets
    train_ds = SequenceDataset(train_conv)
    valid_ds = SequenceDataset(valid_conv)
    test_ds = SequenceDataset(test_conv)
    test_ugr_ds = SequenceDataset(test_ugr_conv)
    
    # Dataloader
    train_dls.append(
        DataLoader(train_ds, batch_size=BATCH_SIZE,
                   shuffle=True, collate_fn=collate_batch))
    valid_dls.append(
        DataLoader(valid_ds, batch_size=BATCH_SIZE,
                   shuffle=False, collate_fn=collate_batch))
    test_dls.append(
        DataLoader(test_ds, batch_size=BATCH_SIZE,
                   shuffle=False, collate_fn=collate_batch))
    test_ugr_dls.append(
        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,
                   shuffle=False, collate_fn=collate_batch))


SWAP1 = 2
SWAP2 = 4
train_dls = []
valid_dls = []
test_dls = []
test_ugr_dls = []
vocabs = []
rvocabs = []
let2idx = lambda task_id, seq: [vocabs[task_id][let] for let in seq]
idx2let = lambda task_id, seq: [rvocabs[task_id][let] for let in seq]

usedpermutations = set()
listUsedPermutations = []
train_convs = []
valid_convs = []
test_convs = []
test_ugr_convs = []
alternate = 0
for t in range(N_TASKS + 1):
    # Create normal tasks
    if t != N_TASKS:
        if alternate == 0:
            temp_letters = copy.copy(letters)

            while str(temp_letters) in usedpermutations:
                random.shuffle(temp_letters)
    
            usedpermutations.add(str(temp_letters))
            listUsedPermutations.append(temp_letters)

            alternate = 1
        else:
            temp_letters = listUsedPermutations[-1]

            #Swap 5th and 6th indice
            temp_letters[SWAP1], temp_letters[SWAP2] = temp_letters[SWAP2], temp_letters[SWAP1]

            usedpermutations.add(str(temp_letters))

            alternate = 0

        # Vocab
        vocabs.append(buildVocab(temp_letters))
        rvocabs.append({v: k for k, v in vocabs[-1].items()})

        # Convert to indices
        train_conv = [let2idx(t, seq) for seq in train_seqs]
        valid_conv = [let2idx(t, seq) for seq in valid_seqs]
        test_conv = [let2idx(t, seq) for seq in test_seqs]
        test_ugr_conv = [let2idx(t, seq) for seq in test_ugr_seqs]

        # Add conv seq to sequence collection
        train_convs.extend(train_conv)
        valid_convs.extend(valid_conv)
        test_convs.extend(test_conv)
        test_ugr_convs.extend(test_ugr_conv)

    # Create joint task
    else:
        train_conv = train_convs
        valid_conv = valid_convs
        test_conv = test_convs
        test_ugr_conv = test_ugr_convs

    # Datasets
    train_ds = SequenceDataset(train_conv)
    valid_ds = SequenceDataset(valid_conv)
    test_ds = SequenceDataset(test_conv)
    test_ugr_ds = SequenceDataset(test_ugr_conv)
    
    # Dataloader
    train_dls.append(
        DataLoader(train_ds, batch_size=BATCH_SIZE,
                   shuffle=True, collate_fn=collate_batch))
    valid_dls.append(
        DataLoader(valid_ds, batch_size=BATCH_SIZE,
                   shuffle=False, collate_fn=collate_batch))
    test_dls.append(
        DataLoader(test_ds, batch_size=BATCH_SIZE,
                   shuffle=False, collate_fn=collate_batch))
    test_ugr_dls.append(
        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,
                   shuffle=False, collate_fn=collate_batch))


print(vocabs[0])
print(vocabs[1])
for i in range(len(valid_dls) - 1):
    for seqs, _ in valid_dls[i]:
        print(f"\nFirst Batch of Task {i}:")
        print(seqs)
        break


let2idx


convTrainSeqs = lambda task_id: [let2idx(task_id, seq) for seq in train_seqs]
convTestSeqs = lambda task_id: [let2idx(task_id, seq) for seq in test_ugr_seqs]
tr1 = convTrainSeqs(0)
tr2 = convTrainSeqs(1)
te1 = convTestSeqs(0)
te2 = convTestSeqs(1)

for seq in tr1:
    if seq in tr2:
        print(se)
        print("hi")

for se in tr2:
    if se in te1:
        print(se)
        print("hi")


for i in range(len(tr1)):
    print(f"{tr1[i]} - {tr2[i]}")


INPUT_DIM = max(vocabs[0].values()) + 1
OUTPUT_DIM = max(vocabs[0].values()) + 1
ENC_EMB_DIM = 30
DEC_EMB_DIM = 30
ENC_HID_DIM = 10
DEC_HID_DIM = 10
ENC_DROPOUT = 0.7
DEC_DROPOUT = 0.7
LEARNING_RATE = 0.01
SRC_PAD_IDX = PAD_TOKEN
TRG_PAD_IDX = PAD_TOKEN
PREFIX = "tr"
N_EPOCHS = 200
CLIP = 1
STEP_SIZE_EVALUATION = 10
TEST_ALL_TASKS = 1  


def plotTransfer(data, title):
    # data = [n_methods, n_tasks, combinedepochs]
    n_methods, n_tasks, n_combinedepochs = data.shape
    fig, axs = plt.subplots(n_tasks, 1)
    colors = ['blue','green','orange','red','yellow','violett']
    
    xvals = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)
    
    for task_idx in range(n_tasks):
        for method_idx in range(n_methods):
            axs[task_idx].plot(
                xvals,
                data[method_idx, task_idx],
                color=colors[method_idx]
            )
            axs[task_idx].set_ylim(0,1.1)
            if task_idx != n_tasks - 1:
                axs[task_idx].tick_params(
                    axis='x',
                    which='both',
                    labelbottom=False
                )
        x_lines = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, N_EPOCHS)
        for xpos in x_lines:
            axs[task_idx].axvline(xpos, color="grey")
    fig.suptitle(title)
    fig.tight_layout()
    
def plotResults(hist_loss, hist_hits, plotLoss=False):
    if plotLoss:
        plotTransfer( hist_loss[:,0,:].unsqueeze(0), "Train Loss")
        plotTransfer( hist_loss[:,1,:].unsqueeze(0), "Test Gr Loss")
        plotTransfer( hist_loss[:,2,:].unsqueeze(0), "Test Ugr Loss")
    plotTransfer( hist_hits[:,0,:].unsqueeze(0), "Train Hits")
    plotTransfer( hist_hits[:,1,:].unsqueeze(0), "Test Gr Hits")
    plotTransfer( hist_hits[:,2,:].unsqueeze(0), "Test Ugr Hits")


def visual_eval(model, test_dl):

    model.eval()
    
    errors = 0
    
    with torch.no_grad():
    
        for i, batch in enumerate(test_dl):

            src, src_len = batch
            trg = src

            output = model(src, src_len, trg, 0) #turn off teacher forcing
            show_batch(output, trg)


def show_batch(output, trg):
    bs = output.shape[1]
    pred = output.argmax(-1)[1:]
    trg = trg[1:]
    for b in range(bs):
        p = cutEndToken(pred[:,b].tolist())
        t = cutEndToken(trg[:,b].tolist())
        status = "same" if p == t else "different"
        print(f"pred = {p} - {status} \ntrg  = {t}\n-")


def accuracy(model):
    for task_id in range(N_TASKS + 1):
        gr_not_hits = evaluate_extra(model, test_dls[task_id], allOrNoneLoss)
        ugr_not_hits = evaluate_extra(model, test_ugr_dls[task_id], allOrNoneLoss)
        gr_hits = 1 - gr_not_hits
        ugr_hits = 1 - ugr_not_hits
        total_acc = (gr_hits + ugr_not_hits) / 2
        print(f"Task {task_id}: Acc {total_acc:2.2}% | Gr acc {gr_hits:2.2} | Ugr acc {ugr_not_hits:2.2}")
        
def accuracyAll(models):
    for model_id in range(len(models)):
        print(f"\nModel {model_id}")
        accuracy(models[model_id])


SEED = 54321
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


models_A = []
hist_losses_A = []
hist_hitsss_A = []
for n_task in range(N_TASKS + TEST_ALL_TASKS):
    SUFFIX = f"A{n_task}"
    title = f"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}"
    LOADNAME = MODELAUTOSAVE + title + ".pt"
    SAVENAME = MODELAUTOSAVE + title + ".pt"
    PLOTSAVE = PLOTSAUTOSAVE + title + ".png"

    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)
    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)

    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)
    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)
    
    print(title)
    print(model.apply(init_weights))
    print(f'The model has {count_parameters(model)} trainable parameters')
    
    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)
    hist_losses_A.append(hist_loss_temp)
    hist_hitsss_A.append(hist_hits_temp)
    models_A.append(model)


hist_loss_A = torch.cat(hist_losses_A, dim=2)
hist_hits_A = torch.cat(hist_hitsss_A, dim=2)

plotResults(hist_loss_A, hist_hits_A)


accuracyAll(models_A)


random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


models_B = []
hist_losses_B = []
hist_hitsss_B = []

attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)
model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)

print(model.apply(init_weights))

for n_task in range(N_TASKS + TEST_ALL_TASKS):
    SUFFIX = f"B{n_task}"
    title = f"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}"
    LOADNAME = MODELAUTOSAVE + title + ".pt"
    SAVENAME = MODELAUTOSAVE + title + ".pt"
    PLOTSAVE = PLOTSAUTOSAVE + title + ".png"
    
    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)
    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)
    
    print(title)
    print(f'The model has {count_parameters(model)} trainable parameters')
    
    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)
    hist_losses_B.append(hist_loss_temp)
    hist_hitsss_B.append(hist_hits_temp)
    models_B.append(copy.deepcopy(model))


hist_loss_B = torch.cat(hist_losses_B, dim=2)
hist_hits_B = torch.cat(hist_hitsss_B, dim=2)

plotResults(hist_loss_B, hist_hits_B)


accuracyAll(models_B)


def applyOnParameters(model, conditions, apply_function):
    for name, param in model.named_parameters():
        # Check every condition
        for condition in conditions:
            # check every keyword
            allincluded = True
            for keyword in condition:
                if keyword not in name:
                    allincluded = False
                    break
            if allincluded:
                apply_function(param)

def freezeParameters(model, conditions):
    def freeze(param):
        param.requires_grad = False
    applyOnParameters(model, conditions, freeze)

def unfreezeParameters(model, conditions):
    def unfreeze(param):
        param.requires_grad = True
    applyOnParameters(model, conditions, unfreeze)

def showModelParameters(model, requires_grad=False):
    for name, param in model.named_parameters():
        if requires_grad:
            if param.requires_grad:
                print(name)
        else:
            print(name)
            
def onTaskUpdate(model):
    # Freeze core weights
    freezeParameters(model, ((""),))    # Freeze everything
    unfreezeParameters(model, (("encoder","embedding"), ("decoder","fc_out"), ("attention",))) # Unfreeze relevant stuff
    
    # Reinitialize
    to_constant = lambda param: nn.init.constant_(param.data, 0)
    applyOnParameters(model, (("decoder","fc_out","bias"),("attn","bias")), to_constant)
    to_normal = lambda param: nn.init.normal_(param.data, mean=0, std=0.01)
    applyOnParameters(model, (("encoder","embedding"),("decoder","fc_out","weight"),("attention","weight")), to_normal)


random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


models_C = []
hist_losses_C = []
hist_hitsss_C = []

attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)
model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)

print(model.apply(init_weights))

for n_task in range(N_TASKS + TEST_ALL_TASKS):
    SUFFIX = f"C{n_task}"
    title = f"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}"
    LOADNAME = MODELAUTOSAVE + title + ".pt"
    SAVENAME = MODELAUTOSAVE + title + ".pt"
    PLOTSAVE = PLOTSAUTOSAVE + title + ".png"
    
    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)
    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)
    
    print(title)
    print(f'The model has {count_parameters(model)} trainable parameters')
    
    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)
    hist_losses_C.append(hist_loss_temp)
    hist_hitsss_C.append(hist_hits_temp)
    models_C.append(copy.deepcopy(model))
    
    # Freeze, reinitialize
    onTaskUpdate(model)


hist_loss_C = torch.cat(hist_losses_C, dim=2)
hist_hits_C = torch.cat(hist_hitsss_C, dim=2)

plotResults(hist_loss_C, hist_hits_C)


accuracyAll(models_C)


def onTaskUpdate_ewc(model, task_id, train_dl, criterion):
    
    model.train()
    optimizer.zero_grad()
    
    #accumulate Gradient
    for it in range(100):
        for seq, seq_len in train_dl:

            optimizer.zero_grad()

            output = model(seq, seq_len, seq, 0)

            if criterion == F.cross_entropy:
              output_dim = output.shape[-1]
                
              output = output[1:].view(-1, output_dim)

              trg = seq[1:].view(-1)

              loss = criterion(output, trg)
            else:
              loss = criterion(output, seq)
            #print(loss)

            loss.backward()
        
    fishers.append({})
    optParams.append({})
    
    for name, param in model.named_parameters():
        fishers[task_id][name] = param.grad.data.clone().pow(2)
        optParams[task_id][name] = param.data.clone()


def train_ewc(model, task_id, dataloader, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for seq, seq_len in dataloader:
        
        optimizer.zero_grad()
        
        output = model(seq, seq_len, seq)
        loss = criterion(output, seq)
        
        if task_id > 0:
            print("-\n", loss)
        
        # EWC Training penalty
        for other_task_id in range(task_id):
            for name, param in model.named_parameters():
                fisher = fishers[other_task_id][name]
                optParam = optParams[other_task_id][name]
                #print(ewc_lambda)
                loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()
                #print((fisher * (optParam - param).pow(2)).sum())
                #print((optParam - param).pow(2).sum())
                #loss += ewc_lambda * (optParam - param).pow(2).sum()
        
        if task_id > 0:
            print(loss)
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(dataloader)


def evaluate_ewc(model, task_id, dataloader, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():

        for seq, seq_len in dataloader:

            output = model(seq, seq_len, seq, 0) #turn off teacher forcing

            loss = criterion(output, seq).type(torch.float)
            
            # EWC Training penalty
            for other_task_id in range(task_id):
                for name, param in model.named_parameters():
                    fisher = fishers[other_task_id][name]
                    optParam = optParams[other_task_id][name]
                    loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()
                    
            epoch_loss += loss.item()

    return epoch_loss / len(dataloader)


def fit_ewc(model, task_id, epochs, step_size_evaluation, clip ):
    best_valid_loss = float('inf')

    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr
    # [task_id, dataset, evaluations]

    for epoch in range(epochs):
        
        start_time = time.time()
        
        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)
        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)
        
        end_time = time.time()
        
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)
        
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), SAVENAME)

        if epoch % STEP_SIZE_EVALUATION == 0:
            idx = epoch//STEP_SIZE_EVALUATION
            for other_id in range(task_id + 1):
                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)
                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)
                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)
                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)
                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)
                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)

        
        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
        
    return total_loss, total_hits


random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


fishers = []
optParams = []
ewc_lambda = 10

models_D = []
hist_losses_D = []
hist_hitsss_D = []

attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)
model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)

print(model.apply(init_weights))

for task_id in range(N_TASKS + TEST_ALL_TASKS):
    SUFFIX = f"D{task_id}"
    title = f"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}"
    LOADNAME = MODELAUTOSAVE + title + ".pt"
    SAVENAME = MODELAUTOSAVE + title + ".pt"
    PLOTSAVE = PLOTSAUTOSAVE + title + ".png"
    
    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)
    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)
    
    hist_loss_temp, hist_hits_temp = fit_ewc(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)
    hist_losses_D.append(hist_loss_temp)
    hist_hitsss_D.append(hist_hits_temp)
    models_D.append(copy.deepcopy(model))
    onTaskUpdate_ewc(model, task_id, train_dls[task_id], F.cross_entropy)


hist_loss_D = torch.cat(hist_losses_D, dim=2)
hist_hits_D = torch.cat(hist_hitsss_D, dim=2)

plotResults(hist_loss_D, hist_hits_D)


hist_loss_D = torch.cat(hist_losses_D, dim=2)
hist_hits_D = torch.cat(hist_hitsss_D, dim=2)

plotResults(hist_loss_D, hist_hits_D)


accuracyAll(models_D)


torch.max(fishers[0]['encoder.embedding.weight'])


def onTaskUpdate_l2reg(model, task_id, train_dl, criterion):
    # Save optimal parameters for each task
    optParams.append({})
    
    for name, param in model.named_parameters():
        optParams[task_id][name] = param.data.clone()


def train_l2reg(model, task_id, dataloader, optimizer, criterion, clip):
    
    model.train()
    
    epoch_loss = 0
    
    for seq, seq_len in dataloader:
        
        optimizer.zero_grad()
        
        output = model(seq, seq_len, seq)
        loss = criterion(output, seq)
        
        # L2 Training penalty
        for other_task_id in range(task_id):
            for name, param in model.named_parameters():
                optParam = optParams[other_task_id][name]
                loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()
        
        loss.backward()
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        optimizer.step()
        
        epoch_loss += loss.item()
        
    return epoch_loss / len(dataloader)


def evaluate_l2reg(model, task_id, dataloader, criterion):
    
    model.eval()
    
    epoch_loss = 0
    
    with torch.no_grad():

        for seq, seq_len in dataloader:

            output = model(seq, seq_len, seq, 0) #turn off teacher forcing

            loss = criterion(output, seq).type(torch.float)
            
            # L2 Training penalty
            for other_task_id in range(task_id):
                for name, param in model.named_parameters():
                    optParam = optParams[other_task_id][name]
                    loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()
                    
            epoch_loss += loss.item()

    return epoch_loss / len(dataloader)


def fit_l2reg(model, task_id, epochs, step_size_evaluation, clip ):
    best_valid_loss = float('inf')

    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr
    # [task_id, dataset, evaluations]

    for epoch in range(epochs):
        
        start_time = time.time()
        
        train_loss = train_l2reg(model, task_id, train_dls[task_id], optimizer, criterion, clip)
        valid_loss = evaluate_l2reg(model, task_id, valid_dls[task_id], criterion)
        
        end_time = time.time()
        
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)
        
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), SAVENAME)

        if epoch % STEP_SIZE_EVALUATION == 0:
            idx = epoch//STEP_SIZE_EVALUATION
            for other_id in range(task_id + 1):
                total_loss[other_id,0,idx] = evaluate_l2reg(model, task_id, train_dls[other_id], criterion)
                total_loss[other_id,1,idx] = evaluate_l2reg(model, task_id, test_dls[other_id], criterion)
                total_loss[other_id,2,idx] = evaluate_l2reg(model, task_id, test_ugr_dls[other_id], criterion)
                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)
                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)
                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)

        
        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
        
    return total_loss, total_hits


LAMBDA_L2REG = 0.02


random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


optParams = []

models_D2 = []
hist_losses_D2 = []
hist_hitsss_D2 = []

attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)
model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)

print(model.apply(init_weights))

for task_id in range(N_TASKS + TEST_ALL_TASKS):
    SUFFIX = f"D2.{task_id}"
    title = f"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}"
    LOADNAME = MODELAUTOSAVE + title + ".pt"
    SAVENAME = MODELAUTOSAVE + title + ".pt"
    PLOTSAVE = PLOTSAUTOSAVE + title + ".png"
    
    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)
    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)
    
    hist_loss_temp, hist_hits_temp = fit_l2reg(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)
    hist_losses_D2.append(hist_loss_temp)
    hist_hitsss_D2.append(hist_hits_temp)
    models_D2.append(copy.deepcopy(model))
    onTaskUpdate_l2reg(model, task_id, train_dls[task_id], F.cross_entropy)


hist_loss_D2 = torch.cat(hist_losses_D2, dim=2)
hist_hits_D2 = torch.cat(hist_hitsss_D2, dim=2)

plotResults(hist_loss_D2, hist_hits_D2)


accuracyAll(models_D2)


class Gating(nn.Module):
    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,
                 n_max_experts, dropout):
        super().__init__()

        self.input_dim = input_dim
        
        self.n_experts = n_experts

        self.n_max_experts = n_max_experts
        
        self.embedding = nn.Embedding(input_dim, embed_dim)
        
        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)

        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, seqs, seqs_len):
        
        # seqs = [seq len, batch_size]
        # seqs_len = [batch_size]
        
        embedded = self.dropout(self.embedding(seqs))
        
        # embedded = [seq len, batch_size, embed_dim]

        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to("cpu"))

        packed_outputs, hidden = self.rnn(packed_embedded)

        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)

        # outputs = [seq len, batch_size, n_experts * num directions]
        # hidden = [n layers * num directions, batch size, n_experts]

        hidden = hidden.squeeze(0)

        # hidden = [batch_size, n_max_experts]

        outputs = outputs[-1]

        outputs = self.fc_out(outputs)

        # outputs = [batch_size, n_max_experts]

        return outputs
        return F.softmax(outputs, dim=1)


class DynaMoE(nn.Module):
    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):
        """
        Parameters
        ----------
        gating: nn.Module
            Gating module
        gating_optimizer: optim
            optimizer for passed Gating module
        expert: list of nn.Module
            list of task experts
        expert_optimizers: list of optim
            list of optimizer for the expert at the same index
        """
        super(DynaMoE, self).__init__()

        assert len(experts) == len(expert_optimizers)
        
        self.gating = gating
        self.gating_optimizer = gating_optimizer
        self.experts = nn.ModuleList(experts)
        self.expert_optimizers = expert_optimizers
        self.n_active_experts = 1
        # set mask
        self.recompute_mask()
    
    def recompute_mask(self):
        gating_mask = torch.zeros(self.gating.n_max_experts).to(device)

        for e_id in range(self.n_active_experts):
            gating_mask[e_id] = 1

        self.gating_mask = gating_mask

    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):
        #seqs = [seqs len, batch size]
        #seqs_len = [batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing

        vocab_size = self.gating.input_dim
        seq_len, batch_size = seqs.shape
        
        # Decide which expert to use
        gatings = self.gating(seqs, seqs_len)

        # gatings = [batch_size, n_max_experts]
        
        masked_gatings = gatings[:,:self.n_active_experts]
        
        # @TODO: Probabilistic vs argmax?
        network_ids = torch.argmax(masked_gatings, dim=1)

        expert_outputs = []
        for e_id in range(self.n_active_experts):
            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,
                                                     teacher_forcing_ratio))

        outputs = torch.empty((seq_len, batch_size, vocab_size))

        for b in range(batch_size):
            network_id = network_ids[b]
            outputs[:,b] = expert_outputs[network_id][:,b]
        
        return outputs

    def add_expert(self):
        # Get new expert
        expert, expert_optimizer = init_expert()
        self.experts.append(expert)
        self.expert_optimizers.append(expert_optimizer)
        self.n_active_experts += 1
        # Recompute mask
        self.recompute_mask()


def compute_loss(outputs, targets, criterion, cutFirstInSequence=True):
    if isinstance(criterion, CosineLoss):
        return criterion(outputs, targets)
    else:
        outputs_dim = outputs.shape[-1]
        
        if cutFirstInSequence:
            outputs = outputs[1:].view(-1, outputs_dim)
            #outputs = [batch size, output dim]
            targets = targets[1:].view(-1)
            #targets = [batch size]
            # print("hi")
        else:
            outputs = outputs.view(-1, outputs_dim)
            targets = targets.view(-1)
        
        # print("######")
        # print(outputs)
        # print(targets)
        # print("######")
        
        return criterion(outputs, targets)


def train_dynamoe_gating(model, iterator, gating_criterion,
                         expert_criterion, clip, verbose=False):
    
    model.gating.train()
    
    epoch_loss = 0
    
    for seqs, seqs_len in iterator:
        
        #seqs = [seq len, batch size]
        #output = [seq len, batch size, vocab sizen]
        batch_size = seqs.shape[1]

        model.gating_optimizer.zero_grad()
        
        gating_outputs = model.gating(seqs, seqs_len)

        # gating_outputs = [batch_size, n_max_experts]

        ## Compute best choice for gating network
        # Compute loss for each expert network
        loss_experts = torch.empty((batch_size, model.n_active_experts))
        expert_trgs = seqs
        for e_id in range(model.n_active_experts):

            model.experts[e_id].eval()
            
            with torch.no_grad():

                # Get model prediction
                expert_outputs = model.experts[e_id](seqs, seqs_len, expert_trgs)

                loss = compute_loss(expert_outputs, expert_trgs,
                                    expert_criterion,
                                    cutFirstInSequence=True)
            
            # Log loss to train gating
            loss_experts[:,e_id] = loss

        # Indices of correct experts to have chosen
        gating_trgs = loss_experts.argmin(dim=1)
        # gating_trgs = [batch_size]

        if verbose:
            print("Target")
            print(gating_trgs)
            print("Output")
            print(gating_outputs.argmax(dim=1))

        gating_trgs = gating_trgs.unsqueeze(0)
        # gating_trgs = [[batch_size]]
        gating_outputs = gating_outputs.unsqueeze(0)
        # gating_ouputs = [[batch_size, n_max_experts]]

        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,
                            cutFirstInSequence=False)
        
        if verbose:
            print(">> Gating Loss")
            print(gating_loss)

        gating_loss.backward()
        
        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        model.gating_optimizer.step()
        
        # Get loss of model gating chose
        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]

        if verbose:
            print("-- Masked Gating")
            print(gating_masked)
        gating_choices = gating_masked.argmax(dim=1)
        # gating_choices = [batch_size]

        loss_chosen_experts = loss_experts[:,gating_choices]
        
        loss_chosen_experts = loss_chosen_experts.mean()
        
        epoch_loss += loss_chosen_experts.item()
        
    return epoch_loss / len(iterator)


def train_dynamoe_both(model, iterator, gating_criterion,
                       expert_criterion, clip):

    model.gating.train()
    
    epoch_loss = 0
    
    for seqs, seqs_len in iterator:
        #seqs = [seq len, batch size]
        #output = [seq len, batch size, vocab sizen]

        batch_size = seqs.shape[1]

        model.gating_optimizer.zero_grad()
        
        gating_outputs = model.gating(seqs, seqs_len)
        # gating_outputs = [batch_size, n_max_experts]

        ## Compute best choice for gating network
        # Compute loss for each expert network
        loss_experts = torch.empty((batch_size, model.n_active_experts))
        # loss_experts = [batch_size, n_active_experts]
        expert_trgs = seqs
        for e_id in range(model.n_active_experts):

            train_model = e_id == model.n_active_experts - 1

            if train_model:
                model.experts[e_id].train()
                model.expert_optimizers[e_id].zero_grad()
                
                # Get model prediction
                expert_outputs = model.experts[e_id](seqs, seqs_len, expert_trgs)

                loss = compute_loss(expert_outputs, expert_trgs, expert_criterion,
                                    cutFirstInSequence=True)

                # Log loss to train gating
                loss_experts[:,e_id] = loss
                
                # Train newly initialized model on new train examples
                reduced_loss = loss.mean()
                reduced_loss.backward()
                model.expert_optimizers[e_id].step()
                
            else:
                model.experts[e_id].eval()
                
                with torch.no_grad():

                    # Get model prediction
                    expert_outputs = model.experts[e_id](seqs, seqs_len, expert_trgs)

                    loss = compute_loss(expert_outputs, expert_trgs, expert_criterion,
                                        cutFirstInSequence=True)

                    # Log loss to train gating
                    loss_experts[:,e_id] = loss

        # Compute expert which should have been chosen
        gating_trgs = loss_experts.argmin(dim=1)
        # gating_trgs = [batch_size]

        gating_trgs = gating_trgs.unsqueeze(0)
        # gating_trgs = [[batch_size]]
        gating_outputs = gating_outputs.unsqueeze(0)
        # gating_ouputs = [[batch_size, n_max_experts]]

        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,
                            cutFirstInSequence=False)

        gating_loss.backward()
        
        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        model.gating_optimizer.step()

        # Get loss of model gating chose
        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]
        gating_choices = gating_masked.argmax(dim=1)
        # gating_choices = [batch_size]

        loss_chosen_experts = loss_experts[:,gating_choices]
        
        loss_chosen_experts = loss_chosen_experts.mean()
        
        epoch_loss += loss_chosen_experts.item()
        
    return epoch_loss / len(iterator)


def init_expert():
    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)
    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)
    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)
    new_expert = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)
    new_expert.apply(init_weights)
    expert_optimizer = optim.Adam(new_expert.parameters(), lr=LEARNING_RATE)
    return new_expert, expert_optimizer


def init_gating():
    gating = Gating(INPUT_DIM, N_GATING_EMBED_DIM, N_GATING_HIDDEN_DIM,
                    N_EXPERTS_START, N_MAX_EXPERTS, GATE_DROPOUT)
    gating.to(device)
    gating_optimizer = optim.Adam(gating.parameters(), lr=LEARNING_RATE)
    return gating, gating_optimizer


def show_expert(model, iterator):
    model.gating.eval()
    with torch.no_grad():
        for i, batch in enumerate(iterator):
            seqs, seqs_len = batch

            batch_size = seqs.shape[1]

            gating_outputs = model.gating(seqs, seqs_len)

            gating_masked = gating_outputs[:,:model.n_active_experts]

            gating_choices = gating_masked.argmax(dim=1)

            for b in range(batch_size):
                print(f"{gating_choices[b]} - {seqs[:,b]}")            


def fit_dynamoe(model, task_id, epochs, step_size_evaluation, clip,
                case = "train_gating_initialized_expert" ):
    """
    Parameters
    ----------
    case : string
        "train_gating_uninitialized_expert" | "train_gating_train_expert" | 
        "train_gating_initialized_expert"
    """
    best_valid_loss = float('inf')

    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr
    # [task_id, dataset, evaluations]

    loss_tracker = torch.zeros((epochs,))

    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT

    for epoch in range(epochs):

        if case == "train_gating_initialized_expert":
            start_time = time.time()
            
            train_loss = train_dynamoe_gating(model, train_dls[task_id],
                                              gating_criterion,
                                              expert_criterion_unreduced, clip)
            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)
            
            end_time = time.time()

            # Log hits
            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id], allOrNoneLoss)

            # Check for improvement in loss
            if epoch > allowed_until_check:
                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - 
                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)
                    and 
                    (valid_loss > PERFORMANCE_TRESHHOLD)
                ):
                    # Case of no improvement:
                    # Switch to train the expert and gating

                    case = "train_gating_train_expert"
                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT
                    print("-----------------------------------")
                    print("------Switch to training both------")
                    print("-----------------------------------")

            
        if case == "train_gating_train_expert":
            start_time = time.time()
            
            train_loss = train_dynamoe_both(model, train_dls[task_id],
                                            gating_criterion,
                                            expert_criterion_unreduced, clip)
            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)
            
            end_time = time.time()

        if case == "train_gating_uninitialized_expert":
            assert len(model.experts) > 0, "Need at least one expert"
            start_time = time.time()
            
            train_loss = train_dynamoe_gating(model, train_dls[task_id],
                                              gating_criterion,
                                              expert_criterion_unreduced, clip)
            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)
            
            end_time = time.time()

            # Log loss
            loss_tracker[epoch] = valid_loss

            # Check for improvement in loss
            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:
                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - 
                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)
                    and 
                    (valid_loss > PERFORMANCE_TRESHHOLD)
                   ):
                    # Case of no improvement:
                    # Initiate new expert and train gating and new expert on it
                    model.add_expert()

                    case = "train_gating_initialized_expert"
                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT
                    print("-----------------------------------")
                    print("-----Added Expert-train Gating-----")
                    print("-----------------------------------")

            
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), SAVENAME)

        if epoch % STEP_SIZE_EVALUATION == 0:
            idx = epoch//STEP_SIZE_EVALUATION
            for other_id in range(task_id + 1):
                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], expert_criterion)
                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], expert_criterion)
                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], expert_criterion)
                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)
                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)
                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)

        epoch_mins, epoch_secs = epoch_time(start_time, end_time)

        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
        
    return total_loss, total_hits


SAVE = N_EPOCHS


N_EXPERTS_START = 1
N_MAX_EXPERTS = 3
GATE_DROPOUT = 0.5
N_GATING_HIDDEN_DIM = 10
N_GATING_EMBED_DIM = 10
N_EPOCHS = SAVE + 200

# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD
# and it stays within ALLOWED_ERROR_VARIANCE for
# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new
# expert is initialized
N_EPOCHS_UNTIL_NEW_EXPERT = 30
ALLOWED_ERROR_VARIANCE = 0.1
PERFORMANCE_TRESHHOLD = 0.3


SEED = 54321
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


models_E = []
hist_losses_E = []
hist_hitsss_E = []

expert, expert_optimizer = init_expert()
gating, gating_optimizer = init_gating()
model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])
print(model.apply(init_weights))

# gating_criterion = CosineLoss(N_MAX_EXPERTS, ignore_index=None)
# Cosine loss is inpractical for Gating because result vectors are low dimensional
gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)
expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)
expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,
                                        reduction="none")

for n_task in range(N_TASKS + TEST_ALL_TASKS):
    SUFFIX = f"E{n_task}"
    title = f"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}"
    LOADNAME = MODELAUTOSAVE + title + ".pt"
    SAVENAME = MODELAUTOSAVE + title + ".pt"
    PLOTSAVE = PLOTSAUTOSAVE + title + ".png"
    print(title)
    print(f'The model has {count_parameters(model)} trainable parameters')
    
    if n_task == 0:
        case = "train_gating_initialized_expert"
    else:
        case = "train_gating_uninitialized_expert"
    hist_loss_temp, hist_hits_temp = fit_dynamoe(model, n_task, N_EPOCHS,
                                                 STEP_SIZE_EVALUATION, CLIP,
                                                 case)
    hist_losses_E.append(hist_loss_temp)
    hist_hitsss_E.append(hist_hits_temp)
    models_E.append(copy.deepcopy(model))


hist_loss_E = torch.cat(hist_losses_E, dim=2)
hist_hits_E = torch.cat(hist_hitsss_E, dim=2)

plotResults(hist_loss_E, hist_hits_E)


accuracyAll(models_E)


models_E[2].n_active_experts


gating_trgts = []
gating_trgts.append( [torch.tensor([1,0,0]) for _ in range(len(train_dls[0]))])
gating_trgts.append( [torch.tensor([0,1,0]) for _ in range(len(train_dls[1]))])


for x,y in train_dls[1]:
    if x[1] == 7:
        print("hi")


gating, gating_optimizer = init_gating()
gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)


gating, gating_optimizer = init_gating()
gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)
for i in range(100):
    for seqs, seqs_len in train_dls[2]:

        gating.train()

        gating_optimizer.zero_grad()

        outputs = gating(seqs, seqs_len)

        #print("seq")
        #print(seqs)
        #print("outputs")
        #print(outputs)

        if seqs[1] == 7:
            trgts = torch.tensor([0])
        else:
            trgts = torch.tensor([1])

        loss = compute_loss(outputs, trgts, gating_criterion,
                            cutFirstInSequence=False)

        loss.backward()

        gating_optimizer.step()

        print("loss")
        print(loss.item())


gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)
expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)
expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,
                                        reduction="none")



expert, expert_optimizer = init_expert()
gating, gating_optimizer = init_gating()
model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])
print(model.apply(init_weights))


model.add_expert()


show_expert(model, train_dls[1])


show_expert(model, train_dls[0])


gating_optimizer = optim.Adam(model.gating.parameters(), lr=0.01)


for name, param in model.gating.named_parameters():
    if param.requires_grad:
        print(name)
        print(param.grad)


gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)


gating_criterion(torch.tensor([[0, 1.0000e+10, 0],
        [0, 1.0000e+10, 0]]),
        torch.tensor([1, 1]))


for x in range(100):
    print( train_dynamoe_gating(model, train_dls[1], gating_criterion,
                                expert_criterion_unreduced,
                                CLIP, verbose=True) )


for x in range(400):
    print( train_dynamoe_both(model, train_dls[1], gating_criterion, expert_criterion_unreduced, CLIP) )


class DynaMoE2(nn.Module):
    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):
        """
        Parameters
        ----------
        gating: nn.Module
            Gating module
        gating_optimizer: optim
            optimizer for passed Gating module
        expert: list of nn.Module
            list of task experts
        expert_optimizers: list of optim
            list of optimizer for the expert at the same index
        """
        super(DynaMoE2, self).__init__()

        assert len(experts) == len(expert_optimizers)
        
        self.gating = gating
        self.gating_optimizer = gating_optimizer
        self.experts = nn.ModuleList(experts)
        self.expert_optimizers = expert_optimizers
        self.n_active_experts = 1

    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio = 0.5):
        #seqs = [seqs len, batch size]
        #seqs_len = [batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing

        vocab_size = self.gating.input_dim
        seq_len, batch_size = seqs.shape
        
        # Decide which expert to use
        gatings = self.gating(seqs, seqs_len)

        # gatings = [batch_size, n_max_experts]
        
        gating_masked = gatings[:,:self.n_active_experts]

        expert_outputs = torch.empty((self.n_active_experts, seq_len, batch_size, vocab_size))
        for e_id in range(self.n_active_experts):
            expert_output = self.experts[e_id](seqs, seqs_len, seqs, teacher_forcing_ratio)
            # expert_output = [seqs_len, batch_size, vocab_size]

            # Weigh every experts output with respective gating weight
            for b in range(batch_size):
                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]
                #expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]
                expert_outputs[e_id,:,b] = expert_output[:,b]
            
            # print("expert_out")
            # print(expert_output)
            # print("gating_masked")
            # print(gating_masked)

        weighted_outputs = expert_outputs.sum(dim=0)
        # weighted_outputs = [seqs_len, batch_size, vocab_size]

        return weighted_outputs

    def add_expert(self):
        # Get new expert
        expert, expert_optimizer = init_expert()
        self.experts.append(expert)
        self.expert_optimizers.append(expert_optimizer)
        self.n_active_experts += 1


def train_dynamoe2_gating(model, iterator, criterion, clip, verbose=False):
    assert isinstance(model, DynaMoE2)
    model.gating.train()
    
    epoch_loss = 0
    
    for seqs, seqs_len in iterator:
        
        #seqs = [seq len, batch size]
        #output = [seq len, batch size, vocab sizen]
        vocab_size = model.gating.input_dim
        seq_len, batch_size = seqs.shape

        model.gating_optimizer.zero_grad()
        
        gating_outputs = model.gating(seqs, seqs_len)

        # gating_outputs = [batch_size, n_max_experts]
        
        gating_masked = gating_outputs[:,:model.n_active_experts]

        ## Compute best choice for gating network
        # Compute loss for each expert network
        expert_outputs = torch.empty((model.n_active_experts, seq_len,
                                      batch_size, vocab_size))
        for e_id in range(model.n_active_experts):

            model.experts[e_id].eval()

            expert_output = model.experts[e_id](seqs, seqs_len, seqs)
            # expert_output = [seqs_len, batch_size, vocab_size]



            # Weigh every experts output with respective gating weight
            for b in range(batch_size):
                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]
                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]
            
        weighted_outputs = expert_outputs.sum(dim=0)
        # weighted_outputs = [seqs_len, batch_size, vocab_size]

        # Gating Loss just is total loss
        gating_loss = compute_loss(weighted_outputs, seqs, expert_criterion,
                            cutFirstInSequence=True)
        
        if verbose:
            print(">> Gating Loss")
            print(gating_loss)

        gating_loss.backward()
        
        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        
        model.gating_optimizer.step()

        if verbose:
            print("-- Masked Gating")
            print(gating_masked)
        
        epoch_loss += gating_loss.item()
        
    return epoch_loss / len(iterator)


def train_dynamoe2_both(model, iterator, criterion, clip):
    assert isinstance(model, DynaMoE2)
    
    model.eval()
    model.experts[model.n_active_experts - 1].train()
    
    epoch_loss = 0
    
    for seqs, seqs_len in iterator:
        
        # model.gating_optimizer.zero_grad()
        model.expert_optimizers[model.n_active_experts - 1].zero_grad()
        
        outputs = model(seqs, seqs_len, seqs)
        
        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)
        
        loss.backward()
        
        # model.gating_optimizer.step()
        model.expert_optimizers[model.n_active_experts - 1].step()
        
        epoch_loss += loss.item()
    
    return epoch_loss / len(iterator)


def fit_dynamoe2(model, task_id, epochs, step_size_evaluation, clip,
                case = "train_gating_initialized_expert" ):
    """
    Parameters
    ----------
    case : string
        "train_gating_uninitialized_expert" | "train_gating_train_expert" | 
        "train_gating_initialized_expert"
    """
    best_valid_loss = float('inf')

    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))
    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr
    # [task_id, dataset, evaluations]

    loss_tracker = torch.zeros((epochs,))

    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT

    for epoch in range(epochs):

        if case == "train_gating_initialized_expert":
            start_time = time.time()
            
            train_loss = train_dynamoe2_gating(model, train_dls[task_id],
                                               criterion, clip)
            valid_loss = evaluate(model, valid_dls[task_id], criterion)
            
            end_time = time.time()

            # Log hits
            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id], allOrNoneLoss)

            # Check for improvement in loss
            if epoch > allowed_until_check:
                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - 
                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)
                    and 
                    (valid_loss > PERFORMANCE_TRESHHOLD)
                ):
                    # Case of no improvement:
                    # Switch to train the expert and gating

                    case = "train_gating_train_expert"
                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT
                    print("-----------------------------------")
                    print("------Switch to training both------")
                    print("-----------------------------------")

            
        if case == "train_gating_train_expert":
            start_time = time.time()
            
            train_loss = train_dynamoe2_both(model, train_dls[task_id],
                                               criterion, clip)
            valid_loss = evaluate(model, valid_dls[task_id], criterion)
            
            end_time = time.time()

        if case == "train_gating_uninitialized_expert":
            assert len(model.experts) > 0, "Need at least one expert"
            start_time = time.time()
            
            train_loss = train_dynamoe2_gating(model, train_dls[task_id],
                                               criterion, clip)
            valid_loss = evaluate(model, valid_dls[task_id], criterion)
            
            end_time = time.time()

            # Log loss
            loss_tracker[epoch] = valid_loss

            # Check for improvement in loss
            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:
                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - 
                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)
                    and 
                    (valid_loss > PERFORMANCE_TRESHHOLD)
                   ):
                    # Case of no improvement:
                    # Initiate new expert and train gating and new expert on it
                    model.add_expert()

                    case = "train_gating_initialized_expert"
                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT
                    print("-----------------------------------")
                    print("-----Added Expert-train Gating-----")
                    print("-----------------------------------")

            
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), SAVENAME)

        if epoch % STEP_SIZE_EVALUATION == 0:
            idx = epoch//STEP_SIZE_EVALUATION
            for other_id in range(task_id + 1):
                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], expert_criterion)
                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], expert_criterion)
                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], expert_criterion)
                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)
                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)
                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)

        epoch_mins, epoch_secs = epoch_time(start_time, end_time)

        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')
        
    return total_loss, total_hits


SAVE = N_EPOCHS


N_EXPERTS_START = 1
N_MAX_EXPERTS = 3
GATE_DROPOUT = 0.5
N_GATING_HIDDEN_DIM = 10
N_GATING_EMBED_DIM = 10
N_EPOCHS = SAVE + 200

# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD
# and it stays within ALLOWED_ERROR_VARIANCE for
# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new
# expert is initialized
N_EPOCHS_UNTIL_NEW_EXPERT = 30
ALLOWED_ERROR_VARIANCE = 0.1
PERFORMANCE_TRESHHOLD = 0.3


SEED = 54321
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.backends.cudnn.deterministic = True


models_F = []
hist_losses_F = []
hist_hitsss_F = []

expert, expert_optimizer = init_expert()
gating, gating_optimizer = init_gating()
model = DynaMoE2(gating, gating_optimizer, [expert,], [expert_optimizer,])

# model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)

print(model.apply(init_weights))

expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)

for n_task in range(N_TASKS + TEST_ALL_TASKS):
    SUFFIX = f"F{n_task}"
    title = f"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}"
    LOADNAME = MODELAUTOSAVE + title + ".pt"
    SAVENAME = MODELAUTOSAVE + title + ".pt"
    PLOTSAVE = PLOTSAUTOSAVE + title + ".png"
    print(title)
    print(f'The model has {count_parameters(model)} trainable parameters')
    
    if n_task == 0:
        case = "train_gating_initialized_expert"
    else:
        case = "train_gating_uninitialized_expert"
    hist_loss_temp, hist_hits_temp = fit_dynamoe2(model, n_task, N_EPOCHS,
                                                 STEP_SIZE_EVALUATION, CLIP,
                                                 case)
    hist_losses_F.append(hist_loss_temp)
    hist_hitsss_F.append(hist_hits_temp)
    models_F.append(copy.deepcopy(model))


hist_loss_F = torch.cat(hist_losses_F, dim=2)
hist_hits_F = torch.cat(hist_hitsss_F, dim=2)

plotResults(hist_loss_F, hist_hits_F)


model.n_active_experts


accuracyAll(models_F)


def show_expert2(model, iterator):
    model.gating.eval()
    with torch.no_grad():
        for i, batch in enumerate(iterator):
            seqs, seqs_len = batch

            batch_size = seqs.shape[1]

            gating_outputs = model.gating(seqs, seqs_len)

            gating_masked = gating_outputs[:,:model.n_active_experts]

            for b in range(batch_size):
                print(f"{gating_masked[b]} - {seqs[:,b]}")            


for seqs, seqs_len in train_dls[2]:
    print(model.gating(seqs, seqs_len))
    break


show_expert2(model, train_dls[2])


expert, expert_optimizer = init_expert()

gating, gating_optimizer = init_gating()

model2 = DynaMoE2(gating, gating_optimizer, [expert,], [expert_optimizer,])

model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)


criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)


for _ in range(100):
    print(train_dynamoe2_gating(model2, train_dls[2], criterion, CLIP))


for seqs, seqs_len in train_dls[1]:
    print(model2.gating(seqs, seqs_len))
    break


for seqs, seqs_len in train_dls[1]:
    print(model2.experts[1](seqs, seqs_len, seqs).argmax(dim=2))
    print(seqs)
    break


N_EPO


for _ in range(100):
    print(train_dynamoe2_both(model2, train_dls[1], criterion, CLIP))


model2.add_expert()


for seqs, seqs_len in train_dls[0]:
    print(seqs)
    print(model2(seqs, seqs_len, seqs))
    break


class Gating(nn.Module):
    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,
                 n_max_experts, dropout):
        super().__init__()

        self.input_dim = input_dim
        
        self.n_experts = n_experts

        self.n_max_experts = n_max_experts
        
        self.embedding = nn.Embedding(input_dim, embed_dim)
        
        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)

        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, seqs, seqs_len, context):
        
        # seqs = [seq len, batch_size]
        # seqs_len = [batch_size]
        
        embedded = self.dropout(self.embedding(seqs))
        
        # embedded = [seq len, batch_size, embed_dim]

        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to("cpu"))

        packed_outputs, hidden = self.rnn(packed_embedded)

        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)

        # outputs = [seq len, batch_size, n_experts * num directions]
        # hidden = [n layers * num directions, batch size, n_experts]

        hidden = hidden.squeeze(0)

        # hidden = [batch_size, n_max_experts]

        outputs = outputs[-1]

        outputs = self.fc_out(outputs)

        # outputs = [batch_size, n_max_experts]

        return outputs
        return F.softmax(outputs, dim=1)


class DynaMoEContext(nn.Module):
    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):
        """
        Parameters
        ----------
        gating: nn.Module
            Gating module
        gating_optimizer: optim
            optimizer for passed Gating module
        expert: list of nn.Module
            list of task experts
        expert_optimizers: list of optim
            list of optimizer for the expert at the same index
        """
        super(DynaMoE, self).__init__()

        assert len(experts) == len(expert_optimizers)
        
        self.gating = gating
        self.gating_optimizer = gating_optimizer
        self.experts = nn.ModuleList(experts)
        self.expert_optimizers = expert_optimizers
        self.n_active_experts = 1
        # set mask
        self.recompute_mask()
    
    def recompute_mask(self):
        gating_mask = torch.zeros(self.gating.n_max_experts).to(device)

        for e_id in range(self.n_active_experts):
            gating_mask[e_id] = 1

        self.gating_mask = gating_mask

    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):
        #seqs = [seqs len, batch size]
        #seqs_len = [batch size]
        #trg = [trg len, batch size]
        #teacher_forcing_ratio is probability to use teacher forcing

        vocab_size = self.gating.input_dim
        seq_len, batch_size = seqs.shape
        
        # Decide which expert to use
        gatings = self.gating(seqs, seqs_len)

        # gatings = [batch_size, n_max_experts]
        
        masked_gatings = gatings[:,:self.n_active_experts]
        
        # @TODO: Probabilistic vs argmax?
        network_ids = torch.argmax(masked_gatings, dim=1)

        expert_outputs = []
        for e_id in range(self.n_active_experts):
            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,
                                                     teacher_forcing_ratio))

        outputs = torch.empty((seq_len, batch_size, vocab_size))

        for b in range(batch_size):
            network_id = network_ids[b]
            outputs[:,b] = expert_outputs[network_id][:,b]
        
        return outputs

    def add_expert(self):
        # Get new expert
        expert, expert_optimizer = init_expert()
        self.experts.append(expert)
        self.expert_optimizers.append(expert_optimizer)
        self.n_active_experts += 1
        # Recompute mask
        self.recompute_mask()
