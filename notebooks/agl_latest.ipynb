{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnant-urt22Y"
   },
   "source": [
    "# AGL Autoencoder\n",
    "\n",
    "Based on https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo collection\n",
    "\n",
    "DynaMoE:\n",
    "* Automatic parameter consolidation (? maybe not needed with training only on instances the expert gets)\n",
    "* Lookahead trainset replication accuracy\n",
    "* Exploring vs exploiting, decreasing over time\n",
    "* New expert only learns from examples it was actually chosen for\n",
    "* consolidation of dynamoe and training gating code into one codeblock\n",
    "\n",
    "General:\n",
    "- Ensembling\n",
    "- EWC\n",
    "- Replay\n",
    "- Run on 3 tasks\n",
    "- Test interleaved training: A -> C -> A, only test on A - what is performance of A?\n",
    "- Let's use a CNN and pictures for the input!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xKTgS0wMOZbb"
   },
   "outputs": [],
   "source": [
    "BASIS = \"../\" #\"drive/MyDrive/Colab_data/\"\n",
    "MODELFOLDER = BASIS + \"/models\"\n",
    "PLOTSFOLDER = BASIS + \"/plots\"\n",
    "MODELAUTOSAVE = MODELFOLDER + \"/autosave/\"\n",
    "PLOTSAUTOSAVE = PLOTSFOLDER + \"/autosave/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8BeTXi1OaMs",
    "outputId": "ca356d96-806e-46bb-9036-0f3c2d8c8139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NKkMOK5QRVp"
   },
   "outputs": [],
   "source": [
    "!cp drive/MyDrive/Colab_data/data.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yjgT9Azct22a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvhH1uQsvnV7"
   },
   "source": [
    "## Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HV39YqA5vnV8"
   },
   "outputs": [],
   "source": [
    "class GrammarGen():\n",
    "    \"\"\"\n",
    "    Generates Grammar sequences from grammars, and offers other functionalities\n",
    "    Grammars are dictionaries:\n",
    "    - always have START\n",
    "    - all paths lead eventually to END\n",
    "    - Entries starting with the same letter\n",
    "      have same output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grammar=None):\n",
    "        if grammar is None:\n",
    "            self.grammar = data.g0()\n",
    "        else:\n",
    "            self.grammar = grammar\n",
    "\n",
    "        # find how many letters in grammar\n",
    "        self.len = len(set([token[0] for token in self.grammar if (token != 'START' and token != 'END')]))\n",
    "\n",
    "        # variable to check how many sequences have been generated for the grammaticality test\n",
    "        self.grammCheckMaxLen = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"Generates n tokens\"\"\"\n",
    "        ret = []\n",
    "        count = 0\n",
    "        hashtrack = set()\n",
    "        while count < n:\n",
    "            token = []\n",
    "            current = 'START'\n",
    "            while current != 'END':\n",
    "                # Append current\n",
    "                if current != 'START':\n",
    "                    token.append(current[0])\n",
    "                # Choose next\n",
    "                r = random.randint(0, len(self.grammar[current]) - 1)\n",
    "                current = self.grammar[current][r]\n",
    "            # Check if seq is already inside\n",
    "            tokenhash = ''.join([str(x) for x in token])\n",
    "            if tokenhash not in hashtrack:\n",
    "                hashtrack.add(tokenhash)\n",
    "                ret.append((token, ))\n",
    "                count += 1\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def generateAllGrammatical(self, maxlen=float('inf')):\n",
    "        \"\"\"Generates all grammatical sequences until length maxlen\"\"\"\n",
    "        def genAllHelp(seq, current):\n",
    "            if current == 'END':\n",
    "                return [seq]\n",
    "            if len(seq) >= maxlen:\n",
    "                return []\n",
    "            # Append Current\n",
    "            if current != 'START':\n",
    "                seq.append(current[0])\n",
    "            # Generate next possibilities\n",
    "            options = range(len(self.grammar[current]))\n",
    "            ret = [(genAllHelp(copy.copy(seq), self.grammar[current][i]))\n",
    "                   for i in options]\n",
    "            return itertools.chain(*ret)\n",
    "        return set([tuple(seq) for seq in genAllHelp([], 'START')])\n",
    "\n",
    "    def isGrammatical(self, seqs):\n",
    "        \"\"\"Check for grammaticality of sequences in seqs\"\"\"\n",
    "        maxlen = max([len(seq) for seq in seqs])\n",
    "        if self.grammCheckMaxLen < maxlen:\n",
    "            self.allGrammatical = self.generateAllGrammatical(maxlen)\n",
    "            self.grammCheckMaxLen = maxlen\n",
    "\n",
    "        return [tuple(seq) in self.allGrammatical for seq in seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKt9wtL6t22s"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Deyfd3OByLSC"
   },
   "source": [
    "\n",
    "\n",
    "### Cosine Loss, Init_weights, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6X9xZXFQvnWF"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class CosineLoss():\n",
    "    def __init__(self, vocabsize, ignore_index, reduction=\"mean\"):\n",
    "        self.vocabsize = vocabsize\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eye = torch.eye(self.vocabsize).to(device)\n",
    "        self.cosSim = nn.CosineSimilarity(dim=1)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, outputs, labels):\n",
    "        maxlen = outputs.shape[0]\n",
    "        bs = outputs.shape[1]\n",
    "\n",
    "        # Deal with positions which should be ignored and make them the same as label\n",
    "        if self.ignore_index is not None:\n",
    "            ignore_positions = (labels == self.ignore_index).to(device)\n",
    "            outputs[ignore_positions] = self.eye[self.ignore_index]\n",
    "\n",
    "        # Convert labels to onehot\n",
    "        labels_onehot = torch.empty((maxlen, bs, self.vocabsize)).to(device)\n",
    "        for idx in range(maxlen):\n",
    "            labels_onehot[idx,:,:] = self.eye[labels[idx,:]]\n",
    "\n",
    "        # Put labels and output in correct form for torch cosSim function\n",
    "        batch_first_labels = labels_onehot.permute(1,0,2)\n",
    "        processed_labels = batch_first_labels.reshape(-1, maxlen * self.vocabsize)\n",
    "        batch_first_outputs = outputs.permute(1,0,2)\n",
    "        processed_outputs = batch_first_outputs.reshape(-1, maxlen * self.vocabsize)\n",
    "\n",
    "        # use cosSim function\n",
    "        res = (1 - self.cosSim(processed_labels, processed_outputs))\n",
    "\n",
    "        # Same interface as native los functions: reductions for the output\n",
    "        if self.reduction == \"none\":\n",
    "            return res\n",
    "        elif self.reduction == \"mean\":\n",
    "            return res.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return res.sum()\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3d2C9Rvt22w"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fut2CtQrt22w"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch\n",
    "        trg = src\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        if isinstance(criterion, CosineLoss):\n",
    "            loss = criterion(output, trg)\n",
    "        else:\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VIT6a8uqt22w"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "            \n",
    "            #seq = [seq_len, batch_size]\n",
    "            #output = [seq_len, batch_size, output_dim]\n",
    "\n",
    "            if isinstance(criterion, CosineLoss):\n",
    "                loss = criterion(output, seq)\n",
    "            else:\n",
    "                output_dim = output.shape[-1]\n",
    "                \n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = seq[1:].view(-1)\n",
    "                \n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #output = [(trg len - 1) * batch size, output dim]\n",
    "                \n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uOtABRKLvnWI"
   },
   "outputs": [],
   "source": [
    "def evaluate_extra(model, dataloader, loss_func):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for seqs, seqs_len in dataloader:\n",
    "\n",
    "            outputs = model(seqs, seqs_len, seqs, 0)\n",
    "\n",
    "            loss = loss_func(outputs, seqs) / seqs.shape[1]\n",
    "\n",
    "            loss_total += loss.item()\n",
    "        \n",
    "#        if loss_func == allOrNoneLoss:\n",
    "#            return loss_total\n",
    "\n",
    "        return loss_total / len(dataloader)\n",
    "\n",
    "def cutEndToken(seq):\n",
    "    ret = []\n",
    "    for stim in seq:\n",
    "        if stim == END_TOKEN:\n",
    "            break\n",
    "        ret.append(stim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def allOrNoneLoss(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    ret = 0\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        ret += not p == t\n",
    "    return torch.tensor(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lW3r-pjXt22x"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUY7o5eGt22x"
   },
   "source": [
    "During Training in addition to collecting the train/validation loss, collect the amount of entirely correct predicted sequences on the train and test gr/ugr set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JAVqiZ3kvnWJ"
   },
   "outputs": [],
   "source": [
    "def fit(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + TEST_ALL_TASKS, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + TEST_ALL_TASKS, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtZwmf8egUu"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9S4PXbSe06C",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P57X-q51t22n"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le1kHQOIt22o"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "P_t5icjNt22o"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v371q1Tkt22q"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ECjR4jZot22q"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.intermediate = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, emb_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        intermediate = self.intermediate(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        prediction = self.fc_out(intermediate)\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toSHWJmEt22q"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
    "\n",
    "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
    "\n",
    "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
    "\n",
    "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
    "\n",
    "The attention at each time-step is stored in the `attentions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OkcTBfr-t22s"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVj3zdDt22d"
   },
   "source": [
    "## Data\n",
    "\n",
    "First, get the training and test sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH6UqaslvnV_"
   },
   "source": [
    "### Sequencedataset\n",
    "Define a Dataset for Sequences:\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rBmNkT78t22d"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int): amount of sequences generated\n",
    "        \"\"\"\n",
    "        self.seqs = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x7ocJD0vnWB"
   },
   "source": [
    "### collate_batch\n",
    "Define collate_batch for the Dataloader: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "\n",
    "Sequences are padded and their non-padded lengths are returned.\n",
    "Since pack_padded_sequences requires sequences to be sorted, they are sorted too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tOBuBfPPt22e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    seq_lens = []\n",
    "    processed_seqs = []\n",
    "    # Sort in descending order\n",
    "    batch.sort(reverse=True, key=(lambda x: len(x)))\n",
    "    # append start and end token\n",
    "    for seq in batch:\n",
    "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
    "        seq_lens.append(len(seq))\n",
    "        processed_seqs.append(torch.tensor(seq))\n",
    "    # pad\n",
    "    padded_seqs = pad_sequence(processed_seqs)\n",
    "    seq_lens = torch.tensor(seq_lens)\n",
    "    return padded_seqs.to(device), seq_lens.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiskh9dalsvW"
   },
   "source": [
    "### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4UgG2QIFlsg3"
   },
   "outputs": [],
   "source": [
    "N_TASKS = 2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "START_TOKEN = 1\n",
    "END_TOKEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2cXtZIVe5_q"
   },
   "source": [
    "### Task loading\n",
    "For reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "HREfj6IuvnWJ"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhSjYo6vnWK"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "jWwIlfPcvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs = data.g0_train()\n",
    "valid_seqs = data.g0_train()\n",
    "test_seqs = data.g0_test_gr()\n",
    "test_ugr_seqs = data.g0_test_ugr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5I5lDz8vnWL"
   },
   "source": [
    "Sort for better perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ocjMQF-RvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs.sort(key=(lambda x: len(x)))\n",
    "valid_seqs.sort(key=(lambda x: len(x)))\n",
    "test_seqs.sort(key=(lambda x: len(x)))\n",
    "test_ugr_seqs.sort(key=(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zMSM652MvnWL"
   },
   "outputs": [],
   "source": [
    "def buildVocab(letterset):\n",
    "    vocab = {'<pad>': PAD_TOKEN, '<sos>': START_TOKEN, '<eos>': END_TOKEN}\n",
    "    counter = len(vocab)\n",
    "    for letter in letterset:\n",
    "        vocab[letter] = counter\n",
    "        counter +=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "yBCWnEM1vnWM"
   },
   "outputs": [],
   "source": [
    "letters = set()\n",
    "for seq in train_seqs:\n",
    "    [letters.add(letter) for letter in seq]\n",
    "letters = list(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPjDWlVPvnWM"
   },
   "source": [
    "Make all tasks, creates an additional task with all sequences from all tasks mashed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "YtuaqeKNvnWM"
   },
   "outputs": [],
   "source": [
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idxs = []\n",
    "idx2lets = []\n",
    "\n",
    "usedpermutations = set()\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        temp_letters = copy.copy(letters)\n",
    "\n",
    "        while str(temp_letters) in usedpermutations:\n",
    "            random.shuffle(temp_letters)\n",
    " \n",
    "        usedpermutations.add(str(temp_letters))\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Conversion Functions\n",
    "        let2idxs.append(lambda seq: [vocabs[-1][let] for let in seq])\n",
    "        idx2lets.append(lambda seq: [rvocabs[-1][let] for let in seq])\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idxs[-1](seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idxs[-1](seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idxs[-1](seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idxs[-1](seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-R9Nw9XB8Iw"
   },
   "source": [
    "### Task loading v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "E2Sn2FU9CAlD"
   },
   "outputs": [],
   "source": [
    "SWAP1 = 2\n",
    "SWAP2 = 4\n",
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idx = lambda task_id, seq: [vocabs[task_id][let] for let in seq]\n",
    "idx2let = lambda task_id, seq: [rvocabs[task_id][let] for let in seq]\n",
    "\n",
    "usedpermutations = set()\n",
    "listUsedPermutations = []\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "alternate = 0\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        if alternate == 0:\n",
    "            temp_letters = copy.copy(letters)\n",
    "\n",
    "            while str(temp_letters) in usedpermutations:\n",
    "                random.shuffle(temp_letters)\n",
    "    \n",
    "            usedpermutations.add(str(temp_letters))\n",
    "            listUsedPermutations.append(temp_letters)\n",
    "\n",
    "            alternate = 1\n",
    "        else:\n",
    "            temp_letters = listUsedPermutations[-1]\n",
    "\n",
    "            #Swap 5th and 6th indice\n",
    "            temp_letters[SWAP1], temp_letters[SWAP2] = temp_letters[SWAP2], temp_letters[SWAP1]\n",
    "\n",
    "            usedpermutations.add(str(temp_letters))\n",
    "\n",
    "            alternate = 0\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idx(t, seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idx(t, seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idx(t, seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idx(t, seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVaRvD2svnWN",
    "outputId": "ddaa8495-e224-4457-af03-78c62225b990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'A': 3, 'C': 4, 'F': 5, 'D': 6, 'G': 7}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'A': 3, 'C': 4, 'G': 5, 'D': 6, 'F': 7}\n",
      "\n",
      "First Batch of Task 0:\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5],\n",
      "        [2]])\n",
      "\n",
      "First Batch of Task 1:\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [4],\n",
      "        [7],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "print(vocabs[0])\n",
    "print(vocabs[1])\n",
    "for i in range(len(valid_dls) - 1):\n",
    "    for seqs, _ in valid_dls[i]:\n",
    "        print(f\"\\nFirst Batch of Task {i}:\")\n",
    "        print(seqs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFM4eGVOs2lZ",
    "outputId": "ef66cff7-2e19-4a9e-8b0e-9fb932381c00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(task_id, seq)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzTfiG5qPpgS",
    "outputId": "be70246f-611c-4bd8-98e7-d46f7b1dceca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5] - [3, 4, 7]\n",
      "[3, 4, 7, 5] - [3, 4, 5, 7]\n",
      "[3, 6, 4, 5] - [3, 6, 4, 7]\n",
      "[3, 4, 5, 4, 7] - [3, 4, 7, 4, 5]\n",
      "[3, 6, 4, 5, 4] - [3, 6, 4, 7, 4]\n",
      "[3, 4, 7, 5, 4, 7] - [3, 4, 5, 7, 4, 5]\n",
      "[3, 6, 4, 5, 4, 7] - [3, 6, 4, 7, 4, 5]\n",
      "[3, 6, 4, 7, 5, 4, 7] - [3, 6, 4, 5, 7, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "convTrainSeqs = lambda task_id: [let2idx(task_id, seq) for seq in train_seqs]\n",
    "convTestSeqs = lambda task_id: [let2idx(task_id, seq) for seq in test_ugr_seqs]\n",
    "tr1 = convTrainSeqs(0)\n",
    "tr2 = convTrainSeqs(1)\n",
    "te1 = convTestSeqs(0)\n",
    "te2 = convTestSeqs(1)\n",
    "\n",
    "for seq in tr1:\n",
    "    if seq in tr2:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "for se in tr2:\n",
    "    if se in te1:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "\n",
    "for i in range(len(tr1)):\n",
    "    print(f\"{tr1[i]} - {tr2[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpwRQ7xLvnWR"
   },
   "source": [
    "## Plotting & Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhO-beIdl9SC"
   },
   "source": [
    "### plotTranser, plotResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "y6vb8He6vnWR"
   },
   "outputs": [],
   "source": [
    "def plotTransfer(data, title):\n",
    "    # data = [n_methods, n_tasks, combinedepochs]\n",
    "    n_methods, n_tasks, n_combinedepochs = data.shape\n",
    "    fig, axs = plt.subplots(n_tasks, 1)\n",
    "    colors = ['blue','green','orange','red','yellow','violett']\n",
    "    \n",
    "    xvals = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    for task_idx in range(n_tasks):\n",
    "        for method_idx in range(n_methods):\n",
    "            axs[task_idx].plot(\n",
    "                xvals,\n",
    "                data[method_idx, task_idx],\n",
    "                color=colors[method_idx]\n",
    "            )\n",
    "            axs[task_idx].set_ylim(0,1.1)\n",
    "            if task_idx != n_tasks - 1:\n",
    "                axs[task_idx].tick_params(\n",
    "                    axis='x',\n",
    "                    which='both',\n",
    "                    labelbottom=False\n",
    "                )\n",
    "        x_lines = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, N_EPOCHS)\n",
    "        for xpos in x_lines:\n",
    "            axs[task_idx].axvline(xpos, color=\"grey\")\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "def plotResults(hist_loss, hist_hits, plotLoss=False):\n",
    "    if torch.is_tensor(hist_loss):\n",
    "        hist_loss = hist_loss.numpy()\n",
    "    if torch.is_tensor(hist_hits):\n",
    "        hist_hits = hist_hits.numpy()\n",
    "    if plotLoss:\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,0,:], 0), \"Train Loss\")\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,1,:], 0), \"Test Gr Loss\")\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,2,:], 0), \"Test Ugr Loss\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,0,:], 0), \"Train Hits\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,1,:], 0), \"Test Gr Hits\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,2,:], 0), \"Test Ugr Hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotAverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAverages(hist_all, datasets=(0,1,2), figsize=(15,9), title=\"\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets: tuple (int)\n",
    "        0 for train, 1 for test gr, 2 for test ugr\n",
    "    title: string\n",
    "        plot title\n",
    "    \"\"\"\n",
    "    # Handle case where only one dataset is displayed\n",
    "    if isinstance(datasets, int):\n",
    "        hist_all = np.expand_dims(hist_all[:,:,:,datasets], axis=-2)\n",
    "    elif len(datasets) == 1:\n",
    "        hist_all = np.expand_dims(hist_all[:,:,:,datasets[0]], axis=-2)\n",
    "    else:\n",
    "        hist_all = hist_all[:,:,:,np.array(datasets)]\n",
    "    \n",
    "    # hist_all = [repetitions, n_train_runs, n_tasks, n_datasets, n_evalsteps]\n",
    "    n_repetitions, n_train_runs, n_tasks, n_datasets, n_evalsteps = hist_all.shape\n",
    "    \n",
    "    # Invert Data\n",
    "    hist_all = 1 - hist_all\n",
    "    # averages\n",
    "    hist_all_avg = np.average(hist_all, axis=0)\n",
    "    hist_all_avg_concat = np.concatenate(hist_all_avg, axis=2)\n",
    "    # standard derivations\n",
    "    hist_all_std = np.std(hist_all, axis=0)\n",
    "    hist_all_std_concat = np.concatenate(hist_all_std, axis=2)\n",
    "    hist_all_min = np.fmax(hist_all_avg_concat - hist_all_std_concat, 0)\n",
    "    hist_all_max = np.fmin(hist_all_avg_concat + hist_all_std_concat, 1)\n",
    "    \n",
    "    n_totalepochs = n_evalsteps * STEP_SIZE_EVALUATION * n_train_runs\n",
    "    n_epochs = n_evalsteps * STEP_SIZE_EVALUATION\n",
    "    \n",
    "    fig, axs = plt.subplots(n_tasks, 1, figsize=figsize)\n",
    "    \n",
    "    # Vertical Lines\n",
    "    lines = range(0, n_totalepochs, n_epochs)\n",
    "    \n",
    "    for dataset_id in range(n_datasets):\n",
    "        for task_id in range(n_tasks):\n",
    "            # Right y limits\n",
    "            axs[task_id].set_ylim(-0.05, 1.05)\n",
    "            \n",
    "            # Hide parts of the graph without training\n",
    "            xvals = range(n_epochs * task_id, n_totalepochs, STEP_SIZE_EVALUATION)\n",
    "\n",
    "            # Average \n",
    "            axs[task_id].plot(xvals,\n",
    "                              hist_all_avg_concat[task_id,dataset_id,n_evalsteps * task_id:])\n",
    "            # Standard deviation\n",
    "            axs[task_id].fill_between(xvals,\n",
    "                                     hist_all_min[task_id,dataset_id,n_evalsteps * task_id:],\n",
    "                                     hist_all_max[task_id,dataset_id,n_evalsteps * task_id:],\n",
    "                                     alpha=0.2)\n",
    "\n",
    "            # prevent ticks in upper subplots\n",
    "            if task_id != n_tasks - 1:\n",
    "                axs[task_id].tick_params(axis=\"x\", \n",
    "                                         which=\"both\",\n",
    "                                         labelbottom=False)\n",
    "            # Epoch label\n",
    "            else:\n",
    "                axs[task_id].set_xlabel(\"Epochs\")\n",
    "\n",
    "            # Task Label\n",
    "            if task_id == 0:\n",
    "                axs[task_id].set_ylabel(f\"Replication percentage average N={n_repetitions}\")\n",
    "\n",
    "            # Vertical lines\n",
    "            axs[task_id].vlines(lines, 0, 1,\n",
    "                                transform=axs[task_id].get_xaxis_transform(),\n",
    "                                colors=\"grey\")\n",
    "            \n",
    "    legends = np.array([\"Train\", \"Test Grammatical\", \"Test Ungrammatical\"])\n",
    "    plt.legend(legends[np.array(datasets)])\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotSpecificTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSpecificTask(hist_all, train_id, task_id, figsize=(13,7), title=\"\"):\n",
    "    # hist_all = [repetitions, n_train_runs, n_tasks, n_datasets, n_evalsteps]\n",
    "    _, _, _, n_datasets, n_evalsteps = hist_all.shape\n",
    "    # Invert data\n",
    "    hist_all = 1 - hist_all\n",
    "    titles = (\"Train\", \"Test Grammatical\", \"Test Ungrammatical\")\n",
    "    # average\n",
    "    hist_all_avg = np.average(hist_all, axis=0)\n",
    "    hist_specific = hist_all_avg[train_id, task_id]\n",
    "    \n",
    "    # standard deviation\n",
    "    hist_all_std = np.std(hist_all, axis=0)\n",
    "    hist_std = hist_all_std[train_id, task_id]\n",
    "    hist_max = np.fmin(hist_specific + hist_std, 1)\n",
    "    hist_min = np.fmax(hist_specific - hist_std, 0)\n",
    "    \n",
    "    xvals = range(0, n_evalsteps * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for dataset_id in range(n_datasets):\n",
    "        plt.plot(xvals, hist_specific[dataset_id])\n",
    "        plt.fill_between(xvals, hist_min[dataset_id], hist_max[dataset_id], alpha=0.2)\n",
    "    \n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.legend(titles)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Replication percentage\")\n",
    "    plt.xlabel(\"Epochs Trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMGZLw18mBw9"
   },
   "source": [
    "### visual_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "kQWqbao6vnWR"
   },
   "outputs": [],
   "source": [
    "def visual_eval(model, test_dl):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    errors = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_dl):\n",
    "\n",
    "            src, src_len = batch\n",
    "            trg = src\n",
    "\n",
    "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "            show_batch(output, trg)\n",
    "\n",
    "\n",
    "def show_batch(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        status = \"same\" if p == t else \"different\"\n",
    "        print(f\"pred = {p} - {status} \\ntrg  = {t}\\n-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xnLRCNmDu1"
   },
   "source": [
    "### accuracy, accuracyAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nPn1QguwvnWR"
   },
   "outputs": [],
   "source": [
    "def accuracy(model):\n",
    "    for task_id in range(N_TASKS + 1):\n",
    "        gr_not_hits = evaluate_extra(model, test_dls[task_id], allOrNoneLoss)\n",
    "        ugr_not_hits = evaluate_extra(model, test_ugr_dls[task_id], allOrNoneLoss)\n",
    "        gr_hits = 1 - gr_not_hits\n",
    "        ugr_hits = 1 - ugr_not_hits\n",
    "        total_acc = (gr_hits + ugr_not_hits) / 2\n",
    "        print(f\"Task {task_id}: Acc {total_acc:2.2}% | Gr acc {gr_hits:2.2} | Ugr acc {ugr_not_hits:2.2}\")\n",
    "        \n",
    "def accuracyAll(models):\n",
    "    for model_id in range(len(models)):\n",
    "        print(f\"\\nModel {model_id}\")\n",
    "        accuracy(models[model_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xoKXlA1vnWQ"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "jc0kqVE-vnWQ"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = max(vocabs[0].values()) + 1\n",
    "OUTPUT_DIM = max(vocabs[0].values()) + 1\n",
    "ENC_EMB_DIM = 30\n",
    "DEC_EMB_DIM = 30\n",
    "ENC_HID_DIM = 10\n",
    "DEC_HID_DIM = 10\n",
    "ENC_DROPOUT = 0.7\n",
    "DEC_DROPOUT = 0.7\n",
    "LEARNING_RATE = 0.01\n",
    "SRC_PAD_IDX = PAD_TOKEN\n",
    "TRG_PAD_IDX = PAD_TOKEN\n",
    "PREFIX = \"tr\"\n",
    "N_EPOCHS = 200\n",
    "CLIP = 1\n",
    "STEP_SIZE_EVALUATION = 10\n",
    "TEST_ALL_TASKS = 1  \n",
    "\n",
    "N_REPETITIONS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline A: Individual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4QnSSsYav-F"
   },
   "source": [
    "### Experiment Individual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "I9vjyK31vnWT"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HS5ogr-8vnWT",
    "outputId": "062a393f-f2da-4723-a4d8-38b61302b32a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ REPETITION   0 ------\n",
      "tr-AE-30-10-0.01-A0\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (intermediate): Linear(in_features=60, out_features=30, bias=True)\n",
      "    (fc_out): Linear(in_features=30, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 7468 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.671 | Train PPL:   1.956\n",
      "\t Val. Loss: 0.538 |  Val. PPL:   1.712\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.646\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.590\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.518\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.481\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.466\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.385\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.383\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.320\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.154\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.146\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "tr-AE-30-10-0.01-A1\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (intermediate): Linear(in_features=60, out_features=30, bias=True)\n",
      "    (fc_out): Linear(in_features=30, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 7468 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.674 | Train PPL:   1.962\n",
      "\t Val. Loss: 0.507 |  Val. PPL:   1.660\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.588\n",
      "\t Val. Loss: 0.448 |  Val. PPL:   1.565\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.653\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.567\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.530\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.547\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.535\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6241d17d78f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The model has {count_parameters(model)} trainable parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mhist_loss_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_hits_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_task\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTEP_SIZE_EVALUATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mhist_all_losses_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepetition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_task\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist_hits_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mhist_all_hitsss_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrepetition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_task\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist_hits_temp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-407288e2b721>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, task_id, epochs, step_size_evaluation, clip)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mSTEP_SIZE_EVALUATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mother_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_id\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mtotal_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ugr_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mother_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c9e147179e16>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, criterion)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#turn off teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m#seq = [seq_len, batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-73ff751608d6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#  and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m#place predictions in a tensor holding predictions for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-9e6943bb2c1c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#weighted = [1, batch size, enc hid dim * 2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_repetitions = N_REPETITIONS\n",
    "hist_all_losses_A = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "hist_all_hitsss_A = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "# hist = [repetitions, task-model-was-trained-on, task-test-was-run-on, dataset-used, timesteps]\n",
    "for repetition in range(n_repetitions):\n",
    "    print(f\"------ REPETITION {repetition:3} ------\")\n",
    "    # Save last iteration model\n",
    "    if repetition == n_repetitions - 1:\n",
    "        models_A = {}\n",
    "\n",
    "    for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "        SUFFIX = f\"A{n_task}\"\n",
    "        title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "        LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "\n",
    "        attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "        enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "        dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "        model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "        criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "        print(title)\n",
    "        print(model.apply(init_weights))\n",
    "        print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "        hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "        hist_all_losses_A[repetition,n_task] = hist_hits_temp\n",
    "        hist_all_hitsss_A[repetition,n_task] = hist_hits_temp\n",
    "        if repetition == n_repetitions - 1:\n",
    "            models_A[SUFFIX] = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_A[:,:,:,1:], figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAVeSfB_vnWU"
   },
   "source": [
    "Plot Results Single (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "WBCWzONDvnWU",
    "outputId": "2c40070d-31a3-4913-98c2-51e44b8432f5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0/0lEQVR4nO3deZRU1bn38e/D0Kg4oICIgLS2iHGIRhvE4TqLUyJe44Ah4Nx0qxnWve8bTfImN8lK7k1MbuJciIo4G+MUElGjximOgBOKmtCI0jTKEEVAZNzvH09VqG6qu6q761Sdqvp91qpVXadOn7M31Zyn9j7P3ttCCIiIiMRNt2IXQEREJBMFKBERiSUFKBERiSUFKBERiSUFKBERiSUFKBERiSUFKJESYGYrzWy3YpdDpJAUoKQsJC/gqcdGM1ud9npcJ473tJldmGWfKjP7sZm9Z2arzGyhmT1iZqM7eK5gZru32vYTM7sj9TqEsHUIYV7yvalm9vOOnEOkFPUodgFE8iGEsHXqZzObD1wYQngi4tPeBwwCJgCvJbcdDZwM/KX1zmbWI4SwPuIyiZQNtaCkrJlZNzO73MwazWyZmd1rZjsk39vCzO5Ibv/UzGaY2QAz+wXwb8C1yRbYtRmOeyxwHDAmhPByCGFt8vFoCOE7afvNN7PLzOxNYJWZdepLYaqVZWZ1wDjge8my/Sn5/mXJFtyKZIvumM6cRyRO1IKScvdt4FTgCGAJcDVwHXA2cA6wHTAEWAPsD6wOIfzQzA4F7ggh3NTGcY8FXg4hNOVQhrPxVtXSrragQgiTzewQoCmE8P8AzGw4cCkwIoTQbGbVQPeunEckDhSgpNxNBC5NBRIz+wnwoZmNB9YBfYHdQwhvArM6cNx+wEepF8lW2TzAgF4hhC3S9r06hLAgy/FeNbONaa+3wLsQc7EB6AXsZWZLQgjzc/w9kVhTF5+Uu6HAg8kuvE+Bd/AL+gDgduAx4B4zazazK8ysZ47HXQYMTL0IIfwzhNAHOBAPFumyBSeAA0IIfVIP4Jc5loMQwlzgu8BPgMVmdo+Z7Zzr74vElQKUlLsFwInpF/8QwhYhhIUhhHUhhJ+GEPYCDgG+iic8AGSb5v9JYISZDc6hDPleMmCz44UQ7gohHIYH5AD8Ks/nFCk4BSgpd5OAX5jZUAAz629mY5I/H2Vm+5pZd+AzvMtvQ/L3PgbaHHcUQvgL8BTwkJkdlEw57wmMirAuKS3KZmbDzexoM+sFfAGsZlM9REqWApSUu6uAacBfzGwF8BJwUPK9nfD7PJ/hXX/PAHek/d7pZvaJmV3dxrFPA/6c/J1PgffxDLsT8l+NFm7G7zd9amYP4V2KvwSW4vfFdgR+EHEZRCJnWrBQRETiSC0oERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJJQUoERGJpR7FOnG/fv1CdXV1l46xbNkyAPr27ZuHEkmc6LMtb/p8Jd2sWbOWhhD6t95etABVXV3NzJkzu3SMqVOnAnDuued2vUASK/psy5s+X0lnZh9k2q4uPhERiaWsAcrMppjZYjN7q433zcyuNrO5ZvammR2Q/2KKiEilyaUFNRU4oZ33TwSGJR91QKLrxRIRkUqX9R5UCOFZM6tuZ5cxwG0hhAC8ZGZ9zGxgCGFRvgqZyapVMGcObL99bvuHAN/9Lpx5Jhx6aJQla2nlSvjOd+C//gt22aVw5y11n3wCCxbAcccVuyQShX339efWn+8WW8D118OQIdmP8dFHcPnl8Lvf5X4dkNKSjySJQcCCtNdNyW2bBSgzq8NbWezSxav1VlvB6tXw+ecefMza3/+vf4Wrr4bXX4dnnunSqTvk1lthyhQ48EC4+OLCnbfUffwxLF/un6+Unw0b/Ln15/v003DllfC//5v9GFdd5f+/jj8ezj473yWUOMhHgMoUGkKmHUMIk4HJALW1tRn3yfmkBjvvDH//O7z4IhxySPv7X3+9Pz/7LLz9Nuy9d1fOnpsQIJHs8GxsjP585WT1ath2W3j++WKXRKKQTOLjqqtabj/rLLjlFvj5z2HLLdv+/TVr4Oab/eeZMxWgylU+sviagPQG+WCgOQ/HzWrHHaF7901BoC0LF8If/wjnnQdVVdn3z5fnnvNgCApQHbV6tXf3SGVpaPDu3d//vv39HngAliyBbbaBGTMKUzYpvHwEqGnAhGQ23yhgedT3n1K6d4eddoJ774WlS9ve76abvEvhhz/0e1C33eb3hqKWSECfPnDssQpQHbFyJaxb1/43aClPRxwBX/pS9i+RiQTU1MCECfDqq5u6DKW85JJmfjfwIjDczJrM7AIzqzez+uQu04F5wFzgRqCgd1oGDoS1a71bIJP16+HGG72fuqbGv6GtWAF33RVtuT7+GO6/H845x28INzZ6l59kN2+ePytAVR4zqK+HV17xwJPJW29570R9PYwc6QlT775b2HJKYWQNUCGEs0MIA0MIPUMIg0MIN4cQJoUQJiXfDyGES0IINSGEfUMIXZseooN694bDD4cbboCNGzd//09/8i6+hgZ/ffDB8OUv+zewKAPGlCneCqiv98C4ejUsKki7svSlWpvq4qtMEyZ4ElRbrahJk6BXL++yHzHCt6mbrzyVxUwSDQ1+UXv88c3fSyRg8GA4+WR/beb7v/46vPxyNOXZsMED5lFHwZ57eoACdfPlKvXvpBZUZerTx5Me7rrLMznTrVzpXfRnngl9+8Iee8DWW3uihJSfsghQp53mCROtv3H94x8etOrqoEdavuK4cf5HHVWyxKOPwgcfbGq1KUB1TGOjf149ijZTpBRbQ4OnoN92W8vtd93lXfSp/1vdu/sQDrWgylNZBKiqKrjgAu/O+/DDTdtvuMEvchde2HL/bbaB8eM9Uyg5qXJeJRKevHHqqf566FDo1k0BKleNjWo9VboDD/Tuu/Su+BB8uMh++8GoUZv2ra2FN97we9FSXsoiQIG3kkLwhAjwez633OJBYuDAzfdvaPCxFKnxGPkyfz5Mn+5BsWdP31ZV5bNIKEDlRgFKwP+PvvOOj10EeOklD0QNDS0H5o8Y4f+X38o4W6iUsrIJUNXVcNJJnlK+bh384Q/wz39u6gpobd99fcqjSZMyJ1d01uTJ/p+nrq7l9poaBahcrFvn3aNKkJCzzvL7Uamu+ETCez/GjWu5X22tP+s+VPkpmwAFHow++ggeesj/mIcP90SF9vafOxeefDI/51+71ke3f/Wrm88lpgCVmw8/9CQTtaBkq63g3HN9UO7bb/t4x/Hj/f5xut12gx120H2oclRWAeqEE/x+z/e/790B9fXtz9F3+unQr1/+kiUeeAAWL87caqup8ftdrbOSpCVl8Em6+npvVY8Z4914mf5vmXkrSi2o8lNWeVLdu8PEifCDH/gF7pxz2t+/Vy84/3yfmPKZZ2C77bp2/muu8W9zo0dv/l56Jt8BWjGrTRoDJemGD4ejj/bJng87DPbZJ/N+tbXwq1/5vWd9uSkfZRWgwLP5fvpT+MY3cpuCf+JE+M1v4Mgj83P+K67wjL3WFKByM3euB6devYpdEomLSy7xAHXJJW3vM2KEdw2/8UbLDD8pbWUXoHbcEWbNyn3tpd1289nQm/MwvW3Pnm2vX6SxULlpbPTPRCTl3//du+xHjmx7n1SixIwZClDlpOwCFHR8KY32/vDzZZttoH9/BahsGhs3BXMR8HtMBx3U/j6DBvnYQ92HKi9llSQRd8rka18IPlGsApR0VCpRQpl85UUBqoAUoNr30Uc+vY0ClHTGiBE+q/mKFcUuieSLAlQB1dTAggWeLiubSwVvBSjpjNpab4W3tUyHlB4FqAKqqfH/QPPnF7sk8aQAJV2hGSXKjwJUASmTr32NjZ6iX11d7JJIKdpxR8/e1X2o8qEAVUAKUO1rbPQpoqqqil0SKVUjRqgFVU4UoApowABfAVgBKjOlmEtX1db639E//1nskkg+KEAVkJkPQlWAykwBSroqtQT8rFnFLYfkhwJUgSnVPLPPPoOlSxWgpGsOPNCfdR+qPOQUoMzsBDN7z8zmmtnlGd4/0syWm9nryceP81/U8lBT44NR87kGVTlQBp/kQ58+MGyY7kOVi6xTHZlZd+A64DigCZhhZtNCCHNa7fpcCOGrEZSxrNTU+Dio5mYYPLjYpYkPBSjJlxEjNq3CK6UtlxbUSGBuCGFeCGEtcA8wJtpilS9l8mWmACX5UlsLTU0+M4mUtlwC1CBgQdrrpuS21g42szfM7BEzyzhdq5nVmdlMM5u5ZMmSThS39O2+uz8rQLXU2OiT6W67bbFLIqUutYr2ffcVtxzSdbkEqExr0oZWr18FhoYQ9gOuAR7KdKAQwuQQQm0IobZ///4dKmi52GUX6NFDAao1ZfBJvuy/v7eiEgmfuUVKVy4BqgkYkvZ6MNBi9aQQwmchhJXJn6cDPc2sX95KWUZ69PBl6RWgWlKAknxqaIA5c+C554pdEumKXALUDGCYme1qZlXAWGBa+g5mtpOZWfLnkcnjLst3YcuFUs1bWrvWJ9FVgJJ8GTvWM/oSiWKXRLoia4AKIawHLgUeA94B7g0hvG1m9WZWn9ztdOAtM3sDuBoYG4Ia121RgGpp/nxPu1eAknzZais45xy4/374+ONil0Y6K6dxUCGE6SGEPUIINSGEXyS3TQohTEr+fG0IYe8Qwn4hhFEhhBeiLHSpq6mBTz7RdCwpyuCTKNTXw7p1cPPNxS6JdJZmkigCpZq3pAAlUdhzTzj6aLjhBtiwodilkc5QgCoCBaiWGht9Et0BA4pdEik3DQ3w4YfwyCPFLol0hgJUEey2mz8rQLnGRv83sUwDGkS6YMwYGDhQyRKlSgGqCHr3hp12UoBKUYq5RKVnT7jwQm9Bvf9+sUsjHaUAVSTK5HMbN/rkuQpQEpWLLvLW+eTJxS6JdJQCVJEoQLlFi+CLLxSgJDpDhsDXvubZfGvWFLs00hEKUEVSUwMLF8Lq1cUuSXEpg08KoaEBliyBBx4odkmkI7IutyHRSF2QZ8709Wsq1euv+7MClETpuOP8byyRgLPPLuy5Q/DxWFVVhT1v1ObP93vpW2wR3TkUoIpkjz38+fDDi1uOOKiq8kl0RaLSrRtMnAjf+x689Rbss0/hzn3llfDf/w1z58J22xXuvFEbPx7Wr4cXX4zuHApQRVJbC3ffDZ9+WuySFN8ee3i2lUiUzjsPfvQjb0Vdd11hzrlhA/zud7B0Kdx2G3zrW4U5b9Rmz4a//Q1+/etoz6MAVSRmPqGliBRGv35wxhlw++3wq1/B1ltHf86HH/aJkLfZxgPjpZeWx3i/SZOgVy8P+lFSkoSIVIyGBlixAu68szDnSyRg553hf/8X3nmnPJaiX7nSg/yZZ0LfvtGeSwFKRCrGwQfDl79cmMUM582Dxx7zcVjjxpXP8h933ulBvqEh+nMpQIlIxTCDiy+GN96Al16K9lw33ODJGRdd5Mt/nHuup7mX8vIfIXiQ3W8/GDUq+vMpQIlIRRk3btM9oaisWQNTpsApp8CgQb6tHJb/eOklD+4NDYW5l6YAJSIVZeutPUX63nthWUTrft93n2fupXeDDR9e+st/JBIe3MeNK8z5FKBEpOI0NHgr55Zbojl+IgG77w7HHLP5eT/8EKZPj+a8UVq2zIP6+PGFyYAEBSgRqUD77AOHHebp0hs35vfYs2fD8897l163VlfYUl7+45ZbPKgXIjkiRQFKRCpSQ4PPBfnEE/k9biLhY4TOPXfz91LLfzz6aGkt/7Fxowfzww4r7CwcClAiUpG+/nXo3z+/rZkVK3yM0FlntT1GKLX8xw035O+8UXviCQ/mhWw9QY4BysxOMLP3zGyumV2e4X0zs6uT779pZgfkv6giIvnTqxecfz5MmwZNTfk55p13+kDW9i7kpbj8RyLhwfzrXy/sebMGKDPrDlwHnAjsBZxtZnu12u1EYFjyUQeUYA+riFSaiRN9bM+NN3b9WKkxQvvvDwcd1P6+DQ2e5Xf//V0/b9SamjyIn3++B/VCymUuvpHA3BDCPAAzuwcYA8xJ22cMcFsIIQAvmVkfMxsYQliU9xKLiOTJrrvCiSd6YFm5smvHWrEC3nzTu+6yjRFKLf/xs5/BrFldO2/UZs/24DtxYuHPbSHLfB9mdjpwQgjhwuTr8cBBIYRL0/b5M/DLEMLfkq+fBC4LIcxsdaw6vIUFMBx4Lw916AcszcNxSkUl1beS6gqqbzmrpLpCx+s7NITQv/XGXFpQmb4LtI5quexDCGEyMDmHc+bMzGaGEGrzecw4q6T6VlJdQfUtZ5VUV8hffXNJkmgChqS9Hgw0d2IfERGRnOUSoGYAw8xsVzOrAsYC01rtMw2YkMzmGwUs1/0nERHpiqxdfCGE9WZ2KfAY0B2YEkJ428zqk+9PAqYDJwFzgc+BiJexaiGvXYYloJLqW0l1BdW3nFVSXSFP9c2aJCEiIlIMmklCRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiKZf1oCLRr1+/UF1d3aVjLFu2DIC+ffvmoUQSJ/psy5s+X0k3a9aspZ1dsDAS1dXVzJw5M/uO7Zg6dSoA5557btcLJLGiz7a86fOVdGb2Qabt6uITEZFYyhqgzGyKmS02s7faeN/M7Gozm2tmb5rZAfkvpoiIVJpcWlBTgRPaef9EYFjyUQckul4sERGpdFkDVAjhWeCf7ewyBrgtuJeAPmY2MF8FlMr00Ufw/POw/fabP447DnJdZ/PnP4exY3M/73/+Z+Zzbr89/Md/dK4u0tKSJfDyy21/vpkeO+8Mb79d7JJLoeUjSWIQsCDtdVNy26LWO5pZHd7KYpdddsnDqaVcffqpB6EJE1pub2qCBx6AZ56BI49s/xgrV8IVV8CKFXD55bD//u3vv2QJXHstjBgBBx7Y8r3HH4c//Ql++9sOVkQ28+qr8MUX0L//5p9vW266Ca66CiZX2sLpFS4fAcoybMv4/TaEMJnkWvW1tbVaa17atGYN9O7tF6V0q1fDoEGQSGQPUHfe6cGpe3ff/4Yb2t9/yhRYu9Yvgnvt1fK9H/wAfv1rWL8eehQt97U8NDb68+67Q11dbr+zapV/nr/+NWy3XXRlk3jJRxZfEzAk7fVgoDkPx5UKtnYtVFVtvn3LLeG887wVtWizNvomIXhQ2m8/OOccuOMOWL687f03bvQAdsQRmwcngJoaD04fftjxukhLc+dCt26ZP9+2NDTA55/DbbdFVy6Jn3wEqGnAhGQ23yhgeQihnUuHSHZr1kCvXpnfq6/3YHHzzW3//ksvwRtv+IUtdXG7/fa293/sMXj/fd83k5oaf059+5fOa2yELbbo2O8ceKB3vSYSud9/lNKXS5r53cCLwHAzazKzC8ys3szqk7tMB+YBc4EbgYsjK61UhJUrYcOGtr9hDxsGxx7rXXEbNmTe5/rrYZttYNw4qK31R3sXt+uvhwED4N//PfP7ClD509joLeGOamiAd96BZ5/Nf5kknnLJ4js7hDAwhNAzhDA4hHBzCGFSCGFS8v0QQrgkhFATQtg3hNC16SGk4jUnO4jbakGBX6wWLICHH978vaVL4d57/Qb81ltv2n/OHHjuuc33/+ADP86FF7YdFAcN8vIoQHVNCDBvXucC1FlnQZ8+/kVDKoNmkpDYSQWo9u5RnHKKpx5nuljdcovfw0rvrhs7tu2L2+TJYNb+Dftu3WDXXRWgumrRIk906WgXH8BWW8G55/r9x48/znvRJIYUoCR2cmlB9egBF13k947mzdu0PZXs8G//BnvvvWn7Vlt5ssT997e8uK1d6ynMJ58M2UY+1NQoQHVV6t+vMy0o8PuP69a1f/9RyocClMTOwoX+3F6AAg9Q3bq1TB9//HG/CGZKdkhd3KZM2bTtwQdh8eK2kyPSpQKUbtJ3XlcD1PDhcPTR/pm3df9RyocClMROc7MHnu7d299v0CDv6psyxQd+gnfh9e8Pp522+f577glHHdXy4pZIeNfd8cdnL1dNjY/HWby4Y/WRTRob/bPtTBdfSkODp/tPn56/ckk8KUBJ7DQ3Z289pTQ0eFLEffd50sSf/gQXXND27zc0eFLEI4940sQzz8DEiX7RzEaZfF3X2OhdqZZpeH+OxoyBgQOVLFEJFKAkdjoSoI45xmckSCTgxhu9+23ixLb3P/VU2Gkn33/SJE/EOP/83M6lANV1jY2b/h07q2dPz7h89FEfuyblSwFKYmfhwtxnGejWze8tvfCCT4t04onQ3kLNqYvbI494tt/pp3uXYC523dW/+StAdV4+AhT4/Uez7NNXSWlTgJJYCaFjLSjw1ONeveCzz3JLdqir84vbypW57Z/SqxcMHqwA1VnLl8OyZfkJUEOGwNe+5tl8a9Z0/XgSTwpQEiuffOIXnI7M09a3r8/Pt+ee3oLKZsgQOPNMGDkSDj20Y+VTqnnnpf7d8hGgYNP9x/vvz8/xJH4UoCRWchkDlcm118Lrr2fP/Eu5/XafVaKjN+sVoDov3wHquOP8WEqWKF8KUBIrqTFQHWlBgQemjgS1Hj06fg7wC+Lixb6Mh3RMvgNU6v7j3/4Gs2fn55gSLwpQEiudbUEVSurimj57heSmsdETUrbZJn/HPO88/1uZNCl/x5T4UICSWMllHr5iUqp55+Urgy9d375+P/H22z3pRcqLApTESnMz7LBDbgNni0EBqvOiCFDgyRIrVviKu1JeYnoZkEq1cKHPUh5Xffp4AFWA6pg1a3ymjygC1KhRvnKyFjMsPwpQEivNzfEOUKBMvs6YP9+DRxQBysxbUW+84SspS/lQgJJYaW72SWDjTAGq4/KdwdfauHGefHH99dEcX4pDAUpiY8MG+Oij0mhBffihL90huYk6QG29NYwf7yspL10azTmk8BSgJDYWL/YgVQoBasMGnxVdctPYCL17w4AB0Z2jocEXoLzllujOIYWlACWxkUoxj3uA2n13f1Y3X+4aG2G33bq2zEY2++wDhx3mE8hu3BjdeaRwcgpQZnaCmb1nZnPN7PIM7x9pZsvN7PXk48f5L6qUu1SAKoV7UKAA1RFRpZi31tDg53r88ejPJdHLGqDMrDtwHXAisBdwtpntlWHX50II+ycfP8tzOaUClEoLauBAX7JcASo3Gzf6zBuFCFBf/7rPVqH5+cpDjxz2GQnMDSHMAzCze4AxwJwoCyaVZ+FCH6Ab5X2KfDDz7qr2AtRll8GOO8J//mfhyhVXzc0+DqoQAapXL1+A8te/hi99afP3t98eHn7YnyX+cglQg4AFaa+bgIMy7Hewmb0BNAP/J4TwdusdzKwOqAPYZZddOl5aKWvNzR6ceuTyV1lk7aWaf/CBXyB79/a1p/I591wpijqDr7XvfhcWLYIvvmi5/bPPfBXeZ5/1ZeMl/nK5FGS6rdl6vParwNAQwkozOwl4CBi22S+FMBmYDFBbW6sx39JCKQzSTamp8fscIWx+43/yZN++ciXccUfHFkUsR4UOUDvtBLfeuvn2zz+HbbeFmTMVoEpFLkkSTcCQtNeD8VbSv4QQPgshrEz+PB3oaWb98lZKqQhxn+YoXU0NrF7t39TTrV0LN90Ep5wCX/mKpt8BD1Ddu0OxO0222soz/WbMKG45JHe5BKgZwDAz29XMqoCxwLT0HcxsJzP/HmlmI5PHXZbvwkp5K7UWFGzezffggz6eq6HBH7NnwwsvFL58cdLYCEOHQs+exS4J1NZ6C6rSvzSUiqwBKoSwHrgUeAx4B7g3hPC2mdWbWX1yt9OBt5L3oK4GxoagPwHJ3Zo1PgNA3FPMU9oKUImEJ1CMHg3f+IZ3KVV6RlmhUsxzMWIELFvmcwNK/OU0DiqEMD2EsEcIoSaE8IvktkkhhEnJn68NIewdQtgvhDAqhFDh3xmloz76yJ9LpQU1dKhnHKYHqDlz4JlnYOJEf693b5gwAf7wB1iypHhlLbY4BajaWn9WN19p0EwSEguppd5LJUBVVfk9lfQANWmSbz/vvE3b6usre/qdTz7xR1wC1L77+mc0c2axSyK5UICSWCiVQbrp0lPNV63yzLEzzvCBoil77w2HH1650+8UOoMvm6oqXztKLajSoAAlsVAq0xylSw9Qd9/t42wypZQ3NPhMCn/5S2HLFwdxC1Dg96FmzarMLwylRgFKYqG52bO8+vYtdklyV1PjN9yXL/dEiH33hUMO2Xy/007zWSUqMVkiFaB226245UhXW+tLxP/978UuiWSjACWxkBoDFeVs1/mWahXcfTe8+qq3lDKVv6oKLrwQ/vxnX0eqkjQ2+uwgW29d7JJsMmKEP+s+VPwpQEkslMJKuq2lAtRPfuIX4G9+s+196+p87M3kyQUpWmzEKYMvZc89fdCu7kPFnwKUxEIpDdJNSV14P/7Yg1N7c+4NHQonn+yzTKxdW5jyxUEcA1SPHnDAAWpBlYISmJZTKkFzsw9uLSXbbOMZe0uW5DbfXkODd/P98pd+gewKMzj0UOjTJ7f9330XhgzxsVld8corPlNGLjZs8K7buAUo8PtQN9wA69e3PTnx+vXw9tue9dcVq1fD++/DXpkWKSphzz7r3bfDh0d3DgUoKbqVKz0DrtRaUAD77w/r1sGXv5x93+OP99V4/+u/8nPus8+Gu+7Kvt9HH/lF9oIL4PrrO3++OXPgoEzrGGSx776dP2dURoyAK6/0OrX12f32t75syquv+ryKnfXDH8I113gm55Ah2fcvBSH4gPTttoOXXoruPApQUnSlmGKect99uSd2dO/u8/J98EHXz3v99T5T+u9+l339rFS34m23eett2207d85EwhM+Hn/c7+Hkolcvn6A1btJnlMgUoDZs2BTME4nO3ztctQqmTPHW2I03ws/KZCnXp5/2VvnUqdGeRwFKiq4UB+mmdPRi379/y4G8nfW97/nsFFOmwPe/3/Z+Gzb4xbW62uefu+MOuPjijp9v5UoPcGec4QOPS93uu/u3/xkzvGXZ2qOP+heJ6mq4805f32u77Tp+nnvu8WEI1dX+ReFHP4rHpLldlUj4oo9nnhnteZQkIUVXygGqWPbcE446yu+jbNjQ9n4PPwwLFsBvftO15T/aG4hcirp1gwMPbDtRIpHwdaXuusvXkbr99s6dJ5Hw2USuucaXZvnjHztf5rhYtMhn7T/vPNhyy2jPpQAlRVdq8/DFRUODf8t/9NG290kk/N91zBjf/6234PnnO3aeENofiFyqRoyAN9/0mfTTzZ8P06f72LWDD/buwM4E9hkzfMaKhgY48UTP5CyHwdo33+xdlvX12fftKgUoKbrmZh9H1Nl7I5Xq1FP9W35bF7158+Cxx+CiizxTrbPLf7zyCrz2WtsDkUtVba0nuLz5Zsvtkyd7Pevq/HVDgydTPPdcx46fSHjW5Pjxfv+xrg7++ld47738lL8YUl3Gxx4LwzZbMz3/FKCk6EpxDFQc9Ozp3/KnT8+8vtENN3hX1kUX+evU8h/33dex5T8SiewDkUtRakaJ9AG7a9d6C+GrX92UcTd2rKfzdySwf/KJ338aN27TF68LLvDPbNKkvBS/KFJdxoXq6lWAkqJTgOq8ujr/tt86y2zNGk+gOOWUltmRqeU/pkzJ7fjLlvmFNttA5FK0yy7Qr1/L+1APPLBpReSUrbaCc86B++/3Qdm5uPVWH/+UfpwBA3xexqlT/b5WKUp1GZ9ySmHOpwAlRbdwYWmmmMfBkCH+bf+mm1reS7nvPl+huPU33Y4u/zF1qh+3XJIj0pl5Kyq9BZW+InK6+nrvDswlsIfgraRRo3ycXLqGBvj0Uw/6paZ1l3EhKEBJUYWgFlRXNTR4l90DD2zalkh4KvUxx2Te//33/WLTno0b/UJ7yCG5DUQuRbW1fn9p1SqfNeLZZzetiJwu16xJgKee8vtMmYL64Yf7jBKlmCzRusu4EBSgpKg++cS/oStAdd7o0f6tP3XRmz3bM/Xq6ze/0ELuy388+STMnVueraeUESM8EL/2WuYVkdPlkjUJ/u+6ww6ZxwiZ+ecyc2ZpzQXYVpdx1BSgpKg0BqrrunXzb/3PPedp5ImEz+Bw7rmZ96+q8hv2Dz/c/vIfiYSvz3X66ZEUOxZSM0o8/fSmgchtDaTOljUJPkbooYc8yG2xReZ9Jkzw+1ql1Ipqq8s4ajkFKDM7wczeM7O5ZnZ5hvfNzK5Ovv+mmXVxKkypFKkxULoH1TXnneeB5ze/8UGlZ53V/uKP2Zb/WLgQpk2D889v+0JbDgYO9L+9K67IPhA5W9Yk+L3A9ev9C0NbttvOU/7vvtt7EEpBe13GUcoaoMysO3AdcCKwF3C2mbWel/dEYFjyUQeU0HcDKSa1oPKjf3//9n/rrT4tUbZvutXVcNJJbS//ceONfq+lvQttuUitsJvLQOS2sibBA9PkyXDccdnHCDU0eJbfbbd1vtyFkq3LOEq55GKMBOaGEOYBmNk9wBhgTto+Y4DbQggBeMnM+pjZwBDCoryXOM3s2f6f6MgjozyLRGnBAn8eOLC45SgHF1/s88btv39us45ffLGvUXXIIZuvePvaaz77ehyXysi3ESN8CqJcBiIPGQJf+xpce61P/Jvu88+hqQmuvjr7OQ84wD+jn/3Mpw2Ks6am9ruMo2Qhy/wdZnY6cEII4cLk6/HAQSGES9P2+TPwyxDC35KvnwQuCyHMbHWsOryFBTAcyMeY6n7A0jwcp1RUUn0rqa6g+pazSqordLy+Q0MIm939y6UFlek7Reuolss+hBAmA3ld9NrMZoYQavN5zDirpPpWUl1B9S1nlVRXyF99c+lRbALSl9kaDDR3Yh8REZGc5RKgZgDDzGxXM6sCxgLTWu0zDZiQzOYbBSyP+v6TiIiUt6xdfCGE9WZ2KfAY0B2YEkJ428zqk+9PAqYDJwFzgc+BNoa6RSKvXYYloJLqW0l1BdW3nFVSXSFP9c2aJCEiIlIMmklCRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiSQFKRERiKZf1oCLRr1+/UF1d3aVjLFu2DIC+ffvmoUQSJ/psy5s+X0k3a9aspZ1dsDAS1dXVzJw5M/uO7Zg6dSoA5xZjLWKJlD7b8qbPV9KZ2QeZtmft4jOzKWa22MzeauN9M7OrzWyumb1pZgd0tbAiIiK53IOaCpzQzvsnAsOSjzog0fViiYhIpctlwcJnzay6nV3GALcFX1jqJTPrY2YDtaKuiLRl3TrYsAHmzWu5fautYKedilMmiZ98ZPENAhakvW5KbhMR2cyHH8ILL8DLL0NNTcvHwIFw/fXFLqHERT6SJCzDtozL9JpZHd4NyC677JKHU4tIqXnvPX8eOhRuvbXle3fdBd/+Nuy5Jxx9dOHLJvGSjwDVBAxJez0YaM60YwhhMsm16mtra7XWvEgFak5eHQYMgAkTWr536qlw8MFwxhnwyiveqpLKlY8uvmnAhGQ23yhgue4/iUhbUgGqV6/N39t2W5g2DUKAU06Bzz4rbNkkXnJJM78beBEYbmZNZnaBmdWbWX1yl+nAPGAucCNwcWSlFZGSt3Ah9OgB3dq4+tTUwB/+4F2B48Z5MoVUplyy+M7O8n4ALslbiUSkrDU3ezJEe445Bq68Er71LfjRj+C//7sgRZOY0Vx8IlJQzc2Zu/dau+QSuOgi+J//gbvvjr5cEj9Fm+pIRCpTrgHKDK69Ft55B845x1tSrfXuDQ8/DIMH57+cUnwKUCJSMBs3wqJFUFWV2/5VVfDAA/DTn8Knn7Z8b/16+P3v4cEHvStQyo8ClIgUzJIlHlhyDVAA/ft7SyqTWbPgL39RgCpXugclIgXTXop5Z4weDU89BWvX5ud4Ei8KUCJSMAsX+nM+A9SqVfDii/k5nsSLApSIFEyqBdWRLr72HHUUdO/u3XxSfhSgRKRgmps9Oy9fAWrbbX1qJAWo8qQAJSIF09wMO+7oQSpfRo/2ZImlS/N3TIkHBSgRKZiFC2FQnhfjGT3a5+578sn8HleKTwFKRAqmuRl23jm/x6ythT591M1XjhSgRKRgoghQ3bvDscd6gApaxKesKECJSEGsWweLF+e/iw+8m6+pCd59N//HluJRgBKRgliUXCUu3y0ogOOO82d185UXBSgRKYjUGKgoAlR1NeyxhwJUuVGAEpGCiDJAgXfzPf00rFkTzfGl8BSgRKQgUtMcRXEPCuD44+Hzz+GFF6I5vhSeApSIFERzM/TsCX37RnP8I4/046ubr3woQIlIQaSWeu8W0VVn663hkEPgsceiOb4UngKUiBREFGOgWhs9Gl57zdPZpfQpQIlIQUQxzVFro0f78xNPRHseKYycApSZnWBm75nZXDO7PMP7R5rZcjN7Pfn4cf6LKiKlrBAtqK98xe9x6T5Ueci65LuZdQeuA44DmoAZZjYthDCn1a7PhRC+GkEZRaTErVoFy5dHH6BaT3uUz1nTpfByaUGNBOaGEOaFENYC9wBjoi2WiJST1CwSUXfxgXfzLVoEjzwS/bkkWrkEqEHAgrTXTcltrR1sZm+Y2SNmtnemA5lZnZnNNLOZS5Ys6URxRaQUpcZARd2CAjjrLNh3X/jmN+Ef/4j+fBKdXAJUpkZy6zmDXwWGhhD2A64BHsp0oBDC5BBCbQihtn///h0qqIiUrqhnkUjXuzf88Y+ezj5mjHctSmnKJUA1AUPSXg8GmtN3CCF8FkJYmfx5OtDTzPrlrZQiUtIKGaAAdt0V7rvPW1DjxsGGDYU5r+RXLgFqBjDMzHY1sypgLDAtfQcz28nMb0ea2cjkcZflu7AiUpoWLvSWzbbbFu6cRx4J11wDDz8MP/hB4c4r+ZM1iy+EsN7MLgUeA7oDU0IIb5tZffL9ScDpQIOZrQdWA2ND0NJhIuJSKeaFzqqrr4c334Qrrth0X0pKR9YABf/qtpveatuktJ+vBa7Nb9FEpFwUYgxUW666Ct55By680JfkGDmyOOWQjtNMEiISuWIGqJ494Q9/8HkATz1VSROlRAFKRCIVQmGmOWpPv34wdaqPj3r00eKVQzpGAUpEIvXpp/DFF8VrQaUcdhj06aNpkEqJApSIRKrQKeZtaT0NksSfApSIRCoVoIrZxZcyejQ0NcG77xa7JJILBSgRiVQhpznK5rjj/FndfKVBAUpEIpVqQQ0cWNxyAFRXe6q5AlRpUIASkUg1N8P228OWWxa7JG70aHj6aVizpjDnW7gQfv/7wpyrkKZOjb5eClAiEqlip5i3Nno0fP45vPBCYc73f/8vjB3rM1qUi3Xr4PvfhzvuiPY8ClAiEqliDtLN5MgjoUePwnTzLV7sk9YCJBLRn69QHnoIPvoIGhqiPY8ClIhEKm4Baptt4JBDChOgpkzx1sZhh3lrY8WK6M9ZCNdf7/fzjj8+2vMoQIlIZDZu9Nkb4hSgwLv5Xn0Volw3dcMGuOEGOOII+M1vYOXK6LvECuGdd/we3sSJPrYsSgpQIhKZxYv9Qh2ne1DgAQrgiSeiO8djj8H8+XDxxT5B7Ve+4t18pT5IeNIkn9/w/POjP5cClIhEJi6zSLR2wAGwww7RdvMlEjBggE9Qa+b3a2bPLlxyRhRWrYJbb4XTT4cdd4z+fApQIhKZuAaoXKY9Sk1y2xkffOALJV54IVRV+bZvfMMXbCzlZIl77vHZ4KNOjkhRgBKRyKQu8HHr4gPv5mtuhjlzMr9/8cUwZAg8+GDHjz15srea6uo2bevdGyZM8KU/orz3FaVEAvbe25M+CkEBSkQi09zsF+oBA4pdks2l7kNl6uZLJPxey3bbwfjxHRvDtHYt3HQTnHwy7LJLy/fq6/39KVM6X+5imTEDZs3y1lOhVkZWgBKRyDQ3e3DqkdPa3YU1ZAh86UubB6innoJvf9sDzOzZ3i13yim5t3oefNCTQzJ1g+29Nxx+uGf3bdzY9ToUUiLhrcDx4wt3TgUoEYlM3MZAtTZ6NDzzjK9XBTBvHpxxBgwbBnfdBYMHbxqUevrp3vrJJpGAXXdte4xQQwO8/75n+ZWKTz7x+0/jxnnALhQFKBGJTNymOWpt9GhYvRqef94H0Y4Z4y2badM2XYhHjoSbb4Znn4XvfKf9482Z4wFv4kTo1sbV9bTTPAOulJIlbr3V/50KlRyRklOAMrMTzOw9M5trZpdneN/M7Ork+2+a2QH5L6qIlJq4t6COOMLH9Dz6KHzzmz4I9d57YffdW+43bhxcdpnfl2ovsEya5Fl77Y0RqqqCCy7wLL8PP8xPPaIUgtdr1CjYf//CnjtrgDKz7sB1wInAXsDZZrZXq91OBIYlH3VACX03EJEorF3r923iHKB69/aMtCuv9FbTb3/r6eeZ/OIXfl/q29/2+1StpY8R6t+//fPW1fmFf/LkLlchck89Be+9V/jWE0Auty5HAnNDCPMAzOweYAyQnpw5BrgthBCAl8ysj5kNDCEsynuJ07z4oo9S/9a3ojyLFMNZZ/mzPtvSlRpfFOcuPvBuvqee8lZNe39v3bv7falRo3zhw9bLh2zYkHs3WHW1B7v/+R+46qouFT9ya9b4oOYzzyz8uS1kmXfDzE4HTgghXJh8PR44KIRwado+fwZ+GUL4W/L1k8BlIYSZrY5Vh7ewAIYD7+WhDv2ApXk4TqmopPpWUl1B9S1nlVRX6Hh9h4YQNmt35tKCypTx3jqq5bIPIYTJQF4btWY2M4RQm89jxlkl1beS6gqqbzmrpLpC/uqbS5JEEzAk7fVgoLkT+4iIiOQslwA1AxhmZruaWRUwFpjWap9pwIRkNt8oYHnU959ERKS8Ze3iCyGsN7NLgceA7sCUEMLbZlaffH8SMB04CZgLfA6cF12RN1MCeTB5VUn1raS6gupbziqprpCn+mZNkhARESkGzSQhIiKxpAAlIiKxVLIBKtv0S6XIzKaY2WIzeytt2w5m9riZ/SP5vH3ae99P1v89M2tjasp4MrMhZvaUmb1jZm+b2XeS28u1vluY2Stm9kayvj9Nbi/L+oLPQmNmryXHSZZ1XQHMbL6ZzTaz181sZnJbWdY5ORnDfWb2bvL/8MGR1DWEUHIPPFmjEdgNqALeAPYqdrnyUK/DgQOAt9K2XQFcnvz5cuBXyZ/3Sta7F7Br8t+je7Hr0IG6DgQOSP68DfD3ZJ3Ktb4GbJ38uSfwMjCqXOubrMN/AHcBf06+Ltu6JusxH+jXaltZ1hm4Fbgw+XMV0CeKupZqC+pf0y+FENYCqemXSloI4Vngn602j8H/GEg+n5q2/Z4QwpoQwvt4BuXIQpQzH0IIi0IIryZ/XgG8AwyifOsbQggrky97Jh+BMq2vmQ0GTgZuSttclnXNouzqbGbb4l+mbwYIIawNIXxKBHUt1QA1CFiQ9ropua0cDQjJMWXJ5x2T28vm38DMqoGv4K2Ksq1vssvrdWAx8HgIoZzreyXwPSB9Wb5yrWtKAP5iZrOS07pBedZ5N2AJcEuyC/cmM+tNBHUt1QCV09RKZa4s/g3MbGvgfuC7IYTP2ts1w7aSqm8IYUMIYX98ppWRZrZPO7uXbH3N7KvA4hDCrFx/JcO2kqhrK4eGEA7AV3e4xMwOb2ffUq5zD/xWRCKE8BVgFd6l15ZO17VUA1QlTa30sZkNBEg+L05uL/l/AzPriQenO0MIDyQ3l219U5LdIU8DJ1Ce9T0UOMXM5uPd70eb2R2UZ13/JYTQnHxeDDyId2OVY52bgKZkDwDAfXjAyntdSzVA5TL9UrmYBpyT/Pkc4I9p28eaWS8z2xVfi+uVIpSvU8zM8D7sd0IIv017q1zr29/M+iR/3hI4FniXMqxvCOH7IYTBIYRq/P/mX0MI36QM65piZr3NbJvUz8Bo4C3KsM4hhI+ABWY2PLnpGHz5pfzXtdjZIF3IIjkJz/xqBH5Y7PLkqU53A4uAdfi3jguAvsCTwD+Szzuk7f/DZP3fA04sdvk7WNfD8Gb+m8DrycdJZVzfLwOvJev7FvDj5PayrG9aHY5kUxZf2dYVvy/zRvLxduqaVK51BvYHZib/nh8Cto+irprqSEREYqlUu/hERKTMKUCJiEgsKUCJiEgsKUCJiEgsKUCJiEgsKUCJiEgsKUCJiEgs/X/hc46sX4p1ZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_A = np.concatenate(hist_all_losses_A[0], axis=2)\n",
    "hist_hits_A = np.concatenate(hist_all_hitsss_A[0], axis=2)\n",
    "\n",
    "plotResults(hist_loss_A, hist_hits_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq_UPpKlvnWV"
   },
   "source": [
    "In numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlY2ObLqvnWV",
    "outputId": "6476a3b7-512d-4bc0-9dd2-dc47319cbef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.81% | Gr acc 0.62 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD_rpqtSvnWV"
   },
   "source": [
    "## Baseline B: Keep Training same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqEL860-a9t9"
   },
   "source": [
    "### Experiment Keep Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "TRGPK1CjvnWV"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9zXm7oSvnWW",
    "outputId": "b2d8e43a-6773-4c10-e7d7-15fe7a662fb2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   0 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.621 | Train PPL:   1.861\n",
      "\t Val. Loss: 0.463 |  Val. PPL:   1.588\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.466\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.552\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.530\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.138\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   1 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.659 | Train PPL:   1.934\n",
      "\t Val. Loss: 0.506 |  Val. PPL:   1.659\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.462 | Train PPL:   1.588\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.461 |  Val. PPL:   1.586\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.533\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.558\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.491\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.481\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.176\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.169\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.466\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   2 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.643 | Train PPL:   1.902\n",
      "\t Val. Loss: 0.478 |  Val. PPL:   1.613\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.488 | Train PPL:   1.629\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.487\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.511\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.497\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.478\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.433\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   3 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.635 | Train PPL:   1.888\n",
      "\t Val. Loss: 0.491 |  Val. PPL:   1.634\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.462 | Train PPL:   1.587\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.561\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.475\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.147\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.311\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.162\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   4 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.677 | Train PPL:   1.967\n",
      "\t Val. Loss: 0.488 |  Val. PPL:   1.628\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.588\n",
      "\t Val. Loss: 0.448 |  Val. PPL:   1.565\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.524\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.475\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.427\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.203\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   5 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.626 | Train PPL:   1.869\n",
      "\t Val. Loss: 0.517 |  Val. PPL:   1.676\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.617\n",
      "\t Val. Loss: 0.491 |  Val. PPL:   1.635\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.472\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.509\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.494\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.138\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   6 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.637 | Train PPL:   1.890\n",
      "\t Val. Loss: 0.469 |  Val. PPL:   1.598\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.549\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.435\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.139\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.157\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.171\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.133\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   7 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.666 | Train PPL:   1.946\n",
      "\t Val. Loss: 0.516 |  Val. PPL:   1.675\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.534\n",
      "\t Val. Loss: 0.500 |  Val. PPL:   1.648\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.524\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.518\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.524\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.457\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.368\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.124\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.138\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   8 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.658 | Train PPL:   1.932\n",
      "\t Val. Loss: 0.510 |  Val. PPL:   1.665\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.497 | Train PPL:   1.645\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.529\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.496\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.497\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.472\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.259\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.146\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.215\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.124\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.378\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.162\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   9 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.632 | Train PPL:   1.882\n",
      "\t Val. Loss: 0.509 |  Val. PPL:   1.664\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.509\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.466\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.488\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.390\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.236\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.157\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  10 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.657 | Train PPL:   1.929\n",
      "\t Val. Loss: 0.477 |  Val. PPL:   1.611\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.523\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.534\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.113\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  11 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.695 | Train PPL:   2.003\n",
      "\t Val. Loss: 0.501 |  Val. PPL:   1.650\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.460 | Train PPL:   1.585\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.533\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.511\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.448 |  Val. PPL:   1.565\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.518\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.457\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.294\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.249\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.250\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.139\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  12 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.490 |  Val. PPL:   1.633\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.533\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.521\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.481\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.466\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.157\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  13 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.649 | Train PPL:   1.913\n",
      "\t Val. Loss: 0.510 |  Val. PPL:   1.666\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.550\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.317 |  Val. PPL:   1.374\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.274\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.124\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  14 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.688 | Train PPL:   1.990\n",
      "\t Val. Loss: 0.505 |  Val. PPL:   1.656\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.612\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.471 |  Val. PPL:   1.602\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.572\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.518\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.201\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  15 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 0.495 |  Val. PPL:   1.641\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.487\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.151\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.223\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.146\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  16 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.648 | Train PPL:   1.912\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.592\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.458 | Train PPL:   1.581\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  17 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.673 | Train PPL:   1.961\n",
      "\t Val. Loss: 0.506 |  Val. PPL:   1.658\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.572\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.479\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.334\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.178\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.162\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  18 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.670 | Train PPL:   1.954\n",
      "\t Val. Loss: 0.512 |  Val. PPL:   1.669\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.549\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.571\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.496\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.357\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.171\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.203\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION  19 ------\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.482 |  Val. PPL:   1.619\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.489 |  Val. PPL:   1.631\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train PPL:   1.536\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.572\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.512\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.254\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.195\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.138\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n"
     ]
    }
   ],
   "source": [
    "hist_all_losses_B = np.empty((N_REPETITIONS, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "hist_all_hitsss_B = np.empty((N_REPETITIONS, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "for repetition in range(N_REPETITIONS):\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n------ REPETITION {repetition:3} ------\")\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "        SUFFIX = f\"B{n_task}\"\n",
    "        title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "        LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "        criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "        print(title)\n",
    "        print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "        hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "        hist_all_losses_B[repetition,n_task] = hist_hits_temp\n",
    "        hist_all_hitsss_B[repetition,n_task] = hist_hits_temp\n",
    "        # models_B.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_B[:,:,:,1:], figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotSpecificTask(hist_all_hitsss_B, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "xVWE0hVcvnWW",
    "outputId": "1ec24514-6caa-4966-b8cd-6235d1bcf63d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 10\n",
    "hist_loss_B = np.concatenate(hist_all_losses_B[idx], axis=2)\n",
    "hist_hits_B = np.concatenate(hist_all_hitsss_B[idx], axis=2)\n",
    "\n",
    "plotResults(hist_loss_B, hist_hits_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ9fLNObvnWW"
   },
   "source": [
    "In numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD9r3RykvnWX",
    "outputId": "e5330dc5-23ab-4038-af99-0fb47c16ec07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.94% | Gr acc 0.88 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUleAoTVvnWY"
   },
   "source": [
    "## Baseline C: Freeze Parameters\n",
    "\n",
    "1. Define functions\n",
    "2. Train model, freeze core weights in between tasks\n",
    "3. Look at performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbcfqNFlbGjA"
   },
   "source": [
    "### onTaskUpdate, applyOnParameters, freezeParameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "EiFSSGphvnWY"
   },
   "outputs": [],
   "source": [
    "def applyOnParameters(model, conditions, apply_function):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check every condition\n",
    "        for condition in conditions:\n",
    "            # check every keyword\n",
    "            allincluded = True\n",
    "            for keyword in condition:\n",
    "                if keyword not in name:\n",
    "                    allincluded = False\n",
    "                    break\n",
    "            if allincluded:\n",
    "                apply_function(param)\n",
    "\n",
    "def freezeParameters(model, conditions):\n",
    "    def freeze(param):\n",
    "        param.requires_grad = False\n",
    "    applyOnParameters(model, conditions, freeze)\n",
    "\n",
    "def unfreezeParameters(model, conditions):\n",
    "    def unfreeze(param):\n",
    "        param.requires_grad = True\n",
    "    applyOnParameters(model, conditions, unfreeze)\n",
    "\n",
    "def showModelParameters(model, requires_grad=False):\n",
    "    for name, param in model.named_parameters():\n",
    "        if requires_grad:\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "        else:\n",
    "            print(name)\n",
    "            \n",
    "def onTaskUpdate(model):\n",
    "    # Freeze core weights\n",
    "    freezeParameters(model, ((\"\"),))    # Freeze everything\n",
    "    unfreezeParameters(model, ((\"encoder\",\"embedding\"), (\"decoder\",\"fc_out\"), (\"attention\",))) # Unfreeze relevant stuff\n",
    "    \n",
    "    # Reinitialize\n",
    "    to_constant = lambda param: nn.init.constant_(param.data, 0)\n",
    "    applyOnParameters(model, ((\"decoder\",\"fc_out\",\"bias\"),(\"attn\",\"bias\")), to_constant)\n",
    "    to_normal = lambda param: nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "    applyOnParameters(model, ((\"encoder\",\"embedding\"),(\"decoder\",\"fc_out\",\"weight\"),(\"attention\",\"weight\")), to_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJlVK1CCvnWZ"
   },
   "source": [
    "### Experiment Freeze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "9_UJzijZvnWZ"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H83P-2BXvnWZ",
    "outputId": "8c4cbe78-90a0-46bc-cc0f-8a9328d47640",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-C0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.184\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-C1\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.147\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "tr-AE-30-10-0.01-C2\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.381\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.342\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = N_REPETITIONS\n",
    "hist_all_losses_C = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "hist_all_hitsss_C = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "for repetition in range(n_repetitions):\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n------ REPETITION {repetition:3} ------\")\n",
    "    # To have single copy\n",
    "    if repetition == n_repetitions - 1:\n",
    "        models_F = []\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "        SUFFIX = f\"C{n_task}\"\n",
    "        title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "        LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "        criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "        print(title)\n",
    "        print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "        hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "        hist_all_losses_C[repetition,n_task] = hist_hits_temp\n",
    "        hist_all_hitsss_C[repetition,n_task] = hist_hits_temp\n",
    "        if repetition == n_repetitions - 1:\n",
    "            models_C.append(copy.deepcopy(model))\n",
    "\n",
    "        # Freeze, reinitialize\n",
    "        onTaskUpdate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "x_07dWu8vnWa",
    "outputId": "22e6837e-a1ee-44e7-cc6c-1ccfd78122e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_C = torch.cat(hist_losses_C, dim=2)\n",
    "hist_hits_C = torch.cat(hist_hitsss_C, dim=2)\n",
    "\n",
    "plotResults(hist_loss_C, hist_hits_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6K7g1tH3vnWa",
    "outputId": "25d75c5a-f339-4071-8aad-0f4df6f05ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkGxECJqvnWb"
   },
   "source": [
    "## Transfer D: EWC\n",
    "\n",
    "1. Create Fisher functions\n",
    "2. Train model on tasks\n",
    "3. Compare results\n",
    "\n",
    "Based on: https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNS4W04AvnWc"
   },
   "source": [
    "Compute optimal parameters and fisher information after training on tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI4QzSeavnWc"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_ewc(model, task_id, train_dl, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #accumulate Gradient\n",
    "    for it in range(100):\n",
    "        for seq, seq_len in train_dl:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0)\n",
    "\n",
    "            if criterion == F.cross_entropy:\n",
    "              output_dim = output.shape[-1]\n",
    "                \n",
    "              output = output[1:].view(-1, output_dim)\n",
    "\n",
    "              trg = seq[1:].view(-1)\n",
    "\n",
    "              loss = criterion(output, trg)\n",
    "            else:\n",
    "              loss = criterion(output, seq)\n",
    "            #print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "        \n",
    "    fishers.append({})\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        fishers[task_id][name] = param.grad.data.clone().pow(2)\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJu-KM7mvnWc"
   },
   "source": [
    "Adapt evaluation and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRW-TlhwvnWd"
   },
   "outputs": [],
   "source": [
    "def train_ewc(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(\"-\\n\", loss)\n",
    "        \n",
    "        # EWC Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fishers[other_task_id][name]\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                #print(ewc_lambda)\n",
    "                loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                #print((fisher * (optParam - param).pow(2)).sum())\n",
    "                #print((optParam - param).pow(2).sum())\n",
    "                #loss += ewc_lambda * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnQbtl1yvnWe"
   },
   "outputs": [],
   "source": [
    "def evaluate_ewc(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # EWC Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    fisher = fishers[other_task_id][name]\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgGMwiHcvnWe"
   },
   "outputs": [],
   "source": [
    "def fit_ewc(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NI--0ZFvnWe"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bB0VOa5lvnWf",
    "outputId": "cfd946be-3342-47d9-a885-90fb65f024f9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1112, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1422, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1009, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1829, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2079, grad_fn=<AddBackward0>)\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0826, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1142, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1145, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1264, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1391, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1517, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1385, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1511, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1339, grad_fn=<AddBackward0>)\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.2965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1228, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1162, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1303, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1152, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1298, grad_fn=<AddBackward0>)\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1306, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2151, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2300, grad_fn=<AddBackward0>)\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1146, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2175, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1107, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1256, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1189, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3172, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3321, grad_fn=<AddBackward0>)\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1111, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2133, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1113, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1131, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1082, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1157, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1317, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1101, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1240, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3278, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3444, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1205, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1088, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1260, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1217, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1379, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1557, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1368, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1546, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1193, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1245, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2326, grad_fn=<AddBackward0>)\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2703, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1196, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1361, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1592, grad_fn=<AddBackward0>)\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1140, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0984, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1472, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1120, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1283, grad_fn=<AddBackward0>)\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.1032, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1286, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1199, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1310, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1318, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1477, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1253, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1187, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1345, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1036, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2772, grad_fn=<AddBackward0>)\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1105, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1284, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1065, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1314, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1461, grad_fn=<AddBackward0>)\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1154, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1300, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1524, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1669, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1488, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1632, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1057, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1153, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1014, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1235, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1148, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1210, grad_fn=<AddBackward0>)\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1168, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1286, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0715, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0826, grad_fn=<AddBackward0>)\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1155, grad_fn=<AddBackward0>)\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0963, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1083, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0856, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0906, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1019, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1067, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1066, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2002, grad_fn=<AddBackward0>)\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1006, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0858, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3459, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1389, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1514, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0938, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1060, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1248, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1234, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1090, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1089, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0802, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1117, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1122, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1255, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1478, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1079, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1024, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1091, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1215, grad_fn=<AddBackward0>)\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1179, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1299, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0830, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1879, grad_fn=<AddBackward0>)\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1551, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1241, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1221, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1364, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1007, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1323, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1464, grad_fn=<AddBackward0>)\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1513, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1658, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1164, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1316, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1349, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1100, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1367, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1294, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1449, grad_fn=<AddBackward0>)\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1716, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1463, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1618, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "-\n",
      " tensor(0.2395, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1288, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1223, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1237, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1160, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1188, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1297, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1470, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1482, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1127, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0959, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1072, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1251, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1132, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1046, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1088, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1149, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1261, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1061, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0957, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1121, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0836, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0938, grad_fn=<AddBackward0>)\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1142, grad_fn=<AddBackward0>)\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0807, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1039, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0882, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0969, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0847, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0824, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0949, grad_fn=<AddBackward0>)\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0985, grad_fn=<AddBackward0>)\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0840, grad_fn=<AddBackward0>)\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0746, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0854, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0820, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0842, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1405, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1522, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1205, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1337, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1113, grad_fn=<AddBackward0>)\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1092, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1181, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1323, grad_fn=<AddBackward0>)\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1124, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1124, grad_fn=<AddBackward0>)\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0884, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0835, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, grad_fn=<AddBackward0>)\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0696, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0815, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0827, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0803, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0813, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0823, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0859, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0822, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0791, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0684, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0719, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0828, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1052, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0924, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0686, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0796, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0757, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0869, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1232, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0809, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1195, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1035, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1079, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1163, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1330, grad_fn=<AddBackward0>)\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1068, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2473, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0940, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2180, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2321, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.4974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1097, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2505, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0849, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1010, grad_fn=<AddBackward0>)\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1374, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1534, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1225, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1385, grad_fn=<AddBackward0>)\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0946, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0953, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1108, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1226, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1370, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0718, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1172, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0974, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2361, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0901, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2765, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.2020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1102, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1247, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1094, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1433, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1483, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1649, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1233, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1053, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1943, grad_fn=<AddBackward0>)\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1054, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1318, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2513, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1615, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1928, grad_fn=<AddBackward0>)\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0927, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1081, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2137, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2329, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1217, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1460, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1192, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1374, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1052, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1175, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3141, grad_fn=<AddBackward0>)\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1038, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1224, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1397, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1585, grad_fn=<AddBackward0>)\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1140, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1293, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1543, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1277, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1116, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2191, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2381, grad_fn=<AddBackward0>)\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2116, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2304, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1173, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1928, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2113, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0988, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1373, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1409, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1601, grad_fn=<AddBackward0>)\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1248, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1656, grad_fn=<AddBackward0>)\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1214, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2758, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1488, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1498, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1203, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1418, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1746, grad_fn=<AddBackward0>)\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1233, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1077, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1786, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1201, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1202, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1416, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2362, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0913, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1462, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3139, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2391, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1224, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1436, grad_fn=<AddBackward0>)\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1399, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1611, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1424, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1583, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1018, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1396, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1359, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1581, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1104, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1143, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1365, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1213, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1149, grad_fn=<AddBackward0>)\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1086, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1279, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1427, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2489, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1143, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1252, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1080, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0740, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0857, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1215, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, grad_fn=<AddBackward0>)\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1358, grad_fn=<AddBackward0>)\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1579, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1068, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1256, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1392, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.1264, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1181, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1087, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0824, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1005, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1176, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1322, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1493, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0904, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.1577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1725, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1030, grad_fn=<AddBackward0>)\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n"
     ]
    }
   ],
   "source": [
    "fishers = []\n",
    "optParams = []\n",
    "ewc_lambda = 10\n",
    "\n",
    "models_D = []\n",
    "hist_losses_D = []\n",
    "hist_hitsss_D = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_ewc(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D.append(hist_loss_temp)\n",
    "    hist_hitsss_D.append(hist_hits_temp)\n",
    "    models_D.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_ewc(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYEBTL-bo_Tr"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QY_YzfCoVkI"
   },
   "source": [
    "L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TVqno83vnWh"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0k0Kp66cvnWh"
   },
   "outputs": [],
   "source": [
    "accuracyAll(models_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17U8oYfQTlma"
   },
   "outputs": [],
   "source": [
    "torch.max(fishers[0]['encoder.embedding.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DnwXlOTXsbM"
   },
   "source": [
    "## Transfer D2: L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iatypwWYytb"
   },
   "source": [
    "### onTaskUpdate_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wzf8QezfX4rt"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_l2reg(model, task_id, train_dl, criterion):\n",
    "    # Save optimal parameters for each task\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-GTTipSYufm"
   },
   "source": [
    "### train_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfX9ClLaYNpv"
   },
   "outputs": [],
   "source": [
    "def train_l2reg(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        # L2 Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGgyR2yZYDfS"
   },
   "source": [
    "### eval_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RsqsxlvY86I"
   },
   "outputs": [],
   "source": [
    "def evaluate_l2reg(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # L2 Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfwySP1EZNc_"
   },
   "source": [
    "### fit_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIQ9dV7WZPX6"
   },
   "outputs": [],
   "source": [
    "def fit_l2reg(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_l2reg(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_l2reg(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_l2reg(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_l2reg(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_l2reg(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWITsrZDZz1z"
   },
   "source": [
    "### Experiment L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bki7JTypZT9E"
   },
   "outputs": [],
   "source": [
    "LAMBDA_L2REG = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08h_J3oUZ5-x"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7MObEkiZ6pa",
    "outputId": "ab9a9f0e-bb75-472d-ea4b-7dbb17755167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.796\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.571\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.533\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.266\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.596 | Train PPL:   1.814\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.613\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.581\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.612\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.583\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.501 |  Val. PPL:   1.651\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.478 |  Val. PPL:   1.613\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.492 |  Val. PPL:   1.635\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.579\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.463 |  Val. PPL:   1.589\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.521 |  Val. PPL:   1.683\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.467 |  Val. PPL:   1.595\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.530\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.483 |  Val. PPL:   1.621\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.596\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.479 |  Val. PPL:   1.614\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.509 |  Val. PPL:   1.663\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.476 |  Val. PPL:   1.610\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.497 |  Val. PPL:   1.644\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.568\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.561\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.588\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.561\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.563\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.538\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.499 |  Val. PPL:   1.647\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.508\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.486 |  Val. PPL:   1.626\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.604\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.505 |  Val. PPL:   1.657\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.550\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.547\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 0.441 |  Val. PPL:   1.554\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.524\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train PPL:   1.536\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.479\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.482 |  Val. PPL:   1.619\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
      "\t Val. Loss: 0.491 |  Val. PPL:   1.635\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.469 |  Val. PPL:   1.599\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.524\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.566\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.489 |  Val. PPL:   1.631\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.575\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.603\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.574\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.582\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.543\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.493 |  Val. PPL:   1.637\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.571\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.541\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.579\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.549\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.528 |  Val. PPL:   1.696\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.580\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.529\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.421\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.529\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.418\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.430\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n"
     ]
    }
   ],
   "source": [
    "optParams = []\n",
    "\n",
    "models_D2 = []\n",
    "hist_losses_D2 = []\n",
    "hist_hitsss_D2 = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D2.{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_l2reg(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D2.append(hist_loss_temp)\n",
    "    hist_hitsss_D2.append(hist_hits_temp)\n",
    "    models_D2.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_l2reg(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "ITGjKqf6c-Bt",
    "outputId": "d49eb2d3-0c6b-4b66-fc1a-744da9729248"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_D2 = torch.cat(hist_losses_D2, dim=2)\n",
    "hist_hits_D2 = torch.cat(hist_hitsss_D2, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D2, hist_hits_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEnIhlRbdPUs",
    "outputId": "36ab6809-9d23-4399-c9b8-c03a0bb6a3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.56% | Gr acc 0.12 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwQE8R-QvnWj"
   },
   "source": [
    "## Transfer E: DynaMoE\n",
    "\n",
    "1. Create DynaMoe network functions:\n",
    "2. Decider Network\n",
    "3. Run experiment\n",
    "\n",
    "Todos:\n",
    "- Training by computing loss of every expert is cheaty?\n",
    "- Training by feeding all new inputs to new expert is cheaty?\n",
    "- Consolidating Network on task change is cheaty!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiVkxi06Nu8y"
   },
   "source": [
    "### Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "JFB0iAdSvnWk"
   },
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pL7WzzmNqBp"
   },
   "source": [
    "### DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "Ysf64peLvnWj"
   },
   "outputs": [],
   "source": [
    "class DynaMoE(nn.Module):\n",
    "    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(DynaMoE, self).__init__()\n",
    "\n",
    "        assert len(experts) == len(expert_optimizers)\n",
    "        \n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        self.n_active_experts = 1\n",
    "        # set mask\n",
    "        self.recompute_mask()\n",
    "    \n",
    "    def recompute_mask(self):\n",
    "        gating_mask = torch.zeros(self.gating.n_max_experts).to(device)\n",
    "\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            gating_mask[e_id] = 1\n",
    "\n",
    "        self.gating_mask = gating_mask\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        masked_gatings = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        # @TODO: Probabilistic vs argmax?\n",
    "        network_ids = torch.argmax(masked_gatings, dim=1)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,\n",
    "                                                     teacher_forcing_ratio))\n",
    "\n",
    "        outputs = torch.empty((seq_len, batch_size, vocab_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            network_id = network_ids[b]\n",
    "            outputs[:,b] = expert_outputs[network_id][:,b]\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1\n",
    "        # Recompute mask\n",
    "        self.recompute_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITHSpNoENze8"
   },
   "source": [
    "### compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "9Opa6EZlw8g-"
   },
   "outputs": [],
   "source": [
    "def compute_loss(outputs, targets, criterion, cutFirstInSequence=True):\n",
    "    if isinstance(criterion, CosineLoss):\n",
    "        return criterion(outputs, targets)\n",
    "    else:\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        \n",
    "        if cutFirstInSequence:\n",
    "            outputs = outputs[1:].view(-1, outputs_dim)\n",
    "            #outputs = [batch size, output dim]\n",
    "            targets = targets[1:].view(-1)\n",
    "            #targets = [batch size]\n",
    "            # print(\"hi\")\n",
    "        else:\n",
    "            outputs = outputs.view(-1, outputs_dim)\n",
    "            targets = targets.view(-1)\n",
    "        \n",
    "        # print(\"######\")\n",
    "        # print(outputs)\n",
    "        # print(targets)\n",
    "        # print(\"######\")\n",
    "        \n",
    "        return criterion(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K75xVqKJN2Km"
   },
   "source": [
    "### train_dynamoe_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "icuVPvGqYI75"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_gating(model, iterator, gating_criterion,\n",
    "                         expert_criterion, clip, verbose=False):\n",
    "    \n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                     expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                    expert_criterion,\n",
    "                                    cutFirstInSequence=True)\n",
    "            \n",
    "            # Log loss to train gating\n",
    "            loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Indices of correct experts to have chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Target\")\n",
    "            print(gating_trgs)\n",
    "            print(\"Output\")\n",
    "            print(gating_outputs.argmax(dim=1))\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs,\n",
    "                                   gating_criterion,\n",
    "                                   cutFirstInSequence=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "        \n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbbZ_pYyN48v"
   },
   "source": [
    "### train_dynamoe_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "MaaIrZLCE1JG"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_both(model, iterator, gating_criterion,\n",
    "                       expert_criterion, clip):\n",
    "\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        # loss_experts = [batch_size, n_active_experts]\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            train_model = e_id == model.n_active_experts - 1\n",
    "\n",
    "            if train_model:\n",
    "                model.experts[e_id].train()\n",
    "                model.expert_optimizers[e_id].zero_grad()\n",
    "                \n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                     expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                    expert_criterion,\n",
    "                                    cutFirstInSequence=True)\n",
    "\n",
    "                # Log loss to train gating\n",
    "                loss_experts[:,e_id] = loss\n",
    "                \n",
    "                # Train newly initialized model on new train examples\n",
    "                reduced_loss = loss.mean()\n",
    "                reduced_loss.backward()\n",
    "                model.expert_optimizers[e_id].step()\n",
    "                \n",
    "            else:\n",
    "                model.experts[e_id].eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Get model prediction\n",
    "                    expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                         expert_trgs)\n",
    "\n",
    "                    loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                        expert_criterion,\n",
    "                                        cutFirstInSequence=True)\n",
    "\n",
    "                    # Log loss to train gating\n",
    "                    loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Compute expert which should have been chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQrjb4QEODx2"
   },
   "source": [
    "### fit_dynamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "tuFUOzDwjZgI"
   },
   "outputs": [],
   "source": [
    "def fit_dynamoe(model, task_id, epochs, step_size_evaluation, clip,\n",
    "                case = \"train_gating_initialized_expert\" ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    case : string\n",
    "        \"train_gating_uninitialized_expert\" | \"train_gating_train_expert\" | \n",
    "        \"train_gating_initialized_expert\"\n",
    "    \"\"\"\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    loss_tracker = torch.zeros((epochs,))\n",
    "\n",
    "    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Look ahead whether to \"consolidate\" expert\n",
    "        \n",
    "        \n",
    "        # Train model depending on case\n",
    "        if case == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id],\n",
    "                                                 allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    # Switch to train the expert and gating\n",
    "\n",
    "                    case = \"train_gating_train_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if case == \"train_gating_train_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_both(model, train_dls[task_id],\n",
    "                                            gating_criterion,\n",
    "                                            expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "        if case == \"train_gating_uninitialized_expert\":\n",
    "            assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    case = \"train_gating_initialized_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate(model,\n",
    "                                                      train_dls[other_id],\n",
    "                                                      expert_criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate(model,\n",
    "                                                      test_dls[other_id],\n",
    "                                                      expert_criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate(model,\n",
    "                                                      test_ugr_dls[other_id],\n",
    "                                                      expert_criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model,\n",
    "                                                            train_dls[other_id],\n",
    "                                                            allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model,\n",
    "                                                            test_dls[other_id],\n",
    "                                                            allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model,\n",
    "                                                            test_ugr_dls[other_id],\n",
    "                                                            allOrNoneLoss)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCiyq9ODN9fG"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "Y4hPup6qpRWp"
   },
   "outputs": [],
   "source": [
    "def init_expert():\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    new_expert = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "    new_expert.apply(init_weights)\n",
    "    expert_optimizer = optim.Adam(new_expert.parameters(), lr=LEARNING_RATE)\n",
    "    return new_expert, expert_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "-gPFPMm_JiO4"
   },
   "outputs": [],
   "source": [
    "def init_gating():\n",
    "    gating = Gating(INPUT_DIM, N_GATING_EMBED_DIM, N_GATING_HIDDEN_DIM,\n",
    "                    N_EXPERTS_START, N_MAX_EXPERTS, GATE_DROPOUT)\n",
    "    gating.to(device)\n",
    "    gating_optimizer = optim.Adam(gating.parameters(), lr=LEARNING_RATE)\n",
    "    return gating, gating_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouWPz-2MdMs1"
   },
   "source": [
    "### show expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "jFW0tm8tdTAl"
   },
   "outputs": [],
   "source": [
    "def show_expert(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            gating_choices = gating_masked.argmax(dim=1)\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_choices[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlpvLkEVfZhR"
   },
   "source": [
    "### Experiment DynaMoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnSZq8sAsGoj"
   },
   "source": [
    "Questions open from the DynaMoE code:\n",
    "- When does the model exactly decide to \"freeze\" the expert\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "iu9jVht1wbg7"
   },
   "outputs": [],
   "source": [
    "SAVE = N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "N_EPOCHS = SAVE + 200\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   0 ------\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-E0\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.036 | Train PPL:   2.819\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.991 | Train PPL:   2.693\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.983 | Train PPL:   2.673\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 1.001 | Train PPL:   2.720\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.973 | Train PPL:   2.646\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 1.032 | Train PPL:   2.807\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 1.058 | Train PPL:   2.882\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 1.046 | Train PPL:   2.847\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 1.037 | Train PPL:   2.822\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.991 | Train PPL:   2.694\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 1.049 | Train PPL:   2.855\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 1.028 | Train PPL:   2.795\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 1.055 | Train PPL:   2.873\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.953 | Train PPL:   2.593\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 1.043 | Train PPL:   2.838\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.868\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 1.030 | Train PPL:   2.801\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 1.034 | Train PPL:   2.811\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 1.093 | Train PPL:   2.982\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.769\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 1.089 | Train PPL:   2.972\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 1.040 | Train PPL:   2.828\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.981 | Train PPL:   2.667\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 1.090 | Train PPL:   2.973\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.879\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 1.108 | Train PPL:   3.028\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 1.055 | Train PPL:   2.873\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 1.035 | Train PPL:   2.816\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.975 | Train PPL:   2.652\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 1.041 | Train PPL:   2.832\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 1.043 | Train PPL:   2.837\n",
      "\t Val. Loss: 1.146 |  Val. PPL:   3.146\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.667 | Train PPL:   1.948\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.786\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.484 |  Val. PPL:   1.622\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.571\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.488\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.511\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.178\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.131\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "tr-AE-30-10-0.01-E1\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.722\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.579 | Train PPL:   1.784\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.681\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.667\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.787\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.568 | Train PPL:   1.764\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.789\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.729\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.732\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.534 | Train PPL:   1.706\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.599 | Train PPL:   1.820\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.722\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.732\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.794\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.727\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.618 | Train PPL:   1.856\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.570 | Train PPL:   1.768\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.798\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.557 | Train PPL:   1.746\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.721\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.671 | Train PPL:   1.956\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.614 | Train PPL:   1.849\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.689\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.559 | Train PPL:   1.749\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.579 | Train PPL:   1.784\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.754\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.702\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.772\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.778\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.534 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.599 | Train PPL:   1.821\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.702\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.537 |  Val. PPL:   1.711\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.700\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.500\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.457\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.466\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.423\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.365\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.359\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.176\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-E2\n",
      "The model has 13219 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.368\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   1 ------\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-E0\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.085 | Train PPL:   2.959\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 1.045 | Train PPL:   2.843\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 1.082 | Train PPL:   2.950\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 1.140 | Train PPL:   3.126\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 1.040 | Train PPL:   2.828\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 1.049 | Train PPL:   2.855\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 1.074 | Train PPL:   2.927\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 1.091 | Train PPL:   2.978\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.919\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 1.124 | Train PPL:   3.077\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 1.064 | Train PPL:   2.899\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 1.093 | Train PPL:   2.982\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 1.041 | Train PPL:   2.833\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 1.148 | Train PPL:   3.152\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 1.056 | Train PPL:   2.876\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.915\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 1.092 | Train PPL:   2.980\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 1.124 | Train PPL:   3.077\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.002\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.000\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 1.018 | Train PPL:   2.768\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 1.076 | Train PPL:   2.933\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 1.129 | Train PPL:   3.093\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 1.092 | Train PPL:   2.980\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 1.029 | Train PPL:   2.799\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 1.037 | Train PPL:   2.822\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 1.096 | Train PPL:   2.993\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.851\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 1.063 | Train PPL:   2.896\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 1.118 | Train PPL:   3.060\n",
      "\t Val. Loss: 1.063 |  Val. PPL:   2.895\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.696 | Train PPL:   2.006\n",
      "\t Val. Loss: 0.570 |  Val. PPL:   1.768\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.491\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.448 |  Val. PPL:   1.566\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.245\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.220\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.138\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-E1\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.696\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.721\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.657 | Train PPL:   1.929\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.790\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.607 | Train PPL:   1.835\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.611 | Train PPL:   1.843\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.617 | Train PPL:   1.854\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.786\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.624 | Train PPL:   1.867\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.638 | Train PPL:   1.893\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.584 | Train PPL:   1.793\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.696 | Train PPL:   2.006\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.797\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.796\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.729\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.648 | Train PPL:   1.912\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.647 | Train PPL:   1.909\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.625 | Train PPL:   1.867\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.681\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.583 | Train PPL:   1.792\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.618 | Train PPL:   1.856\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.771\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.692 | Train PPL:   1.998\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.629 | Train PPL:   1.875\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.559 | Train PPL:   1.750\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.653\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.623 | Train PPL:   1.865\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.790\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.790\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.632\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.789\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.757\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.648 | Train PPL:   1.912\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.635 | Train PPL:   1.888\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.663 | Train PPL:   1.941\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.636 | Train PPL:   1.890\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.631 | Train PPL:   1.880\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.631 | Train PPL:   1.879\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.610 | Train PPL:   1.840\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.568 | Train PPL:   1.765\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.495 | Train PPL:   1.641\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.577 | Train PPL:   1.781\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.734\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.594 | Train PPL:   1.812\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.568 | Train PPL:   1.764\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.594 | Train PPL:   1.811\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.663 | Train PPL:   1.942\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.595 | Train PPL:   1.813\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.636 | Train PPL:   1.889\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.647 | Train PPL:   1.910\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.622 | Train PPL:   1.864\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.622 | Train PPL:   1.863\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.874\n",
      "\t Val. Loss: 0.512 |  Val. PPL:   1.669\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.620\n",
      "\t Val. Loss: 0.448 |  Val. PPL:   1.566\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.567\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.481\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.482\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.457\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.427\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.178\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.242\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.191\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-E2\n",
      "The model has 13219 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = N_REPETITIONS\n",
    "hist_all_losses_E = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "hist_all_hitsss_E = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "for repetition in range(n_repetitions):\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n------ REPETITION {repetition:3} ------\")\n",
    "    expert, expert_optimizer = init_expert()\n",
    "    gating, gating_optimizer = init_gating()\n",
    "    model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    # gating_criterion = CosineLoss(N_MAX_EXPERTS, ignore_index=None)\n",
    "    # Cosine loss is inpractical for Gating because result vectors are low dimensional\n",
    "    gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "    expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                            reduction=\"none\")\n",
    "\n",
    "    for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "        SUFFIX = f\"E{n_task}\"\n",
    "        title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "        LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "        print(title)\n",
    "        print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "        if n_task == 0:\n",
    "            case = \"train_gating_initialized_expert\"\n",
    "        else:\n",
    "            case = \"train_gating_uninitialized_expert\"\n",
    "        hist_loss_temp, hist_hits_temp = fit_dynamoe(model, n_task, N_EPOCHS,\n",
    "                                                     STEP_SIZE_EVALUATION, CLIP,\n",
    "                                                     case)\n",
    "        hist_all_losses_E[repetition,n_task] = hist_hits_temp\n",
    "        hist_all_hitsss_E[repetition,n_task] = hist_hits_temp\n",
    "        # models_E.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_E[:,:,:,1:], figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "bIK6DRy6oZOF",
    "outputId": "9232242e-2678-4b38-c20e-2641256d1b4f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_losses_E' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-ad834e68d5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist_loss_E\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_losses_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist_hits_E\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_hitsss_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplotResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_loss_E\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_hits_E\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist_losses_E' is not defined"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "hist_loss_E = torch.cat(hist_losses_E[idx], dim=2)\n",
    "hist_hits_E = torch.cat(hist_hitsss_E[idx], dim=2)\n",
    "\n",
    "plotResults(hist_loss_E, hist_hits_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVUf91lhquoH",
    "outputId": "e3521937-8666-4089-8de0-8f067d906fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xROT6lswtwba",
    "outputId": "4e09b763-60b2-4d1a-8dd3-bbfcbfff2973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_E[2].n_active_experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5kSqVvW6e7M"
   },
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_trgts = []\n",
    "gating_trgts.append( [torch.tensor([1,0,0]) for _ in range(len(train_dls[0]))])\n",
    "gating_trgts.append( [torch.tensor([0,1,0]) for _ in range(len(train_dls[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dls[1]:\n",
    "    if x[1] == 7:\n",
    "        print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0990233421325684\n",
      "loss\n",
      "1.0725314617156982\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.06223726272583\n",
      "loss\n",
      "1.1580368280410767\n",
      "loss\n",
      "0.994653046131134\n",
      "loss\n",
      "1.0136317014694214\n",
      "loss\n",
      "1.1363170146942139\n",
      "loss\n",
      "1.0377978086471558\n",
      "loss\n",
      "1.273862361907959\n",
      "loss\n",
      "1.1935420036315918\n",
      "loss\n",
      "1.2392826080322266\n",
      "loss\n",
      "0.843536376953125\n",
      "loss\n",
      "1.0642420053482056\n",
      "loss\n",
      "0.8814082145690918\n",
      "loss\n",
      "1.1417649984359741\n",
      "loss\n",
      "1.2824432849884033\n",
      "loss\n",
      "0.8938198089599609\n",
      "loss\n",
      "0.9051280617713928\n",
      "loss\n",
      "0.9723299145698547\n",
      "loss\n",
      "1.2795649766921997\n",
      "loss\n",
      "1.1691962480545044\n",
      "loss\n",
      "1.2110319137573242\n",
      "loss\n",
      "0.7188755869865417\n",
      "loss\n",
      "1.3590319156646729\n",
      "loss\n",
      "0.9704457521438599\n",
      "loss\n",
      "0.9680742025375366\n",
      "loss\n",
      "1.385023593902588\n",
      "loss\n",
      "0.7331454157829285\n",
      "loss\n",
      "0.7141051888465881\n",
      "loss\n",
      "1.245851993560791\n",
      "loss\n",
      "0.7735995054244995\n",
      "loss\n",
      "1.3736876249313354\n",
      "loss\n",
      "0.8231424689292908\n",
      "loss\n",
      "1.3750696182250977\n",
      "loss\n",
      "1.51996910572052\n",
      "loss\n",
      "1.2661633491516113\n",
      "loss\n",
      "1.3473231792449951\n",
      "loss\n",
      "1.233702301979065\n",
      "loss\n",
      "0.7255655527114868\n",
      "loss\n",
      "0.6757468581199646\n",
      "loss\n",
      "0.6531469225883484\n",
      "loss\n",
      "0.745249330997467\n",
      "loss\n",
      "0.7534412145614624\n",
      "loss\n",
      "1.4418754577636719\n",
      "loss\n",
      "0.7937403321266174\n",
      "loss\n",
      "0.6716989278793335\n",
      "loss\n",
      "1.2280333042144775\n",
      "loss\n",
      "0.6496949195861816\n",
      "loss\n",
      "1.5460278987884521\n",
      "loss\n",
      "1.5514756441116333\n",
      "loss\n",
      "1.1636765003204346\n",
      "loss\n",
      "1.3178153038024902\n",
      "loss\n",
      "0.5455304384231567\n",
      "loss\n",
      "0.5333239436149597\n",
      "loss\n",
      "1.4688210487365723\n",
      "loss\n",
      "1.375108003616333\n",
      "loss\n",
      "0.5335070490837097\n",
      "loss\n",
      "1.430232048034668\n",
      "loss\n",
      "1.1741782426834106\n",
      "loss\n",
      "1.2446054220199585\n",
      "loss\n",
      "0.6195405721664429\n",
      "loss\n",
      "0.49828198552131653\n",
      "loss\n",
      "0.7075855135917664\n",
      "loss\n",
      "0.6252626776695251\n",
      "loss\n",
      "1.3688387870788574\n",
      "loss\n",
      "1.3391385078430176\n",
      "loss\n",
      "1.1697885990142822\n",
      "loss\n",
      "0.37812450528144836\n",
      "loss\n",
      "1.1592947244644165\n",
      "loss\n",
      "0.6913880109786987\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.6907056570053101\n",
      "loss\n",
      "0.4162207841873169\n",
      "loss\n",
      "0.5808714032173157\n",
      "loss\n",
      "1.1500698328018188\n",
      "loss\n",
      "0.352736234664917\n",
      "loss\n",
      "0.39313051104545593\n",
      "loss\n",
      "0.41192883253097534\n",
      "loss\n",
      "0.5116475820541382\n",
      "loss\n",
      "1.1172572374343872\n",
      "loss\n",
      "1.4131906032562256\n",
      "loss\n",
      "0.33028650283813477\n",
      "loss\n",
      "1.489363193511963\n",
      "loss\n",
      "1.2112787961959839\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.4293568730354309\n",
      "loss\n",
      "0.4328417181968689\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.47999051213264465\n",
      "loss\n",
      "0.19354216754436493\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.1361033916473389\n",
      "loss\n",
      "0.1798943281173706\n",
      "loss\n",
      "0.12139580398797989\n",
      "loss\n",
      "0.21990478038787842\n",
      "loss\n",
      "0.2244793325662613\n",
      "loss\n",
      "0.1716667264699936\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.5843265056610107\n",
      "loss\n",
      "0.11034240573644638\n",
      "loss\n",
      "0.3461153209209442\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.14620114862918854\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.13816894590854645\n",
      "loss\n",
      "0.15988250076770782\n",
      "loss\n",
      "0.2077673375606537\n",
      "loss\n",
      "0.13088884949684143\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.3269919157028198\n",
      "loss\n",
      "0.12710054218769073\n",
      "loss\n",
      "1.4896562099456787\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.10144913196563721\n",
      "loss\n",
      "0.07071235775947571\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.40675270557403564\n",
      "loss\n",
      "0.06046057492494583\n",
      "loss\n",
      "0.04141584411263466\n",
      "loss\n",
      "1.0897676944732666\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.07548543810844421\n",
      "loss\n",
      "1.0544909238815308\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.059570763260126114\n",
      "loss\n",
      "0.06236014887690544\n",
      "loss\n",
      "0.04088432341814041\n",
      "loss\n",
      "0.6670161485671997\n",
      "loss\n",
      "0.09925808757543564\n",
      "loss\n",
      "0.06430821120738983\n",
      "loss\n",
      "0.5717656016349792\n",
      "loss\n",
      "0.5106241106987\n",
      "loss\n",
      "0.6109991073608398\n",
      "loss\n",
      "0.4585453271865845\n",
      "loss\n",
      "0.40513986349105835\n",
      "loss\n",
      "0.028454262763261795\n",
      "loss\n",
      "0.10292236506938934\n",
      "loss\n",
      "0.14290152490139008\n",
      "loss\n",
      "0.08874966204166412\n",
      "loss\n",
      "0.1695457398891449\n",
      "loss\n",
      "0.06828229129314423\n",
      "loss\n",
      "0.03192349523305893\n",
      "loss\n",
      "0.10639900714159012\n",
      "loss\n",
      "0.13714158535003662\n",
      "loss\n",
      "0.04405619949102402\n",
      "loss\n",
      "0.020778479054570198\n",
      "loss\n",
      "0.04888411983847618\n",
      "loss\n",
      "0.02315611019730568\n",
      "loss\n",
      "0.03884715959429741\n",
      "loss\n",
      "0.0185548085719347\n",
      "loss\n",
      "0.026514191180467606\n",
      "loss\n",
      "0.015334682539105415\n",
      "loss\n",
      "0.16082657873630524\n",
      "loss\n",
      "0.16628775000572205\n",
      "loss\n",
      "0.03266807645559311\n",
      "loss\n",
      "0.024600008502602577\n",
      "loss\n",
      "0.030816741287708282\n",
      "loss\n",
      "0.021857907995581627\n",
      "loss\n",
      "0.009852115996181965\n",
      "loss\n",
      "0.0058709559962153435\n",
      "loss\n",
      "0.017790623009204865\n",
      "loss\n",
      "0.015698540955781937\n",
      "loss\n",
      "0.08955177664756775\n",
      "loss\n",
      "0.020790155977010727\n",
      "loss\n",
      "0.037340905517339706\n",
      "loss\n",
      "0.030650736764073372\n",
      "loss\n",
      "0.024169908836483955\n",
      "loss\n",
      "0.011669941246509552\n",
      "loss\n",
      "0.8218876719474792\n",
      "loss\n",
      "0.0061847250908613205\n",
      "loss\n",
      "0.04579256847500801\n",
      "loss\n",
      "0.014900343492627144\n",
      "loss\n",
      "0.021984562277793884\n",
      "loss\n",
      "0.008054869249463081\n",
      "loss\n",
      "0.017524754628539085\n",
      "loss\n",
      "0.008446445688605309\n",
      "loss\n",
      "0.005841090343892574\n",
      "loss\n",
      "0.02205546200275421\n",
      "loss\n",
      "0.01886228285729885\n",
      "loss\n",
      "0.012258898466825485\n",
      "loss\n",
      "0.00998903438448906\n",
      "loss\n",
      "0.007056789472699165\n",
      "loss\n",
      "0.010330774821341038\n",
      "loss\n",
      "0.00817713513970375\n",
      "loss\n",
      "0.006179037969559431\n",
      "loss\n",
      "0.0073410761542618275\n",
      "loss\n",
      "0.010899768210947514\n",
      "loss\n",
      "0.026390664279460907\n",
      "loss\n",
      "0.016640907153487206\n",
      "loss\n",
      "0.00597974332049489\n",
      "loss\n",
      "0.009672206826508045\n",
      "loss\n",
      "0.005973581690341234\n",
      "loss\n",
      "3.6694374084472656\n",
      "loss\n",
      "0.00877902377396822\n",
      "loss\n",
      "4.256886959075928\n",
      "loss\n",
      "0.017297949641942978\n",
      "loss\n",
      "0.011129443533718586\n",
      "loss\n",
      "0.015219627879559994\n",
      "loss\n",
      "0.0030019478872418404\n",
      "loss\n",
      "0.021291498094797134\n",
      "loss\n",
      "0.018529647961258888\n",
      "loss\n",
      "0.009522965177893639\n",
      "loss\n",
      "0.012272440828382969\n",
      "loss\n",
      "0.008545498363673687\n",
      "loss\n",
      "0.008923650719225407\n",
      "loss\n",
      "0.0023492376785725355\n",
      "loss\n",
      "0.08297166228294373\n",
      "loss\n",
      "0.006391085684299469\n",
      "loss\n",
      "0.2621001899242401\n",
      "loss\n",
      "0.0033736478071659803\n",
      "loss\n",
      "0.06227020546793938\n",
      "loss\n",
      "0.10268152505159378\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.001490197260864079\n",
      "loss\n",
      "0.0031985098030418158\n",
      "loss\n",
      "0.19377416372299194\n",
      "loss\n",
      "0.43188416957855225\n",
      "loss\n",
      "0.010465031489729881\n",
      "loss\n",
      "0.313730925321579\n",
      "loss\n",
      "0.052673447877168655\n",
      "loss\n",
      "0.010588418692350388\n",
      "loss\n",
      "0.16984111070632935\n",
      "loss\n",
      "0.003559921169653535\n",
      "loss\n",
      "0.05207546055316925\n",
      "loss\n",
      "0.02877179905772209\n",
      "loss\n",
      "0.09439631551504135\n",
      "loss\n",
      "0.003439465072005987\n",
      "loss\n",
      "0.01418695505708456\n",
      "loss\n",
      "0.005863133817911148\n",
      "loss\n",
      "0.010725353844463825\n",
      "loss\n",
      "0.01660596765577793\n",
      "loss\n",
      "0.008103824220597744\n",
      "loss\n",
      "0.020882155746221542\n",
      "loss\n",
      "0.015934040769934654\n",
      "loss\n",
      "0.009476440958678722\n",
      "loss\n",
      "0.011041258461773396\n",
      "loss\n",
      "0.0038036394398659468\n",
      "loss\n",
      "0.015622024424374104\n",
      "loss\n",
      "0.004314637742936611\n",
      "loss\n",
      "0.002865258837118745\n",
      "loss\n",
      "0.003520245896652341\n",
      "loss\n",
      "0.00621197372674942\n",
      "loss\n",
      "0.3402809202671051\n",
      "loss\n",
      "1.1181387901306152\n",
      "loss\n",
      "0.016513574868440628\n",
      "loss\n",
      "0.005002601072192192\n",
      "loss\n",
      "0.005188333801925182\n",
      "loss\n",
      "0.0484754703938961\n",
      "loss\n",
      "0.011777392588555813\n",
      "loss\n",
      "0.00955106783658266\n",
      "loss\n",
      "0.0032876271288841963\n",
      "loss\n",
      "2.739438056945801\n",
      "loss\n",
      "0.08617750555276871\n",
      "loss\n",
      "0.0030841901898384094\n",
      "loss\n",
      "0.005740703083574772\n",
      "loss\n",
      "0.022282473742961884\n",
      "loss\n",
      "0.049073684960603714\n",
      "loss\n",
      "0.010887740179896355\n",
      "loss\n",
      "0.0028799984138458967\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.020307835191488266\n",
      "loss\n",
      "0.007002928759902716\n",
      "loss\n",
      "0.004172904882580042\n",
      "loss\n",
      "0.011328539811074734\n",
      "loss\n",
      "0.002724746707826853\n",
      "loss\n",
      "0.011057999916374683\n",
      "loss\n",
      "0.007479520980268717\n",
      "loss\n",
      "0.007599018048495054\n",
      "loss\n",
      "0.019894583150744438\n",
      "loss\n",
      "0.011453117243945599\n",
      "loss\n",
      "0.006259953137487173\n",
      "loss\n",
      "0.005067836493253708\n",
      "loss\n",
      "0.07912437617778778\n",
      "loss\n",
      "0.018125612288713455\n",
      "loss\n",
      "0.00830671563744545\n",
      "loss\n",
      "0.003499813610687852\n",
      "loss\n",
      "0.009177522733807564\n",
      "loss\n",
      "0.007959552109241486\n",
      "loss\n",
      "0.0048975031822919846\n",
      "loss\n",
      "0.00563117815181613\n",
      "loss\n",
      "0.011946087703108788\n",
      "loss\n",
      "0.12055058032274246\n",
      "loss\n",
      "0.0014368696138262749\n",
      "loss\n",
      "0.0018726922571659088\n",
      "loss\n",
      "0.0020918408408761024\n",
      "loss\n",
      "0.01692248322069645\n",
      "loss\n",
      "0.05113114416599274\n",
      "loss\n",
      "0.0038523285184055567\n",
      "loss\n",
      "0.0021715410985052586\n",
      "loss\n",
      "0.0018234307644888759\n",
      "loss\n",
      "0.03622610867023468\n",
      "loss\n",
      "0.004267395939677954\n",
      "loss\n",
      "0.004066176246851683\n",
      "loss\n",
      "1.5602083206176758\n",
      "loss\n",
      "0.004498837050050497\n",
      "loss\n",
      "0.006695810705423355\n",
      "loss\n",
      "0.023411402478814125\n",
      "loss\n",
      "0.011220568791031837\n",
      "loss\n",
      "0.005698625463992357\n",
      "loss\n",
      "0.2322503626346588\n",
      "loss\n",
      "0.00617643166333437\n",
      "loss\n",
      "0.001796538126654923\n",
      "loss\n",
      "0.005663539282977581\n",
      "loss\n",
      "0.015758037567138672\n",
      "loss\n",
      "0.0018020119750872254\n",
      "loss\n",
      "0.005692461505532265\n",
      "loss\n",
      "0.016637155786156654\n",
      "loss\n",
      "0.004839136730879545\n",
      "loss\n",
      "0.010831727646291256\n",
      "loss\n",
      "0.0023832509759813547\n",
      "loss\n",
      "0.004562444519251585\n",
      "loss\n",
      "0.009627577848732471\n",
      "loss\n",
      "0.024295106530189514\n",
      "loss\n",
      "0.011150899343192577\n",
      "loss\n",
      "0.009857428260147572\n",
      "loss\n",
      "0.00823152344673872\n",
      "loss\n",
      "0.006388835143297911\n",
      "loss\n",
      "0.006659812293946743\n",
      "loss\n",
      "0.005407468415796757\n",
      "loss\n",
      "0.006669285707175732\n",
      "loss\n",
      "0.007078569382429123\n",
      "loss\n",
      "0.00319589558057487\n",
      "loss\n",
      "0.004212554078549147\n",
      "loss\n",
      "0.00865777675062418\n",
      "loss\n",
      "0.0035727499052882195\n",
      "loss\n",
      "0.0016208856832236052\n",
      "loss\n",
      "0.006474115885794163\n",
      "loss\n",
      "0.011410925537347794\n",
      "loss\n",
      "0.01145629957318306\n",
      "loss\n",
      "0.007725598756223917\n",
      "loss\n",
      "0.0021824846044182777\n",
      "loss\n",
      "0.0015101945027709007\n",
      "loss\n",
      "0.01465426292270422\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.006637786515057087\n",
      "loss\n",
      "0.002001427114009857\n",
      "loss\n",
      "0.0010892179561778903\n",
      "loss\n",
      "0.005189519841223955\n",
      "loss\n",
      "0.005124291870743036\n",
      "loss\n",
      "0.016665644943714142\n",
      "loss\n",
      "0.005322452634572983\n",
      "loss\n",
      "0.0194013062864542\n",
      "loss\n",
      "0.0016494491137564182\n",
      "loss\n",
      "0.01149224303662777\n",
      "loss\n",
      "0.03479050472378731\n",
      "loss\n",
      "0.006425435654819012\n",
      "loss\n",
      "0.005977491848170757\n",
      "loss\n",
      "0.00912188645452261\n",
      "loss\n",
      "2.2605345249176025\n",
      "loss\n",
      "0.006742346566170454\n",
      "loss\n",
      "0.003994344733655453\n",
      "loss\n",
      "0.02096504159271717\n",
      "loss\n",
      "0.0022115076426416636\n",
      "loss\n",
      "0.011889898218214512\n",
      "loss\n",
      "0.0265306755900383\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.00421172333881259\n",
      "loss\n",
      "0.015857893973588943\n",
      "loss\n",
      "0.1096530631184578\n",
      "loss\n",
      "0.018577391281723976\n",
      "loss\n",
      "0.0031585826072841883\n",
      "loss\n",
      "0.007590973284095526\n",
      "loss\n",
      "0.01869463361799717\n",
      "loss\n",
      "0.0005868143052794039\n",
      "loss\n",
      "0.0009062950266525149\n",
      "loss\n",
      "0.0009920443408191204\n",
      "loss\n",
      "0.022964363917708397\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "1.710546851158142\n",
      "loss\n",
      "0.020992590114474297\n",
      "loss\n",
      "0.0007769426447339356\n",
      "loss\n",
      "0.0007874249131418765\n",
      "loss\n",
      "0.002684087259694934\n",
      "loss\n",
      "0.0014432977186515927\n",
      "loss\n",
      "0.014400719664990902\n",
      "loss\n",
      "0.01647898182272911\n",
      "loss\n",
      "0.0025668551679700613\n",
      "loss\n",
      "0.02713252790272236\n",
      "loss\n",
      "0.0029743739869445562\n",
      "loss\n",
      "0.006955931894481182\n",
      "loss\n",
      "0.02879623882472515\n",
      "loss\n",
      "0.009999656118452549\n",
      "loss\n",
      "0.002252900041639805\n",
      "loss\n",
      "0.0019014865392819047\n",
      "loss\n",
      "0.004394398536533117\n",
      "loss\n",
      "0.0068917665630578995\n",
      "loss\n",
      "0.0008858094224706292\n",
      "loss\n",
      "0.009932027198374271\n",
      "loss\n",
      "0.014354773797094822\n",
      "loss\n",
      "0.003590567270293832\n",
      "loss\n",
      "0.007239537313580513\n",
      "loss\n",
      "0.010349651798605919\n",
      "loss\n",
      "0.0017913023475557566\n",
      "loss\n",
      "0.005552583374083042\n",
      "loss\n",
      "0.005063566844910383\n",
      "loss\n",
      "0.00753796985372901\n",
      "loss\n",
      "0.005215846933424473\n",
      "loss\n",
      "0.019643884152173996\n",
      "loss\n",
      "0.010137026198208332\n",
      "loss\n",
      "0.009279691614210606\n",
      "loss\n",
      "0.24630846083164215\n",
      "loss\n",
      "0.20919343829154968\n",
      "loss\n",
      "0.011986136436462402\n",
      "loss\n",
      "0.03142803534865379\n",
      "loss\n",
      "0.0016289787599816918\n",
      "loss\n",
      "0.007744170259684324\n",
      "loss\n",
      "0.0020404488313943148\n",
      "loss\n",
      "0.0019769188947975636\n",
      "loss\n",
      "0.0036828566808253527\n",
      "loss\n",
      "0.001430917764082551\n",
      "loss\n",
      "0.0032458023633807898\n",
      "loss\n",
      "0.9635264873504639\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.0015397133538499475\n",
      "loss\n",
      "0.01138865016400814\n",
      "loss\n",
      "0.006255806889384985\n",
      "loss\n",
      "0.0012344843707978725\n",
      "loss\n",
      "0.012749706394970417\n",
      "loss\n",
      "0.016048895195126534\n",
      "loss\n",
      "0.016542652621865273\n",
      "loss\n",
      "0.009411019273102283\n",
      "loss\n",
      "0.016764706000685692\n",
      "loss\n",
      "0.0016001766780391335\n",
      "loss\n",
      "0.024112075567245483\n",
      "loss\n",
      "0.0008298290777020156\n",
      "loss\n",
      "0.0038938906509429216\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0008660380262881517\n",
      "loss\n",
      "0.005714982748031616\n",
      "loss\n",
      "0.005052180495113134\n",
      "loss\n",
      "0.006232587620615959\n",
      "loss\n",
      "0.01502776425331831\n",
      "loss\n",
      "0.005707752425223589\n",
      "loss\n",
      "0.002473630243912339\n",
      "loss\n",
      "0.009997768327593803\n",
      "loss\n",
      "0.012457652017474174\n",
      "loss\n",
      "0.011125553399324417\n",
      "loss\n",
      "0.0012016226537525654\n",
      "loss\n",
      "0.0020697140134871006\n",
      "loss\n",
      "0.0006506709614768624\n",
      "loss\n",
      "0.014135828241705894\n",
      "loss\n",
      "0.004716699477285147\n",
      "loss\n",
      "0.0012061471352353692\n",
      "loss\n",
      "1.7048331499099731\n",
      "loss\n",
      "0.0016477829776704311\n",
      "loss\n",
      "0.007360128220170736\n",
      "loss\n",
      "0.0018893502419814467\n",
      "loss\n",
      "0.0011075560469180346\n",
      "loss\n",
      "0.0037392713129520416\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.003224414074793458\n",
      "loss\n",
      "0.0066930875182151794\n",
      "loss\n",
      "0.002837918698787689\n",
      "loss\n",
      "0.012988959439098835\n",
      "loss\n",
      "0.005118361674249172\n",
      "loss\n",
      "0.002372785471379757\n",
      "loss\n",
      "0.002551397541537881\n",
      "loss\n",
      "0.002693360671401024\n",
      "loss\n",
      "0.005424067843705416\n",
      "loss\n",
      "0.029661312699317932\n",
      "loss\n",
      "0.004786224570125341\n",
      "loss\n",
      "0.002189621329307556\n",
      "loss\n",
      "0.003100590081885457\n",
      "loss\n",
      "0.0031093843281269073\n",
      "loss\n",
      "0.009124367497861385\n",
      "loss\n",
      "0.003724069334566593\n",
      "loss\n",
      "0.02314271405339241\n",
      "loss\n",
      "0.0025100174825638533\n",
      "loss\n",
      "0.0035209585912525654\n",
      "loss\n",
      "0.002444852376356721\n",
      "loss\n",
      "0.005885177291929722\n",
      "loss\n",
      "0.013309091329574585\n",
      "loss\n",
      "0.10153012722730637\n",
      "loss\n",
      "0.004611808806657791\n",
      "loss\n",
      "0.0028217521030455828\n",
      "loss\n",
      "0.015542215667665005\n",
      "loss\n",
      "0.002033786615356803\n",
      "loss\n",
      "0.0014035383937880397\n",
      "loss\n",
      "0.0029042467940598726\n",
      "loss\n",
      "0.0021230080164968967\n",
      "loss\n",
      "0.003525234991684556\n",
      "loss\n",
      "0.0052865236066281796\n",
      "loss\n",
      "0.05563344433903694\n",
      "loss\n",
      "1.2306580543518066\n",
      "loss\n",
      "0.002805347554385662\n",
      "loss\n",
      "0.008296666666865349\n",
      "loss\n",
      "0.0025618611834943295\n",
      "loss\n",
      "0.0029922020621597767\n",
      "loss\n",
      "0.001327824778854847\n",
      "loss\n",
      "0.004977691452950239\n",
      "loss\n",
      "0.008818963542580605\n",
      "loss\n",
      "0.001708239782601595\n",
      "loss\n",
      "0.7690757513046265\n",
      "loss\n",
      "0.015876196324825287\n",
      "loss\n",
      "0.003787725931033492\n",
      "loss\n",
      "0.032406941056251526\n",
      "loss\n",
      "0.0013309201458469033\n",
      "loss\n",
      "0.0024917051196098328\n",
      "loss\n",
      "0.001479484373703599\n",
      "loss\n",
      "0.003427822608500719\n",
      "loss\n",
      "0.003249604720622301\n",
      "loss\n",
      "0.004893707111477852\n",
      "loss\n",
      "0.011703873984515667\n",
      "loss\n",
      "0.005132000893354416\n",
      "loss\n",
      "0.018576689064502716\n",
      "loss\n",
      "0.009199729189276695\n",
      "loss\n",
      "0.005308579187840223\n",
      "loss\n",
      "0.007215393707156181\n",
      "loss\n",
      "0.0033021229319274426\n",
      "loss\n",
      "0.025185471400618553\n",
      "loss\n",
      "0.001508289948105812\n",
      "loss\n",
      "0.007013701368123293\n",
      "loss\n",
      "0.05054481700062752\n",
      "loss\n",
      "0.0034468306694179773\n",
      "loss\n",
      "0.006219674367457628\n",
      "loss\n",
      "0.0019921474158763885\n",
      "loss\n",
      "0.0024717275518924\n",
      "loss\n",
      "0.0010458719916641712\n",
      "loss\n",
      "0.0013863962376490235\n",
      "loss\n",
      "0.00099871342536062\n",
      "loss\n",
      "0.003458235412836075\n",
      "loss\n",
      "0.0014266322832554579\n",
      "loss\n",
      "0.0684390440583229\n",
      "loss\n",
      "0.0028136686887592077\n",
      "loss\n",
      "0.0014552014181390405\n",
      "loss\n",
      "0.0019932182040065527\n",
      "loss\n",
      "0.0014911495381966233\n",
      "loss\n",
      "0.07449249923229218\n",
      "loss\n",
      "0.009254061616957188\n",
      "loss\n",
      "0.0026251161471009254\n",
      "loss\n",
      "0.0013711584033444524\n",
      "loss\n",
      "0.005050757434219122\n",
      "loss\n",
      "0.00182819040492177\n",
      "loss\n",
      "0.002353043295443058\n",
      "loss\n",
      "0.0033620046451687813\n",
      "loss\n",
      "0.0028780964203178883\n",
      "loss\n",
      "0.001177094760350883\n",
      "loss\n",
      "0.0012528197839856148\n",
      "loss\n",
      "0.005170782096683979\n",
      "loss\n",
      "0.006078684702515602\n",
      "loss\n",
      "0.0027772923931479454\n",
      "loss\n",
      "0.0025754161179065704\n",
      "loss\n",
      "0.0015905360924080014\n",
      "loss\n",
      "0.06839874386787415\n",
      "loss\n",
      "0.0038691910449415445\n",
      "loss\n",
      "0.00432888139039278\n",
      "loss\n",
      "0.0014771036803722382\n",
      "loss\n",
      "0.0012421043356880546\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0018992258701473475\n",
      "loss\n",
      "0.0033263610675930977\n",
      "loss\n",
      "0.0013111574808135629\n",
      "loss\n",
      "0.0015920833684504032\n",
      "loss\n",
      "0.0006268443539738655\n",
      "loss\n",
      "0.001416633022017777\n",
      "loss\n",
      "0.0014332984574139118\n",
      "loss\n",
      "0.004780292976647615\n",
      "loss\n",
      "0.1666361540555954\n",
      "loss\n",
      "0.0019267105963081121\n",
      "loss\n",
      "0.001359730027616024\n",
      "loss\n",
      "0.007523654028773308\n",
      "loss\n",
      "0.0007602662080898881\n",
      "loss\n",
      "0.0053190141916275024\n",
      "loss\n",
      "0.0015573289711028337\n",
      "loss\n",
      "0.0013621109537780285\n",
      "loss\n",
      "0.0012852036161348224\n",
      "loss\n",
      "0.0010589712765067816\n",
      "loss\n",
      "0.0019512200960889459\n",
      "loss\n",
      "0.002201516181230545\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0015368566382676363\n",
      "loss\n",
      "0.005285100545734167\n",
      "loss\n",
      "0.00283316383138299\n",
      "loss\n",
      "0.002679450437426567\n",
      "loss\n",
      "0.0008553183870390058\n",
      "loss\n",
      "0.005783846136182547\n",
      "loss\n",
      "0.0013325868640094995\n",
      "loss\n",
      "0.0020808966364711523\n",
      "loss\n",
      "0.0010180057724937797\n",
      "loss\n",
      "0.0015910121146589518\n",
      "loss\n",
      "0.002381110331043601\n",
      "loss\n",
      "0.0018386616138741374\n",
      "loss\n",
      "0.0021241975482553244\n",
      "loss\n",
      "0.0017482249531894922\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007312007946893573\n",
      "loss\n",
      "0.0012354368809610605\n",
      "loss\n",
      "0.0022005646023899317\n",
      "loss\n",
      "0.003978315275162458\n",
      "loss\n",
      "0.0013494918821379542\n",
      "loss\n",
      "0.002420830773189664\n",
      "loss\n",
      "0.003981996327638626\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.0021873614750802517\n",
      "loss\n",
      "0.001004667836241424\n",
      "loss\n",
      "0.0024986020289361477\n",
      "loss\n",
      "0.004319860599935055\n",
      "loss\n",
      "0.0005863377591595054\n",
      "loss\n",
      "0.0010850501712411642\n",
      "loss\n",
      "0.0016085079405456781\n",
      "loss\n",
      "0.0014430596493184566\n",
      "loss\n",
      "0.0023011888843029737\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.006295610684901476\n",
      "loss\n",
      "0.001369729870930314\n",
      "loss\n",
      "0.0011634016409516335\n",
      "loss\n",
      "0.0015768486773595214\n",
      "loss\n",
      "0.0007338214782066643\n",
      "loss\n",
      "0.001019553979858756\n",
      "loss\n",
      "0.0008622265886515379\n",
      "loss\n",
      "0.0012063853209838271\n",
      "loss\n",
      "0.0012644876260310411\n",
      "loss\n",
      "0.0010768335778266191\n",
      "loss\n",
      "0.0011632826644927263\n",
      "loss\n",
      "0.0032733690459281206\n",
      "loss\n",
      "0.001688008545897901\n",
      "loss\n",
      "0.0009617946925573051\n",
      "loss\n",
      "0.0008934320067055523\n",
      "loss\n",
      "0.0010639727115631104\n",
      "loss\n",
      "0.004857524763792753\n",
      "loss\n",
      "0.0022261380217969418\n",
      "loss\n",
      "0.003475104458630085\n",
      "loss\n",
      "0.0010546842822805047\n",
      "loss\n",
      "0.0016202905680984259\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001911718980409205\n",
      "loss\n",
      "0.0008288762182928622\n",
      "loss\n",
      "0.0038984029088169336\n",
      "loss\n",
      "0.0010794533882290125\n",
      "loss\n",
      "0.0025000290479511023\n",
      "loss\n",
      "0.06439148634672165\n",
      "loss\n",
      "0.0005739472107961774\n",
      "loss\n",
      "0.0867927223443985\n",
      "loss\n",
      "0.0008388814167119563\n",
      "loss\n",
      "0.0008179179858416319\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0008789013954810798\n",
      "loss\n",
      "0.001069450518116355\n",
      "loss\n",
      "0.0004976941272616386\n",
      "loss\n",
      "0.0016221948899328709\n",
      "loss\n",
      "0.00044431351125240326\n",
      "loss\n",
      "0.0019490785198286176\n",
      "loss\n",
      "0.0010580186499282718\n",
      "loss\n",
      "0.003044258337467909\n",
      "loss\n",
      "0.0017691688844934106\n",
      "loss\n",
      "0.0010022860951721668\n",
      "loss\n",
      "0.0004898302140645683\n",
      "loss\n",
      "0.000834117061458528\n",
      "loss\n",
      "0.0032225127797573805\n",
      "loss\n",
      "0.006845948286354542\n",
      "loss\n",
      "0.005816676188260317\n",
      "loss\n",
      "0.0025216706562787294\n",
      "loss\n",
      "0.0019045800436288118\n",
      "loss\n",
      "0.013794328086078167\n",
      "loss\n",
      "0.0015889888163655996\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.0013323486782610416\n",
      "loss\n",
      "0.0006561510381288826\n",
      "loss\n",
      "0.010727358050644398\n",
      "loss\n",
      "0.0008112476789392531\n",
      "loss\n",
      "0.0013940150383859873\n",
      "loss\n",
      "0.0067525296472013\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "1.133132815361023\n",
      "loss\n",
      "0.0005555993411689997\n",
      "loss\n",
      "0.000635183765552938\n",
      "loss\n",
      "0.003932482097297907\n",
      "loss\n",
      "0.0025434307754039764\n",
      "loss\n",
      "0.002568519674241543\n",
      "loss\n",
      "0.0027159492019563913\n",
      "loss\n",
      "0.5777732133865356\n",
      "loss\n",
      "0.0015739921946078539\n",
      "loss\n",
      "0.009132400155067444\n",
      "loss\n",
      "0.0021394239738583565\n",
      "loss\n",
      "0.0009253510506823659\n",
      "loss\n",
      "0.004545712377876043\n",
      "loss\n",
      "0.0010368215152993798\n",
      "loss\n",
      "0.0029525042045861483\n",
      "loss\n",
      "0.0005034133209846914\n",
      "loss\n",
      "0.00035565727739594877\n",
      "loss\n",
      "0.0013454442378133535\n",
      "loss\n",
      "0.0008417400531470776\n",
      "loss\n",
      "0.00573691027238965\n",
      "loss\n",
      "0.0008646087371744215\n",
      "loss\n",
      "0.004140377044677734\n",
      "loss\n",
      "0.002090056659653783\n",
      "loss\n",
      "0.0017508429009467363\n",
      "loss\n",
      "0.0005458295345306396\n",
      "loss\n",
      "0.007576303090900183\n",
      "loss\n",
      "0.002320099389180541\n",
      "loss\n",
      "0.006270022597163916\n",
      "loss\n",
      "0.0028695380315184593\n",
      "loss\n",
      "0.0013546108966693282\n",
      "loss\n",
      "0.0011938833631575108\n",
      "loss\n",
      "0.0004612335760612041\n",
      "loss\n",
      "0.0020207001361995935\n",
      "loss\n",
      "0.0028019000310450792\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0005999195855110884\n",
      "loss\n",
      "0.0004219118563923985\n",
      "loss\n",
      "0.0046008918434381485\n",
      "loss\n",
      "0.002042233245447278\n",
      "loss\n",
      "0.006754186935722828\n",
      "loss\n",
      "0.0007593132322654128\n",
      "loss\n",
      "0.00041547726141288877\n",
      "loss\n",
      "0.0008174415561370552\n",
      "loss\n",
      "0.0022442173212766647\n",
      "loss\n",
      "0.000581572181545198\n",
      "loss\n",
      "0.003397290362045169\n",
      "loss\n",
      "0.0006323245470412076\n",
      "loss\n",
      "0.0019155264599248767\n",
      "loss\n",
      "0.0005765683017671108\n",
      "loss\n",
      "0.0003532739356160164\n",
      "loss\n",
      "0.002211745595559478\n",
      "loss\n",
      "0.03702251985669136\n",
      "loss\n",
      "0.0009205871028825641\n",
      "loss\n",
      "0.008095311000943184\n",
      "loss\n",
      "0.0005775213940069079\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0011156531982123852\n",
      "loss\n",
      "0.0019378946162760258\n",
      "loss\n",
      "0.003487696871161461\n",
      "loss\n",
      "0.0017197832930833101\n",
      "loss\n",
      "0.0022155519109219313\n",
      "loss\n",
      "0.0010141950333490968\n",
      "loss\n",
      "0.07641874998807907\n",
      "loss\n",
      "0.0013409203384071589\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "0.0014016337227076292\n",
      "loss\n",
      "0.017542092129588127\n",
      "loss\n",
      "0.0032928551081568003\n",
      "loss\n",
      "0.0005334384622983634\n",
      "loss\n",
      "0.0008803306263871491\n",
      "loss\n",
      "0.0032828745897859335\n",
      "loss\n",
      "0.0023209319915622473\n",
      "loss\n",
      "0.0004161922261118889\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.0006065912893973291\n",
      "loss\n",
      "0.0072313714772462845\n",
      "loss\n",
      "0.0024629279505461454\n",
      "loss\n",
      "0.0021594080608338118\n",
      "loss\n",
      "0.0022205475252121687\n",
      "loss\n",
      "0.0010596857173368335\n",
      "loss\n",
      "0.001319729257375002\n",
      "loss\n",
      "0.009429323486983776\n",
      "loss\n",
      "0.002771110739558935\n",
      "loss\n",
      "0.001105888863094151\n",
      "loss\n",
      "0.006941134110093117\n",
      "loss\n",
      "0.0014033003244549036\n",
      "loss\n",
      "0.00024911639047786593\n",
      "loss\n",
      "0.00403613829985261\n",
      "loss\n",
      "0.0013616346986964345\n",
      "loss\n",
      "0.002226970624178648\n",
      "loss\n",
      "0.0029974314384162426\n",
      "loss\n",
      "0.0010115751065313816\n",
      "loss\n",
      "0.0009703694959171116\n",
      "loss\n",
      "0.0023328252136707306\n",
      "loss\n",
      "0.0012416280806064606\n",
      "loss\n",
      "0.008411219343543053\n",
      "loss\n",
      "0.0014676999999210238\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000969535845797509\n",
      "loss\n",
      "0.00026556302327662706\n",
      "loss\n",
      "0.00041261743172071874\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.001522930571809411\n",
      "loss\n",
      "0.0008607972995378077\n",
      "loss\n",
      "0.001702408422715962\n",
      "loss\n",
      "0.00075049843871966\n",
      "loss\n",
      "0.0002951186615973711\n",
      "loss\n",
      "0.0009332115878351033\n",
      "loss\n",
      "0.0014611531514674425\n",
      "loss\n",
      "0.0015103134792298079\n",
      "loss\n",
      "0.0012911563972011209\n",
      "loss\n",
      "0.000649956171400845\n",
      "loss\n",
      "0.002857889048755169\n",
      "loss\n",
      "0.001141611486673355\n",
      "loss\n",
      "0.0008865240379236639\n",
      "loss\n",
      "0.004283539019525051\n",
      "loss\n",
      "0.0007805161876603961\n",
      "loss\n",
      "0.0007681279676035047\n",
      "loss\n",
      "0.0013560395454987884\n",
      "loss\n",
      "0.0003937899600714445\n",
      "loss\n",
      "0.002218644367530942\n",
      "loss\n",
      "0.0014017528155818582\n",
      "loss\n",
      "0.0006868863711133599\n",
      "loss\n",
      "0.026876559481024742\n",
      "loss\n",
      "0.002644615015015006\n",
      "loss\n",
      "0.0009517907164990902\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.003852209774777293\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.00044764988706447184\n",
      "loss\n",
      "0.001259963377378881\n",
      "loss\n",
      "0.002960229991003871\n",
      "loss\n",
      "0.001384491566568613\n",
      "loss\n",
      "0.0019179059891030192\n",
      "loss\n",
      "0.0013440155889838934\n",
      "loss\n",
      "0.0005482124397531152\n",
      "loss\n",
      "0.005588384345173836\n",
      "loss\n",
      "0.0002812943421304226\n",
      "loss\n",
      "0.0008534126682206988\n",
      "loss\n",
      "0.0009110590908676386\n",
      "loss\n",
      "0.0007183355046436191\n",
      "loss\n",
      "0.0003687655262183398\n",
      "loss\n",
      "0.0008818790083751082\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.002041638595983386\n",
      "loss\n",
      "0.002261463785544038\n",
      "loss\n",
      "0.0010599239030852914\n",
      "loss\n",
      "0.0013654442736878991\n",
      "loss\n",
      "0.004441278520971537\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.010698935016989708\n",
      "loss\n",
      "0.000696654780767858\n",
      "loss\n",
      "0.0009991897968575358\n",
      "loss\n",
      "0.00774192251265049\n",
      "loss\n",
      "0.0011189873330295086\n",
      "loss\n",
      "0.0007273888913914561\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.00035232058144174516\n",
      "loss\n",
      "0.003114256775006652\n",
      "loss\n",
      "0.005950829479843378\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.00035398892941884696\n",
      "loss\n",
      "0.0009184433147311211\n",
      "loss\n",
      "0.00034850722295232117\n",
      "loss\n",
      "0.0016182672698050737\n",
      "loss\n",
      "0.0009114163694903255\n",
      "loss\n",
      "0.010211721062660217\n",
      "loss\n",
      "0.0008002892718650401\n",
      "loss\n",
      "0.0028330450877547264\n",
      "loss\n",
      "0.000977038755081594\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0009078433504328132\n",
      "loss\n",
      "0.0004047528200317174\n",
      "loss\n",
      "0.0012709167785942554\n",
      "loss\n",
      "0.0021843877620995045\n",
      "loss\n",
      "0.0014002051902934909\n",
      "loss\n",
      "0.001053255284205079\n",
      "loss\n",
      "0.0003840185818262398\n",
      "loss\n",
      "0.0006049233488738537\n",
      "loss\n",
      "0.0006123098428361118\n",
      "loss\n",
      "0.0007838514284230769\n",
      "loss\n",
      "0.005385533440858126\n",
      "loss\n",
      "0.0015470929211005569\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.002130145439878106\n",
      "loss\n",
      "0.000745018885936588\n",
      "loss\n",
      "0.0004693360242526978\n",
      "loss\n",
      "0.0018774517811834812\n",
      "loss\n",
      "0.0003051292151212692\n",
      "loss\n",
      "0.0018917298875749111\n",
      "loss\n",
      "0.0010977915953844786\n",
      "loss\n",
      "0.001210909802466631\n",
      "loss\n",
      "0.0024791003670543432\n",
      "loss\n",
      "0.002086844528093934\n",
      "loss\n",
      "0.0011520899133756757\n",
      "loss\n",
      "0.00046957432641647756\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "0.0006725909770466387\n",
      "loss\n",
      "0.0010973153403028846\n",
      "loss\n",
      "0.0013992529129609466\n",
      "loss\n",
      "0.0011435167398303747\n",
      "loss\n",
      "0.0009629856795072556\n",
      "loss\n",
      "0.009789552539587021\n",
      "loss\n",
      "0.00024577934527769685\n",
      "loss\n",
      "0.0004914983292110264\n",
      "loss\n",
      "0.00042906138696707785\n",
      "loss\n",
      "0.005800202023237944\n",
      "loss\n",
      "0.0008683010237291455\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0016599221853539348\n",
      "loss\n",
      "0.0010418231831863523\n",
      "loss\n",
      "0.0007952864980325103\n",
      "loss\n",
      "0.0007488307310268283\n",
      "loss\n",
      "0.00200130813755095\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.0005787128466181457\n",
      "loss\n",
      "0.0008704449282959104\n",
      "loss\n",
      "0.0008831891464069486\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0006168370018713176\n",
      "loss\n",
      "0.0010122895473614335\n",
      "loss\n",
      "0.000797192333266139\n",
      "loss\n",
      "0.001840565470047295\n",
      "loss\n",
      "0.0009284476400353014\n",
      "loss\n",
      "0.002931585069745779\n",
      "loss\n",
      "0.00023958197562023997\n",
      "loss\n",
      "0.0002543602604418993\n",
      "loss\n",
      "0.0008665143977850676\n",
      "loss\n",
      "0.0005872909096069634\n",
      "loss\n",
      "0.0009951406391337514\n",
      "loss\n",
      "0.0030088413041085005\n",
      "loss\n",
      "4.661959648132324\n",
      "loss\n",
      "0.012682733125984669\n",
      "loss\n",
      "0.0011266082292422652\n",
      "loss\n",
      "0.0011711412807926536\n",
      "loss\n",
      "0.0006866481271572411\n",
      "loss\n",
      "0.005445883143693209\n",
      "loss\n",
      "0.0044141001999378204\n",
      "loss\n",
      "0.0148676922544837\n",
      "loss\n",
      "0.0033791130408644676\n",
      "loss\n",
      "0.01798722706735134\n",
      "loss\n",
      "0.0019363479223102331\n",
      "loss\n",
      "0.0010003806091845036\n",
      "loss\n",
      "0.001105412608012557\n",
      "loss\n",
      "0.000695463502779603\n",
      "loss\n",
      "0.0007178590167313814\n",
      "loss\n",
      "0.058921705931425095\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.002141089178621769\n",
      "loss\n",
      "0.0011211306555196643\n",
      "loss\n",
      "0.06521012634038925\n",
      "loss\n",
      "0.0007843278581276536\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.0006530536338686943\n",
      "loss\n",
      "0.002709410386160016\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.007267585955560207\n",
      "loss\n",
      "0.005277866963297129\n",
      "loss\n",
      "0.0009411911014467478\n",
      "loss\n",
      "0.0005138983833603561\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.002880473854020238\n",
      "loss\n",
      "0.026496661826968193\n",
      "loss\n",
      "0.001375206047669053\n",
      "loss\n",
      "0.002098502591252327\n",
      "loss\n",
      "0.37235745787620544\n",
      "loss\n",
      "0.0003971264814026654\n",
      "loss\n",
      "0.0007009433466009796\n",
      "loss\n",
      "0.006032118573784828\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.001023483811877668\n",
      "loss\n",
      "0.00044550508027896285\n",
      "loss\n",
      "0.0014649622607976198\n",
      "loss\n",
      "0.0023430532310158014\n",
      "loss\n",
      "0.0051649706438183784\n",
      "loss\n",
      "0.0011328000109642744\n",
      "loss\n",
      "0.012277033179998398\n",
      "loss\n",
      "0.0009029601933434606\n",
      "loss\n",
      "0.0016807490028440952\n",
      "loss\n",
      "0.0008746135863475502\n",
      "loss\n",
      "0.0005560758872888982\n",
      "loss\n",
      "0.0037347583565860987\n",
      "loss\n",
      "0.0024947968777269125\n",
      "loss\n",
      "0.005204224959015846\n",
      "loss\n",
      "0.0009472650708630681\n",
      "loss\n",
      "0.000876757490914315\n",
      "loss\n",
      "0.0005915798828937113\n",
      "loss\n",
      "0.0004741021548397839\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007281036232598126\n",
      "loss\n",
      "0.001086598145775497\n",
      "loss\n",
      "0.0012180536286905408\n",
      "loss\n",
      "0.0010181248653680086\n",
      "loss\n",
      "0.0006835508393123746\n",
      "loss\n",
      "0.0008760428754612803\n",
      "loss\n",
      "0.001959786517545581\n",
      "loss\n",
      "0.0006316096987575293\n",
      "loss\n",
      "0.005911367479711771\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0007014198345132172\n",
      "loss\n",
      "0.001320443581789732\n",
      "loss\n",
      "0.0007500219508074224\n",
      "loss\n",
      "0.0008264940115623176\n",
      "loss\n",
      "0.0005551227368414402\n",
      "loss\n",
      "0.0007950482540763915\n",
      "loss\n",
      "0.002404181519523263\n",
      "loss\n",
      "0.0028921226039528847\n",
      "loss\n",
      "0.06963595002889633\n",
      "loss\n",
      "0.0018892312655225396\n",
      "loss\n",
      "0.002776341512799263\n",
      "loss\n",
      "1.097870945930481\n",
      "loss\n",
      "0.0005777596961706877\n",
      "loss\n",
      "0.0008172033121809363\n",
      "loss\n",
      "0.009694874286651611\n",
      "loss\n",
      "0.006388006266206503\n",
      "loss\n",
      "0.0017229963559657335\n",
      "loss\n",
      "0.001210671616718173\n",
      "loss\n",
      "0.0014313939027488232\n",
      "loss\n",
      "0.008590529672801495\n",
      "loss\n",
      "0.00047743841423653066\n",
      "loss\n",
      "0.0011382774682715535\n",
      "loss\n",
      "0.0026242840103805065\n",
      "loss\n",
      "0.0003644755925051868\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.00157803890760988\n",
      "loss\n",
      "0.0009129646932706237\n",
      "loss\n",
      "0.0011154150124639273\n",
      "loss\n",
      "0.001625765347853303\n",
      "loss\n",
      "2.8665642738342285\n",
      "loss\n",
      "0.012164924293756485\n",
      "loss\n",
      "0.0004950728034600616\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.005643506534397602\n",
      "loss\n",
      "0.00040451448876410723\n",
      "loss\n",
      "0.00030298411729745567\n",
      "loss\n",
      "0.04739883169531822\n",
      "loss\n",
      "0.036705754697322845\n",
      "loss\n",
      "0.00945920031517744\n",
      "loss\n",
      "0.00024720950750634074\n",
      "loss\n",
      "0.061474040150642395\n",
      "loss\n",
      "0.0006235085893422365\n",
      "loss\n",
      "0.007738137152045965\n",
      "loss\n",
      "0.00026246439665555954\n",
      "loss\n",
      "0.04900194704532623\n",
      "loss\n",
      "0.00022837892174720764\n",
      "loss\n",
      "0.021302001550793648\n",
      "loss\n",
      "0.07682917267084122\n",
      "loss\n",
      "0.0003003622987307608\n",
      "loss\n",
      "0.03357384353876114\n",
      "loss\n",
      "0.0002033503697020933\n",
      "loss\n",
      "0.00017331528943032026\n",
      "loss\n",
      "0.0002858230145648122\n",
      "loss\n",
      "0.012821382842957973\n",
      "loss\n",
      "0.016321489587426186\n",
      "loss\n",
      "0.08305186033248901\n",
      "loss\n",
      "0.0014668668154627085\n",
      "loss\n",
      "0.00033158526639454067\n",
      "loss\n",
      "0.018014157190918922\n",
      "loss\n",
      "0.00038211196078918874\n",
      "loss\n",
      "0.05664663016796112\n",
      "loss\n",
      "0.0001652104256208986\n",
      "loss\n",
      "0.0010937429033219814\n",
      "loss\n",
      "0.0409710630774498\n",
      "loss\n",
      "0.040607012808322906\n",
      "loss\n",
      "0.0007090438157320023\n",
      "loss\n",
      "0.005363598000258207\n",
      "loss\n",
      "0.00033968876232393086\n",
      "loss\n",
      "0.0037461596075445414\n",
      "loss\n",
      "0.000534868217073381\n",
      "loss\n",
      "0.00042715485324151814\n",
      "loss\n",
      "0.005437346640974283\n",
      "loss\n",
      "0.0005265279905870557\n",
      "loss\n",
      "0.018740614876151085\n",
      "loss\n",
      "0.005633667577058077\n",
      "loss\n",
      "0.002406084444373846\n",
      "loss\n",
      "0.0005832401220686734\n",
      "loss\n",
      "0.0005103239673189819\n",
      "loss\n",
      "0.0033570146188139915\n",
      "loss\n",
      "0.007710220292210579\n",
      "loss\n",
      "0.0002157455455744639\n",
      "loss\n",
      "0.004921702668070793\n",
      "loss\n",
      "0.00028391621890477836\n",
      "loss\n",
      "0.009648121893405914\n",
      "loss\n",
      "0.0002779574424494058\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.00031704644788987935\n",
      "loss\n",
      "0.002370406873524189\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.0037905762437731028\n",
      "loss\n",
      "0.003954449202865362\n",
      "loss\n",
      "0.00046695294440723956\n",
      "loss\n",
      "0.00443332688882947\n",
      "loss\n",
      "0.0001833270798670128\n",
      "loss\n",
      "0.0022333934903144836\n",
      "loss\n",
      "0.003699603257700801\n",
      "loss\n",
      "0.0019722788129001856\n",
      "loss\n",
      "0.010431881994009018\n",
      "loss\n",
      "0.0002743821241892874\n",
      "loss\n",
      "6.502380847930908\n",
      "loss\n",
      "0.0034477810841053724\n",
      "loss\n",
      "0.0003575639275368303\n",
      "loss\n",
      "0.0002674698771443218\n",
      "loss\n",
      "0.0030373651534318924\n",
      "loss\n",
      "0.0006556744920089841\n",
      "loss\n",
      "0.002501931507140398\n",
      "loss\n",
      "0.0031947072129696608\n",
      "loss\n",
      "0.0007147617870941758\n",
      "loss\n",
      "0.0033837463706731796\n",
      "loss\n",
      "0.001416394836269319\n",
      "loss\n",
      "0.0024562685284763575\n",
      "loss\n",
      "0.00471147894859314\n",
      "loss\n",
      "0.0022033003624528646\n",
      "loss\n",
      "0.002617269055917859\n",
      "loss\n",
      "0.00243831193074584\n",
      "loss\n",
      "0.0016720612766221166\n",
      "loss\n",
      "0.12599332630634308\n",
      "loss\n",
      "0.0017636949196457863\n",
      "loss\n",
      "0.0017951102927327156\n",
      "loss\n",
      "0.00887355301529169\n",
      "loss\n",
      "0.007288059685379267\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.0016561138909310102\n",
      "loss\n",
      "0.005817742552608252\n",
      "loss\n",
      "0.0010451575508341193\n",
      "loss\n",
      "0.0009803733555600047\n",
      "loss\n",
      "0.022383779287338257\n",
      "loss\n",
      "0.0032427129335701466\n",
      "loss\n",
      "0.0009266611887142062\n",
      "loss\n",
      "0.019285082817077637\n",
      "loss\n",
      "0.0009420248097740114\n",
      "loss\n",
      "0.0016465928638353944\n",
      "loss\n",
      "0.0023178397677838802\n",
      "loss\n",
      "0.002938360208645463\n",
      "loss\n",
      "0.08495984971523285\n",
      "loss\n",
      "0.0014054430648684502\n",
      "loss\n",
      "0.0009221353684552014\n",
      "loss\n",
      "0.00436816830188036\n",
      "loss\n",
      "0.0030544791370630264\n",
      "loss\n",
      "0.005048622377216816\n",
      "loss\n",
      "0.0060799880884587765\n",
      "loss\n",
      "0.016540072858333588\n",
      "loss\n",
      "0.2260076254606247\n",
      "loss\n",
      "0.007220128085464239\n",
      "loss\n",
      "0.004577396437525749\n",
      "loss\n",
      "0.0031294680666178465\n",
      "loss\n",
      "0.0011528043542057276\n",
      "loss\n",
      "0.008339227177202702\n",
      "loss\n",
      "0.0013830630341544747\n",
      "loss\n",
      "0.0027998790610581636\n",
      "loss\n",
      "0.001560185570269823\n",
      "loss\n",
      "0.0018848287872970104\n",
      "loss\n",
      "0.030961912125349045\n",
      "loss\n",
      "0.0022762122098356485\n",
      "loss\n",
      "0.003579401643946767\n",
      "loss\n",
      "0.0034686895087361336\n",
      "loss\n",
      "0.002000237349420786\n",
      "loss\n",
      "0.001800346071831882\n",
      "loss\n",
      "0.0035957936197519302\n",
      "loss\n",
      "0.014768090099096298\n",
      "loss\n",
      "0.0046996138989925385\n",
      "loss\n",
      "0.0067700534127652645\n",
      "loss\n",
      "0.0009396428358741105\n",
      "loss\n",
      "0.0017360866768285632\n",
      "loss\n",
      "0.001462581567466259\n",
      "loss\n",
      "0.0004752936656586826\n",
      "loss\n",
      "0.0007898071780800819\n",
      "loss\n",
      "0.0008774721063673496\n",
      "loss\n",
      "0.0011689979583024979\n",
      "loss\n",
      "0.004408997017890215\n",
      "loss\n",
      "0.01529852394014597\n",
      "loss\n",
      "0.0017256144201382995\n",
      "loss\n",
      "0.0005324853118509054\n",
      "loss\n",
      "0.00284944917075336\n",
      "loss\n",
      "0.3101171851158142\n",
      "loss\n",
      "0.0016018429305404425\n",
      "loss\n",
      "0.0008465044084005058\n",
      "loss\n",
      "0.00684417225420475\n",
      "loss\n",
      "0.0010065733222290874\n",
      "loss\n",
      "0.001503290724940598\n",
      "loss\n",
      "0.01349575724452734\n",
      "loss\n",
      "0.0005980133428238332\n",
      "loss\n",
      "0.005173865240067244\n",
      "loss\n",
      "0.0010562323732301593\n",
      "loss\n",
      "0.04587842524051666\n",
      "loss\n",
      "0.001095886342227459\n",
      "loss\n",
      "0.0011753087164834142\n",
      "loss\n",
      "0.000433112756581977\n",
      "loss\n",
      "0.0035584955476224422\n",
      "loss\n",
      "0.03456712141633034\n",
      "loss\n",
      "0.01112897228449583\n",
      "loss\n",
      "0.19578249752521515\n",
      "loss\n",
      "0.0008631794480606914\n",
      "loss\n",
      "0.0010082405060529709\n",
      "loss\n",
      "0.0028364923782646656\n",
      "loss\n",
      "0.0018275955226272345\n",
      "loss\n",
      "0.0023895539343357086\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.00041214076918549836\n",
      "loss\n",
      "0.0006101653561927378\n",
      "loss\n",
      "0.0008522216230630875\n",
      "loss\n",
      "0.0012179345358163118\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0016235039802268147\n",
      "loss\n",
      "0.0009777533123269677\n",
      "loss\n",
      "0.001444011926651001\n",
      "loss\n",
      "0.0035663354210555553\n",
      "loss\n",
      "0.01225253939628601\n",
      "loss\n",
      "0.000615407363511622\n",
      "loss\n",
      "0.002785851713269949\n",
      "loss\n",
      "0.0010449193650856614\n",
      "loss\n",
      "0.0018184330547228456\n",
      "loss\n",
      "0.0007669368060305715\n",
      "loss\n",
      "0.0027184458449482918\n",
      "loss\n",
      "0.001904699020087719\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "0.0010415849974378943\n",
      "loss\n",
      "0.000568228424526751\n",
      "loss\n",
      "0.0031387372873723507\n",
      "loss\n",
      "0.00719101307913661\n",
      "loss\n",
      "0.0006353028584271669\n",
      "loss\n",
      "0.000486970558995381\n",
      "loss\n",
      "0.0007688426994718611\n",
      "loss\n",
      "0.0007824220228940248\n",
      "loss\n",
      "0.0013804440386593342\n",
      "loss\n",
      "0.0009111781837418675\n",
      "loss\n",
      "0.0014972201315686107\n",
      "loss\n",
      "0.0006042085005901754\n",
      "loss\n",
      "0.0012735360069200397\n",
      "loss\n",
      "0.0013263961300253868\n",
      "loss\n",
      "0.0005981324939057231\n",
      "loss\n",
      "0.0006130246329121292\n",
      "loss\n",
      "0.0023889592848718166\n",
      "loss\n",
      "0.0011750705307349563\n",
      "loss\n",
      "0.0005891970940865576\n",
      "loss\n",
      "0.0004895919119007885\n",
      "loss\n",
      "0.0005679901223629713\n",
      "loss\n",
      "0.5016879439353943\n",
      "loss\n",
      "0.0017500099493190646\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.0005539313424378633\n",
      "loss\n",
      "0.003807914676144719\n",
      "loss\n",
      "0.028101855888962746\n",
      "loss\n",
      "0.0008391196606680751\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.00046433156239800155\n",
      "loss\n",
      "0.1355494260787964\n",
      "loss\n",
      "0.000809818331617862\n",
      "loss\n",
      "0.005297314375638962\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.0007186928996816278\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.0024447336327284575\n",
      "loss\n",
      "0.0013624681159853935\n",
      "loss\n",
      "0.000847933697514236\n",
      "loss\n",
      "0.0033197076991200447\n",
      "loss\n",
      "0.0002531684876885265\n",
      "loss\n",
      "0.0006457865820266306\n",
      "loss\n",
      "0.00021288513380568475\n",
      "loss\n",
      "0.0028309053741395473\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "5.774600982666016\n",
      "loss\n",
      "0.0007955246837809682\n",
      "loss\n",
      "0.0029955299105495214\n",
      "loss\n",
      "1.699110507965088\n",
      "loss\n",
      "0.07665915042161942\n",
      "loss\n",
      "0.0006833125371485949\n",
      "loss\n",
      "0.0008222059695981443\n",
      "loss\n",
      "0.001655756845138967\n",
      "loss\n",
      "0.000842692912556231\n",
      "loss\n",
      "0.001057184999808669\n",
      "loss\n",
      "0.0017527469899505377\n",
      "loss\n",
      "0.0015608996618539095\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001906721736304462\n",
      "loss\n",
      "0.0029006809927523136\n",
      "loss\n",
      "0.0015824426664039493\n",
      "loss\n",
      "0.006690955720841885\n",
      "loss\n",
      "0.026949666440486908\n",
      "loss\n",
      "0.005086457822471857\n",
      "loss\n",
      "0.0062572285532951355\n",
      "loss\n",
      "0.0013733012601733208\n",
      "loss\n",
      "0.015596204437315464\n",
      "loss\n",
      "0.004771632142364979\n",
      "loss\n",
      "0.010210777632892132\n",
      "loss\n",
      "0.0018556771101430058\n",
      "loss\n",
      "0.0012656782055273652\n",
      "loss\n",
      "0.001777260797098279\n",
      "loss\n",
      "0.0009158230968751013\n",
      "loss\n",
      "0.00970514491200447\n",
      "loss\n",
      "0.0015851801726967096\n",
      "loss\n",
      "0.007525546941906214\n",
      "loss\n",
      "0.0019395602867007256\n",
      "loss\n",
      "0.14365465939044952\n",
      "loss\n",
      "0.006396060809493065\n",
      "loss\n",
      "0.0021193204447627068\n",
      "loss\n",
      "0.007879721000790596\n",
      "loss\n",
      "0.03223589062690735\n",
      "loss\n",
      "0.0014585343888029456\n",
      "loss\n",
      "0.12759247422218323\n",
      "loss\n",
      "0.0017432268941774964\n",
      "loss\n",
      "0.0009039129945449531\n",
      "loss\n",
      "0.02098698727786541\n",
      "loss\n",
      "0.0011002921964973211\n",
      "loss\n",
      "0.015343839302659035\n",
      "loss\n",
      "0.0017241863533854485\n",
      "loss\n",
      "0.012848097831010818\n",
      "loss\n",
      "0.016196461394429207\n",
      "loss\n",
      "0.007988644763827324\n",
      "loss\n",
      "0.0008741371566429734\n",
      "loss\n",
      "0.0033289750572293997\n",
      "loss\n",
      "0.002584690460935235\n",
      "loss\n",
      "0.3376161456108093\n",
      "loss\n",
      "0.17622306942939758\n",
      "loss\n",
      "0.005285219289362431\n",
      "loss\n",
      "0.0030551922973245382\n",
      "loss\n",
      "0.0048184944316744804\n",
      "loss\n",
      "0.004540016409009695\n",
      "loss\n",
      "0.004049673210829496\n",
      "loss\n",
      "0.004293984733521938\n",
      "loss\n",
      "0.0020384264644235373\n",
      "loss\n",
      "0.6701012849807739\n",
      "loss\n",
      "0.006541150622069836\n",
      "loss\n",
      "0.0017422748496755958\n",
      "loss\n",
      "0.03246106579899788\n",
      "loss\n",
      "0.0052717006765306\n",
      "loss\n",
      "0.005346760619431734\n",
      "loss\n",
      "0.0010355116100981832\n",
      "loss\n",
      "0.0005392765742726624\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0026057357899844646\n",
      "loss\n",
      "0.003418674925342202\n",
      "loss\n",
      "0.0008293526479974389\n",
      "loss\n",
      "0.009370513260364532\n",
      "loss\n",
      "0.0006006343755871058\n",
      "loss\n",
      "0.003137667663395405\n",
      "loss\n",
      "0.0011179156135767698\n",
      "loss\n",
      "0.01869744248688221\n",
      "loss\n",
      "0.0004447901446837932\n",
      "loss\n",
      "0.001640761154703796\n",
      "loss\n",
      "0.0004435985756572336\n",
      "loss\n",
      "0.004437480587512255\n",
      "loss\n",
      "0.0004259632551111281\n",
      "loss\n",
      "0.02258472703397274\n",
      "loss\n",
      "0.0006771179032512009\n",
      "loss\n",
      "0.0015863704029470682\n",
      "loss\n",
      "0.0030540036968886852\n",
      "loss\n",
      "0.0007546676206402481\n",
      "loss\n",
      "0.002878334140405059\n",
      "loss\n",
      "0.0005322470096871257\n",
      "loss\n",
      "0.026510940864682198\n",
      "loss\n",
      "0.009304258041083813\n",
      "loss\n",
      "0.021776961162686348\n",
      "loss\n",
      "0.001404728856869042\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0008524598088115454\n",
      "loss\n",
      "0.0027340196538716555\n",
      "loss\n",
      "0.002498839981853962\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.00899465661495924\n",
      "loss\n",
      "0.0012816318776458502\n",
      "loss\n",
      "0.001966211013495922\n",
      "loss\n",
      "0.0018600797047838569\n",
      "loss\n",
      "0.0019061268540099263\n",
      "loss\n",
      "0.0014174662064760923\n",
      "loss\n",
      "0.003535688389092684\n",
      "loss\n",
      "0.0014075858052819967\n",
      "loss\n",
      "0.0005794276366941631\n",
      "loss\n",
      "0.0028871302492916584\n",
      "loss\n",
      "0.0061088986694812775\n",
      "loss\n",
      "0.00042572495294734836\n",
      "loss\n",
      "0.000428108120104298\n",
      "loss\n",
      "0.0028246049769222736\n",
      "loss\n",
      "0.007277645170688629\n",
      "loss\n",
      "0.0010798105504363775\n",
      "loss\n",
      "0.0030859727412462234\n",
      "loss\n",
      "0.0006139777251519263\n",
      "loss\n",
      "0.0005782362422905862\n",
      "loss\n",
      "0.010593726299703121\n",
      "loss\n",
      "0.0012159105390310287\n",
      "loss\n",
      "0.0005053196800872684\n",
      "loss\n",
      "0.001134705264121294\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000846266164444387\n",
      "loss\n",
      "0.0016955060418695211\n",
      "loss\n",
      "0.001279369811527431\n",
      "loss\n",
      "0.00779361417517066\n",
      "loss\n",
      "0.0016464737709611654\n",
      "loss\n",
      "0.003351786872372031\n",
      "loss\n",
      "0.005628214683383703\n",
      "loss\n",
      "0.0035672858357429504\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.0006885541952215135\n",
      "loss\n",
      "0.0004690977220889181\n",
      "loss\n",
      "0.0005258131423033774\n",
      "loss\n",
      "0.0017419178038835526\n",
      "loss\n",
      "0.001593511551618576\n",
      "loss\n",
      "0.0007316772826015949\n",
      "loss\n",
      "0.000707971747033298\n",
      "loss\n",
      "0.0006911749369464815\n",
      "loss\n",
      "0.002665896899998188\n",
      "loss\n",
      "0.0006254147156141698\n",
      "loss\n",
      "0.0015022194711491466\n",
      "loss\n",
      "0.009392124600708485\n",
      "loss\n",
      "0.012946008704602718\n",
      "loss\n",
      "0.0010157431242987514\n",
      "loss\n",
      "0.0014280608156695962\n",
      "loss\n",
      "0.003851853543892503\n",
      "loss\n",
      "0.000559173640795052\n",
      "loss\n",
      "0.0003511289251036942\n",
      "loss\n",
      "0.004013104364275932\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.030644262209534645\n",
      "loss\n",
      "0.0011582816950976849\n",
      "loss\n",
      "0.0014397265622392297\n",
      "loss\n",
      "0.0021531034726649523\n",
      "loss\n",
      "0.001191621064208448\n",
      "loss\n",
      "0.0016000575851649046\n",
      "loss\n",
      "0.0017477489309385419\n",
      "loss\n",
      "0.008792140521109104\n",
      "loss\n",
      "0.0028020190075039864\n",
      "loss\n",
      "0.0006766413571313024\n",
      "loss\n",
      "0.0018878034316003323\n",
      "loss\n",
      "0.0017471539322286844\n",
      "loss\n",
      "0.001256510615348816\n",
      "loss\n",
      "0.001255677198059857\n",
      "loss\n",
      "0.0038440159987658262\n",
      "loss\n",
      "0.0017959432443603873\n",
      "loss\n",
      "0.0016683719586580992\n",
      "loss\n",
      "0.0008314966107718647\n",
      "loss\n",
      "0.0022165034897625446\n",
      "loss\n",
      "0.0023671959061175585\n",
      "loss\n",
      "0.0002982171718031168\n",
      "loss\n",
      "0.014729914255440235\n",
      "loss\n",
      "0.0027272433508187532\n",
      "loss\n",
      "0.0008842610404826701\n",
      "loss\n",
      "0.0004936429904773831\n",
      "loss\n",
      "0.0007914748275652528\n",
      "loss\n",
      "0.0013392536202445626\n",
      "loss\n",
      "0.0038484097458422184\n",
      "loss\n",
      "0.0009913297835737467\n",
      "loss\n",
      "0.03474237769842148\n",
      "loss\n",
      "0.0005641775787808001\n",
      "loss\n",
      "0.0003849719068966806\n",
      "loss\n",
      "0.0013365155318751931\n",
      "loss\n",
      "0.0008220868767239153\n",
      "loss\n",
      "0.0014574630185961723\n",
      "loss\n",
      "0.0016183863626793027\n",
      "loss\n",
      "0.00040356122190132737\n",
      "loss\n",
      "0.0005911033367738128\n",
      "loss\n",
      "0.0027939353603869677\n",
      "loss\n",
      "0.00039641151670366526\n",
      "loss\n",
      "0.0027140469755977392\n",
      "loss\n",
      "0.0018346159486100078\n",
      "loss\n",
      "0.002303329762071371\n",
      "loss\n",
      "0.0006030171643942595\n",
      "loss\n",
      "0.009124013595283031\n",
      "loss\n",
      "0.0012161486083641648\n",
      "loss\n",
      "0.0027712297160178423\n",
      "loss\n",
      "0.0002739054325502366\n",
      "loss\n",
      "0.0006935574929229915\n",
      "loss\n",
      "0.0003129946126136929\n",
      "loss\n",
      "0.000476246903417632\n",
      "loss\n",
      "0.00044288364006206393\n",
      "loss\n",
      "0.0018682897789403796\n",
      "loss\n",
      "0.0003418338019400835\n",
      "loss\n",
      "0.0012497241841629148\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0023768290411680937\n",
      "loss\n",
      "0.02574142999947071\n",
      "loss\n",
      "0.0017401328077539802\n",
      "loss\n",
      "0.004134322516620159\n",
      "loss\n",
      "0.0008989107445813715\n",
      "loss\n",
      "0.0005607224884442985\n",
      "loss\n",
      "0.001046705641783774\n",
      "loss\n",
      "0.0007827793597243726\n",
      "loss\n",
      "0.0006280356901697814\n",
      "loss\n",
      "0.02786920592188835\n",
      "loss\n",
      "0.0008538890979252756\n",
      "loss\n",
      "0.0010851691477000713\n",
      "loss\n",
      "0.00240013818256557\n",
      "loss\n",
      "0.0021174170542508364\n",
      "loss\n",
      "0.002991488901898265\n",
      "loss\n",
      "0.0014588914345949888\n",
      "loss\n",
      "0.004241281189024448\n",
      "loss\n",
      "0.003736658487468958\n",
      "loss\n",
      "0.0022800182923674583\n",
      "loss\n",
      "0.0007152383332140744\n",
      "loss\n",
      "0.000924993772059679\n",
      "loss\n",
      "0.003994819708168507\n",
      "loss\n",
      "0.000894146622158587\n",
      "loss\n",
      "0.0027646913658827543\n",
      "loss\n",
      "0.0007594323833473027\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.0012949660886079073\n",
      "loss\n",
      "0.002144657773897052\n",
      "loss\n",
      "0.0016444505890831351\n",
      "loss\n",
      "0.0005864569102413952\n",
      "loss\n",
      "0.00039188333903439343\n",
      "loss\n",
      "0.0029439465142786503\n",
      "loss\n",
      "0.0022638426162302494\n",
      "loss\n",
      "0.0009483369067311287\n",
      "loss\n",
      "0.0116086695343256\n",
      "loss\n",
      "0.0008537700050510466\n",
      "loss\n",
      "0.001016814960166812\n",
      "loss\n",
      "0.0008986725588329136\n",
      "loss\n",
      "0.000701658078469336\n",
      "loss\n",
      "0.0017089537577703595\n",
      "loss\n",
      "0.0011794761521741748\n",
      "loss\n",
      "0.0004247716860845685\n",
      "loss\n",
      "0.0005683475756086409\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0010266992030665278\n",
      "loss\n",
      "0.0019143365789204836\n",
      "loss\n",
      "0.0005214046686887741\n",
      "loss\n",
      "0.001148994080722332\n",
      "loss\n",
      "0.004422408062964678\n",
      "loss\n",
      "0.0005122303264215589\n",
      "loss\n",
      "0.0011468507582321763\n",
      "loss\n",
      "0.00022957073815632612\n",
      "loss\n",
      "0.0027576773427426815\n",
      "loss\n",
      "0.000952267087996006\n",
      "loss\n",
      "0.016635630279779434\n",
      "loss\n",
      "0.0007250064518302679\n",
      "loss\n",
      "0.0006623458466492593\n",
      "loss\n",
      "0.0007408496458083391\n",
      "loss\n",
      "0.002483262214809656\n",
      "loss\n",
      "0.0021470370702445507\n",
      "loss\n",
      "0.0009137984015978873\n",
      "loss\n",
      "0.00023457636416424066\n",
      "loss\n",
      "0.0023837266489863396\n",
      "loss\n",
      "0.0011623300379142165\n",
      "loss\n",
      "0.0013184197014197707\n",
      "loss\n",
      "0.001476865611039102\n",
      "loss\n",
      "0.003979858942329884\n",
      "loss\n",
      "0.0005212855176068842\n",
      "loss\n",
      "0.0005154472892172635\n",
      "loss\n",
      "0.00042965717148035765\n",
      "loss\n",
      "0.000764792668633163\n",
      "loss\n",
      "0.0009112972766160965\n",
      "loss\n",
      "0.0008796160109341145\n",
      "loss\n",
      "0.0008474572678096592\n",
      "loss\n",
      "0.0005744237569160759\n",
      "loss\n",
      "0.0019026764202862978\n",
      "loss\n",
      "0.002003092784434557\n",
      "loss\n",
      "0.42928773164749146\n",
      "loss\n",
      "0.0008839037618599832\n",
      "loss\n",
      "0.001987983239814639\n",
      "loss\n",
      "0.014038268476724625\n",
      "loss\n",
      "0.0011518517276272178\n",
      "loss\n",
      "0.00032658010604791343\n",
      "loss\n",
      "0.0008891443139873445\n",
      "loss\n",
      "0.000750736624468118\n",
      "loss\n",
      "0.013025554828345776\n",
      "loss\n",
      "0.004036019556224346\n",
      "loss\n",
      "0.005081120412796736\n",
      "loss\n",
      "0.0002636561985127628\n",
      "loss\n",
      "0.003246871754527092\n",
      "loss\n",
      "0.00031418632715940475\n",
      "loss\n",
      "0.005515950731933117\n",
      "loss\n",
      "0.0016540905926376581\n",
      "loss\n",
      "0.00022194306075107306\n",
      "loss\n",
      "0.0026214304380118847\n",
      "loss\n",
      "0.016800694167613983\n",
      "loss\n",
      "0.004386090207844973\n",
      "loss\n",
      "0.0003225283289793879\n",
      "loss\n",
      "0.000758955895435065\n",
      "loss\n",
      "0.00035613393993116915\n",
      "loss\n",
      "0.00037448544753715396\n",
      "loss\n",
      "0.0017446548445150256\n",
      "loss\n",
      "0.00015090756642166525\n",
      "loss\n",
      "0.0002574589161667973\n",
      "loss\n",
      "0.0007256020326167345\n",
      "loss\n",
      "0.001990600721910596\n",
      "loss\n",
      "0.00012468514614738524\n",
      "loss\n",
      "0.0026844439562410116\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.000788258679676801\n",
      "loss\n",
      "0.00028796817059628665\n",
      "loss\n",
      "0.0013338964199647307\n",
      "loss\n",
      "0.00020621081057470292\n",
      "loss\n",
      "0.05171101912856102\n",
      "loss\n",
      "0.0030126445926725864\n",
      "loss\n",
      "0.005566809326410294\n",
      "loss\n",
      "0.00016234986833296716\n",
      "loss\n",
      "0.00017736769223120064\n",
      "loss\n",
      "0.0002821285743266344\n",
      "loss\n",
      "0.0006914132391102612\n",
      "loss\n",
      "0.0004648081958293915\n",
      "loss\n",
      "0.0008348317351192236\n",
      "loss\n",
      "0.0019875073339790106\n",
      "loss\n",
      "0.0002169373765354976\n",
      "loss\n",
      "0.0012260308722034097\n",
      "loss\n",
      "0.00018523407925385982\n",
      "loss\n",
      "0.00041571559268049896\n",
      "loss\n",
      "0.0006481691962108016\n",
      "loss\n",
      "0.0010397987207397819\n",
      "loss\n",
      "0.0006696127820760012\n",
      "loss\n",
      "0.011480340734124184\n",
      "loss\n",
      "0.0003594706067815423\n",
      "loss\n",
      "0.0024949158541858196\n",
      "loss\n",
      "0.011587812565267086\n",
      "loss\n",
      "0.0006804534932598472\n",
      "loss\n",
      "0.0003280101518612355\n",
      "loss\n",
      "0.0011869773734360933\n",
      "loss\n",
      "0.0009833505610004067\n",
      "loss\n",
      "0.00016330339713022113\n",
      "loss\n",
      "0.0008563903393223882\n",
      "loss\n",
      "0.0004122599493712187\n",
      "loss\n",
      "0.0005706112715415657\n",
      "loss\n",
      "0.003935806918889284\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0002915434306487441\n",
      "loss\n",
      "0.00020215852418914437\n",
      "loss\n",
      "0.00126008247025311\n",
      "loss\n",
      "0.0009531007381156087\n",
      "loss\n",
      "0.00030632095877081156\n",
      "loss\n",
      "0.0005411829333752394\n"
     ]
    }
   ],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "for i in range(100):\n",
    "    for seqs, seqs_len in train_dls[2]:\n",
    "\n",
    "        gating.train()\n",
    "\n",
    "        gating_optimizer.zero_grad()\n",
    "\n",
    "        outputs = gating(seqs, seqs_len)\n",
    "\n",
    "        #print(\"seq\")\n",
    "        #print(seqs)\n",
    "        #print(\"outputs\")\n",
    "        #print(outputs)\n",
    "\n",
    "        if seqs[1] == 7:\n",
    "            trgts = torch.tensor([0])\n",
    "        else:\n",
    "            trgts = torch.tensor([1])\n",
    "\n",
    "        loss = compute_loss(outputs, trgts, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        gating_optimizer.step()\n",
    "\n",
    "        print(\"loss\")\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ONMUcvfWeipD"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                        reduction=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcqtDHZcJah8",
    "outputId": "a48e44cc-67ab-474d-8e5b-5267ec501dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "gating, gating_optimizer = init_gating()\n",
    "model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "print(model.apply(init_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "UYduyRNYJolw"
   },
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARkJQvgNgU27",
    "outputId": "0012ec60-7122-4876-8fb5-5a863be5f25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 5, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 7, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 2])\n",
      "1 - tensor([1, 5, 3, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kO7XbXIIgjWN",
    "outputId": "67eb26e2-95b1-4844-e801-752078589da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 7, 4, 6, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 2])\n",
      "1 - tensor([1, 7, 3, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "XQStSnwP-bwf"
   },
   "outputs": [],
   "source": [
    "gating_optimizer = optim.Adam(model.gating.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xtz-JxQ--n-U",
    "outputId": "a4164e4b-10f8-4a69-acb9-d454881779b5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3362e-14,\n",
      "          0.0000e+00,  0.0000e+00, -1.0627e-14,  1.2231e-14,  0.0000e+00],\n",
      "        [ 9.5526e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5729e-09,\n",
      "         -8.0322e-09,  0.0000e+00, -1.3070e-08,  2.0716e-08,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.2082e-14, -4.8358e-12,  6.6748e-12, -6.2617e-12,\n",
      "         -5.3810e-12,  7.6332e-12, -2.3784e-12,  0.0000e+00, -7.3044e-12],\n",
      "        [-4.1353e-13, -4.9267e-13, -3.5623e-13,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  5.5728e-13,  0.0000e+00,  4.3358e-13, -5.3819e-13],\n",
      "        [ 0.0000e+00,  0.0000e+00, -8.9321e-11,  5.7395e-14,  0.0000e+00,\n",
      "         -1.0017e-10,  1.4144e-10,  0.0000e+00,  5.0259e-14, -6.3196e-14],\n",
      "        [ 0.0000e+00, -1.0079e-14,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -8.3384e-15,  1.0995e-14, -6.3370e-15,  0.0000e+00, -1.1091e-14]])\n",
      "rnn.weight_ih_l0\n",
      "tensor([[-1.2741e-11, -4.1343e-15, -1.2010e-12,  5.5348e-14, -6.1908e-12,\n",
      "         -2.3396e-11,  1.1962e-12,  1.5775e-12,  1.0668e-11, -6.6139e-14],\n",
      "        [-1.7986e-11, -6.1930e-15, -1.7422e-12,  8.5238e-14, -8.7462e-12,\n",
      "         -3.3065e-11,  1.7365e-12,  2.2192e-12,  1.5059e-11, -1.0169e-13],\n",
      "        [-1.7355e-11, -5.7870e-15, -1.6031e-12,  7.9681e-14, -8.4371e-12,\n",
      "         -3.1844e-11,  1.5982e-12,  2.1442e-12,  1.4531e-11, -9.5055e-14],\n",
      "        [-1.8238e-11, -6.5488e-15, -1.8412e-12,  8.8614e-14, -8.8711e-12,\n",
      "         -3.3589e-11,  1.8348e-12,  2.2480e-12,  1.5270e-11, -1.0583e-13],\n",
      "        [-9.1758e-12, -2.9753e-15, -9.0931e-13,  3.4315e-14, -4.4529e-12,\n",
      "         -1.6883e-11,  9.0369e-13,  1.1421e-12,  7.6826e-12, -4.1435e-14],\n",
      "        [-1.2727e-17, -2.0106e-19, -7.9940e-19,  1.0168e-20, -3.3603e-17,\n",
      "         -2.3203e-17,  1.0065e-18, -1.8089e-17,  2.2752e-17, -2.1822e-19],\n",
      "        [-2.1118e-11, -6.7207e-15, -1.9434e-12,  9.6933e-14, -1.0267e-11,\n",
      "         -3.8743e-11,  1.9376e-12,  2.6091e-12,  1.7681e-11, -1.1530e-13],\n",
      "        [-4.2235e-12, -1.5118e-15, -4.4800e-13,  1.6007e-14, -2.0497e-12,\n",
      "         -7.7942e-12,  4.4502e-13,  5.2544e-13,  3.5362e-12, -1.9453e-14],\n",
      "        [-1.2104e-11, -4.2045e-15, -1.2392e-12,  4.8817e-14, -5.8774e-12,\n",
      "         -2.2303e-11,  1.2321e-12,  1.5028e-12,  1.0134e-11, -5.8929e-14],\n",
      "        [-4.7408e-12, -2.2792e-15, -5.7470e-13,  2.1609e-14, -2.3041e-12,\n",
      "         -8.8055e-12,  5.7121e-13,  5.8587e-13,  3.9693e-12, -2.6509e-14],\n",
      "        [ 6.6286e-13,  7.3984e-16, -8.5020e-14, -1.5201e-15,  3.2065e-13,\n",
      "          1.0991e-12,  8.3078e-14, -8.3417e-14, -5.5514e-13,  2.3342e-15],\n",
      "        [ 8.7248e-13,  8.9356e-16, -1.1475e-13, -1.8182e-15,  4.2183e-13,\n",
      "          1.4442e-12,  1.1224e-13, -1.1005e-13, -7.3065e-13,  2.8197e-15],\n",
      "        [ 8.4736e-13,  8.5097e-16, -1.0343e-13, -1.5827e-15,  4.0949e-13,\n",
      "          1.4090e-12,  1.0117e-13, -1.0709e-13, -7.0961e-13,  2.5146e-15],\n",
      "        [ 1.0759e-12,  9.2496e-16, -1.3016e-13, -2.1993e-15,  5.2013e-13,\n",
      "          1.7902e-12,  1.2728e-13, -1.3580e-13, -9.0093e-13,  3.2712e-15],\n",
      "        [ 5.6700e-13,  8.6599e-16, -8.4782e-14, -2.0148e-15,  2.7489e-13,\n",
      "          9.3052e-13,  8.2685e-14, -7.0587e-14, -4.7490e-13,  2.9716e-15],\n",
      "        [ 3.2364e-17,  1.4174e-16,  3.2161e-17, -3.8775e-17,  2.1664e-16,\n",
      "          1.3156e-16, -1.3289e-16,  2.2662e-16, -1.2378e-16,  1.6023e-16],\n",
      "        [ 1.1006e-12,  8.1159e-16, -1.1776e-13, -1.7806e-15,  5.3170e-13,\n",
      "          1.8437e-12,  1.1522e-13, -1.3944e-13, -9.2167e-13,  2.7080e-15],\n",
      "        [ 3.1208e-13,  9.5127e-16, -5.4686e-14, -1.5326e-15,  1.5156e-13,\n",
      "          5.0549e-13,  5.3179e-14, -3.8384e-14, -2.6151e-13,  2.4646e-15],\n",
      "        [ 6.8898e-13,  1.1269e-15, -1.1249e-13, -2.6129e-15,  3.3411e-13,\n",
      "          1.1229e-12,  1.0977e-13, -8.5628e-14, -5.7703e-13,  3.8803e-15],\n",
      "        [ 4.0820e-13,  1.7776e-15, -8.1602e-14, -2.9815e-15,  1.9877e-13,\n",
      "          6.5251e-13,  7.9180e-14, -4.9381e-14, -3.4215e-13,  4.7378e-15],\n",
      "        [-1.1690e-10, -4.0569e-14, -9.9090e-12,  5.7459e-13, -5.6869e-11,\n",
      "         -2.1379e-10,  9.9007e-12,  1.4401e-11,  9.7877e-11, -6.8428e-13],\n",
      "        [ 1.5179e-10,  5.5814e-14,  1.3358e-11, -8.0338e-13,  7.3899e-11,\n",
      "          2.7800e-10, -1.3354e-11, -1.8637e-11, -1.2709e-10,  9.5584e-13],\n",
      "        [ 1.4691e-10,  5.1973e-14,  1.2321e-11, -7.5753e-13,  7.1503e-11,\n",
      "          2.6858e-10, -1.2322e-11, -1.8059e-11, -1.2300e-10,  9.0062e-13],\n",
      "        [ 1.5032e-10,  5.7241e-14,  1.3668e-11, -8.0311e-13,  7.3187e-11,\n",
      "          2.7565e-10, -1.3659e-11, -1.8447e-11, -1.2585e-10,  9.5701e-13],\n",
      "        [ 7.5015e-11,  2.6564e-14,  6.6023e-12, -3.2660e-13,  3.6450e-11,\n",
      "          1.3737e-10, -6.5820e-12, -9.2864e-12, -6.2807e-11,  3.9260e-13],\n",
      "        [-4.9563e-15, -4.4676e-18, -2.3274e-16,  6.1259e-18, -2.7919e-15,\n",
      "         -8.9162e-15,  2.3483e-16,  3.4348e-16,  4.3254e-15, -1.1415e-17],\n",
      "        [-1.7836e-10, -5.9934e-14, -1.4637e-11,  8.8339e-13, -8.6773e-11,\n",
      "         -3.2581e-10,  1.4634e-11,  2.1964e-11,  1.4933e-10, -1.0495e-12],\n",
      "        [-3.8141e-11, -1.4796e-14, -3.5249e-12,  1.7314e-13, -1.8539e-11,\n",
      "         -6.9977e-11,  3.5140e-12,  4.7135e-12,  3.1934e-11, -2.0884e-13],\n",
      "        [ 9.0623e-11,  3.4656e-14,  8.3105e-12, -4.2677e-13,  4.4064e-11,\n",
      "          1.6622e-10, -8.2893e-12, -1.1184e-11, -7.5875e-11,  5.1302e-13],\n",
      "        [-3.8155e-11, -2.0677e-14, -4.1315e-12,  2.1317e-13, -1.8584e-11,\n",
      "         -7.0482e-11,  4.1218e-12,  4.6718e-12,  3.1946e-11, -2.5969e-13]])\n",
      "rnn.weight_hh_l0\n",
      "tensor([[ 3.2406e-11, -3.2207e-11, -3.2240e-11, -3.2168e-11, -3.2604e-11,\n",
      "          3.2936e-11,  3.2080e-11,  3.2774e-11, -3.2534e-11,  3.2767e-11],\n",
      "        [ 4.5892e-11, -4.5609e-11, -4.5657e-11, -4.5555e-11, -4.6172e-11,\n",
      "          4.6641e-11,  4.5431e-11,  4.6412e-11, -4.6072e-11,  4.6402e-11],\n",
      "        [ 4.4040e-11, -4.3768e-11, -4.3814e-11, -4.3715e-11, -4.4309e-11,\n",
      "          4.4760e-11,  4.3596e-11,  4.4539e-11, -4.4213e-11,  4.4530e-11],\n",
      "        [ 4.6769e-11, -4.6481e-11, -4.6530e-11, -4.6426e-11, -4.7053e-11,\n",
      "          4.7530e-11,  4.6300e-11,  4.7297e-11, -4.6952e-11,  4.7287e-11],\n",
      "        [ 2.3477e-11, -2.3332e-11, -2.3357e-11, -2.3304e-11, -2.3620e-11,\n",
      "          2.3859e-11,  2.3241e-11,  2.3742e-11, -2.3569e-11,  2.3737e-11],\n",
      "        [ 3.1307e-17, -3.1110e-17, -3.1140e-17, -3.1068e-17, -3.1490e-17,\n",
      "          3.1876e-17,  3.0985e-17,  3.1657e-17, -3.1424e-17,  3.1647e-17],\n",
      "        [ 5.3566e-11, -5.3236e-11, -5.3291e-11, -5.3171e-11, -5.3893e-11,\n",
      "          5.4443e-11,  5.3027e-11,  5.4174e-11, -5.3777e-11,  5.4162e-11],\n",
      "        [ 1.0898e-11, -1.0831e-11, -1.0842e-11, -1.0818e-11, -1.0964e-11,\n",
      "          1.1075e-11,  1.0789e-11,  1.1021e-11, -1.0940e-11,  1.1018e-11],\n",
      "        [ 3.1093e-11, -3.0902e-11, -3.0934e-11, -3.0865e-11, -3.1282e-11,\n",
      "          3.1599e-11,  3.0781e-11,  3.1444e-11, -3.1215e-11,  3.1437e-11],\n",
      "        [ 1.2456e-11, -1.2380e-11, -1.2393e-11, -1.2366e-11, -1.2531e-11,\n",
      "          1.2657e-11,  1.2332e-11,  1.2595e-11, -1.2505e-11,  1.2593e-11],\n",
      "        [-1.2247e-12,  1.2160e-12,  1.2177e-12,  1.2142e-12,  1.2331e-12,\n",
      "         -1.2481e-12, -1.2107e-12, -1.2407e-12,  1.2299e-12, -1.2403e-12],\n",
      "        [-1.6030e-12,  1.5915e-12,  1.5938e-12,  1.5892e-12,  1.6139e-12,\n",
      "         -1.6337e-12, -1.5846e-12, -1.6239e-12,  1.6098e-12, -1.6235e-12],\n",
      "        [-1.5819e-12,  1.5707e-12,  1.5729e-12,  1.5684e-12,  1.5927e-12,\n",
      "         -1.6119e-12, -1.5639e-12, -1.6024e-12,  1.5886e-12, -1.6020e-12],\n",
      "        [-2.0123e-12,  1.9981e-12,  2.0009e-12,  1.9952e-12,  2.0260e-12,\n",
      "         -2.0505e-12, -1.9894e-12, -2.0384e-12,  2.0208e-12, -2.0378e-12],\n",
      "        [-1.0099e-12,  1.0026e-12,  1.0041e-12,  1.0011e-12,  1.0169e-12,\n",
      "         -1.0296e-12, -9.9817e-13, -1.0233e-12,  1.0142e-12, -1.0230e-12],\n",
      "        [-2.7163e-16,  2.6840e-16,  2.6669e-16,  2.6575e-16,  2.6862e-16,\n",
      "         -3.1099e-16, -2.6572e-16, -2.7043e-16,  2.6912e-16, -2.6736e-16],\n",
      "        [-2.1069e-12,  2.0921e-12,  2.0950e-12,  2.0891e-12,  2.1210e-12,\n",
      "         -2.1463e-12, -2.0831e-12, -2.1339e-12,  2.1157e-12, -2.1333e-12],\n",
      "        [-5.3080e-13,  5.2684e-13,  5.2764e-13,  5.2603e-13,  5.3453e-13,\n",
      "         -5.4151e-13, -5.2447e-13, -5.3800e-13,  5.3309e-13, -5.3782e-13],\n",
      "        [-1.1973e-12,  1.1885e-12,  1.1903e-12,  1.1867e-12,  1.2056e-12,\n",
      "         -1.2209e-12, -1.1832e-12, -1.2134e-12,  1.2024e-12, -1.2130e-12],\n",
      "        [-6.6248e-13,  6.5742e-13,  6.5846e-13,  6.5637e-13,  6.6724e-13,\n",
      "         -6.7629e-13, -6.5440e-13, -6.7170e-13,  6.6538e-13, -6.7146e-13],\n",
      "        [ 2.7702e-10, -2.7530e-10, -2.7560e-10, -2.7497e-10, -2.7872e-10,\n",
      "          2.8157e-10,  2.7422e-10,  2.8018e-10, -2.7811e-10,  2.8012e-10],\n",
      "        [-3.5715e-10,  3.5494e-10,  3.5531e-10,  3.5451e-10,  3.5933e-10,\n",
      "         -3.6301e-10, -3.5355e-10, -3.6121e-10,  3.5856e-10, -3.6113e-10],\n",
      "        [-3.4445e-10,  3.4232e-10,  3.4268e-10,  3.4190e-10,  3.4656e-10,\n",
      "         -3.5012e-10, -3.4097e-10, -3.4838e-10,  3.4581e-10, -3.4830e-10],\n",
      "        [-3.5395e-10,  3.5176e-10,  3.5213e-10,  3.5134e-10,  3.5611e-10,\n",
      "         -3.5975e-10, -3.5038e-10, -3.5797e-10,  3.5534e-10, -3.5789e-10],\n",
      "        [-1.7851e-10,  1.7741e-10,  1.7759e-10,  1.7719e-10,  1.7960e-10,\n",
      "         -1.8144e-10, -1.7671e-10, -1.8054e-10,  1.7921e-10, -1.8050e-10],\n",
      "        [ 1.1874e-14, -1.1799e-14, -1.1812e-14, -1.1784e-14, -1.1948e-14,\n",
      "          1.2074e-14,  1.1752e-14,  1.2012e-14, -1.1921e-14,  1.2009e-14],\n",
      "        [ 4.1527e-10, -4.1270e-10, -4.1313e-10, -4.1219e-10, -4.1782e-10,\n",
      "          4.2211e-10,  4.1107e-10,  4.2001e-10, -4.1691e-10,  4.1992e-10],\n",
      "        [ 9.2472e-11, -9.1901e-11, -9.1998e-11, -9.1790e-11, -9.3037e-11,\n",
      "          9.3986e-11,  9.1541e-11,  9.3522e-11, -9.2836e-11,  9.3501e-11],\n",
      "        [-2.1470e-10,  2.1337e-10,  2.1360e-10,  2.1312e-10,  2.1601e-10,\n",
      "         -2.1821e-10, -2.1254e-10, -2.1714e-10,  2.1554e-10, -2.1709e-10],\n",
      "        [ 9.3705e-11, -9.3130e-11, -9.3226e-11, -9.3019e-11, -9.4273e-11,\n",
      "          9.5226e-11,  9.2767e-11,  9.4760e-11, -9.4072e-11,  9.4740e-11]])\n",
      "rnn.bias_ih_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.9870e-10, -3.8939e-10, -3.7496e-10, -3.8699e-10, -1.9244e-10,\n",
      "         1.3200e-14,  4.5421e-10,  9.8371e-11, -2.3352e-10,  1.0031e-10])\n",
      "rnn.bias_hh_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.8158e-10, -3.6301e-10, -3.5012e-10, -3.5975e-10, -1.8144e-10,\n",
      "         1.2938e-14,  4.2211e-10,  9.3988e-11, -2.1822e-10,  9.5228e-11])\n",
      "rnn.weight_ih_l0_reverse\n",
      "tensor([[-8.4594e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.0747e-11,\n",
      "         -1.4896e-10,  0.0000e+00,  1.0874e-11,  7.0829e-11,  0.0000e+00],\n",
      "        [-2.9883e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4394e-11,\n",
      "         -5.2622e-11,  0.0000e+00,  3.8413e-12,  2.5020e-11,  0.0000e+00],\n",
      "        [-5.3683e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.5858e-12,\n",
      "         -9.4532e-12,  0.0000e+00,  6.9007e-13,  4.4948e-12,  0.0000e+00],\n",
      "        [-1.1488e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5334e-11,\n",
      "         -2.0229e-10,  0.0000e+00,  1.4767e-11,  9.6184e-11,  0.0000e+00],\n",
      "        [-1.3635e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.5675e-11,\n",
      "         -2.4010e-10,  0.0000e+00,  1.7527e-11,  1.1416e-10,  0.0000e+00],\n",
      "        [-1.1582e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5790e-12,\n",
      "         -2.0396e-11,  0.0000e+00,  1.4889e-12,  9.6977e-12,  0.0000e+00],\n",
      "        [-5.7277e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7589e-12,\n",
      "         -1.0086e-11,  0.0000e+00,  7.3627e-13,  4.7957e-12,  0.0000e+00],\n",
      "        [ 1.6456e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.9265e-12,\n",
      "          2.8978e-11,  0.0000e+00, -2.1153e-12, -1.3778e-11,  0.0000e+00],\n",
      "        [-9.6630e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.6545e-12,\n",
      "         -1.7016e-11,  0.0000e+00,  1.2421e-12,  8.0906e-12,  0.0000e+00],\n",
      "        [-4.0936e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9718e-11,\n",
      "         -7.2086e-11,  0.0000e+00,  5.2621e-12,  3.4275e-11,  0.0000e+00],\n",
      "        [ 8.6252e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1546e-10,\n",
      "          1.5188e-09,  0.0000e+00, -1.1087e-10, -7.2217e-10,  0.0000e+00],\n",
      "        [ 6.7716e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2618e-11,\n",
      "          1.1924e-10,  0.0000e+00, -8.7046e-12, -5.6697e-11,  0.0000e+00],\n",
      "        [ 3.7308e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7971e-10,\n",
      "          6.5697e-10,  0.0000e+00, -4.7958e-11, -3.1237e-10,  0.0000e+00],\n",
      "        [ 8.7162e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1984e-10,\n",
      "          1.5349e-09,  0.0000e+00, -1.1204e-10, -7.2979e-10,  0.0000e+00],\n",
      "        [ 8.1539e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9276e-10,\n",
      "          1.4359e-09,  0.0000e+00, -1.0482e-10, -6.8271e-10,  0.0000e+00],\n",
      "        [ 5.1821e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4961e-10,\n",
      "          9.1254e-10,  0.0000e+00, -6.6614e-11, -4.3389e-10,  0.0000e+00],\n",
      "        [-5.1632e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4870e-11,\n",
      "         -9.0921e-11,  0.0000e+00,  6.6371e-12,  4.3231e-11,  0.0000e+00],\n",
      "        [-1.8575e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.9473e-11,\n",
      "         -3.2710e-10,  0.0000e+00,  2.3878e-11,  1.5553e-10,  0.0000e+00],\n",
      "        [-2.9914e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4409e-11,\n",
      "         -5.2677e-11,  0.0000e+00,  3.8454e-12,  2.5047e-11,  0.0000e+00],\n",
      "        [ 1.2659e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.0975e-11,\n",
      "          2.2291e-10,  0.0000e+00, -1.6272e-11, -1.0599e-10,  0.0000e+00],\n",
      "        [ 8.4579e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0740e-10,\n",
      "          1.4894e-09,  0.0000e+00, -1.0872e-10, -7.0816e-10,  0.0000e+00],\n",
      "        [-1.3305e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.4088e-10,\n",
      "         -2.3429e-09,  0.0000e+00,  1.7103e-10,  1.1140e-09,  0.0000e+00],\n",
      "        [-1.0231e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9283e-11,\n",
      "         -1.8017e-10,  0.0000e+00,  1.3152e-11,  8.5665e-11,  0.0000e+00],\n",
      "        [ 1.1314e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4499e-10,\n",
      "          1.9924e-09,  0.0000e+00, -1.4544e-10, -9.4733e-10,  0.0000e+00],\n",
      "        [-1.4078e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.7809e-10,\n",
      "         -2.4790e-09,  0.0000e+00,  1.8096e-10,  1.1787e-09,  0.0000e+00],\n",
      "        [-1.6939e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.1590e-11,\n",
      "         -2.9828e-10,  0.0000e+00,  2.1774e-11,  1.4182e-10,  0.0000e+00],\n",
      "        [ 1.1274e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4306e-10,\n",
      "          1.9853e-09,  0.0000e+00, -1.4493e-10, -9.4398e-10,  0.0000e+00],\n",
      "        [-7.6532e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.6864e-10,\n",
      "         -1.3477e-09,  0.0000e+00,  9.8378e-11,  6.4078e-10,  0.0000e+00],\n",
      "        [ 1.0906e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2532e-10,\n",
      "          1.9205e-09,  0.0000e+00, -1.4019e-10, -9.1313e-10,  0.0000e+00],\n",
      "        [-1.3831e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.6623e-10,\n",
      "         -2.4356e-09,  0.0000e+00,  1.7780e-10,  1.1581e-09,  0.0000e+00]])\n",
      "rnn.weight_hh_l0_reverse\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "rnn.bias_ih_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.9353e-09,  3.0444e-09,  2.3411e-10, -2.5889e-09,  3.2212e-09,\n",
      "         3.8758e-10, -2.5797e-09,  1.7512e-09, -2.4954e-09,  3.1648e-09])\n",
      "rnn.bias_hh_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.4298e-09,  1.0445e-09,  2.2012e-10, -1.7819e-09,  2.0395e-09,\n",
      "         3.5252e-10, -7.8725e-10,  4.3729e-10, -7.7968e-10,  1.1508e-09])\n",
      "fc_out.weight\n",
      "tensor([[-9.4410e-09,  9.3946e-09,  9.4020e-09,  9.3900e-09,  9.4874e-09,\n",
      "         -9.5659e-09, -9.3705e-09, -9.5267e-09,  9.4705e-09, -9.5252e-09,\n",
      "          5.9604e-09, -2.1056e-10, -8.9559e-09,  5.1718e-09, -4.0032e-09,\n",
      "         -8.5015e-09, -1.5177e-10,  5.1033e-10, -8.8546e-11, -4.0043e-10],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.2325e-08,  1.2264e-08,  1.2274e-08,  1.2258e-08,  1.2385e-08,\n",
      "         -1.2488e-08, -1.2233e-08, -1.2437e-08,  1.2363e-08, -1.2435e-08,\n",
      "          7.7809e-09, -2.7487e-10, -1.1691e-08,  6.7515e-09, -5.2260e-09,\n",
      "         -1.1098e-08, -1.9813e-10,  6.6621e-10, -1.1559e-10, -5.2274e-10]])\n",
      "fc_out.bias\n",
      "tensor([-9.5659e-09,  0.0000e+00, -1.2488e-08])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.gating.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgyhIfAtNrto"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKTw6jmppP8y",
    "outputId": "6161027d-7195-48ea-952c-005a6461f81b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 238,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_criterion(torch.tensor([[0, 1.0000e+10, 0],\n",
    "        [0, 1.0000e+10, 0]]),\n",
    "        torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbNMxqrZp_S-",
    "outputId": "7d2a76f9-b577-4a74-bb77-4af9d2f93c01",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([2])\n",
      ">> Gating Loss\n",
      "tensor(1.0986, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[-2.3803e-05]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0837, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0650, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0255]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0419, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0131, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9796, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0922]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9351, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1283]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8878, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1678]], grad_fn=<SliceBackward>)\n",
      "0.82974424213171\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8212, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2259]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.7432, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2976]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.6585, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.4752, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5942]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.3468, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.7856]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.9747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.2153]], grad_fn=<SliceBackward>)\n",
      "0.9061286374926567\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.4879]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.7517]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.9076]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.1951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.4174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.5413]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.8154]], grad_fn=<SliceBackward>)\n",
      "0.9074996039271355\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0053, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.9616]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0032, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2070]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.4383]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3716]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6711]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6474]], grad_fn=<SliceBackward>)\n",
      "0.9339357018470764\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6148]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6527]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2055]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9430]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1320]], grad_fn=<SliceBackward>)\n",
      "0.8914671167731285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3627]], grad_fn=<SliceBackward>)\n",
      "0.9092058017849922\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2314]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4475]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4427]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4040]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.8662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4927]], grad_fn=<SliceBackward>)\n",
      "0.9086853638291359\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7027]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5852]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4688]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8051]], grad_fn=<SliceBackward>)\n",
      "0.9540341049432755\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5801]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5563]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3491]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5286]], grad_fn=<SliceBackward>)\n",
      "0.9324938133358955\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3866]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6850]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7848]], grad_fn=<SliceBackward>)\n",
      "0.9254426285624504\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7740]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8104e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2422]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2047]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3928]], grad_fn=<SliceBackward>)\n",
      "0.8661048337817192\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6382]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7866e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9646]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4085]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6154]], grad_fn=<SliceBackward>)\n",
      "0.8831527382135391\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9931]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7738]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4841]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6868]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4436]], grad_fn=<SliceBackward>)\n",
      "0.880535438656807\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4818]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0234]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7940]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8329]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4648e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9808]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5697]], grad_fn=<SliceBackward>)\n",
      "0.8675431832671165\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6845]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4355]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6103]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9230]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6864]], grad_fn=<SliceBackward>)\n",
      "0.9496602267026901\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4083]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7828]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7782]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3674]], grad_fn=<SliceBackward>)\n",
      "0.9153327122330666\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4941]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9431]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0011]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9210]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7318]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6719]], grad_fn=<SliceBackward>)\n",
      "0.9284641966223717\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2209]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7534]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6434]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5050]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4922]], grad_fn=<SliceBackward>)\n",
      "0.9609360843896866\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6555e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4290e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7235]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1793]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7433]], grad_fn=<SliceBackward>)\n",
      "0.9041190296411514\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3575e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9867]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6078e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6897]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6544]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7483]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8843]], grad_fn=<SliceBackward>)\n",
      "0.903328500688076\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8428]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7444]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4540]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2109]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7608]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7668]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3372]], grad_fn=<SliceBackward>)\n",
      "0.9555044546723366\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8617]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5408]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7394]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2313]], grad_fn=<SliceBackward>)\n",
      "0.9302210956811905\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2986]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3337e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9876]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5304]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4039e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0407]], grad_fn=<SliceBackward>)\n",
      "0.8923574611544609\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5188]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7251]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1046e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7108]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6158]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5218e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0967]], grad_fn=<SliceBackward>)\n",
      "0.9684783220291138\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8901e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1406]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4026e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9114]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8457]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1415]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2771]], grad_fn=<SliceBackward>)\n",
      "0.9525922313332558\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5614]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6529e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9403e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0997]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3187]], grad_fn=<SliceBackward>)\n",
      "0.8704231753945351\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5723]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0595e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0026]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7264]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5248]], grad_fn=<SliceBackward>)\n",
      "0.8824385702610016\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5601e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9758]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4923]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7087]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1972]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8807e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0128]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0074]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2956]], grad_fn=<SliceBackward>)\n",
      "0.9546280577778816\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3962]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3098e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0331e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1300]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7753]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6634]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9485]], grad_fn=<SliceBackward>)\n",
      "0.9295287430286407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3370]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9178]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1839]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2145e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9944]], grad_fn=<SliceBackward>)\n",
      "0.8718064799904823\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6463]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8458]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5933e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0913]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5339]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8529]], grad_fn=<SliceBackward>)\n",
      "0.8350213691592216\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3111]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8939e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1571]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1373]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0569e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1284]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6946]], grad_fn=<SliceBackward>)\n",
      "0.941309466958046\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6921]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7919]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3554]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1072e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8788]], grad_fn=<SliceBackward>)\n",
      "0.9300222471356392\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.6980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3648]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2035]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2370e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3549e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1079]], grad_fn=<SliceBackward>)\n",
      "0.9206623733043671\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2090]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2973]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8679]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6243]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7694]], grad_fn=<SliceBackward>)\n",
      "0.9505183324217796\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3803]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1271]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6526]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2224]], grad_fn=<SliceBackward>)\n",
      "0.8610163629055023\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8198e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0000]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8999]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8926e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5534]], grad_fn=<SliceBackward>)\n",
      "0.9559815302491188\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5469e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9510]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7809]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7747e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9638]], grad_fn=<SliceBackward>)\n",
      "0.982656717300415\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7947e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1464]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1478]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0726]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1562]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1804]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1315]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1703]], grad_fn=<SliceBackward>)\n",
      "0.9076273813843727\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7778]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7198]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1668e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4522]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0199e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0213]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9854e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0903]], grad_fn=<SliceBackward>)\n",
      "0.9130949825048447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6566]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7200]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9442]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.1178e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0568]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3338]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1155]], grad_fn=<SliceBackward>)\n",
      "0.9232373982667923\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2943]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5479]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3715]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9175]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4886e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3081]], grad_fn=<SliceBackward>)\n",
      "0.9844113141298294\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3553]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7245e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2013e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0521]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1420]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0777]], grad_fn=<SliceBackward>)\n",
      "0.906353659927845\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2683]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1621]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5701]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "0.9535688981413841\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3641]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0211]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4322]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4635e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0359]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2119e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1173]], grad_fn=<SliceBackward>)\n",
      "0.97125643491745\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9165e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0093]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5485]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6330]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1748]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5840e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9730]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1825]], grad_fn=<SliceBackward>)\n",
      "1.0099817141890526\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5900]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1847]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2959]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3417e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2336]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1123]], grad_fn=<SliceBackward>)\n",
      "0.9761429280042648\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7194]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1948e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5258]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6055]], grad_fn=<SliceBackward>)\n",
      "0.9192996472120285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0276]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3697]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2895]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2797]], grad_fn=<SliceBackward>)\n",
      "0.9878424927592278\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6423e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0256]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1844]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.2106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1912]], grad_fn=<SliceBackward>)\n",
      "0.9360367059707642\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2252]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3832]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3678]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2162]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3179]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9967]], grad_fn=<SliceBackward>)\n",
      "0.9009067863225937\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9006]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0940e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4516e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0364]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3992]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2383e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0727]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3520]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6362]], grad_fn=<SliceBackward>)\n",
      "0.9399682730436325\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5874]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3456e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9861]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4212]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3954]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0737]], grad_fn=<SliceBackward>)\n",
      "0.9046507701277733\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2467]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4278]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9299]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0054e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4131]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0379]], grad_fn=<SliceBackward>)\n",
      "0.8922909200191498\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3813e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9843]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2239]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4288]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2247]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9376]], grad_fn=<SliceBackward>)\n",
      "0.9415184482932091\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2638]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4342]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6291e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0882]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1603]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4346]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5226]], grad_fn=<SliceBackward>)\n",
      "0.9168690741062164\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7180]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0374]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9205]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2857]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8505]], grad_fn=<SliceBackward>)\n",
      "0.9186825603246689\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6391]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3396]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1430]], grad_fn=<SliceBackward>)\n",
      "1.0298770293593407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3640]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8706]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6598]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4381]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2565]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5695e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3518]], grad_fn=<SliceBackward>)\n",
      "0.9632200226187706\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3154]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2023]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4719]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3268]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8894]], grad_fn=<SliceBackward>)\n",
      "0.8725502789020538\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1332]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4833]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6266]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5859]], grad_fn=<SliceBackward>)\n",
      "0.9114488065242767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2089]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5034]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2518]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3826]], grad_fn=<SliceBackward>)\n",
      "0.997397854924202\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8003]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3097]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4366]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4170]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1381]], grad_fn=<SliceBackward>)\n",
      "0.8821030855178833\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3788e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1053]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9641e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0062]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5363e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9755]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4294]], grad_fn=<SliceBackward>)\n",
      "0.9253259226679802\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6894]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8625]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5305]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6994]], grad_fn=<SliceBackward>)\n",
      "0.826819896697998\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3774]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7934e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9637]], grad_fn=<SliceBackward>)\n",
      "0.9356672465801239\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5946e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0275]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1360]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6202]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2711]], grad_fn=<SliceBackward>)\n",
      "0.9173995330929756\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5826]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3270]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7982]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7323]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7734e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4181]], grad_fn=<SliceBackward>)\n",
      "0.9404812529683113\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2763]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6613]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5192e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2514]], grad_fn=<SliceBackward>)\n",
      "0.908241368830204\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5550e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5391e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1784]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6141]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2168]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6144]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4947]], grad_fn=<SliceBackward>)\n",
      "0.960708424448967\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9057]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9271e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0684]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3351]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6400]], grad_fn=<SliceBackward>)\n",
      "0.9716712683439255\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9403]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4854]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0977]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8236]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9031]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.3085e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6196]], grad_fn=<SliceBackward>)\n",
      "0.9475336000323296\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6060]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6597]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2132]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6303]], grad_fn=<SliceBackward>)\n",
      "0.9496114104986191\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3421]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2233]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9064]], grad_fn=<SliceBackward>)\n",
      "0.9413476660847664\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6966]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4809e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9695]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8241]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8081]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8603]], grad_fn=<SliceBackward>)\n",
      "0.8706775084137917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2751]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0582e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0599]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7073]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3231]], grad_fn=<SliceBackward>)\n",
      "0.9316414818167686\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0994e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5410]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6669]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7125]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2988]], grad_fn=<SliceBackward>)\n",
      "0.9206378161907196\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7277]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2903]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7470e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4822e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3551]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3291]], grad_fn=<SliceBackward>)\n",
      "0.8711266815662384\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0027e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7609]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5503]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3892]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3649]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6760]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3847]], grad_fn=<SliceBackward>)\n",
      "0.9439148530364037\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9087e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5720]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5509]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8880]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1769]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8602]], grad_fn=<SliceBackward>)\n",
      "0.9201693758368492\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1868e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6908]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5094]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8538]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3829]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5983]], grad_fn=<SliceBackward>)\n",
      "0.9166541695594788\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7206]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1399]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5801e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1612]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2305e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5201]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5670]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6542]], grad_fn=<SliceBackward>)\n",
      "0.9151469171047211\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0888e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4002]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9099]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5418e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3490]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5680]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7409]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4017]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1219e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "0.8847057297825813\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3875]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1602]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8996]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4501]], grad_fn=<SliceBackward>)\n",
      "0.9680259078741074\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5604]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9340]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7770]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0650e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4043]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7885]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8000e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8142]], grad_fn=<SliceBackward>)\n",
      "0.9062342792749405\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2459]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2098]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8056]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4743]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4388]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2013]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8037]], grad_fn=<SliceBackward>)\n",
      "0.8556824177503586\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6071]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7881e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2281]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6260]], grad_fn=<SliceBackward>)\n",
      "0.8968541398644447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4615]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9842]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4981]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8386]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6171]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6759]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6369]], grad_fn=<SliceBackward>)\n",
      "0.9463448226451874\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4136]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8466]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7012]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2749]], grad_fn=<SliceBackward>)\n",
      "0.9251849204301834\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6263]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8456]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9686]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6559e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4523]], grad_fn=<SliceBackward>)\n",
      "0.9190128594636917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2278e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0058]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4835e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2537]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1821]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4788]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9957]], grad_fn=<SliceBackward>)\n",
      "0.9369889944791794\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8579]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6681]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6682]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8702]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6698]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4175e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1353]], grad_fn=<SliceBackward>)\n",
      "0.8986769914627075\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2790]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5072]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5080]], grad_fn=<SliceBackward>)\n",
      "0.8923800513148308\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2570]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7044]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8404]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1791e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1504]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1086e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5827e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2201]], grad_fn=<SliceBackward>)\n",
      "0.8838892206549644\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4253]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3228]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8626]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9010]], grad_fn=<SliceBackward>)\n",
      "0.8662857860326767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3378]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5552]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5242]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8772]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5620]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0331]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6600]], grad_fn=<SliceBackward>)\n",
      "0.8832837343215942\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9028]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8647]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5368]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7733]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0967e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7280]], grad_fn=<SliceBackward>)\n",
      "0.9058996215462685\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1378e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2865]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0713]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9529]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7744]], grad_fn=<SliceBackward>)\n",
      "0.9235536307096481\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5516]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9810]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7508]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4873e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5926]], grad_fn=<SliceBackward>)\n",
      "0.8536463901400566\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3014]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9365]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "0.9422603696584702\n"
     ]
    }
   ],
   "source": [
    "for x in range(100):\n",
    "    print( train_dynamoe_gating(model, train_dls[1], gating_criterion,\n",
    "                                expert_criterion_unreduced,\n",
    "                                CLIP, verbose=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fcaN82bJ-f5",
    "outputId": "2c94f843-e801-4dfb-d79c-89ab704be8fd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630012683570385\n",
      "0.47744666039943695\n",
      "0.44646283984184265\n",
      "0.4113902524113655\n",
      "0.4127246364951134\n",
      "0.37290962785482407\n",
      "0.43906231224536896\n",
      "0.385328508913517\n",
      "0.37053677439689636\n",
      "0.3609902262687683\n",
      "0.3726344630122185\n",
      "0.3744181916117668\n",
      "0.36561162024736404\n",
      "0.3524796664714813\n",
      "0.39585812389850616\n",
      "0.32904189825057983\n",
      "0.3342840299010277\n",
      "0.3753737062215805\n",
      "0.40356797724962234\n",
      "0.3463684171438217\n",
      "0.35821671038866043\n",
      "0.36753934621810913\n",
      "0.31112828850746155\n",
      "0.3395487293601036\n",
      "0.32247451692819595\n",
      "0.31689079105854034\n",
      "0.3214147612452507\n",
      "0.3431061953306198\n",
      "0.32723822444677353\n",
      "0.3066282793879509\n",
      "0.2927127256989479\n",
      "0.3012479245662689\n",
      "0.2619609013199806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d7e8f4aa73e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_dynamoe_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgating_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_criterion_unreduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-00addaa6fd57>\u001b[0m in \u001b[0;36mtrain_dynamoe_both\u001b[0;34m(model, iterator, gating_criterion, expert_criterion, clip)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Train newly initialized model on new train examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mreduced_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mreduced_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(400):\n",
    "    print( train_dynamoe_both(model, train_dls[1], gating_criterion, expert_criterion_unreduced, CLIP) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAASoarxMs3k"
   },
   "source": [
    "## Transfer F: Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "ztDsiI2WM98Y"
   },
   "outputs": [],
   "source": [
    "class Ensembler(nn.Module):\n",
    "    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(Ensembler, self).__init__()\n",
    "\n",
    "        assert len(experts) == len(expert_optimizers)\n",
    "        \n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        self.n_active_experts = 1\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio = 0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gatings[:,:self.n_active_experts]\n",
    "\n",
    "        expert_outputs = torch.empty((self.n_active_experts, seq_len, batch_size, vocab_size))\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_output = self.experts[e_id](seqs, seqs_len, seqs, teacher_forcing_ratio)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                # expert_outputs[e_id,:,b] = expert_output[:,b]\n",
    "            \n",
    "            # print(\"expert_out\")\n",
    "            # print(expert_output)\n",
    "            # print(\"gating_masked\")\n",
    "            # print(gating_masked)\n",
    "\n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "        return weighted_outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_ensembler_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler_gating(model, iterator, criterion, clip, verbose=False):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        vocab_size = model.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        expert_outputs = torch.empty((model.n_active_experts, seq_len,\n",
    "                                      batch_size, vocab_size))\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "\n",
    "            expert_output = model.experts[e_id](seqs, seqs_len, seqs)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "            \n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "        # Gating Loss just is total loss\n",
    "        gating_loss = compute_loss(weighted_outputs, seqs, criterion,\n",
    "                            cutFirstInSequence=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        \n",
    "        epoch_loss += gating_loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_ensembler_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler_both(model, iterator, criterion, clip):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    \n",
    "    model.eval()\n",
    "    model.experts[model.n_active_experts - 1].train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        # model.gating_optimizer.zero_grad()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].zero_grad()\n",
    "        \n",
    "        outputs = model(seqs, seqs_len, seqs)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # model.gating_optimizer.step()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensembler(model, task_id, epochs, step_size_evaluation, clip,\n",
    "                 case = \"train_gating_initialized_expert\" ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    case : string\n",
    "        \"train_gating_uninitialized_expert\" | \"train_gating_train_expert\" | \n",
    "        \"train_gating_initialized_expert\"\n",
    "    \"\"\"\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    loss_tracker = torch.zeros((epochs,))\n",
    "\n",
    "    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        if case == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id], allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    # Switch to train the expert and gating\n",
    "\n",
    "                    case = \"train_gating_train_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if case == \"train_gating_train_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_both(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "        if case == \"train_gating_uninitialized_expert\":\n",
    "            assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    case = \"train_gating_initialized_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], expert_criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], expert_criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], expert_criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show_expert2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_expert2(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_masked[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "iu9jVht1wbg7"
   },
   "outputs": [],
   "source": [
    "SAVE = N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "N_EPOCHS = SAVE + 200\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   0 ------\n",
      "Ensembler(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-F0\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.003 | Train PPL:   2.726\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.967 | Train PPL:   2.630\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.965 | Train PPL:   2.624\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.980 | Train PPL:   2.663\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.980 | Train PPL:   2.664\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.962 | Train PPL:   2.617\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 1.035 | Train PPL:   2.816\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.991 | Train PPL:   2.695\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.999 | Train PPL:   2.717\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.983 | Train PPL:   2.673\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 1.024 | Train PPL:   2.785\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.995 | Train PPL:   2.704\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.965 | Train PPL:   2.624\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.946 | Train PPL:   2.575\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.990 | Train PPL:   2.690\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 1.011 | Train PPL:   2.748\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.974 | Train PPL:   2.648\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.979 | Train PPL:   2.663\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.997 | Train PPL:   2.710\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.954 | Train PPL:   2.597\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.986 | Train PPL:   2.682\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 1.000 | Train PPL:   2.717\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.969 | Train PPL:   2.636\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 1.031 | Train PPL:   2.803\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 1.004 | Train PPL:   2.728\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.970 | Train PPL:   2.637\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 1.008 | Train PPL:   2.739\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.919 | Train PPL:   2.506\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.981 | Train PPL:   2.666\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.982 | Train PPL:   2.670\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 1.028 | Train PPL:   2.794\n",
      "\t Val. Loss: 0.906 |  Val. PPL:   2.475\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.644 | Train PPL:   1.905\n",
      "\t Val. Loss: 0.525 |  Val. PPL:   1.690\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train PPL:   1.536\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.457\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.476\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.385\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.360\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.231\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-F1\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.786\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.575 | Train PPL:   1.777\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.818\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.577 | Train PPL:   1.781\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.732\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.637 | Train PPL:   1.891\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.557 | Train PPL:   1.746\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.593 | Train PPL:   1.809\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.594 | Train PPL:   1.812\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.579 | Train PPL:   1.785\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.626 | Train PPL:   1.870\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.597 | Train PPL:   1.818\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.612 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.593 | Train PPL:   1.810\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.819\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.596 | Train PPL:   1.815\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.703\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.789\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.627 | Train PPL:   1.872\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.790\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.818\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.787\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.594 | Train PPL:   1.811\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.578 | Train PPL:   1.782\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.589 | Train PPL:   1.803\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.833\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.591 | Train PPL:   1.805\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.874\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.867 | Train PPL:   2.380\n",
      "\t Val. Loss: 1.445 |  Val. PPL:   4.240\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.896 | Train PPL:   2.449\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.612 | Train PPL:   1.843\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.578 | Train PPL:   1.782\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.719 | Train PPL:   2.053\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.861 | Train PPL:   2.366\n",
      "\t Val. Loss: 1.445 |  Val. PPL:   4.240\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 1.398 | Train PPL:   4.049\n",
      "\t Val. Loss: 1.445 |  Val. PPL:   4.240\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 1.419 | Train PPL:   4.133\n",
      "\t Val. Loss: 1.445 |  Val. PPL:   4.240\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 1.396 | Train PPL:   4.041\n",
      "\t Val. Loss: 1.445 |  Val. PPL:   4.240\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 1.389 | Train PPL:   4.009\n",
      "\t Val. Loss: 1.444 |  Val. PPL:   4.239\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 1.284 | Train PPL:   3.610\n",
      "\t Val. Loss: 1.443 |  Val. PPL:   4.232\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.873 | Train PPL:   2.394\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.575 | Train PPL:   1.776\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.741\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.594 | Train PPL:   1.812\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.790\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.578 | Train PPL:   1.783\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.578 | Train PPL:   1.782\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.597 | Train PPL:   1.817\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.787\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.622 | Train PPL:   1.863\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.583 | Train PPL:   1.791\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.743\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.789\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.608 | Train PPL:   1.837\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.583 | Train PPL:   1.791\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.734\n",
      "\t Val. Loss: 0.555 |  Val. PPL:   1.742\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.834\n",
      "\t Val. Loss: 0.492 |  Val. PPL:   1.636\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.471 |  Val. PPL:   1.602\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
      "\t Val. Loss: 0.448 |  Val. PPL:   1.565\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.479 |  Val. PPL:   1.614\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.485\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.472\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.271\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "tr-AE-30-10-0.01-F2\n",
      "The model has 13219 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "------ REPETITION   1 ------\n",
      "Ensembler(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-F0\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.005 | Train PPL:   2.733\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 1.004 | Train PPL:   2.729\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 1.006 | Train PPL:   2.733\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.976 | Train PPL:   2.655\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.942 | Train PPL:   2.565\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.961 | Train PPL:   2.614\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.945 | Train PPL:   2.574\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.950 | Train PPL:   2.587\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.959 | Train PPL:   2.609\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.998 | Train PPL:   2.713\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.993 | Train PPL:   2.699\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.990 | Train PPL:   2.692\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.921 | Train PPL:   2.513\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.992 | Train PPL:   2.698\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.963 | Train PPL:   2.618\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 1.007 | Train PPL:   2.737\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.992 | Train PPL:   2.697\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.974 | Train PPL:   2.648\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.981 | Train PPL:   2.666\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 1.002 | Train PPL:   2.725\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.967 | Train PPL:   2.631\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.981 | Train PPL:   2.668\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.988 | Train PPL:   2.686\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.938 | Train PPL:   2.556\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.984 | Train PPL:   2.675\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.925 | Train PPL:   2.521\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.995 | Train PPL:   2.704\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.977 | Train PPL:   2.657\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.957 | Train PPL:   2.605\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 1.029 | Train PPL:   2.798\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.993 | Train PPL:   2.698\n",
      "\t Val. Loss: 0.962 |  Val. PPL:   2.617\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.644 | Train PPL:   1.905\n",
      "\t Val. Loss: 0.526 |  Val. PPL:   1.692\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.508 |  Val. PPL:   1.662\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.492 | Train PPL:   1.635\n",
      "\t Val. Loss: 0.495 |  Val. PPL:   1.640\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.547\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.509\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.521\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.147\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "tr-AE-30-10-0.01-F1\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.666 | Train PPL:   1.946\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.541 |  Val. PPL:   1.718\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 1.025 | Train PPL:   2.786\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.308\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 1.458 | Train PPL:   4.299\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 1.460 | Train PPL:   4.307\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 1.460 | Train PPL:   4.304\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.300\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 1.462 | Train PPL:   4.315\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.304\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.309\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.302\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.302\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.312\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.311\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.302\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 1.460 | Train PPL:   4.306\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.302\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 1.457 | Train PPL:   4.292\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.310\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 1.458 | Train PPL:   4.297\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 1.458 | Train PPL:   4.299\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 1.462 | Train PPL:   4.315\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.312\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.302\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 1.457 | Train PPL:   4.292\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 1.459 | Train PPL:   4.301\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 1.462 | Train PPL:   4.313\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 1.461 | Train PPL:   4.310\n",
      "\t Val. Loss: 1.459 |  Val. PPL:   4.300\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 1.436 | Train PPL:   4.203\n",
      "\t Val. Loss: 1.335 |  Val. PPL:   3.799\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 1.095 | Train PPL:   2.989\n",
      "\t Val. Loss: 0.844 |  Val. PPL:   2.327\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.778 | Train PPL:   2.177\n",
      "\t Val. Loss: 0.710 |  Val. PPL:   2.034\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.688 | Train PPL:   1.991\n",
      "\t Val. Loss: 0.670 |  Val. PPL:   1.954\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.664 | Train PPL:   1.942\n",
      "\t Val. Loss: 0.658 |  Val. PPL:   1.931\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.654 | Train PPL:   1.924\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.651 | Train PPL:   1.917\n",
      "\t Val. Loss: 0.647 |  Val. PPL:   1.909\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.643 | Train PPL:   1.903\n",
      "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.638 | Train PPL:   1.893\n",
      "\t Val. Loss: 0.637 |  Val. PPL:   1.891\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.632 | Train PPL:   1.881\n",
      "\t Val. Loss: 0.643 |  Val. PPL:   1.903\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.632 | Train PPL:   1.881\n",
      "\t Val. Loss: 0.642 |  Val. PPL:   1.901\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.631 | Train PPL:   1.879\n",
      "\t Val. Loss: 0.642 |  Val. PPL:   1.900\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.627 | Train PPL:   1.872\n",
      "\t Val. Loss: 0.641 |  Val. PPL:   1.899\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.641 |  Val. PPL:   1.898\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.605 | Train PPL:   1.831\n",
      "\t Val. Loss: 0.641 |  Val. PPL:   1.899\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.818\n",
      "\t Val. Loss: 0.643 |  Val. PPL:   1.902\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.600 | Train PPL:   1.823\n",
      "\t Val. Loss: 0.645 |  Val. PPL:   1.905\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.591 | Train PPL:   1.806\n",
      "\t Val. Loss: 0.647 |  Val. PPL:   1.910\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.593 | Train PPL:   1.809\n",
      "\t Val. Loss: 0.650 |  Val. PPL:   1.915\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.578 | Train PPL:   1.782\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
      "\t Val. Loss: 0.609 |  Val. PPL:   1.839\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.567 | Train PPL:   1.762\n",
      "\t Val. Loss: 0.609 |  Val. PPL:   1.838\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.733\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.750\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.739\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.726\n",
      "\t Val. Loss: 0.630 |  Val. PPL:   1.877\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.609 |  Val. PPL:   1.838\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.795\n",
      "\t Val. Loss: 0.676 |  Val. PPL:   1.966\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.676 |  Val. PPL:   1.966\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.730\n",
      "\t Val. Loss: 0.680 |  Val. PPL:   1.975\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
      "\t Val. Loss: 0.608 |  Val. PPL:   1.836\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.691\n",
      "\t Val. Loss: 0.609 |  Val. PPL:   1.838\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.694 |  Val. PPL:   2.001\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.701 |  Val. PPL:   2.016\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
      "\t Val. Loss: 0.615 |  Val. PPL:   1.850\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.521 | Train PPL:   1.685\n",
      "\t Val. Loss: 0.616 |  Val. PPL:   1.851\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.681 |  Val. PPL:   1.976\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.492 | Train PPL:   1.636\n",
      "\t Val. Loss: 0.721 |  Val. PPL:   2.057\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.646\n",
      "\t Val. Loss: 0.726 |  Val. PPL:   2.067\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 0.721 |  Val. PPL:   2.057\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.699 |  Val. PPL:   2.012\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
      "\t Val. Loss: 0.647 |  Val. PPL:   1.909\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.655\n",
      "\t Val. Loss: 0.610 |  Val. PPL:   1.841\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.657\n",
      "\t Val. Loss: 0.610 |  Val. PPL:   1.840\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.684 |  Val. PPL:   1.981\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.631 |  Val. PPL:   1.880\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.614 |  Val. PPL:   1.848\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.648\n",
      "\t Val. Loss: 0.707 |  Val. PPL:   2.028\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.704 |  Val. PPL:   2.021\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.737\n",
      "\t Val. Loss: 0.631 |  Val. PPL:   1.879\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.599\n",
      "\t Val. Loss: 0.639 |  Val. PPL:   1.895\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.497 | Train PPL:   1.644\n",
      "\t Val. Loss: 0.608 |  Val. PPL:   1.837\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.521 | Train PPL:   1.685\n",
      "\t Val. Loss: 0.622 |  Val. PPL:   1.863\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.657\n",
      "\t Val. Loss: 0.622 |  Val. PPL:   1.862\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.693 |  Val. PPL:   2.000\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.677 |  Val. PPL:   1.967\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
      "\t Val. Loss: 0.605 |  Val. PPL:   1.831\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.492 | Train PPL:   1.636\n",
      "\t Val. Loss: 0.605 |  Val. PPL:   1.832\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.619 |  Val. PPL:   1.857\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.619 |  Val. PPL:   1.857\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.689 |  Val. PPL:   1.992\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.662\n",
      "\t Val. Loss: 0.633 |  Val. PPL:   1.883\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.604 |  Val. PPL:   1.830\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 0.622 |  Val. PPL:   1.863\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.653\n",
      "\t Val. Loss: 0.699 |  Val. PPL:   2.011\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.757\n",
      "\t Val. Loss: 0.691 |  Val. PPL:   1.997\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.721\n",
      "\t Val. Loss: 0.618 |  Val. PPL:   1.855\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.670\n",
      "\t Val. Loss: 0.632 |  Val. PPL:   1.882\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.685\n",
      "\t Val. Loss: 0.602 |  Val. PPL:   1.826\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.498 | Train PPL:   1.646\n",
      "\t Val. Loss: 0.603 |  Val. PPL:   1.827\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.683\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.681 |  Val. PPL:   1.977\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.656\n",
      "\t Val. Loss: 0.657 |  Val. PPL:   1.928\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.604 |  Val. PPL:   1.829\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.573 | Train PPL:   1.773\n",
      "\t Val. Loss: 0.679 |  Val. PPL:   1.973\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.599 |  Val. PPL:   1.820\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.685\n",
      "\t Val. Loss: 0.626 |  Val. PPL:   1.869\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.672 |  Val. PPL:   1.958\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.639\n",
      "\t Val. Loss: 0.672 |  Val. PPL:   1.957\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.671 |  Val. PPL:   1.956\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.671 |  Val. PPL:   1.957\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.617\n",
      "\t Val. Loss: 0.676 |  Val. PPL:   1.966\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.629 |  Val. PPL:   1.877\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.666\n",
      "\t Val. Loss: 0.620 |  Val. PPL:   1.858\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.623\n",
      "\t Val. Loss: 0.674 |  Val. PPL:   1.962\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.650 |  Val. PPL:   1.916\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.602 |  Val. PPL:   1.826\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 0.604 |  Val. PPL:   1.829\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.685 |  Val. PPL:   1.984\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.678\n",
      "\t Val. Loss: 0.603 |  Val. PPL:   1.828\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
      "\t Val. Loss: 0.604 |  Val. PPL:   1.829\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.687 |  Val. PPL:   1.988\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.684 |  Val. PPL:   1.981\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.703\n",
      "\t Val. Loss: 0.601 |  Val. PPL:   1.824\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.638\n",
      "\t Val. Loss: 0.601 |  Val. PPL:   1.823\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.674 |  Val. PPL:   1.962\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.741\n",
      "\t Val. Loss: 0.675 |  Val. PPL:   1.964\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.675 |  Val. PPL:   1.963\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.703\n",
      "\t Val. Loss: 0.600 |  Val. PPL:   1.823\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
      "\t Val. Loss: 0.599 |  Val. PPL:   1.821\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.666\n",
      "\t Val. Loss: 0.674 |  Val. PPL:   1.961\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.653\n",
      "\t Val. Loss: 0.673 |  Val. PPL:   1.959\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.570 | Train PPL:   1.769\n",
      "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.934\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.658 |  Val. PPL:   1.930\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
      "\t Val. Loss: 0.659 |  Val. PPL:   1.933\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.659 |  Val. PPL:   1.934\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.630\n",
      "\t Val. Loss: 0.658 |  Val. PPL:   1.932\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
      "\t Val. Loss: 0.596 |  Val. PPL:   1.814\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.596 |  Val. PPL:   1.815\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.730\n",
      "\t Val. Loss: 0.596 |  Val. PPL:   1.814\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.940\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.473 | Train PPL:   1.605\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.945\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.535 | Train PPL:   1.708\n",
      "\t Val. Loss: 0.623 |  Val. PPL:   1.865\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.668 |  Val. PPL:   1.950\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.666 |  Val. PPL:   1.947\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.710\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.687\n",
      "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.935\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.655\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.934\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.934\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.497 | Train PPL:   1.643\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.473 | Train PPL:   1.605\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.486 | Train PPL:   1.625\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.616\n",
      "\t Val. Loss: 0.595 |  Val. PPL:   1.813\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.660\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
      "\t Val. Loss: 0.662 |  Val. PPL:   1.938\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.497 | Train PPL:   1.643\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.941\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.662\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.597 |  Val. PPL:   1.816\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.597 |  Val. PPL:   1.817\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.669 |  Val. PPL:   1.953\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.620\n",
      "\t Val. Loss: 0.669 |  Val. PPL:   1.952\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.462 | Train PPL:   1.587\n",
      "\t Val. Loss: 0.597 |  Val. PPL:   1.816\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.673 |  Val. PPL:   1.960\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.673 |  Val. PPL:   1.960\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.668 |  Val. PPL:   1.950\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.662 |  Val. PPL:   1.939\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.599\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.753\n",
      "\t Val. Loss: 0.667 |  Val. PPL:   1.949\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.656\n",
      "\t Val. Loss: 0.656 |  Val. PPL:   1.926\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.698\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.945\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.934\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.937\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.685\n",
      "\t Val. Loss: 0.595 |  Val. PPL:   1.813\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
      "\t Val. Loss: 0.595 |  Val. PPL:   1.813\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.676\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.662 |  Val. PPL:   1.938\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.496 | Train PPL:   1.643\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.941\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.649\n",
      "\t Val. Loss: 0.667 |  Val. PPL:   1.948\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.498 | Train PPL:   1.645\n",
      "\t Val. Loss: 0.667 |  Val. PPL:   1.949\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.661\n",
      "\t Val. Loss: 0.597 |  Val. PPL:   1.816\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
      "\t Val. Loss: 0.596 |  Val. PPL:   1.815\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.668 |  Val. PPL:   1.950\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.667\n",
      "\t Val. Loss: 0.667 |  Val. PPL:   1.949\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.536 | Train PPL:   1.709\n",
      "\t Val. Loss: 0.666 |  Val. PPL:   1.947\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.681\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.940\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.534 | Train PPL:   1.706\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.935\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.617\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.638\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.937\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.940\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.731\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.691\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.757\n",
      "\t Val. Loss: 0.659 |  Val. PPL:   1.933\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.729\n",
      "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.514 | Train PPL:   1.672\n",
      "\t Val. Loss: 0.655 |  Val. PPL:   1.925\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.741\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.649\n",
      "\t Val. Loss: 0.650 |  Val. PPL:   1.916\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.599 |  Val. PPL:   1.821\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.497 | Train PPL:   1.644\n",
      "\t Val. Loss: 0.589 |  Val. PPL:   1.802\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
      "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.589 |  Val. PPL:   1.802\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.922\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.698\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.535 | Train PPL:   1.707\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.918\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.658\n",
      "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.655\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.922\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.662\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.922\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.678\n",
      "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.671\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.721\n",
      "\t Val. Loss: 0.591 |  Val. PPL:   1.805\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.639\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.657 |  Val. PPL:   1.929\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.657 |  Val. PPL:   1.928\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.657 |  Val. PPL:   1.930\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.811\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.696\n",
      "\t Val. Loss: 0.659 |  Val. PPL:   1.933\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.659 |  Val. PPL:   1.932\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.696\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.812\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.653\n",
      "\t Val. Loss: 0.618 |  Val. PPL:   1.855\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.535 | Train PPL:   1.708\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.676\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.486 | Train PPL:   1.626\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.655 |  Val. PPL:   1.926\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.491 | Train PPL:   1.633\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.591 |  Val. PPL:   1.807\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.498 | Train PPL:   1.646\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.807\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.661\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.657\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.937\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.934\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.632\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.934\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.676\n",
      "\t Val. Loss: 0.659 |  Val. PPL:   1.934\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.741\n",
      "\t Val. Loss: 0.658 |  Val. PPL:   1.931\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.726\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.921\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.649 |  Val. PPL:   1.914\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
      "\t Val. Loss: 0.647 |  Val. PPL:   1.910\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.657\n",
      "\t Val. Loss: 0.648 |  Val. PPL:   1.912\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.567 | Train PPL:   1.763\n",
      "\t Val. Loss: 0.647 |  Val. PPL:   1.910\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.589 |  Val. PPL:   1.801\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.681\n",
      "\t Val. Loss: 0.648 |  Val. PPL:   1.912\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.650 |  Val. PPL:   1.915\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
      "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.495 | Train PPL:   1.640\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.922\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.681\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.922\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.514 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.922\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.648\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.922\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.617\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.655 |  Val. PPL:   1.924\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.920\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.510 | Train PPL:   1.665\n",
      "\t Val. Loss: 0.588 |  Val. PPL:   1.800\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.588 |  Val. PPL:   1.800\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.633\n",
      "\t Val. Loss: 0.651 |  Val. PPL:   1.918\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.638\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.923\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.484 | Train PPL:   1.623\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.654 |  Val. PPL:   1.924\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.655 |  Val. PPL:   1.925\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.498 | Train PPL:   1.646\n",
      "\t Val. Loss: 0.588 |  Val. PPL:   1.800\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.589 |  Val. PPL:   1.802\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.627\n",
      "\t Val. Loss: 0.659 |  Val. PPL:   1.934\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.936\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.610\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.937\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.754\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.938\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.594 |  Val. PPL:   1.810\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.511 | Train PPL:   1.667\n",
      "\t Val. Loss: 0.667 |  Val. PPL:   1.948\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.668 |  Val. PPL:   1.951\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.491 | Train PPL:   1.633\n",
      "\t Val. Loss: 0.668 |  Val. PPL:   1.951\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.942\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.486 | Train PPL:   1.625\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.651\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.943\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.568 | Train PPL:   1.764\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.807\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.941\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.807\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.941\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.936\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.590 |  Val. PPL:   1.804\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.591 |  Val. PPL:   1.806\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.663 |  Val. PPL:   1.940\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.733\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.936\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.657 |  Val. PPL:   1.929\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.655 |  Val. PPL:   1.925\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.653 |  Val. PPL:   1.922\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.588 |  Val. PPL:   1.800\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.652 |  Val. PPL:   1.919\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.497 | Train PPL:   1.644\n",
      "\t Val. Loss: 0.588 |  Val. PPL:   1.800\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.655\n",
      "\t Val. Loss: 0.655 |  Val. PPL:   1.924\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.656 |  Val. PPL:   1.927\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.657 |  Val. PPL:   1.928\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.490 | Train PPL:   1.632\n",
      "\t Val. Loss: 0.590 |  Val. PPL:   1.804\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.657 |  Val. PPL:   1.929\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.658\n",
      "\t Val. Loss: 0.658 |  Val. PPL:   1.931\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.508 | Train PPL:   1.661\n",
      "\t Val. Loss: 0.590 |  Val. PPL:   1.803\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.658\n",
      "\t Val. Loss: 0.590 |  Val. PPL:   1.804\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.658 |  Val. PPL:   1.932\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.651\n",
      "\t Val. Loss: 0.660 |  Val. PPL:   1.934\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.627\n",
      "\t Val. Loss: 0.590 |  Val. PPL:   1.805\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.661 |  Val. PPL:   1.937\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.591 |  Val. PPL:   1.806\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.807\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.945\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.460 | Train PPL:   1.585\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.639\n",
      "\t Val. Loss: 0.671 |  Val. PPL:   1.956\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.648\n",
      "\t Val. Loss: 0.674 |  Val. PPL:   1.961\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.492 | Train PPL:   1.636\n",
      "\t Val. Loss: 0.674 |  Val. PPL:   1.961\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.552\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.809\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
      "\t Val. Loss: 0.679 |  Val. PPL:   1.972\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.593 |  Val. PPL:   1.810\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.696\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.672 |  Val. PPL:   1.958\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.668 |  Val. PPL:   1.950\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.666 |  Val. PPL:   1.946\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.657\n",
      "\t Val. Loss: 0.664 |  Val. PPL:   1.942\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.700\n",
      "\t Val. Loss: 0.662 |  Val. PPL:   1.938\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.662 |  Val. PPL:   1.938\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.495 | Train PPL:   1.641\n",
      "\t Val. Loss: 0.589 |  Val. PPL:   1.802\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
      "\t Val. Loss: 0.589 |  Val. PPL:   1.802\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.486 | Train PPL:   1.626\n",
      "\t Val. Loss: 0.665 |  Val. PPL:   1.944\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
      "\t Val. Loss: 0.667 |  Val. PPL:   1.948\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.667 |  Val. PPL:   1.949\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.591 |  Val. PPL:   1.806\n",
      "tr-AE-30-10-0.01-F2\n",
      "The model has 13219 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.759\n",
      "\t Val. Loss: 0.598 |  Val. PPL:   1.818\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.902 | Train PPL:   2.464\n",
      "\t Val. Loss: 1.283 |  Val. PPL:   3.607\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.875\n",
      "\t Val. Loss: 0.592 |  Val. PPL:   1.808\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.768 | Train PPL:   2.155\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.491\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.430 | Train PPL:   1.537\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.341\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.330\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.330\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.336\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.330\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.328\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = N_REPETITIONS\n",
    "hist_all_losses_F = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "hist_all_hitsss_F = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "for repetition in range(n_repetitions):\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n------ REPETITION {repetition:3} ------\")\n",
    "    # To have single copy\n",
    "    if repetition == n_repetitions - 1:\n",
    "        models_F = []\n",
    "    expert, expert_optimizer = init_expert()\n",
    "    gating, gating_optimizer = init_gating()\n",
    "    model = Ensembler(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "\n",
    "    # model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "\n",
    "    for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "        SUFFIX = f\"F{n_task}\"\n",
    "        title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "        LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "        print(title)\n",
    "        print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "        if n_task == 0:\n",
    "            case = \"train_gating_initialized_expert\"\n",
    "        else:\n",
    "            case = \"train_gating_uninitialized_expert\"\n",
    "        hist_loss_temp, hist_hits_temp = fit_ensembler(model, n_task, N_EPOCHS,\n",
    "                                                      STEP_SIZE_EVALUATION, CLIP,\n",
    "                                                      case)\n",
    "        hist_all_losses_F[repetition,n_task] = hist_hits_temp\n",
    "        hist_all_hitsss_F[repetition,n_task] = hist_hits_temp\n",
    "        if repetition == n_repetitions - 1:\n",
    "            models_F.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_F = torch.cat(hist_losses_F, dim=2)\n",
    "hist_hits_F = torch.cat(hist_hitsss_F, dim=2)\n",
    "\n",
    "plotResults(hist_loss_F, hist_hits_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_F, datasets=(1,2), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2000e+01,  1.7990e-01, -1.0471e-02],\n",
      "        [ 6.7971e+00,  6.8288e+00, -2.9813e-02]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[2]:\n",
    "    print(model.gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22.0830, -0.2584]) - tensor([1, 3, 7, 5, 4, 5, 2])\n",
      "tensor([21.2003, -0.2421]) - tensor([1, 3, 5, 4, 5, 6, 2])\n",
      "tensor([20.5874, -0.2038]) - tensor([1, 3, 5, 6, 4, 2])\n",
      "tensor([21.1779, -0.2220]) - tensor([1, 3, 5, 4, 2])\n",
      "tensor([21.1373, -0.2487]) - tensor([1, 3, 7, 5, 6, 4, 5, 6, 2])\n",
      "tensor([22.0578, -0.2568]) - tensor([1, 3, 7, 5, 4, 2])\n",
      "tensor([22.0680, -0.2647]) - tensor([1, 3, 7, 5, 4, 5, 6, 2])\n",
      "tensor([20.6297, -0.2455]) - tensor([1, 3, 5, 6, 4, 5, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert2(model, train_dls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "TCvCl-WOjvYN"
   },
   "outputs": [],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "\n",
    "gating, gating_optimizer = init_gating()\n",
    "\n",
    "model2 = Ensembler(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "\n",
    "model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09185882657766342\n",
      "0.09303067624568939\n",
      "0.09172848239541054\n",
      "0.09473882988095284\n",
      "0.09154968708753586\n",
      "0.09308598190546036\n",
      "0.09350227937102318\n",
      "0.09345363080501556\n",
      "0.09185237810015678\n",
      "0.09177808463573456\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(train_ensembler_gating(model, train_dls[2], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[1]:\n",
    "    print(model2.gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2788e+01,  4.3714e-02, -1.4698e-02]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[0]:\n",
    "    print(model.gating(seqs, seqs_len))\n",
    "    print(model.experts[0](seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print(model.experts[1](seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print(model(seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print((model.experts[0](seqs, seqs_len, seqs) * model.gating(seqs, seqs_len)[0][0] + model.experts[1](seqs, seqs_len, seqs) * model.gating(seqs, seqs_len)[0][1]).argmax(dim=2) )\n",
    "    print(seqs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16881714761257172\n",
      "0.15141833573579788\n",
      "0.15692844986915588\n",
      "0.15707916021347046\n",
      "0.15791257470846176\n",
      "0.18426920473575592\n",
      "0.15594960004091263\n",
      "0.2099481225013733\n",
      "0.19191593676805496\n",
      "0.16780265420675278\n",
      "0.16559413820505142\n",
      "0.14507802575826645\n",
      "0.14517706632614136\n",
      "0.15876172482967377\n",
      "0.18021392822265625\n",
      "0.14854232966899872\n",
      "0.17011234909296036\n",
      "0.19770054519176483\n",
      "0.15062608569860458\n",
      "0.16328270733356476\n",
      "0.1630496382713318\n",
      "0.15208538621664047\n",
      "0.1585157811641693\n",
      "0.1476331725716591\n",
      "0.19418861716985703\n",
      "0.14303864538669586\n",
      "0.15178367495536804\n",
      "0.13727358728647232\n",
      "0.14693152904510498\n",
      "0.16229414194822311\n",
      "0.16068880259990692\n",
      "0.13847041130065918\n",
      "0.17334415018558502\n",
      "0.15148300677537918\n",
      "0.16502273082733154\n",
      "0.14539223909378052\n",
      "0.12727899849414825\n",
      "0.1345638856291771\n",
      "0.14078588038682938\n",
      "0.16145791113376617\n",
      "0.1524231731891632\n",
      "0.1419893577694893\n",
      "0.12537699937820435\n",
      "0.14602867513895035\n",
      "0.14317089319229126\n",
      "0.16259494423866272\n",
      "0.17060024291276932\n",
      "0.1443256139755249\n",
      "0.13479702174663544\n",
      "0.15025698393583298\n",
      "0.14225276559591293\n",
      "0.16034740954637527\n",
      "0.12438227236270905\n",
      "0.12518545240163803\n",
      "0.14440368115901947\n",
      "0.13735894113779068\n",
      "0.19062084704637527\n",
      "0.13652129471302032\n",
      "0.17650730162858963\n",
      "0.14275554567575455\n",
      "0.16762355715036392\n",
      "0.14181502908468246\n",
      "0.1351715624332428\n",
      "0.12215232849121094\n",
      "0.1263386607170105\n",
      "0.11908707767724991\n",
      "0.12115544825792313\n",
      "0.1282174363732338\n",
      "0.1253407672047615\n",
      "0.12408994138240814\n",
      "0.14472167938947678\n",
      "0.128596231341362\n",
      "0.12068396806716919\n",
      "0.12849827855825424\n",
      "0.1212557777762413\n",
      "0.1384652704000473\n",
      "0.172116719186306\n",
      "0.11886386573314667\n",
      "0.11503040790557861\n",
      "0.1102377399802208\n",
      "0.11419399827718735\n",
      "0.13094515353441238\n",
      "0.11548421531915665\n",
      "0.12197885662317276\n",
      "0.11363319307565689\n",
      "0.11196141690015793\n",
      "0.11577392369508743\n",
      "0.11174105852842331\n",
      "0.1156032532453537\n",
      "0.1134130209684372\n",
      "0.12646576017141342\n",
      "0.12065742164850235\n",
      "0.11694453656673431\n",
      "0.13224681466817856\n",
      "0.10931586474180222\n",
      "0.13207688927650452\n",
      "0.123417429625988\n",
      "0.10937980562448502\n",
      "0.12781494110822678\n",
      "0.11629518866539001\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    print(train_dynamoe2_both(model2, train_dls[1], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "pJE4efjNcUCM"
   },
   "outputs": [],
   "source": [
    "model2.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVuFGE3vj6K-",
    "outputId": "80edc0df-4dcb-4f7c-f48f-6e818ad0a2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [4],\n",
      "        [6],\n",
      "        [7],\n",
      "        [2]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.6035e-04, -3.0651e-03, -3.0967e+00,  5.0364e+00, -2.0193e+00,\n",
      "           1.1399e+01, -1.7679e+00,  4.7675e+00]],\n",
      "\n",
      "        [[-1.7267e-02, -1.7725e-02, -2.5892e+00,  8.6882e+00, -3.9250e-02,\n",
      "           2.2423e+00,  2.9604e+00,  1.6000e+00]],\n",
      "\n",
      "        [[-6.3461e-03, -2.2191e-02, -3.4556e+00,  2.0296e-01,  1.4208e-01,\n",
      "          -6.5787e-02,  1.4856e+01,  2.9764e+00]],\n",
      "\n",
      "        [[-1.5827e-02, -6.2566e-03, -2.2252e+00,  1.3526e+00,  6.0539e+00,\n",
      "          -3.2105e-01,  1.5760e+00,  7.5781e+00]],\n",
      "\n",
      "        [[-1.3362e-02,  1.7968e-02,  3.8569e+00, -1.9823e+00,  8.1386e+00,\n",
      "           1.5963e+00, -1.1647e-02,  1.3524e+00]],\n",
      "\n",
      "        [[-5.1290e-03, -1.0336e-02,  3.7318e+00,  1.4661e+00, -1.7547e+00,\n",
      "          -1.2773e+00,  1.0091e+01,  2.9304e+00]],\n",
      "\n",
      "        [[-6.9810e-03,  1.6720e-02,  7.0399e+00, -3.2432e-01, -3.1907e+00,\n",
      "           1.1463e-01,  1.1009e+00,  1.0808e+01]],\n",
      "\n",
      "        [[-1.7517e-02,  9.3911e-03,  1.1679e+01, -1.5853e+00, -6.2468e-01,\n",
      "           1.4161e+00, -1.5484e+00,  6.6679e+00]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[0]:\n",
    "    print(seqs)\n",
    "    print(model2(seqs, seqs_len, seqs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hz95pA8kDHf"
   },
   "source": [
    "## Transfer G: DynaMoE with context vector\n",
    "\n",
    "1. Give DynaMoE additional artificial context onehot vector encoding\n",
    "2. Use Continual Learning technique to make gating network remember task\n",
    "3. In this case, replay (maybe later EWC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GatingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len, context):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaMoEContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaMoEContext(nn.Module):\n",
    "    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(DynaMoE, self).__init__()\n",
    "\n",
    "        assert len(experts) == len(expert_optimizers)\n",
    "        \n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        self.n_active_experts = 1\n",
    "        # set mask\n",
    "        self.recompute_mask()\n",
    "    \n",
    "    def recompute_mask(self):\n",
    "        gating_mask = torch.zeros(self.gating.n_max_experts).to(device)\n",
    "\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            gating_mask[e_id] = 1\n",
    "\n",
    "        self.gating_mask = gating_mask\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        masked_gatings = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        # @TODO: Probabilistic vs argmax?\n",
    "        network_ids = torch.argmax(masked_gatings, dim=1)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,\n",
    "                                                     teacher_forcing_ratio))\n",
    "\n",
    "        outputs = torch.empty((seq_len, batch_size, vocab_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            network_id = network_ids[b]\n",
    "            outputs[:,b] = expert_outputs[network_id][:,b]\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1\n",
    "        # Recompute mask\n",
    "        self.recompute_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DynaMoE with Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "agl_autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "fa0c181e15994ab92b77e0a9eeb7815659805982e17e67ed50eb685ba3cdee26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
