{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnant-urt22Y"
   },
   "source": [
    "# AGL Autoencoder\n",
    "\n",
    "Based on https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "xKTgS0wMOZbb"
   },
   "outputs": [],
   "source": [
    "BASIS = \"../\"\n",
    "MODELFOLDER = BASIS + \"/models\"\n",
    "PLOTSFOLDER = BASIS + \"/plots\"\n",
    "MODELAUTOSAVE = MODELFOLDER + \"/autosave/\"\n",
    "PLOTSAUTOSAVE = PLOTSFOLDER + \"/autosave/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8BeTXi1OaMs",
    "outputId": "ca356d96-806e-46bb-9036-0f3c2d8c8139"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NKkMOK5QRVp"
   },
   "outputs": [],
   "source": [
    "!cp drive/MyDrive/Colab_data/data.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yjgT9Azct22a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvhH1uQsvnV7"
   },
   "source": [
    "## Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HV39YqA5vnV8"
   },
   "outputs": [],
   "source": [
    "class GrammarGen():\n",
    "    \"\"\"\n",
    "    Generates Grammar sequences from grammars, and offers other functionalities\n",
    "    Grammars are dictionaries:\n",
    "    - always have START\n",
    "    - all paths lead eventually to END\n",
    "    - Entries starting with the same letter\n",
    "      have same output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grammar=None):\n",
    "        if grammar is None:\n",
    "            self.grammar = data.g0()\n",
    "        else:\n",
    "            self.grammar = grammar\n",
    "\n",
    "        # find how many letters in grammar\n",
    "        self.len = len(set([token[0] for token in self.grammar if (token != 'START' and token != 'END')]))\n",
    "\n",
    "        # variable to check how many sequences have been generated for the grammaticality test\n",
    "        self.grammCheckMaxLen = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"Generates n tokens\"\"\"\n",
    "        ret = []\n",
    "        count = 0\n",
    "        hashtrack = set()\n",
    "        while count < n:\n",
    "            token = []\n",
    "            current = 'START'\n",
    "            while current != 'END':\n",
    "                # Append current\n",
    "                if current != 'START':\n",
    "                    token.append(current[0])\n",
    "                # Choose next\n",
    "                r = random.randint(0, len(self.grammar[current]) - 1)\n",
    "                current = self.grammar[current][r]\n",
    "            # Check if seq is already inside\n",
    "            tokenhash = ''.join([str(x) for x in token])\n",
    "            if tokenhash not in hashtrack:\n",
    "                hashtrack.add(tokenhash)\n",
    "                ret.append((token, ))\n",
    "                count += 1\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def generateAllGrammatical(self, maxlen=float('inf')):\n",
    "        \"\"\"Generates all grammatical sequences until length maxlen\"\"\"\n",
    "        def genAllHelp(seq, current):\n",
    "            if current == 'END':\n",
    "                return [seq]\n",
    "            if len(seq) >= maxlen:\n",
    "                return []\n",
    "            # Append Current\n",
    "            if current != 'START':\n",
    "                seq.append(current[0])\n",
    "            # Generate next possibilities\n",
    "            options = range(len(self.grammar[current]))\n",
    "            ret = [(genAllHelp(copy.copy(seq), self.grammar[current][i]))\n",
    "                   for i in options]\n",
    "            return itertools.chain(*ret)\n",
    "        return set([tuple(seq) for seq in genAllHelp([], 'START')])\n",
    "\n",
    "    def isGrammatical(self, seqs):\n",
    "        \"\"\"Check for grammaticality of sequences in seqs\"\"\"\n",
    "        maxlen = max([len(seq) for seq in seqs])\n",
    "        if self.grammCheckMaxLen < maxlen:\n",
    "            self.allGrammatical = self.generateAllGrammatical(maxlen)\n",
    "            self.grammCheckMaxLen = maxlen\n",
    "\n",
    "        return [tuple(seq) in self.allGrammatical for seq in seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKt9wtL6t22s"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Deyfd3OByLSC"
   },
   "source": [
    "\n",
    "\n",
    "### Cosine Loss, Init_weights, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6X9xZXFQvnWF"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class CosineLoss():\n",
    "    def __init__(self, vocabsize, ignore_index, reduction=\"mean\"):\n",
    "        self.vocabsize = vocabsize\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eye = torch.eye(self.vocabsize).to(device)\n",
    "        self.cosSim = nn.CosineSimilarity(dim=1)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, outputs, labels):\n",
    "        maxlen = outputs.shape[0]\n",
    "        bs = outputs.shape[1]\n",
    "\n",
    "        # Deal with positions which should be ignored and make them the same as label\n",
    "        if self.ignore_index is not None:\n",
    "            ignore_positions = (labels == self.ignore_index).to(device)\n",
    "            outputs[ignore_positions] = self.eye[self.ignore_index]\n",
    "\n",
    "        # Convert labels to onehot\n",
    "        labels_onehot = torch.empty((maxlen, bs, self.vocabsize)).to(device)\n",
    "        for idx in range(maxlen):\n",
    "            labels_onehot[idx,:,:] = self.eye[labels[idx,:]]\n",
    "\n",
    "        # Put labels and output in correct form for torch cosSim function\n",
    "        batch_first_labels = labels_onehot.permute(1,0,2)\n",
    "        processed_labels = batch_first_labels.reshape(-1, maxlen * self.vocabsize)\n",
    "        batch_first_outputs = outputs.permute(1,0,2)\n",
    "        processed_outputs = batch_first_outputs.reshape(-1, maxlen * self.vocabsize)\n",
    "\n",
    "        # use cosSim function\n",
    "        res = (1 - self.cosSim(processed_labels, processed_outputs))\n",
    "\n",
    "        # Same interface as native los functions: reductions for the output\n",
    "        if self.reduction == \"none\":\n",
    "            return res\n",
    "        elif self.reduction == \"mean\":\n",
    "            return res.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return res.sum()\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3d2C9Rvt22w"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Fut2CtQrt22w"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch\n",
    "        trg = src\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        if isinstance(criterion, CosineLoss):\n",
    "            loss = criterion(output, trg)\n",
    "        else:\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VIT6a8uqt22w"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "            \n",
    "            #seq = [seq_len, batch_size]\n",
    "            #output = [seq_len, batch_size, output_dim]\n",
    "\n",
    "            if isinstance(criterion, CosineLoss):\n",
    "                loss = criterion(output, seq)\n",
    "            else:\n",
    "                output_dim = output.shape[-1]\n",
    "                \n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = seq[1:].view(-1)\n",
    "                \n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #output = [(trg len - 1) * batch size, output dim]\n",
    "                \n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "uOtABRKLvnWI"
   },
   "outputs": [],
   "source": [
    "def evaluate_extra(model, dataloader, loss_func):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for seqs, seqs_len in dataloader:\n",
    "\n",
    "            outputs = model(seqs, seqs_len, seqs, 0)\n",
    "\n",
    "            loss = loss_func(outputs, seqs) / seqs.shape[1]\n",
    "\n",
    "            loss_total += loss.item()\n",
    "        \n",
    "#        if loss_func == allOrNoneLoss:\n",
    "#            return loss_total\n",
    "\n",
    "        return loss_total / len(dataloader)\n",
    "\n",
    "def cutEndToken(seq):\n",
    "    ret = []\n",
    "    for stim in seq:\n",
    "        if stim == END_TOKEN:\n",
    "            break\n",
    "        ret.append(stim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def allOrNoneLoss(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    ret = 0\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        ret += not p == t\n",
    "    return torch.tensor(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lW3r-pjXt22x"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUY7o5eGt22x"
   },
   "source": [
    "During Training in addition to collecting the train/validation loss, collect the amount of entirely correct predicted sequences on the train and test gr/ugr set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JAVqiZ3kvnWJ"
   },
   "outputs": [],
   "source": [
    "def fit(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtZwmf8egUu"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9S4PXbSe06C"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P57X-q51t22n"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le1kHQOIt22o"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P_t5icjNt22o"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v371q1Tkt22q"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ECjR4jZot22q"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toSHWJmEt22q"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
    "\n",
    "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
    "\n",
    "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
    "\n",
    "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
    "\n",
    "The attention at each time-step is stored in the `attentions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OkcTBfr-t22s"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVj3zdDt22d"
   },
   "source": [
    "## Data\n",
    "\n",
    "First, get the training and test sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH6UqaslvnV_"
   },
   "source": [
    "### Sequencedataset\n",
    "Define a Dataset for Sequences:\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rBmNkT78t22d"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int): amount of sequences generated\n",
    "        \"\"\"\n",
    "        self.seqs = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x7ocJD0vnWB"
   },
   "source": [
    "### collate_batch\n",
    "Define collate_batch for the Dataloader: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "\n",
    "Sequences are padded and their non-padded lengths are returned.\n",
    "Since pack_padded_sequences requires sequences to be sorted, they are sorted too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tOBuBfPPt22e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    seq_lens = []\n",
    "    processed_seqs = []\n",
    "    # Sort in descending order\n",
    "    batch.sort(reverse=True, key=(lambda x: len(x)))\n",
    "    # append start and end token\n",
    "    for seq in batch:\n",
    "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
    "        seq_lens.append(len(seq))\n",
    "        processed_seqs.append(torch.tensor(seq))\n",
    "    # pad\n",
    "    padded_seqs = pad_sequence(processed_seqs)\n",
    "    seq_lens = torch.tensor(seq_lens)\n",
    "    return padded_seqs.to(device), seq_lens.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiskh9dalsvW"
   },
   "source": [
    "### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "4UgG2QIFlsg3"
   },
   "outputs": [],
   "source": [
    "N_TASKS = 2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "START_TOKEN = 1\n",
    "END_TOKEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2cXtZIVe5_q"
   },
   "source": [
    "### Task loading\n",
    "For reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "HREfj6IuvnWJ"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhSjYo6vnWK"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jWwIlfPcvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs = data.g0_train()\n",
    "valid_seqs = data.g0_train()\n",
    "test_seqs = data.g0_test_gr()\n",
    "test_ugr_seqs = data.g0_test_ugr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5I5lDz8vnWL"
   },
   "source": [
    "Sort for better perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ocjMQF-RvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs.sort(key=(lambda x: len(x)))\n",
    "valid_seqs.sort(key=(lambda x: len(x)))\n",
    "test_seqs.sort(key=(lambda x: len(x)))\n",
    "test_ugr_seqs.sort(key=(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "zMSM652MvnWL"
   },
   "outputs": [],
   "source": [
    "def buildVocab(letterset):\n",
    "    vocab = {'<pad>': PAD_TOKEN, '<sos>': START_TOKEN, '<eos>': END_TOKEN}\n",
    "    counter = len(vocab)\n",
    "    for letter in letterset:\n",
    "        vocab[letter] = counter\n",
    "        counter +=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "yBCWnEM1vnWM"
   },
   "outputs": [],
   "source": [
    "letters = set()\n",
    "for seq in train_seqs:\n",
    "    [letters.add(letter) for letter in seq]\n",
    "letters = list(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPjDWlVPvnWM"
   },
   "source": [
    "Make all tasks, creates an additional task with all sequences from all tasks mashed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "YtuaqeKNvnWM"
   },
   "outputs": [],
   "source": [
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idxs = []\n",
    "idx2lets = []\n",
    "\n",
    "usedpermutations = set()\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        temp_letters = copy.copy(letters)\n",
    "\n",
    "        while str(temp_letters) in usedpermutations:\n",
    "            random.shuffle(temp_letters)\n",
    " \n",
    "        usedpermutations.add(str(temp_letters))\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Conversion Functions\n",
    "        let2idxs.append(lambda seq: [vocabs[-1][let] for let in seq])\n",
    "        idx2lets.append(lambda seq: [rvocabs[-1][let] for let in seq])\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idxs[-1](seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idxs[-1](seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idxs[-1](seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idxs[-1](seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-R9Nw9XB8Iw"
   },
   "source": [
    "### Task loading v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "E2Sn2FU9CAlD"
   },
   "outputs": [],
   "source": [
    "SWAP1 = 2\n",
    "SWAP2 = 4\n",
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idx = lambda task_id, seq: [vocabs[task_id][let] for let in seq]\n",
    "idx2let = lambda task_id, seq: [rvocabs[task_id][let] for let in seq]\n",
    "\n",
    "usedpermutations = set()\n",
    "listUsedPermutations = []\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "alternate = 0\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        if alternate == 0:\n",
    "            temp_letters = copy.copy(letters)\n",
    "\n",
    "            while str(temp_letters) in usedpermutations:\n",
    "                random.shuffle(temp_letters)\n",
    "    \n",
    "            usedpermutations.add(str(temp_letters))\n",
    "            listUsedPermutations.append(temp_letters)\n",
    "\n",
    "            alternate = 1\n",
    "        else:\n",
    "            temp_letters = listUsedPermutations[-1]\n",
    "\n",
    "            #Swap 5th and 6th indice\n",
    "            temp_letters[SWAP1], temp_letters[SWAP2] = temp_letters[SWAP2], temp_letters[SWAP1]\n",
    "\n",
    "            usedpermutations.add(str(temp_letters))\n",
    "\n",
    "            alternate = 0\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idx(t, seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idx(t, seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idx(t, seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idx(t, seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVaRvD2svnWN",
    "outputId": "ddaa8495-e224-4457-af03-78c62225b990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'A': 3, 'D': 4, 'F': 5, 'G': 6, 'C': 7}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'A': 3, 'D': 4, 'C': 5, 'G': 6, 'F': 7}\n",
      "\n",
      "First Batch of Task 0:\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [2]])\n",
      "\n",
      "First Batch of Task 1:\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "print(vocabs[0])\n",
    "print(vocabs[1])\n",
    "for i in range(len(valid_dls) - 1):\n",
    "    for seqs, _ in valid_dls[i]:\n",
    "        print(f\"\\nFirst Batch of Task {i}:\")\n",
    "        print(seqs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFM4eGVOs2lZ",
    "outputId": "ef66cff7-2e19-4a9e-8b0e-9fb932381c00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(task_id, seq)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzTfiG5qPpgS",
    "outputId": "be70246f-611c-4bd8-98e7-d46f7b1dceca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 5] - [3, 5, 7]\n",
      "[3, 7, 6, 5] - [3, 5, 6, 7]\n",
      "[3, 4, 7, 5] - [3, 4, 5, 7]\n",
      "[3, 7, 5, 7, 6] - [3, 5, 7, 5, 6]\n",
      "[3, 4, 7, 5, 7] - [3, 4, 5, 7, 5]\n",
      "[3, 7, 6, 5, 7, 6] - [3, 5, 6, 7, 5, 6]\n",
      "[3, 4, 7, 5, 7, 6] - [3, 4, 5, 7, 5, 6]\n",
      "[3, 4, 7, 6, 5, 7, 6] - [3, 4, 5, 6, 7, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "convTrainSeqs = lambda task_id: [let2idx(task_id, seq) for seq in train_seqs]\n",
    "convTestSeqs = lambda task_id: [let2idx(task_id, seq) for seq in test_ugr_seqs]\n",
    "tr1 = convTrainSeqs(0)\n",
    "tr2 = convTrainSeqs(1)\n",
    "te1 = convTestSeqs(0)\n",
    "te2 = convTestSeqs(1)\n",
    "\n",
    "for seq in tr1:\n",
    "    if seq in tr2:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "for se in tr2:\n",
    "    if se in te1:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "\n",
    "for i in range(len(tr1)):\n",
    "    print(f\"{tr1[i]} - {tr2[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xoKXlA1vnWQ"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "jc0kqVE-vnWQ"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = max(vocabs[0].values()) + 1\n",
    "OUTPUT_DIM = max(vocabs[0].values()) + 1\n",
    "ENC_EMB_DIM = 30\n",
    "DEC_EMB_DIM = 30\n",
    "ENC_HID_DIM = 10\n",
    "DEC_HID_DIM = 10\n",
    "ENC_DROPOUT = 0.7\n",
    "DEC_DROPOUT = 0.7\n",
    "LEARNING_RATE = 0.01\n",
    "SRC_PAD_IDX = PAD_TOKEN\n",
    "TRG_PAD_IDX = PAD_TOKEN\n",
    "PREFIX = \"tr\"\n",
    "N_EPOCHS = 200\n",
    "CLIP = 1\n",
    "STEP_SIZE_EVALUATION = 10\n",
    "TEST_ALL_TASKS = 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpwRQ7xLvnWR"
   },
   "source": [
    "## Plotting & Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhO-beIdl9SC"
   },
   "source": [
    "### plotTranser, plotResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "y6vb8He6vnWR"
   },
   "outputs": [],
   "source": [
    "def plotTransfer(data, title):\n",
    "    # data = [n_methods, n_tasks, combinedepochs]\n",
    "    n_methods, n_tasks, n_combinedepochs = data.shape\n",
    "    fig, axs = plt.subplots(n_tasks, 1)\n",
    "    colors = ['blue','green','orange','red','yellow','violett']\n",
    "    \n",
    "    xvals = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    for task_idx in range(n_tasks):\n",
    "        for method_idx in range(n_methods):\n",
    "            axs[task_idx].plot(\n",
    "                xvals,\n",
    "                data[method_idx, task_idx],\n",
    "                color=colors[method_idx]\n",
    "            )\n",
    "            axs[task_idx].set_ylim(0,1.1)\n",
    "            if task_idx != n_tasks - 1:\n",
    "                axs[task_idx].tick_params(\n",
    "                    axis='x',\n",
    "                    which='both',\n",
    "                    labelbottom=False\n",
    "                )\n",
    "        x_lines = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, N_EPOCHS)\n",
    "        for xpos in x_lines:\n",
    "            axs[task_idx].axvline(xpos, color=\"grey\")\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "def plotResults(hist_loss, hist_hits, plotLoss=False):\n",
    "    if plotLoss:\n",
    "        plotTransfer( hist_loss[:,0,:].unsqueeze(0), \"Train Loss\")\n",
    "        plotTransfer( hist_loss[:,1,:].unsqueeze(0), \"Test Gr Loss\")\n",
    "        plotTransfer( hist_loss[:,2,:].unsqueeze(0), \"Test Ugr Loss\")\n",
    "    plotTransfer( hist_hits[:,0,:].unsqueeze(0), \"Train Hits\")\n",
    "    plotTransfer( hist_hits[:,1,:].unsqueeze(0), \"Test Gr Hits\")\n",
    "    plotTransfer( hist_hits[:,2,:].unsqueeze(0), \"Test Ugr Hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMGZLw18mBw9"
   },
   "source": [
    "### visual_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "kQWqbao6vnWR"
   },
   "outputs": [],
   "source": [
    "def visual_eval(model, test_dl):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    errors = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_dl):\n",
    "\n",
    "            src, src_len = batch\n",
    "            trg = src\n",
    "\n",
    "            output = model(src, src_len, trg, 0) #turn off teacher forcing\n",
    "            show_batch(output, trg)\n",
    "\n",
    "\n",
    "def show_batch(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        status = \"same\" if p == t else \"different\"\n",
    "        print(f\"pred = {p} - {status} \\ntrg  = {t}\\n-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xnLRCNmDu1"
   },
   "source": [
    "### accuracy, accuracyAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "nPn1QguwvnWR"
   },
   "outputs": [],
   "source": [
    "def accuracy(model):\n",
    "    for task_id in range(N_TASKS + 1):\n",
    "        gr_not_hits = evaluate_extra(model, test_dls[task_id], allOrNoneLoss)\n",
    "        ugr_not_hits = evaluate_extra(model, test_ugr_dls[task_id], allOrNoneLoss)\n",
    "        gr_hits = 1 - gr_not_hits\n",
    "        ugr_hits = 1 - ugr_not_hits\n",
    "        total_acc = (gr_hits + ugr_not_hits) / 2\n",
    "        print(f\"Task {task_id}: Acc {total_acc:2.2}% | Gr acc {gr_hits:2.2} | Ugr acc {ugr_not_hits:2.2}\")\n",
    "        \n",
    "def accuracyAll(models):\n",
    "    for model_id in range(len(models)):\n",
    "        print(f\"\\nModel {model_id}\")\n",
    "        accuracy(models[model_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VCOcUbMvnWS"
   },
   "source": [
    "## Baseline A: Individual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4QnSSsYav-F"
   },
   "source": [
    "### Experiment Individual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "I9vjyK31vnWT"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HS5ogr-8vnWT",
    "outputId": "062a393f-f2da-4723-a4d8-38b61302b32a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr-AE-30-10-0.01-A0\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.184\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-A1\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.622 | Train PPL:   1.863\n",
      "\t Val. Loss: 0.469 |  Val. PPL:   1.599\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.494\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.561\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.467\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.473\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.440\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.451\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.341\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.139\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "tr-AE-30-10-0.01-A2\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.772\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.574\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.458 | Train PPL:   1.581\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.518\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.493\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.453\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.379\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.354\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.220\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.203\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.212\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.174\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n"
     ]
    }
   ],
   "source": [
    "models_A = []\n",
    "hist_losses_A = []\n",
    "hist_hitsss_A = []\n",
    "for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"A{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    print(title)\n",
    "    print(model.apply(init_weights))\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_A.append(hist_loss_temp)\n",
    "    hist_hitsss_A.append(hist_hits_temp)\n",
    "    models_A.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAVeSfB_vnWU"
   },
   "source": [
    "Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "WBCWzONDvnWU",
    "outputId": "2c40070d-31a3-4913-98c2-51e44b8432f5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_A = torch.cat(hist_losses_A, dim=2)\n",
    "hist_hits_A = torch.cat(hist_hitsss_A, dim=2)\n",
    "\n",
    "plotResults(hist_loss_A, hist_hits_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq_UPpKlvnWV"
   },
   "source": [
    "In numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlY2ObLqvnWV",
    "outputId": "6476a3b7-512d-4bc0-9dd2-dc47319cbef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.81% | Gr acc 0.62 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD_rpqtSvnWV"
   },
   "source": [
    "## Baseline B: Keep Training same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqEL860-a9t9"
   },
   "source": [
    "### Experiment Keep Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "TRGPK1CjvnWV"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9zXm7oSvnWW",
    "outputId": "b2d8e43a-6773-4c10-e7d7-15fe7a662fb2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-B0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.184\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-B1\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "tr-AE-30-10-0.01-B2\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.178\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.176\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n"
     ]
    }
   ],
   "source": [
    "models_B = []\n",
    "hist_losses_B = []\n",
    "hist_hitsss_B = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"B{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    print(title)\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_B.append(hist_loss_temp)\n",
    "    hist_hitsss_B.append(hist_hits_temp)\n",
    "    models_B.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "xVWE0hVcvnWW",
    "outputId": "1ec24514-6caa-4966-b8cd-6235d1bcf63d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAi2UlEQVR4nO3de5xVdb3/8deHEUgCRBgUBGQAkd/BKzAQZpld5AdG4mVMqCS8EXh51Klz0n6e6tfVfvk7diJNwjS08nLSUo6hdtTyUpozEKho1IgXZoYQCEXEuH7OH9+1ZTvsmdnD7D1rzVrv5+OxH3vvtdes+XwZ4D3f7/qu7zJ3R0REJGm6xV2AiIhIIQooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJFMnM7jOzT5fx+KvM7ORyHV+kqzFdByVpZmZb8972ArYDu6P3n3H3n3dSHS8BF7r7g3nb5kTb3ldg//8LHOHun+qM+kSS6IC4CxApJ3fvnXtdKCTyPjvA3Xd1Zm0i0joN8UkmmdnJZtZgZpeb2d+An5jZwWZ2r5ltMLPN0euheV/zOzO7MHo9x8weN7P/H+37oplN62BNL5nZR8xsKvB/gHPMbKuZrcz7nmvM7I3o+32yI99PJOkUUJJlg4D+wHBgLuHfw0+i94cDbwHXtvL17wFWA5XAd4Ebzcw6WpS73w98G7jD3Xu7+3Fm9m5gATDN3fsA7wVWdPR7iSSZhvgky/YAX3X37dH7t4C7ch+a2beA37by9S+7+w3RvjcDPwQOBf7Wwv53m1n+MGIPYHk76z3azF5x93XAunZ8rUiXox6UZNkGd/9H7o2Z9TKzH5nZy2a2BXgU6GdmFS18/dtB5O7bope9W9gX4HR375d7ABcXW6i7vwmcA8wD1pnZr83sfxX79SJdkQJKsqz5FNYvAGOA97h7X+CkaHuHh+32wz7Ta939AXc/BRgM/Bm4odOrEulECiiRvfoQhvleM7P+wFdjrGU9UGVm3QDM7FAzOy06F7Ud2Mre6fIiqaSAEtnrP4ADgY3Ak8D9Mdbyi+h5k5ktJ/xb/QLQBPwd+ADtGCIU6Yp0oa6IiCSSelAiIpJICigREUkkBZSIiCSSAkpERBJJASUiIomkgBIRkURSQImISCIpoEREJJEUUCIikkgKKBERSSQFlIiIJJICSkREEkkBJSIiiaSAEhGRRFJAiYhIIimgREQkkRRQIiKSSAfE9Y0rKyu9qqqqQ8fYtGkTAAMGDChBRZIk+tmmm36+km/ZsmUb3X1g8+2xBVRVVRV1dXUdOsbixYsBmDNnTscLkkTRzzbd9POVfGb2cqHtGuITEZFEajOgzOwmM3vVzJ5t4XMzswVmVm9mT5vZ+NKXKSIiWVNMD2oxMLWVz6cBo6PHXOD6jpclIiJZ1+Y5KHd/1MyqWtllBnCLuzvwpJn1M7PB7r6uVEW25NlnYc8eOOWU4vbv1g2uvBJOOqm8dUn53H8/XHMNuMddiXTEscfC6NFxVyFJV4pJEkOAtXnvG6Jt+wSUmc0l9LI4/PDDO/yN9+yB3bth27bi9n/uOfjyl+GRRzr8rSUmP/0pPP44jBsXdyXSEX//O6xfH3cVknSlCCgrsK3g77fuvghYBFBdXd3h34GPPTY8f//7xe1/9dXwxS/CqlVw1FEd/e4Sh8ZGmDABHnss7kqkIy67DDZvjrsKSbpSzOJrAIblvR8KNJXguCV33nnQsydcr7NkXVZTExx2WNxVSEf17w9btoSHSEtKEVBLgNnRbL7JwOudcf5pf1RWwtlnwy23wNatcVcj7eWugEqLgw8OP8/f/S7uSiTJiplmfhvwBDDGzBrM7AIzm2dm86JdlgJrgHrgBuDislVbAvPnwxtvwK23xl2JtNcbb8Cbbyqg0qBv3zBp6Te/ibsSSbJiZvHNauNzBy4pWUVldsIJ4dzV9dfDRReBFTqDJonU2BiehwyJtw7puG7doF8/+K//irsSSbLMrSRhFnpRK1bAH/8YdzXSHk3RmU31oNKhf3/461/hxRfjrkSSKnMBBfDJT0Lv3pos0dUooNLl4IPDs4b5pCWZDKg+feDcc+GOOyBaVFm6AAVUuvTqBcOGKaCkZZkMKAjDfNu3Q7SosnQBjY3h5Hrv3nFXIqUyZQo89BDs2hV3JZJEmQ2oY46BE0+EhQvDihSSfJpinj5TpsDrr0NtbdyVSBJlNqAg9KLq68NvcJJ8Cqj0+fCHw8QlDfNJIZkOqJqacPGuJkt0DQqo9BkwAKqrFVBSWKYDqmdPOP98WLJk7zU2kkx79oSA0jVQ6TNlSrjk47XX4q5EkibTAQXwmc+E//xuuCHuSqQ1mzbBzp3qQaXRlCnhrgS//W3clUjSZD6gRo6EqVNDQO3cGXc10hJNMU+vyZPDzEwN80lzmQ8oCJMlmpq07EqSKaDSq0cP+OAHFVCyLwUUcOqpcPjhmiyRZFqHL92mTIE1a+CFF+KuRJJEAQVUVMDcufDgg/CXv8RdjRSS60ENGhRvHVIeU6aEZ/WiJJ8CKnLBBXDAAeHCXUmepqZwSUDPnnFXIuUwejRUVSmg5J0UUJFBg+DMM8PSR2+9FXc10pymmKebWehFPfywJivJXgqoPPPnw+bNYRFZSZbGRk2QSLspU8It4J96Ku5KJCkUUHk+8AH4p3/SZIkk0ioS6ff+94fnJ5+Mtw5JDgVUHjOYNy/8Brd8edzVSM6uXbB+vQIq7Q45JNx+o64u7kokKRRQzcyeHe5To15UcqxfD+46B5UFEydqZXPZSwHVTL9+MGsW3HpruA2AxC93DZR6UOlXXR2uhdq8Oe5KJAmKCigzm2pmq82s3syuKPD5yWb2upmtiB5fKX2pnWf+fNi2DW65Je5KBLSKRJZMnBieNcwnUERAmVkFcB0wDRgLzDKzsQV2fczdj48eXy9xnZ1qwoTwD+X668PQksRLAZUdEyaEZwWUQHE9qElAvbuvcfcdwO3AjPKWFb/58+H55+HRR+OuRBobw2ofhxwSdyVSbgcfDEccofNQEhQTUEOAtXnvG6JtzZ1gZivN7D4zO6rQgcxsrpnVmVndhg0b9qPcznPOOeF8lCZLxK+pKVxIXVERdyXSGSZOVA9KgmICygpsaz7wtRwY7u7HAT8A7i50IHdf5O7V7l49cODAdhXa2Xr1gjlz4Je/DLPIJD66Bipbqqth7Vr9u5PiAqoBGJb3fijQlL+Du29x963R66VAdzOrLFmVMZk3Lyy7cuONcVeSbQqobNFECckpJqBqgdFmNsLMegAzgSX5O5jZIDOz6PWk6LibSl1sZxszBj70IfjRj8IdPyUejY26BipLxo2Dbt10HkqKCCh33wVcCjwAPA/8p7uvMrN5ZjYv2q0GeNbMVgILgJnu6Zj/Nn8+vPIK3Hdf3JVk01tvhWti1IPKjt69w5Jj6kHJAcXsFA3bLW22bWHe62uBa0tbWjLMmAGDB4fJEtOnx11N9qxbF54VUNlSXR1+KXQPS5BJNhUVUFnWvTtcdBF84xvw4oswYkTh/dzDvWwKndjt0QPOOEP3MtofugYqmyZOhJtvhoaGsD6fZJMCqggXXQTf+hYsWgRXXVV4n+99D77whZaPcfXV8C//Up760ky3es+m6urwXFurgMoyrcVXhKFD4WMfC7P5tm/f9/P774d//ddww8MXXtj38d73hjv17tnT+bV3depBZdNxx4U7XOs8VLYpoIo0fz5s2AB33fXO7atXw8yZcMwxYe2+kSP3fVxySQiqBx+Mp/aurKkpDI0efHDclUhnete7wr8pzeTLNgVUkT7yERg16p0rS2zeDKedFs4x3XMPvPvdhb/2rLNg4ECtSrE/crd614ny7MmtKJGO+cCyPxRQRerWLVy4+/jj8Mwz4SZ6s2aFiRN33QXDh7f8tT17wvnnw5Il4aSvFE+3es+uiRPhtdfC6INkkwKqHc47L4TNwoVw+eXwwAPwwx/uvVV1az7zmfCb4A03lL/ONNEqEtmVmyih81DZpYBqhwED4OMfDyFzzTVw2WVw4YXFfe2IETB1avjanTvLW2eaKKCy66ijwrkonYfKLgVUO118cQiYD384hFR7zJ8fLjy9557y1JY2u3fDm29qinlWde8Oxx+vHlSWKaDaafJkeOQR+NWvwjTY9jj1VDj8cE2WKFZuSr96UNk1cSIsW6a1MLNKAbUfTjoJ+vRp/9dVVMDcufDww2F6urROASXV1aEXrX8v2aSA6mQXXBB6XgsXtr1v1u3YEZ4VUNmVu/WGzkNlkwKqkw0aFFacWLwYtm2Lu5pkUw9KjjwyrG6u81DZpICKwfz54fqOO+6Iu5Jk27ED+vYN/0FJNlVUwIQJ6kFllQIqBh/4QLjfjSZLtG77dvWeJJyHWrFCl2dkkQIqBmZhVYraWnj00birSa4dOxRQEs5Dbd8OTzwRdyXS2RRQMZkzJywkO3Pm3ltKyDtt365roAROOSX8PZg9OyzYLNmhgIpJ377hgt033oDTTw+3Npd3Ug9KAPr3D9cdrl8PNTV7Z3dK+imgYnT00fDzn4cLES+8UKs259u5M/x5KKAEwjDfjTeGIfHLLtO/laxQQMXstNPgm9+EW2+F73437mqSQ1PMpblPfAKuuCLc2fqHP4y7GukMRQWUmU01s9VmVm9mVxT43MxsQfT502Y2vvSlpteXvgTnnBOe77037mqSITeMo3NQku+b34Tp0+Gznw0rski6tRlQZlYBXAdMA8YCs8xsbLPdpgGjo8dcQBOo28EMbroJxo0LvyU+91zcFcVPPSgppKIiDIsfeSScfbbuFZV2xSx3Ogmod/c1AGZ2OzADyP9vdAZwi7s78KSZ9TOzwe6+ruQVp1SvXnD33WGsffp0OOOMuCuKV25x0EGD4q1Dkqdv33Dzz0mTwgLM06fHXVF2HXFEWHigXMzbONtoZjXAVHe/MHp/LvAed780b597ge+4++PR+4eAy929rtmx5hJ6WABjgFIsAVkJbCzBcbqKLLU3S20FtTfNstRWaH97h7v7wOYbi+lBWYFtzVOtmH1w90XAoiK+Z9HMrM7dq0t5zCTLUnuz1FZQe9MsS22F0rW3mEkSDcCwvPdDgab92EdERKRoxQRULTDazEaYWQ9gJrCk2T5LgNnRbL7JwOs6/yQiIh3R5hCfu+8ys0uBB4AK4CZ3X2Vm86LPFwJLgVOBemAbcF75St5HSYcMu4AstTdLbQW1N82y1FYoUXvbnCQhIiISB60kISIiiaSAEhGRRFJAiYhIIimgREQkkRRQIiKSSAooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJiEgiFXM/qLKorKz0qqqqDh1j06ZNAAwYMKAEFUmS6Gebbvr5Sr5ly5Zt3N8bFpZFVVUVdXV1be/YisWLFwMwZ86cjhckiaKfbbrp5yv5zOzlQts1xCciIonUZkCZ2U1m9qqZPdvC52ZmC8ys3syeNrPxpS9TRESyppge1GJgaiufTwNGR4+5wPUdL0tERLKumDvqPmpmVa3sMgO4xcOdD580s35mNli3fJeOWL8eNmyIuwoRiVMpzkENAdbmvW+Itu3DzOaaWZ2Z1W3Q/z7Sis2bYdMm2Lo17kpEJC6lCCgrsK3gfeTdfZG7V7t79cCB+8woFHnb9u3hec2aeOsQkfiUIqAagGF574cCTSU4rmTYjh3hub4+3jpEJD6lCKglwOxoNt9k4HWdf5KOyvWgXngh3jpEJD5tTpIws9uAk4FKM2sAvgp0B3D3hcBS4FSgHtgGnFeuYiUbtm6F3bvDawWUSHYVM4tvVhufO3BJySqSzGvKGyBWQIlkl1aSkMTJBVT37gookSxTQEni5ALqoIPglVdg58546xGReCigJHEaG8PzQQeFc1EvF1xGUkTSTgElidPUBN26QZ8+4b2G+USySQElidPUBD17wrveFd4roESySQElidPUBD167A0pBZRINimgJHEaG0M4AYwcqYASySoFlCSK+94hPoBRoxRQIlmlgJJE2bw5LHPUo0d4P2pUWDDWCy4/LCJppoCSRMldA5Xfg9q2Df72t/hqEpF4KKAkUXLXQOX3oEDDfCJZpICSRCnUgwIFlEgWKaAkUXIBletBVVWFi3YVUCLZo4CSRGlqgv79QyhBCKphwxRQIlmkgJJEaWyEww575zZNNRfJJgWUJEpTkwJKRAIFlCRKUxMMGfLObaNGwcaNsGVLPDWJSDwUUJIYu3eH650K9aBAvSiRrFFASWK8+moIKQWUiIACShIkN8VcASUiUGRAmdlUM1ttZvVmdkWBz082s9fNbEX0+ErpS5W0ywVU83NQfftCZaUCSiRrDmhrBzOrAK4DTgEagFozW+LuzzXb9TF3n16GGiUj8ntQq1a98zPN5BPJnmJ6UJOAendf4+47gNuBGeUtS7KosRHM4NBD9/1MASWSPcUE1BBgbd77hmhbcyeY2Uozu8/Mjip0IDOba2Z1Zla3YcOG/ShX0qypKYTTAQX69aNGwdq1sGNH59clIvEoJqCswLbmd+dZDgx39+OAHwB3FzqQuy9y92p3rx44cGC7CpX0K3QNVM6oUbBnD7z0UqeWJCIxKiagGoBhee+HAk35O7j7FnffGr1eCnQ3s8qSVSmZUGiZoxzN5BPJnmICqhYYbWYjzKwHMBNYkr+DmQ0yM4teT4qOu6nUxUq6FVrmKEcBJZI9bc7ic/ddZnYp8ABQAdzk7qvMbF70+UKgBphvZruAt4CZ7rpJtxRv+/awnFFLQ3yDBkGvXgookSxpM6Dg7WG7pc22Lcx7fS1wbWlLkyzJ3dK9pR6UGYwcqYASyRKtJCGJkLvVe0sBBZpqLpI1CihJhJaWOco3ahSsWRNm84lI+imgJBFaWuYo36hR8I9/wLp1nVOTiMRLASWJ0NQE3bvDgAEt76OZfCLZooCSRMhdA2WFLguPKKBEskUBJYnQ2ioSOcOHQ0WFAkokKxRQkgitXaSb0707HH64AkokK4q6Dkqk3JqaYMqUtvcbNQpWrIB77933s7Fjw7VSIpIOCiiJ3datsGVL2z0ogGOPhQcfhI99bN/PevWCP/wBjjuu9DWKSOdTQEnsiplinvPtb8MnPgHNF9L6xz9g1iw47TSorYVDDil9nSLSuRRQErtiLtLN6dkTJkwo/Nndd8P73gc1NaGX1aNHyUoUkRhokoTErj0B1ZoJE+AnP4HHHoNLL923lyUiXYt6UBK7YtbhK9bMmfDMM2Eo8Ljj4JJLOn5MEYmHelASu6Ym6N0b+vYtzfG+8Y0wieKzn4WHHy7NMUWk86kHJbEr5hqo9ujWDX72M3jve+Hss+GJJ6CqqnTHb65799ZXwJDiuevPUvZSD0piV+qAgtAbu+ee8HrMmDC5olyP6dNh587S1p9FDz0EgwfDnXfGXYkkhXpQErvGxtDbKbVRo+CRR2DJktIfO2f9eliwAD73ObjuuvJ9n7R74YXQ2928GWbPDj+7cePirkripoCSWLmXpweVc/TR4VFOPXvC1VfDMcfAvHnl/V5ptGVLuH7NDH7/ezjnHJgxI1zPduihcVcncdIQn8Rq82bYvr18AdUZrroKpk2Dyy4LPTYp3u7d8MlPwurV8ItfhJ70PffAxo1w5pnh74ZklwJKYlWqa6DiVFEBt90WhqXOOgtefDHuirqOL385rKv4/e/Dhz4Uto0fD4sXh2Wr5s/X9WxZVlRAmdlUM1ttZvVmdkWBz83MFkSfP21m40tfqqRR7hqoYpY5SrKDDgrnunbvDsNTW7fGXVHyvfpq6H3OnQsXX/zOzz7+cfi3fwsXXi9YEE99Er82A8rMKoDrgGnAWGCWmY1ttts0YHT0mAtcX+I6JaXS0IPKOfJIuOMOWLUqnOjfsyfuipLrjTfCsN773w8/+EHhqeVf+1oI+89/Hn7zm86vUeJXzCSJSUC9u68BMLPbgRnAc3n7zABucXcHnjSzfmY22N3XlbziPM88E35jPfnkcn4XKae1a8Pz4MHx1lEqU6bAv/87/PM/w8SJ0KdP3BUl0+jR4fqxu+5qec3Ebt3gpz8N56VqasLQnyTLuHHwve+V7/jmbQzwmlkNMNXdL4zenwu8x90vzdvnXuA77v549P4h4HJ3r2t2rLmEHhbAGGB1CdpQCWwswXG6iiy1N0ttBbU3zbLUVmh/e4e7+8DmG4vpQRW6rrt5qhWzD+6+CFhUxPcsmpnVuXt1KY+ZZFlqb5baCmpvmmWprVC69hYzSaIBGJb3fijQtB/7iIiIFK2YgKoFRpvZCDPrAcwEml+bvwSYHc3mmwy8Xu7zTyIikm5tDvG5+y4zuxR4AKgAbnL3VWY2L/p8IbAUOBWoB7YB55Wv5H2UdMiwC8hSe7PUVlB70yxLbYUStbfNSRIiIiJx0EoSIiKSSAooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJiEgiKaBERCSRFFAiIpJICigREUkkBZSIiCRSMfeDKovKykqvqqrq0DE2bdoEwIABA0pQkSSJfrbppp+v5Fu2bNnG/b1hYVlUVVVRV1fX9o6tWLx4MQBz5szpeEGSKPrZppt+vpLPzF4utL3NIT4zu8nMXjWzZ1v43MxsgZnVm9nTZja+o8WKiIgUcw5qMTC1lc+nAaOjx1zg+o6XJSIiWddmQLn7o8DfW9llBnCLB08C/cxscKkKFBGRbCrFLL4hwNq89w3RNhERkf1WioCyAtsK3qbXzOaaWZ2Z1W3YsKEE31pERNKqFAHVAAzLez8UaCq0o7svcvdqd68eOHCfGYUiIiJvK0VALQFmR7P5JgOvu/u6EhxXREQyrM3roMzsNuBkoNLMGoCvAt0B3H0hsBQ4FagHtgHnlatYERHJjjYDyt1ntfG5A5eUrCIRERG0Fp+IiCSUAkpERBJJASUiIomkgBIRkURSQImISCIpoEREJJEUUCIikkgKKBERSSQFlIiIJJICSkREEkkBJSIiiaSAEhGRRFJAiYhIIimgREQkkRRQIiKSSAooERFJJAWUiIgkkgJKREQSSQElIiKJpIASEZFEUkCJiEgiFRVQZjbVzFabWb2ZXVHg85PN7HUzWxE9vlL6UkVEJEsOaGsHM6sArgNOARqAWjNb4u7PNdv1MXefXoYaRUQkg4rpQU0C6t19jbvvAG4HZpS3LBFJq7Vr4fe/h/Xr465Ekq6YgBoCrM173xBta+4EM1tpZveZ2VGFDmRmc82szszqNmzYsB/likhX9+ijsGsXrF4NTzwRdzWSZMUElBXY5s3eLweGu/txwA+AuwsdyN0XuXu1u1cPHDiwXYWKSDrU1kK3btCzJ5xxRuhRiRRSTEA1AMPy3g8FmvJ3cPct7r41er0U6G5mlSWrUkRSo64OeveGY46Bbdvg9NPDs0hzxQRULTDazEaYWQ9gJrAkfwczG2RmFr2eFB13U6mLFZGubdcuWL4c+vSBXr3g1lvhT3+C888Hbz4uI5nXZkC5+y7gUuAB4HngP919lZnNM7N50W41wLNmthJYAMx01183EXmn55+Ht94KAQUwfTpcdRXccUd4FsnX5jRzeHvYbmmzbQvzXl8LXFva0kQkbWprw3MuoAC++EV4+mm48ko46iiYoTnCEtFKEiLSaerqoG/fMLyXYwY//jFUV8OnPgVr1sRXnySLAkpEOk1tLUyYsO/2Aw+EO++EN9+Em2/u/LokmRRQItIptm+HlStDT6mQ4cPhpJNCUImAAkpEOskzz8DOnTBxYsv71NTAc8+Fh4gCSkQ6RV1deG6pBwVw5pnh+a67yl+PJJ8CSkQ6RW0tDBgAVVUt73PYYXDiiRrmk0ABJSKdoq4u9J6s0OJpeWpqwrTzv/ylc+qS5FJAiUjZbdsGq1a1fv4p56yzwrOG+UQBJSJlt2IF7N7d+vmnnGHDYPJkDfOJAkpEOkFuBYlielAQhvmWL9dFu1mngBKRsqurg8GDwySIYmiYT0ABJSKdoLa2+N4ThJl+1dUa5ss6BZSIlNWWLeHuucWcf8pXUwNPPQUvv1yeuiT5FFAiUlbLloXn9vSgYO8w3y9/Wdp6pOtQQIlIWRWzgkQhRxwBxx+vYb4sU0CJSFnV1oZzSpWV7f/amhr4wx+gsbHkZUkXoIASkbLKrSCxP2pqwrOG+bJJASUiZbNxI7z4YvvPP+WMGQNHH61hvqxSQIlI2ezvBIl8NTXw2GNhqSTJFgWUiJRNbgWJ8eP3/xjnnhtuEz9+PHz1q/DWW6WpTZKvqIAys6lmttrM6s3sigKfm5ktiD5/2sw68NdRRNKiri4M0x100P4fY+RI+POf4eyz4etfD0N+S5eWrkZJrjYDyswqgOuAacBYYJaZjW222zRgdPSYC1xf4jpFpAuqrd3/CRL5Bg2Cn/0MHn4YevSAj3403NzwlVc6fmxJrgOK2GcSUO/uawDM7HZgBpB/U+YZwC3u7sCTZtbPzAa7+7qSV5zniSfCCsmXXVbO7yJxOOec8KyfbdflDm++WZqAyvngB2HlSrjmmtCbGjkSDjywdMeX9jnpJPj1r8t3fAuZ0soOZjXAVHe/MHp/LvAed780b597ge+4++PR+4eAy929rtmx5hJ6WABjgNUlaEMlsLEEx+kqstTeLLUV1N40y1Jbof3tHe7uA5tvLKYHVej+l81TrZh9cPdFwKIivmfRzKzO3Uv4O1qyZam9WWorqL1plqW2QunaW8wkiQZgWN77oUDTfuwjIiJStGICqhYYbWYjzKwHMBNY0myfJcDsaDbfZOD1cp9/EhGRdGtziM/dd5nZpcADQAVwk7uvMrN50ecLgaXAqUA9sA04r3wl76OkQ4ZdQJbam6W2gtqbZllqK5SovW1OkhAREYmDVpIQEZFEUkCJiEgiddmAamv5pa7IzG4ys1fN7Nm8bf3N7L/N7K/R88F5n30pav9qM/vf8VS9f8xsmJn91syeN7NVZvbZaHta2/suM3vKzFZG7f1atD2V7YWwCo2Z/Sm6TjLVbQUws5fM7BkzW2FmddG2VLY5WozhTjP7c/Rv+ISytNXdu9yDMFnjBWAk0ANYCYyNu64StOskYDzwbN627wJXRK+vAP5f9Hps1O6ewIjoz6Mi7ja0o62DgfHR6z7AX6I2pbW9BvSOXncH/ghMTmt7ozZ8HrgVuDd6n9q2Ru14Cahsti2VbQZuBi6MXvcA+pWjrV21B/X28kvuvgPILb/Upbn7o8Dfm22eQfjLQPR8et722919u7u/SJhBOakz6iwFd1/n7suj128AzwNDSG973d23Rm+7Rw8npe01s6HAR4Ef521OZVvbkLo2m1lfwi/TNwK4+w53f40ytLWrBtQQYG3e+4ZoWxod6tE1ZdHzIdH21PwZmFkVMI7Qq0hte6MhrxXAq8B/u3ua2/sfwBeBPXnb0trWHAd+Y2bLomXdIJ1tHglsAH4SDeH+2MzeTRna2lUDqqillVIuFX8GZtYbuAv4nLtvaW3XAtu6VHvdfbe7H09YaWWSmR3dyu5dtr1mNh141d2XFfslBbZ1ibY2c6K7jyfc3eESMzuplX27cpsPIJyKuN7dxwFvEob0WrLfbe2qAZWlpZXWm9lggOj51Wh7l/8zMLPuhHD6ubv/Mtqc2vbmRMMhvwOmks72ngicZmYvEYbfP2RmPyOdbX2buzdFz68CvyIMY6WxzQ1AQzQCAHAnIbBK3tauGlDFLL+UFkuAT0evPw3ck7d9ppn1NLMRhHtxPRVDffvFzIwwhv28u1+T91Fa2zvQzPpFrw8EPgL8mRS2192/5O5D3b2K8G/zYXf/FClsa46ZvdvM+uReA1OAZ0lhm939b8BaMxsTbfow4fZLpW9r3LNBOjCL5FTCzK8XgCvjrqdEbboNWAfsJPzWcQEwAHgI+Gv03D9v/yuj9q8GpsVdfzvb+j5CN/9pYEX0ODXF7T0W+FPU3meBr0TbU9nevDaczN5ZfKltK+G8zMrosSr3f1Ja2wwcD9RFf5/vBg4uR1u11JGIiCRSVx3iExGRlFNAiYhIIimgREQkkRRQIiKSSAooERFJJAWUiIgkkgJKREQS6X8AxOrABW4jUNsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_B = torch.cat(hist_losses_B, dim=2)\n",
    "hist_hits_B = torch.cat(hist_hitsss_B, dim=2)\n",
    "\n",
    "plotResults(hist_loss_B, hist_hits_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQ9fLNObvnWW"
   },
   "source": [
    "In numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD9r3RykvnWX",
    "outputId": "e5330dc5-23ab-4038-af99-0fb47c16ec07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.94% | Gr acc 0.88 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUleAoTVvnWY"
   },
   "source": [
    "## Baseline C: Freeze Parameters\n",
    "\n",
    "1. Define functions\n",
    "2. Train model, freeze core weights in between tasks\n",
    "3. Look at performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbcfqNFlbGjA"
   },
   "source": [
    "### onTaskUpdate, applyOnParameters, freezeParameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "EiFSSGphvnWY"
   },
   "outputs": [],
   "source": [
    "def applyOnParameters(model, conditions, apply_function):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check every condition\n",
    "        for condition in conditions:\n",
    "            # check every keyword\n",
    "            allincluded = True\n",
    "            for keyword in condition:\n",
    "                if keyword not in name:\n",
    "                    allincluded = False\n",
    "                    break\n",
    "            if allincluded:\n",
    "                apply_function(param)\n",
    "\n",
    "def freezeParameters(model, conditions):\n",
    "    def freeze(param):\n",
    "        param.requires_grad = False\n",
    "    applyOnParameters(model, conditions, freeze)\n",
    "\n",
    "def unfreezeParameters(model, conditions):\n",
    "    def unfreeze(param):\n",
    "        param.requires_grad = True\n",
    "    applyOnParameters(model, conditions, unfreeze)\n",
    "\n",
    "def showModelParameters(model, requires_grad=False):\n",
    "    for name, param in model.named_parameters():\n",
    "        if requires_grad:\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "        else:\n",
    "            print(name)\n",
    "            \n",
    "def onTaskUpdate(model):\n",
    "    # Freeze core weights\n",
    "    freezeParameters(model, ((\"\"),))    # Freeze everything\n",
    "    unfreezeParameters(model, ((\"encoder\",\"embedding\"), (\"decoder\",\"fc_out\"), (\"attention\",))) # Unfreeze relevant stuff\n",
    "    \n",
    "    # Reinitialize\n",
    "    to_constant = lambda param: nn.init.constant_(param.data, 0)\n",
    "    applyOnParameters(model, ((\"decoder\",\"fc_out\",\"bias\"),(\"attn\",\"bias\")), to_constant)\n",
    "    to_normal = lambda param: nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "    applyOnParameters(model, ((\"encoder\",\"embedding\"),(\"decoder\",\"fc_out\",\"weight\"),(\"attention\",\"weight\")), to_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJlVK1CCvnWZ"
   },
   "source": [
    "### Experiment Freeze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "9_UJzijZvnWZ"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H83P-2BXvnWZ",
    "outputId": "8c4cbe78-90a0-46bc-cc0f-8a9328d47640",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-C0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.184\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-C1\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.147\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "tr-AE-30-10-0.01-C2\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.381\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.342\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n"
     ]
    }
   ],
   "source": [
    "models_C = []\n",
    "hist_losses_C = []\n",
    "hist_hitsss_C = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"C{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    print(title)\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_C.append(hist_loss_temp)\n",
    "    hist_hitsss_C.append(hist_hits_temp)\n",
    "    models_C.append(copy.deepcopy(model))\n",
    "    \n",
    "    # Freeze, reinitialize\n",
    "    onTaskUpdate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "x_07dWu8vnWa",
    "outputId": "22e6837e-a1ee-44e7-cc6c-1ccfd78122e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_C = torch.cat(hist_losses_C, dim=2)\n",
    "hist_hits_C = torch.cat(hist_hitsss_C, dim=2)\n",
    "\n",
    "plotResults(hist_loss_C, hist_hits_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6K7g1tH3vnWa",
    "outputId": "25d75c5a-f339-4071-8aad-0f4df6f05ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkGxECJqvnWb"
   },
   "source": [
    "## Transfer D: EWC\n",
    "\n",
    "1. Create Fisher functions\n",
    "2. Train model on tasks\n",
    "3. Compare results\n",
    "\n",
    "Based on: https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNS4W04AvnWc"
   },
   "source": [
    "Compute optimal parameters and fisher information after training on tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI4QzSeavnWc"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_ewc(model, task_id, train_dl, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #accumulate Gradient\n",
    "    for it in range(100):\n",
    "        for seq, seq_len in train_dl:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0)\n",
    "\n",
    "            if criterion == F.cross_entropy:\n",
    "              output_dim = output.shape[-1]\n",
    "                \n",
    "              output = output[1:].view(-1, output_dim)\n",
    "\n",
    "              trg = seq[1:].view(-1)\n",
    "\n",
    "              loss = criterion(output, trg)\n",
    "            else:\n",
    "              loss = criterion(output, seq)\n",
    "            #print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "        \n",
    "    fishers.append({})\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        fishers[task_id][name] = param.grad.data.clone().pow(2)\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJu-KM7mvnWc"
   },
   "source": [
    "Adapt evaluation and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRW-TlhwvnWd"
   },
   "outputs": [],
   "source": [
    "def train_ewc(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(\"-\\n\", loss)\n",
    "        \n",
    "        # EWC Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fishers[other_task_id][name]\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                #print(ewc_lambda)\n",
    "                loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                #print((fisher * (optParam - param).pow(2)).sum())\n",
    "                #print((optParam - param).pow(2).sum())\n",
    "                #loss += ewc_lambda * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnQbtl1yvnWe"
   },
   "outputs": [],
   "source": [
    "def evaluate_ewc(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # EWC Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    fisher = fishers[other_task_id][name]\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgGMwiHcvnWe"
   },
   "outputs": [],
   "source": [
    "def fit_ewc(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NI--0ZFvnWe"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bB0VOa5lvnWf",
    "outputId": "cfd946be-3342-47d9-a885-90fb65f024f9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1112, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1422, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1009, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1829, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2079, grad_fn=<AddBackward0>)\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0826, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1142, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1145, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1264, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1391, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1517, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1385, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1511, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1339, grad_fn=<AddBackward0>)\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.2965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1228, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1162, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1303, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1152, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1298, grad_fn=<AddBackward0>)\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1306, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2151, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2300, grad_fn=<AddBackward0>)\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1146, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2175, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1107, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1256, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1189, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3172, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3321, grad_fn=<AddBackward0>)\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1111, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2133, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1113, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1131, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1082, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1157, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1317, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1101, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1240, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3278, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3444, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1205, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1088, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1260, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1217, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1379, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1557, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1368, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1546, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1193, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1245, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2326, grad_fn=<AddBackward0>)\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2703, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1196, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1361, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1592, grad_fn=<AddBackward0>)\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1140, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0984, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1472, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1120, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1283, grad_fn=<AddBackward0>)\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.1032, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1286, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1199, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1310, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1318, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1477, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1253, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1187, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1345, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1036, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2772, grad_fn=<AddBackward0>)\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1105, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1284, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1065, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1314, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1461, grad_fn=<AddBackward0>)\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1154, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1300, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1524, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1669, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1488, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1632, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1057, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1153, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1014, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1235, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1148, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1210, grad_fn=<AddBackward0>)\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1168, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1286, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0715, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0826, grad_fn=<AddBackward0>)\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1155, grad_fn=<AddBackward0>)\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0963, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1083, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0856, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0906, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1019, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1067, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1066, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2002, grad_fn=<AddBackward0>)\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1006, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0858, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3459, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1389, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1514, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0938, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1060, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1248, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1234, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1090, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1089, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0802, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1117, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1122, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1255, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1478, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1079, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1024, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1091, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1215, grad_fn=<AddBackward0>)\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1179, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1299, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0830, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1879, grad_fn=<AddBackward0>)\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1551, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1241, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1221, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1364, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1007, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1323, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1464, grad_fn=<AddBackward0>)\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1513, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1658, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1164, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1316, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1349, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1100, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1367, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1294, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1449, grad_fn=<AddBackward0>)\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1716, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1463, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1618, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "-\n",
      " tensor(0.2395, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1288, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1223, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1237, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1160, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1188, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1297, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1470, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1482, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1127, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0959, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1072, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1251, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1132, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1046, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1088, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1149, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1261, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1061, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0957, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1121, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0836, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0938, grad_fn=<AddBackward0>)\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1142, grad_fn=<AddBackward0>)\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0807, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1039, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0882, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0969, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0847, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0824, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0949, grad_fn=<AddBackward0>)\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0985, grad_fn=<AddBackward0>)\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0840, grad_fn=<AddBackward0>)\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0746, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0854, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0820, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0842, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1405, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1522, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1205, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1337, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1113, grad_fn=<AddBackward0>)\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1092, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1181, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1323, grad_fn=<AddBackward0>)\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1124, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1124, grad_fn=<AddBackward0>)\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0884, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0835, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, grad_fn=<AddBackward0>)\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0696, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0815, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0827, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0803, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0813, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0823, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0859, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0822, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0791, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0684, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0719, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0828, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1052, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0924, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0686, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0796, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0757, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0869, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1232, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0809, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1195, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1035, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1079, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1163, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1330, grad_fn=<AddBackward0>)\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1068, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2473, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0940, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2180, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2321, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.4974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1097, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2505, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0849, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1010, grad_fn=<AddBackward0>)\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1374, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1534, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1225, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1385, grad_fn=<AddBackward0>)\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0946, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0953, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1108, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1226, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1370, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0718, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1172, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0974, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2361, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0901, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2765, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.2020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1102, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1247, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1094, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1433, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1483, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1649, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1233, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1053, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1943, grad_fn=<AddBackward0>)\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1054, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1318, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2513, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1615, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1928, grad_fn=<AddBackward0>)\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0927, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1081, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2137, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2329, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1217, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1460, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1192, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1374, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1052, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1175, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3141, grad_fn=<AddBackward0>)\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1038, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1224, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1397, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1585, grad_fn=<AddBackward0>)\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1140, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1293, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1543, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1277, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1116, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2191, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2381, grad_fn=<AddBackward0>)\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2116, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2304, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1173, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1928, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2113, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0988, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1373, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1409, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1601, grad_fn=<AddBackward0>)\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1248, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1656, grad_fn=<AddBackward0>)\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1214, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2758, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1488, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1498, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1203, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1418, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1746, grad_fn=<AddBackward0>)\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1233, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1077, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1786, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1201, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1202, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1416, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2362, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0913, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1462, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3139, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2391, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1224, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1436, grad_fn=<AddBackward0>)\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1399, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1611, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1424, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1583, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1018, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1396, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1359, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1581, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1104, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1143, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1365, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1213, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1149, grad_fn=<AddBackward0>)\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1086, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1279, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1427, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2489, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1143, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1252, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1080, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0740, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0857, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1215, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, grad_fn=<AddBackward0>)\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1358, grad_fn=<AddBackward0>)\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1579, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1068, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1256, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1392, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.1264, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1181, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1087, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0824, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1005, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1176, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1322, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1493, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0904, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.1577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1725, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1030, grad_fn=<AddBackward0>)\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n"
     ]
    }
   ],
   "source": [
    "fishers = []\n",
    "optParams = []\n",
    "ewc_lambda = 10\n",
    "\n",
    "models_D = []\n",
    "hist_losses_D = []\n",
    "hist_hitsss_D = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_ewc(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D.append(hist_loss_temp)\n",
    "    hist_hitsss_D.append(hist_hits_temp)\n",
    "    models_D.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_ewc(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYEBTL-bo_Tr"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QY_YzfCoVkI"
   },
   "source": [
    "L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TVqno83vnWh"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0k0Kp66cvnWh"
   },
   "outputs": [],
   "source": [
    "accuracyAll(models_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17U8oYfQTlma"
   },
   "outputs": [],
   "source": [
    "torch.max(fishers[0]['encoder.embedding.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DnwXlOTXsbM"
   },
   "source": [
    "## Transfer D2: L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iatypwWYytb"
   },
   "source": [
    "### onTaskUpdate_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wzf8QezfX4rt"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_l2reg(model, task_id, train_dl, criterion):\n",
    "    # Save optimal parameters for each task\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-GTTipSYufm"
   },
   "source": [
    "### train_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfX9ClLaYNpv"
   },
   "outputs": [],
   "source": [
    "def train_l2reg(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        # L2 Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGgyR2yZYDfS"
   },
   "source": [
    "### eval_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RsqsxlvY86I"
   },
   "outputs": [],
   "source": [
    "def evaluate_l2reg(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # L2 Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfwySP1EZNc_"
   },
   "source": [
    "### fit_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIQ9dV7WZPX6"
   },
   "outputs": [],
   "source": [
    "def fit_l2reg(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_l2reg(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_l2reg(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_l2reg(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_l2reg(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_l2reg(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWITsrZDZz1z"
   },
   "source": [
    "### Experiment L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bki7JTypZT9E"
   },
   "outputs": [],
   "source": [
    "LAMBDA_L2REG = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08h_J3oUZ5-x"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7MObEkiZ6pa",
    "outputId": "ab9a9f0e-bb75-472d-ea4b-7dbb17755167"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.796\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.571\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.533\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.266\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.596 | Train PPL:   1.814\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.613\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.581\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.612\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.583\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.501 |  Val. PPL:   1.651\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.478 |  Val. PPL:   1.613\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.492 |  Val. PPL:   1.635\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.579\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.463 |  Val. PPL:   1.589\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.521 |  Val. PPL:   1.683\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.467 |  Val. PPL:   1.595\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.530\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.483 |  Val. PPL:   1.621\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.596\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.479 |  Val. PPL:   1.614\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.509 |  Val. PPL:   1.663\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.476 |  Val. PPL:   1.610\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.497 |  Val. PPL:   1.644\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.568\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.561\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.588\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.561\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.563\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.538\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.499 |  Val. PPL:   1.647\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.508\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.486 |  Val. PPL:   1.626\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.604\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.505 |  Val. PPL:   1.657\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.550\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.547\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 0.441 |  Val. PPL:   1.554\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.524\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train PPL:   1.536\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.479\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.482 |  Val. PPL:   1.619\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
      "\t Val. Loss: 0.491 |  Val. PPL:   1.635\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.469 |  Val. PPL:   1.599\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.524\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.566\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.489 |  Val. PPL:   1.631\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.575\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.603\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.574\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.582\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.543\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.493 |  Val. PPL:   1.637\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.571\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.541\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.579\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.549\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.528 |  Val. PPL:   1.696\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.580\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.529\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.421\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.529\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.418\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.430\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n"
     ]
    }
   ],
   "source": [
    "optParams = []\n",
    "\n",
    "models_D2 = []\n",
    "hist_losses_D2 = []\n",
    "hist_hitsss_D2 = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D2.{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_l2reg(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D2.append(hist_loss_temp)\n",
    "    hist_hitsss_D2.append(hist_hits_temp)\n",
    "    models_D2.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_l2reg(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "ITGjKqf6c-Bt",
    "outputId": "d49eb2d3-0c6b-4b66-fc1a-744da9729248"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_D2 = torch.cat(hist_losses_D2, dim=2)\n",
    "hist_hits_D2 = torch.cat(hist_hitsss_D2, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D2, hist_hits_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEnIhlRbdPUs",
    "outputId": "36ab6809-9d23-4399-c9b8-c03a0bb6a3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.56% | Gr acc 0.12 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwQE8R-QvnWj"
   },
   "source": [
    "## Transfer E: DynaMoE\n",
    "\n",
    "1. Create DynaMoe network functions:\n",
    "2. Decider Network\n",
    "3. Run experiment\n",
    "\n",
    "Todos:\n",
    "- Training by computing loss of every expert is cheaty?\n",
    "- Training by feeding all new inputs to new expert is cheaty?\n",
    "- Consolidating Network on task change is cheaty!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiVkxi06Nu8y"
   },
   "source": [
    "### Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "JFB0iAdSvnWk"
   },
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        return outputs\n",
    "        return F.softmax(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pL7WzzmNqBp"
   },
   "source": [
    "### DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Ysf64peLvnWj"
   },
   "outputs": [],
   "source": [
    "class DynaMoE(nn.Module):\n",
    "    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(DynaMoE, self).__init__()\n",
    "\n",
    "        assert len(experts) == len(expert_optimizers)\n",
    "        \n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        self.n_active_experts = 1\n",
    "        # set mask\n",
    "        self.recompute_mask()\n",
    "    \n",
    "    def recompute_mask(self):\n",
    "        gating_mask = torch.zeros(self.gating.n_max_experts).to(device)\n",
    "\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            gating_mask[e_id] = 1\n",
    "\n",
    "        self.gating_mask = gating_mask\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        masked_gatings = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        # @TODO: Probabilistic vs argmax?\n",
    "        network_ids = torch.argmax(masked_gatings, dim=1)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,\n",
    "                                                     teacher_forcing_ratio))\n",
    "\n",
    "        outputs = torch.empty((seq_len, batch_size, vocab_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            network_id = network_ids[b]\n",
    "            outputs[:,b] = expert_outputs[network_id][:,b]\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1\n",
    "        # Recompute mask\n",
    "        self.recompute_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITHSpNoENze8"
   },
   "source": [
    "### compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "9Opa6EZlw8g-"
   },
   "outputs": [],
   "source": [
    "def compute_loss(outputs, targets, criterion, cutFirstInSequence=True):\n",
    "    if isinstance(criterion, CosineLoss):\n",
    "        return criterion(outputs, targets)\n",
    "    else:\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        \n",
    "        if cutFirstInSequence:\n",
    "            outputs = outputs[1:].view(-1, outputs_dim)\n",
    "            #outputs = [batch size, output dim]\n",
    "            targets = targets[1:].view(-1)\n",
    "            #targets = [batch size]\n",
    "            # print(\"hi\")\n",
    "        else:\n",
    "            outputs = outputs.view(-1, outputs_dim)\n",
    "            targets = targets.view(-1)\n",
    "        \n",
    "        # print(\"######\")\n",
    "        # print(outputs)\n",
    "        # print(targets)\n",
    "        # print(\"######\")\n",
    "        \n",
    "        return criterion(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K75xVqKJN2Km"
   },
   "source": [
    "### train_dynamoe_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "icuVPvGqYI75"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_gating(model, iterator, gating_criterion,\n",
    "                         expert_criterion, clip, verbose=False):\n",
    "    \n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len, expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                    expert_criterion,\n",
    "                                    cutFirstInSequence=True)\n",
    "            \n",
    "            # Log loss to train gating\n",
    "            loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Indices of correct experts to have chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Target\")\n",
    "            print(gating_trgs)\n",
    "            print(\"Output\")\n",
    "            print(gating_outputs.argmax(dim=1))\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "        \n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbbZ_pYyN48v"
   },
   "source": [
    "### train_dynamoe_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "MaaIrZLCE1JG"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_both(model, iterator, gating_criterion,\n",
    "                       expert_criterion, clip):\n",
    "\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        # loss_experts = [batch_size, n_active_experts]\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            train_model = e_id == model.n_active_experts - 1\n",
    "\n",
    "            if train_model:\n",
    "                model.experts[e_id].train()\n",
    "                model.expert_optimizers[e_id].zero_grad()\n",
    "                \n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len, expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs, expert_criterion,\n",
    "                                    cutFirstInSequence=True)\n",
    "\n",
    "                # Log loss to train gating\n",
    "                loss_experts[:,e_id] = loss\n",
    "                \n",
    "                # Train newly initialized model on new train examples\n",
    "                reduced_loss = loss.mean()\n",
    "                reduced_loss.backward()\n",
    "                model.expert_optimizers[e_id].step()\n",
    "                \n",
    "            else:\n",
    "                model.experts[e_id].eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Get model prediction\n",
    "                    expert_outputs = model.experts[e_id](seqs, seqs_len, expert_trgs)\n",
    "\n",
    "                    loss = compute_loss(expert_outputs, expert_trgs, expert_criterion,\n",
    "                                        cutFirstInSequence=True)\n",
    "\n",
    "                    # Log loss to train gating\n",
    "                    loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Compute expert which should have been chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCiyq9ODN9fG"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Y4hPup6qpRWp"
   },
   "outputs": [],
   "source": [
    "def init_expert():\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    new_expert = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "    new_expert.apply(init_weights)\n",
    "    expert_optimizer = optim.Adam(new_expert.parameters(), lr=LEARNING_RATE)\n",
    "    return new_expert, expert_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "-gPFPMm_JiO4"
   },
   "outputs": [],
   "source": [
    "def init_gating():\n",
    "    gating = Gating(INPUT_DIM, N_GATING_EMBED_DIM, N_GATING_HIDDEN_DIM,\n",
    "                    N_EXPERTS_START, N_MAX_EXPERTS, GATE_DROPOUT)\n",
    "    gating.to(device)\n",
    "    gating_optimizer = optim.Adam(gating.parameters(), lr=LEARNING_RATE)\n",
    "    return gating, gating_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouWPz-2MdMs1"
   },
   "source": [
    "### show expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "jFW0tm8tdTAl"
   },
   "outputs": [],
   "source": [
    "def show_expert(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            gating_choices = gating_masked.argmax(dim=1)\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_choices[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQrjb4QEODx2"
   },
   "source": [
    "### fit_dynamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "tuFUOzDwjZgI"
   },
   "outputs": [],
   "source": [
    "def fit_dynamoe(model, task_id, epochs, step_size_evaluation, clip,\n",
    "                case = \"train_gating_initialized_expert\" ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    case : string\n",
    "        \"train_gating_uninitialized_expert\" | \"train_gating_train_expert\" | \n",
    "        \"train_gating_initialized_expert\"\n",
    "    \"\"\"\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    loss_tracker = torch.zeros((epochs,))\n",
    "\n",
    "    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        if case == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id], allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    # Switch to train the expert and gating\n",
    "\n",
    "                    case = \"train_gating_train_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if case == \"train_gating_train_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_both(model, train_dls[task_id],\n",
    "                                            gating_criterion,\n",
    "                                            expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "        if case == \"train_gating_uninitialized_expert\":\n",
    "            assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    case = \"train_gating_initialized_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], expert_criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], expert_criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], expert_criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlpvLkEVfZhR"
   },
   "source": [
    "### Experiment DynaMoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnSZq8sAsGoj"
   },
   "source": [
    "Questions open from the DynaMoE code:\n",
    "- When does the model exactly decide to \"freeze\" the expert\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "iu9jVht1wbg7"
   },
   "outputs": [],
   "source": [
    "SAVE = N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "N_EPOCHS = SAVE + 200\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-E0\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.949\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 1.034 | Train PPL:   2.813\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 1.044 | Train PPL:   2.841\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 1.092 | Train PPL:   2.981\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.001\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 1.118 | Train PPL:   3.059\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 1.033 | Train PPL:   2.809\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 1.127 | Train PPL:   3.085\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 1.115 | Train PPL:   3.050\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 1.143 | Train PPL:   3.138\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 1.035 | Train PPL:   2.814\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 1.061 | Train PPL:   2.890\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 1.106 | Train PPL:   3.022\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.923\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.997\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 1.126 | Train PPL:   3.084\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 1.055 | Train PPL:   2.872\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 1.117 | Train PPL:   3.055\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 1.091 | Train PPL:   2.977\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 1.096 | Train PPL:   2.991\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 1.050 | Train PPL:   2.857\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.955\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.986 | Train PPL:   2.681\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 1.082 | Train PPL:   2.951\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 1.058 | Train PPL:   2.882\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 1.101 | Train PPL:   3.008\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.064\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 1.016 | Train PPL:   2.763\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.827\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 1.109 | Train PPL:   3.031\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 1.076 | Train PPL:   2.933\n",
      "\t Val. Loss: 1.126 |  Val. PPL:   3.085\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.662 | Train PPL:   1.938\n",
      "\t Val. Loss: 0.559 |  Val. PPL:   1.749\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.639\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.544\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.478\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.338\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.165\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-E1\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.732\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.734\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.698\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.674\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.734\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.668\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.753\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.731\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.743\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.734\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.534 | Train PPL:   1.706\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.575 | Train PPL:   1.776\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.571 | Train PPL:   1.769\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.710\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.694\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.505 | Train PPL:   1.658\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.724\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.721\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.731\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.687\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.527 | Train PPL:   1.693\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.536 | Train PPL:   1.709\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.536 | Train PPL:   1.710\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.724\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.691\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.729\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.536 | Train PPL:   1.708\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.760\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.514 | Train PPL:   1.672\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.515 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.700\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.698\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.554 | Train PPL:   1.740\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.685\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.715\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.536 | Train PPL:   1.709\n",
      "\t Val. Loss: 0.527 |  Val. PPL:   1.694\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.495 | Train PPL:   1.640\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.512\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.488\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.444\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.475\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.399\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.352\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.379\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.437\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.157\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "tr-AE-30-10-0.01-E2\n",
      "The model has 13219 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.385\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n"
     ]
    }
   ],
   "source": [
    "models_E = []\n",
    "hist_losses_E = []\n",
    "hist_hitsss_E = []\n",
    "\n",
    "expert, expert_optimizer = init_expert()\n",
    "gating, gating_optimizer = init_gating()\n",
    "model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "# gating_criterion = CosineLoss(N_MAX_EXPERTS, ignore_index=None)\n",
    "# Cosine loss is inpractical for Gating because result vectors are low dimensional\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                        reduction=\"none\")\n",
    "\n",
    "for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"E{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    print(title)\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    if n_task == 0:\n",
    "        case = \"train_gating_initialized_expert\"\n",
    "    else:\n",
    "        case = \"train_gating_uninitialized_expert\"\n",
    "    hist_loss_temp, hist_hits_temp = fit_dynamoe(model, n_task, N_EPOCHS,\n",
    "                                                 STEP_SIZE_EVALUATION, CLIP,\n",
    "                                                 case)\n",
    "    hist_losses_E.append(hist_loss_temp)\n",
    "    hist_hitsss_E.append(hist_hits_temp)\n",
    "    models_E.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "bIK6DRy6oZOF",
    "outputId": "9232242e-2678-4b38-c20e-2641256d1b4f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_E = torch.cat(hist_losses_E, dim=2)\n",
    "hist_hits_E = torch.cat(hist_hitsss_E, dim=2)\n",
    "\n",
    "plotResults(hist_loss_E, hist_hits_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVUf91lhquoH",
    "outputId": "e3521937-8666-4089-8de0-8f067d906fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xROT6lswtwba",
    "outputId": "4e09b763-60b2-4d1a-8dd3-bbfcbfff2973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_E[2].n_active_experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5kSqVvW6e7M"
   },
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_trgts = []\n",
    "gating_trgts.append( [torch.tensor([1,0,0]) for _ in range(len(train_dls[0]))])\n",
    "gating_trgts.append( [torch.tensor([0,1,0]) for _ in range(len(train_dls[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dls[1]:\n",
    "    if x[1] == 7:\n",
    "        print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0990233421325684\n",
      "loss\n",
      "1.0725314617156982\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.06223726272583\n",
      "loss\n",
      "1.1580368280410767\n",
      "loss\n",
      "0.994653046131134\n",
      "loss\n",
      "1.0136317014694214\n",
      "loss\n",
      "1.1363170146942139\n",
      "loss\n",
      "1.0377978086471558\n",
      "loss\n",
      "1.273862361907959\n",
      "loss\n",
      "1.1935420036315918\n",
      "loss\n",
      "1.2392826080322266\n",
      "loss\n",
      "0.843536376953125\n",
      "loss\n",
      "1.0642420053482056\n",
      "loss\n",
      "0.8814082145690918\n",
      "loss\n",
      "1.1417649984359741\n",
      "loss\n",
      "1.2824432849884033\n",
      "loss\n",
      "0.8938198089599609\n",
      "loss\n",
      "0.9051280617713928\n",
      "loss\n",
      "0.9723299145698547\n",
      "loss\n",
      "1.2795649766921997\n",
      "loss\n",
      "1.1691962480545044\n",
      "loss\n",
      "1.2110319137573242\n",
      "loss\n",
      "0.7188755869865417\n",
      "loss\n",
      "1.3590319156646729\n",
      "loss\n",
      "0.9704457521438599\n",
      "loss\n",
      "0.9680742025375366\n",
      "loss\n",
      "1.385023593902588\n",
      "loss\n",
      "0.7331454157829285\n",
      "loss\n",
      "0.7141051888465881\n",
      "loss\n",
      "1.245851993560791\n",
      "loss\n",
      "0.7735995054244995\n",
      "loss\n",
      "1.3736876249313354\n",
      "loss\n",
      "0.8231424689292908\n",
      "loss\n",
      "1.3750696182250977\n",
      "loss\n",
      "1.51996910572052\n",
      "loss\n",
      "1.2661633491516113\n",
      "loss\n",
      "1.3473231792449951\n",
      "loss\n",
      "1.233702301979065\n",
      "loss\n",
      "0.7255655527114868\n",
      "loss\n",
      "0.6757468581199646\n",
      "loss\n",
      "0.6531469225883484\n",
      "loss\n",
      "0.745249330997467\n",
      "loss\n",
      "0.7534412145614624\n",
      "loss\n",
      "1.4418754577636719\n",
      "loss\n",
      "0.7937403321266174\n",
      "loss\n",
      "0.6716989278793335\n",
      "loss\n",
      "1.2280333042144775\n",
      "loss\n",
      "0.6496949195861816\n",
      "loss\n",
      "1.5460278987884521\n",
      "loss\n",
      "1.5514756441116333\n",
      "loss\n",
      "1.1636765003204346\n",
      "loss\n",
      "1.3178153038024902\n",
      "loss\n",
      "0.5455304384231567\n",
      "loss\n",
      "0.5333239436149597\n",
      "loss\n",
      "1.4688210487365723\n",
      "loss\n",
      "1.375108003616333\n",
      "loss\n",
      "0.5335070490837097\n",
      "loss\n",
      "1.430232048034668\n",
      "loss\n",
      "1.1741782426834106\n",
      "loss\n",
      "1.2446054220199585\n",
      "loss\n",
      "0.6195405721664429\n",
      "loss\n",
      "0.49828198552131653\n",
      "loss\n",
      "0.7075855135917664\n",
      "loss\n",
      "0.6252626776695251\n",
      "loss\n",
      "1.3688387870788574\n",
      "loss\n",
      "1.3391385078430176\n",
      "loss\n",
      "1.1697885990142822\n",
      "loss\n",
      "0.37812450528144836\n",
      "loss\n",
      "1.1592947244644165\n",
      "loss\n",
      "0.6913880109786987\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.6907056570053101\n",
      "loss\n",
      "0.4162207841873169\n",
      "loss\n",
      "0.5808714032173157\n",
      "loss\n",
      "1.1500698328018188\n",
      "loss\n",
      "0.352736234664917\n",
      "loss\n",
      "0.39313051104545593\n",
      "loss\n",
      "0.41192883253097534\n",
      "loss\n",
      "0.5116475820541382\n",
      "loss\n",
      "1.1172572374343872\n",
      "loss\n",
      "1.4131906032562256\n",
      "loss\n",
      "0.33028650283813477\n",
      "loss\n",
      "1.489363193511963\n",
      "loss\n",
      "1.2112787961959839\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.4293568730354309\n",
      "loss\n",
      "0.4328417181968689\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.47999051213264465\n",
      "loss\n",
      "0.19354216754436493\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.1361033916473389\n",
      "loss\n",
      "0.1798943281173706\n",
      "loss\n",
      "0.12139580398797989\n",
      "loss\n",
      "0.21990478038787842\n",
      "loss\n",
      "0.2244793325662613\n",
      "loss\n",
      "0.1716667264699936\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.5843265056610107\n",
      "loss\n",
      "0.11034240573644638\n",
      "loss\n",
      "0.3461153209209442\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.14620114862918854\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.13816894590854645\n",
      "loss\n",
      "0.15988250076770782\n",
      "loss\n",
      "0.2077673375606537\n",
      "loss\n",
      "0.13088884949684143\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.3269919157028198\n",
      "loss\n",
      "0.12710054218769073\n",
      "loss\n",
      "1.4896562099456787\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.10144913196563721\n",
      "loss\n",
      "0.07071235775947571\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.40675270557403564\n",
      "loss\n",
      "0.06046057492494583\n",
      "loss\n",
      "0.04141584411263466\n",
      "loss\n",
      "1.0897676944732666\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.07548543810844421\n",
      "loss\n",
      "1.0544909238815308\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.059570763260126114\n",
      "loss\n",
      "0.06236014887690544\n",
      "loss\n",
      "0.04088432341814041\n",
      "loss\n",
      "0.6670161485671997\n",
      "loss\n",
      "0.09925808757543564\n",
      "loss\n",
      "0.06430821120738983\n",
      "loss\n",
      "0.5717656016349792\n",
      "loss\n",
      "0.5106241106987\n",
      "loss\n",
      "0.6109991073608398\n",
      "loss\n",
      "0.4585453271865845\n",
      "loss\n",
      "0.40513986349105835\n",
      "loss\n",
      "0.028454262763261795\n",
      "loss\n",
      "0.10292236506938934\n",
      "loss\n",
      "0.14290152490139008\n",
      "loss\n",
      "0.08874966204166412\n",
      "loss\n",
      "0.1695457398891449\n",
      "loss\n",
      "0.06828229129314423\n",
      "loss\n",
      "0.03192349523305893\n",
      "loss\n",
      "0.10639900714159012\n",
      "loss\n",
      "0.13714158535003662\n",
      "loss\n",
      "0.04405619949102402\n",
      "loss\n",
      "0.020778479054570198\n",
      "loss\n",
      "0.04888411983847618\n",
      "loss\n",
      "0.02315611019730568\n",
      "loss\n",
      "0.03884715959429741\n",
      "loss\n",
      "0.0185548085719347\n",
      "loss\n",
      "0.026514191180467606\n",
      "loss\n",
      "0.015334682539105415\n",
      "loss\n",
      "0.16082657873630524\n",
      "loss\n",
      "0.16628775000572205\n",
      "loss\n",
      "0.03266807645559311\n",
      "loss\n",
      "0.024600008502602577\n",
      "loss\n",
      "0.030816741287708282\n",
      "loss\n",
      "0.021857907995581627\n",
      "loss\n",
      "0.009852115996181965\n",
      "loss\n",
      "0.0058709559962153435\n",
      "loss\n",
      "0.017790623009204865\n",
      "loss\n",
      "0.015698540955781937\n",
      "loss\n",
      "0.08955177664756775\n",
      "loss\n",
      "0.020790155977010727\n",
      "loss\n",
      "0.037340905517339706\n",
      "loss\n",
      "0.030650736764073372\n",
      "loss\n",
      "0.024169908836483955\n",
      "loss\n",
      "0.011669941246509552\n",
      "loss\n",
      "0.8218876719474792\n",
      "loss\n",
      "0.0061847250908613205\n",
      "loss\n",
      "0.04579256847500801\n",
      "loss\n",
      "0.014900343492627144\n",
      "loss\n",
      "0.021984562277793884\n",
      "loss\n",
      "0.008054869249463081\n",
      "loss\n",
      "0.017524754628539085\n",
      "loss\n",
      "0.008446445688605309\n",
      "loss\n",
      "0.005841090343892574\n",
      "loss\n",
      "0.02205546200275421\n",
      "loss\n",
      "0.01886228285729885\n",
      "loss\n",
      "0.012258898466825485\n",
      "loss\n",
      "0.00998903438448906\n",
      "loss\n",
      "0.007056789472699165\n",
      "loss\n",
      "0.010330774821341038\n",
      "loss\n",
      "0.00817713513970375\n",
      "loss\n",
      "0.006179037969559431\n",
      "loss\n",
      "0.0073410761542618275\n",
      "loss\n",
      "0.010899768210947514\n",
      "loss\n",
      "0.026390664279460907\n",
      "loss\n",
      "0.016640907153487206\n",
      "loss\n",
      "0.00597974332049489\n",
      "loss\n",
      "0.009672206826508045\n",
      "loss\n",
      "0.005973581690341234\n",
      "loss\n",
      "3.6694374084472656\n",
      "loss\n",
      "0.00877902377396822\n",
      "loss\n",
      "4.256886959075928\n",
      "loss\n",
      "0.017297949641942978\n",
      "loss\n",
      "0.011129443533718586\n",
      "loss\n",
      "0.015219627879559994\n",
      "loss\n",
      "0.0030019478872418404\n",
      "loss\n",
      "0.021291498094797134\n",
      "loss\n",
      "0.018529647961258888\n",
      "loss\n",
      "0.009522965177893639\n",
      "loss\n",
      "0.012272440828382969\n",
      "loss\n",
      "0.008545498363673687\n",
      "loss\n",
      "0.008923650719225407\n",
      "loss\n",
      "0.0023492376785725355\n",
      "loss\n",
      "0.08297166228294373\n",
      "loss\n",
      "0.006391085684299469\n",
      "loss\n",
      "0.2621001899242401\n",
      "loss\n",
      "0.0033736478071659803\n",
      "loss\n",
      "0.06227020546793938\n",
      "loss\n",
      "0.10268152505159378\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.001490197260864079\n",
      "loss\n",
      "0.0031985098030418158\n",
      "loss\n",
      "0.19377416372299194\n",
      "loss\n",
      "0.43188416957855225\n",
      "loss\n",
      "0.010465031489729881\n",
      "loss\n",
      "0.313730925321579\n",
      "loss\n",
      "0.052673447877168655\n",
      "loss\n",
      "0.010588418692350388\n",
      "loss\n",
      "0.16984111070632935\n",
      "loss\n",
      "0.003559921169653535\n",
      "loss\n",
      "0.05207546055316925\n",
      "loss\n",
      "0.02877179905772209\n",
      "loss\n",
      "0.09439631551504135\n",
      "loss\n",
      "0.003439465072005987\n",
      "loss\n",
      "0.01418695505708456\n",
      "loss\n",
      "0.005863133817911148\n",
      "loss\n",
      "0.010725353844463825\n",
      "loss\n",
      "0.01660596765577793\n",
      "loss\n",
      "0.008103824220597744\n",
      "loss\n",
      "0.020882155746221542\n",
      "loss\n",
      "0.015934040769934654\n",
      "loss\n",
      "0.009476440958678722\n",
      "loss\n",
      "0.011041258461773396\n",
      "loss\n",
      "0.0038036394398659468\n",
      "loss\n",
      "0.015622024424374104\n",
      "loss\n",
      "0.004314637742936611\n",
      "loss\n",
      "0.002865258837118745\n",
      "loss\n",
      "0.003520245896652341\n",
      "loss\n",
      "0.00621197372674942\n",
      "loss\n",
      "0.3402809202671051\n",
      "loss\n",
      "1.1181387901306152\n",
      "loss\n",
      "0.016513574868440628\n",
      "loss\n",
      "0.005002601072192192\n",
      "loss\n",
      "0.005188333801925182\n",
      "loss\n",
      "0.0484754703938961\n",
      "loss\n",
      "0.011777392588555813\n",
      "loss\n",
      "0.00955106783658266\n",
      "loss\n",
      "0.0032876271288841963\n",
      "loss\n",
      "2.739438056945801\n",
      "loss\n",
      "0.08617750555276871\n",
      "loss\n",
      "0.0030841901898384094\n",
      "loss\n",
      "0.005740703083574772\n",
      "loss\n",
      "0.022282473742961884\n",
      "loss\n",
      "0.049073684960603714\n",
      "loss\n",
      "0.010887740179896355\n",
      "loss\n",
      "0.0028799984138458967\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.020307835191488266\n",
      "loss\n",
      "0.007002928759902716\n",
      "loss\n",
      "0.004172904882580042\n",
      "loss\n",
      "0.011328539811074734\n",
      "loss\n",
      "0.002724746707826853\n",
      "loss\n",
      "0.011057999916374683\n",
      "loss\n",
      "0.007479520980268717\n",
      "loss\n",
      "0.007599018048495054\n",
      "loss\n",
      "0.019894583150744438\n",
      "loss\n",
      "0.011453117243945599\n",
      "loss\n",
      "0.006259953137487173\n",
      "loss\n",
      "0.005067836493253708\n",
      "loss\n",
      "0.07912437617778778\n",
      "loss\n",
      "0.018125612288713455\n",
      "loss\n",
      "0.00830671563744545\n",
      "loss\n",
      "0.003499813610687852\n",
      "loss\n",
      "0.009177522733807564\n",
      "loss\n",
      "0.007959552109241486\n",
      "loss\n",
      "0.0048975031822919846\n",
      "loss\n",
      "0.00563117815181613\n",
      "loss\n",
      "0.011946087703108788\n",
      "loss\n",
      "0.12055058032274246\n",
      "loss\n",
      "0.0014368696138262749\n",
      "loss\n",
      "0.0018726922571659088\n",
      "loss\n",
      "0.0020918408408761024\n",
      "loss\n",
      "0.01692248322069645\n",
      "loss\n",
      "0.05113114416599274\n",
      "loss\n",
      "0.0038523285184055567\n",
      "loss\n",
      "0.0021715410985052586\n",
      "loss\n",
      "0.0018234307644888759\n",
      "loss\n",
      "0.03622610867023468\n",
      "loss\n",
      "0.004267395939677954\n",
      "loss\n",
      "0.004066176246851683\n",
      "loss\n",
      "1.5602083206176758\n",
      "loss\n",
      "0.004498837050050497\n",
      "loss\n",
      "0.006695810705423355\n",
      "loss\n",
      "0.023411402478814125\n",
      "loss\n",
      "0.011220568791031837\n",
      "loss\n",
      "0.005698625463992357\n",
      "loss\n",
      "0.2322503626346588\n",
      "loss\n",
      "0.00617643166333437\n",
      "loss\n",
      "0.001796538126654923\n",
      "loss\n",
      "0.005663539282977581\n",
      "loss\n",
      "0.015758037567138672\n",
      "loss\n",
      "0.0018020119750872254\n",
      "loss\n",
      "0.005692461505532265\n",
      "loss\n",
      "0.016637155786156654\n",
      "loss\n",
      "0.004839136730879545\n",
      "loss\n",
      "0.010831727646291256\n",
      "loss\n",
      "0.0023832509759813547\n",
      "loss\n",
      "0.004562444519251585\n",
      "loss\n",
      "0.009627577848732471\n",
      "loss\n",
      "0.024295106530189514\n",
      "loss\n",
      "0.011150899343192577\n",
      "loss\n",
      "0.009857428260147572\n",
      "loss\n",
      "0.00823152344673872\n",
      "loss\n",
      "0.006388835143297911\n",
      "loss\n",
      "0.006659812293946743\n",
      "loss\n",
      "0.005407468415796757\n",
      "loss\n",
      "0.006669285707175732\n",
      "loss\n",
      "0.007078569382429123\n",
      "loss\n",
      "0.00319589558057487\n",
      "loss\n",
      "0.004212554078549147\n",
      "loss\n",
      "0.00865777675062418\n",
      "loss\n",
      "0.0035727499052882195\n",
      "loss\n",
      "0.0016208856832236052\n",
      "loss\n",
      "0.006474115885794163\n",
      "loss\n",
      "0.011410925537347794\n",
      "loss\n",
      "0.01145629957318306\n",
      "loss\n",
      "0.007725598756223917\n",
      "loss\n",
      "0.0021824846044182777\n",
      "loss\n",
      "0.0015101945027709007\n",
      "loss\n",
      "0.01465426292270422\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.006637786515057087\n",
      "loss\n",
      "0.002001427114009857\n",
      "loss\n",
      "0.0010892179561778903\n",
      "loss\n",
      "0.005189519841223955\n",
      "loss\n",
      "0.005124291870743036\n",
      "loss\n",
      "0.016665644943714142\n",
      "loss\n",
      "0.005322452634572983\n",
      "loss\n",
      "0.0194013062864542\n",
      "loss\n",
      "0.0016494491137564182\n",
      "loss\n",
      "0.01149224303662777\n",
      "loss\n",
      "0.03479050472378731\n",
      "loss\n",
      "0.006425435654819012\n",
      "loss\n",
      "0.005977491848170757\n",
      "loss\n",
      "0.00912188645452261\n",
      "loss\n",
      "2.2605345249176025\n",
      "loss\n",
      "0.006742346566170454\n",
      "loss\n",
      "0.003994344733655453\n",
      "loss\n",
      "0.02096504159271717\n",
      "loss\n",
      "0.0022115076426416636\n",
      "loss\n",
      "0.011889898218214512\n",
      "loss\n",
      "0.0265306755900383\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.00421172333881259\n",
      "loss\n",
      "0.015857893973588943\n",
      "loss\n",
      "0.1096530631184578\n",
      "loss\n",
      "0.018577391281723976\n",
      "loss\n",
      "0.0031585826072841883\n",
      "loss\n",
      "0.007590973284095526\n",
      "loss\n",
      "0.01869463361799717\n",
      "loss\n",
      "0.0005868143052794039\n",
      "loss\n",
      "0.0009062950266525149\n",
      "loss\n",
      "0.0009920443408191204\n",
      "loss\n",
      "0.022964363917708397\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "1.710546851158142\n",
      "loss\n",
      "0.020992590114474297\n",
      "loss\n",
      "0.0007769426447339356\n",
      "loss\n",
      "0.0007874249131418765\n",
      "loss\n",
      "0.002684087259694934\n",
      "loss\n",
      "0.0014432977186515927\n",
      "loss\n",
      "0.014400719664990902\n",
      "loss\n",
      "0.01647898182272911\n",
      "loss\n",
      "0.0025668551679700613\n",
      "loss\n",
      "0.02713252790272236\n",
      "loss\n",
      "0.0029743739869445562\n",
      "loss\n",
      "0.006955931894481182\n",
      "loss\n",
      "0.02879623882472515\n",
      "loss\n",
      "0.009999656118452549\n",
      "loss\n",
      "0.002252900041639805\n",
      "loss\n",
      "0.0019014865392819047\n",
      "loss\n",
      "0.004394398536533117\n",
      "loss\n",
      "0.0068917665630578995\n",
      "loss\n",
      "0.0008858094224706292\n",
      "loss\n",
      "0.009932027198374271\n",
      "loss\n",
      "0.014354773797094822\n",
      "loss\n",
      "0.003590567270293832\n",
      "loss\n",
      "0.007239537313580513\n",
      "loss\n",
      "0.010349651798605919\n",
      "loss\n",
      "0.0017913023475557566\n",
      "loss\n",
      "0.005552583374083042\n",
      "loss\n",
      "0.005063566844910383\n",
      "loss\n",
      "0.00753796985372901\n",
      "loss\n",
      "0.005215846933424473\n",
      "loss\n",
      "0.019643884152173996\n",
      "loss\n",
      "0.010137026198208332\n",
      "loss\n",
      "0.009279691614210606\n",
      "loss\n",
      "0.24630846083164215\n",
      "loss\n",
      "0.20919343829154968\n",
      "loss\n",
      "0.011986136436462402\n",
      "loss\n",
      "0.03142803534865379\n",
      "loss\n",
      "0.0016289787599816918\n",
      "loss\n",
      "0.007744170259684324\n",
      "loss\n",
      "0.0020404488313943148\n",
      "loss\n",
      "0.0019769188947975636\n",
      "loss\n",
      "0.0036828566808253527\n",
      "loss\n",
      "0.001430917764082551\n",
      "loss\n",
      "0.0032458023633807898\n",
      "loss\n",
      "0.9635264873504639\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.0015397133538499475\n",
      "loss\n",
      "0.01138865016400814\n",
      "loss\n",
      "0.006255806889384985\n",
      "loss\n",
      "0.0012344843707978725\n",
      "loss\n",
      "0.012749706394970417\n",
      "loss\n",
      "0.016048895195126534\n",
      "loss\n",
      "0.016542652621865273\n",
      "loss\n",
      "0.009411019273102283\n",
      "loss\n",
      "0.016764706000685692\n",
      "loss\n",
      "0.0016001766780391335\n",
      "loss\n",
      "0.024112075567245483\n",
      "loss\n",
      "0.0008298290777020156\n",
      "loss\n",
      "0.0038938906509429216\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0008660380262881517\n",
      "loss\n",
      "0.005714982748031616\n",
      "loss\n",
      "0.005052180495113134\n",
      "loss\n",
      "0.006232587620615959\n",
      "loss\n",
      "0.01502776425331831\n",
      "loss\n",
      "0.005707752425223589\n",
      "loss\n",
      "0.002473630243912339\n",
      "loss\n",
      "0.009997768327593803\n",
      "loss\n",
      "0.012457652017474174\n",
      "loss\n",
      "0.011125553399324417\n",
      "loss\n",
      "0.0012016226537525654\n",
      "loss\n",
      "0.0020697140134871006\n",
      "loss\n",
      "0.0006506709614768624\n",
      "loss\n",
      "0.014135828241705894\n",
      "loss\n",
      "0.004716699477285147\n",
      "loss\n",
      "0.0012061471352353692\n",
      "loss\n",
      "1.7048331499099731\n",
      "loss\n",
      "0.0016477829776704311\n",
      "loss\n",
      "0.007360128220170736\n",
      "loss\n",
      "0.0018893502419814467\n",
      "loss\n",
      "0.0011075560469180346\n",
      "loss\n",
      "0.0037392713129520416\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.003224414074793458\n",
      "loss\n",
      "0.0066930875182151794\n",
      "loss\n",
      "0.002837918698787689\n",
      "loss\n",
      "0.012988959439098835\n",
      "loss\n",
      "0.005118361674249172\n",
      "loss\n",
      "0.002372785471379757\n",
      "loss\n",
      "0.002551397541537881\n",
      "loss\n",
      "0.002693360671401024\n",
      "loss\n",
      "0.005424067843705416\n",
      "loss\n",
      "0.029661312699317932\n",
      "loss\n",
      "0.004786224570125341\n",
      "loss\n",
      "0.002189621329307556\n",
      "loss\n",
      "0.003100590081885457\n",
      "loss\n",
      "0.0031093843281269073\n",
      "loss\n",
      "0.009124367497861385\n",
      "loss\n",
      "0.003724069334566593\n",
      "loss\n",
      "0.02314271405339241\n",
      "loss\n",
      "0.0025100174825638533\n",
      "loss\n",
      "0.0035209585912525654\n",
      "loss\n",
      "0.002444852376356721\n",
      "loss\n",
      "0.005885177291929722\n",
      "loss\n",
      "0.013309091329574585\n",
      "loss\n",
      "0.10153012722730637\n",
      "loss\n",
      "0.004611808806657791\n",
      "loss\n",
      "0.0028217521030455828\n",
      "loss\n",
      "0.015542215667665005\n",
      "loss\n",
      "0.002033786615356803\n",
      "loss\n",
      "0.0014035383937880397\n",
      "loss\n",
      "0.0029042467940598726\n",
      "loss\n",
      "0.0021230080164968967\n",
      "loss\n",
      "0.003525234991684556\n",
      "loss\n",
      "0.0052865236066281796\n",
      "loss\n",
      "0.05563344433903694\n",
      "loss\n",
      "1.2306580543518066\n",
      "loss\n",
      "0.002805347554385662\n",
      "loss\n",
      "0.008296666666865349\n",
      "loss\n",
      "0.0025618611834943295\n",
      "loss\n",
      "0.0029922020621597767\n",
      "loss\n",
      "0.001327824778854847\n",
      "loss\n",
      "0.004977691452950239\n",
      "loss\n",
      "0.008818963542580605\n",
      "loss\n",
      "0.001708239782601595\n",
      "loss\n",
      "0.7690757513046265\n",
      "loss\n",
      "0.015876196324825287\n",
      "loss\n",
      "0.003787725931033492\n",
      "loss\n",
      "0.032406941056251526\n",
      "loss\n",
      "0.0013309201458469033\n",
      "loss\n",
      "0.0024917051196098328\n",
      "loss\n",
      "0.001479484373703599\n",
      "loss\n",
      "0.003427822608500719\n",
      "loss\n",
      "0.003249604720622301\n",
      "loss\n",
      "0.004893707111477852\n",
      "loss\n",
      "0.011703873984515667\n",
      "loss\n",
      "0.005132000893354416\n",
      "loss\n",
      "0.018576689064502716\n",
      "loss\n",
      "0.009199729189276695\n",
      "loss\n",
      "0.005308579187840223\n",
      "loss\n",
      "0.007215393707156181\n",
      "loss\n",
      "0.0033021229319274426\n",
      "loss\n",
      "0.025185471400618553\n",
      "loss\n",
      "0.001508289948105812\n",
      "loss\n",
      "0.007013701368123293\n",
      "loss\n",
      "0.05054481700062752\n",
      "loss\n",
      "0.0034468306694179773\n",
      "loss\n",
      "0.006219674367457628\n",
      "loss\n",
      "0.0019921474158763885\n",
      "loss\n",
      "0.0024717275518924\n",
      "loss\n",
      "0.0010458719916641712\n",
      "loss\n",
      "0.0013863962376490235\n",
      "loss\n",
      "0.00099871342536062\n",
      "loss\n",
      "0.003458235412836075\n",
      "loss\n",
      "0.0014266322832554579\n",
      "loss\n",
      "0.0684390440583229\n",
      "loss\n",
      "0.0028136686887592077\n",
      "loss\n",
      "0.0014552014181390405\n",
      "loss\n",
      "0.0019932182040065527\n",
      "loss\n",
      "0.0014911495381966233\n",
      "loss\n",
      "0.07449249923229218\n",
      "loss\n",
      "0.009254061616957188\n",
      "loss\n",
      "0.0026251161471009254\n",
      "loss\n",
      "0.0013711584033444524\n",
      "loss\n",
      "0.005050757434219122\n",
      "loss\n",
      "0.00182819040492177\n",
      "loss\n",
      "0.002353043295443058\n",
      "loss\n",
      "0.0033620046451687813\n",
      "loss\n",
      "0.0028780964203178883\n",
      "loss\n",
      "0.001177094760350883\n",
      "loss\n",
      "0.0012528197839856148\n",
      "loss\n",
      "0.005170782096683979\n",
      "loss\n",
      "0.006078684702515602\n",
      "loss\n",
      "0.0027772923931479454\n",
      "loss\n",
      "0.0025754161179065704\n",
      "loss\n",
      "0.0015905360924080014\n",
      "loss\n",
      "0.06839874386787415\n",
      "loss\n",
      "0.0038691910449415445\n",
      "loss\n",
      "0.00432888139039278\n",
      "loss\n",
      "0.0014771036803722382\n",
      "loss\n",
      "0.0012421043356880546\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0018992258701473475\n",
      "loss\n",
      "0.0033263610675930977\n",
      "loss\n",
      "0.0013111574808135629\n",
      "loss\n",
      "0.0015920833684504032\n",
      "loss\n",
      "0.0006268443539738655\n",
      "loss\n",
      "0.001416633022017777\n",
      "loss\n",
      "0.0014332984574139118\n",
      "loss\n",
      "0.004780292976647615\n",
      "loss\n",
      "0.1666361540555954\n",
      "loss\n",
      "0.0019267105963081121\n",
      "loss\n",
      "0.001359730027616024\n",
      "loss\n",
      "0.007523654028773308\n",
      "loss\n",
      "0.0007602662080898881\n",
      "loss\n",
      "0.0053190141916275024\n",
      "loss\n",
      "0.0015573289711028337\n",
      "loss\n",
      "0.0013621109537780285\n",
      "loss\n",
      "0.0012852036161348224\n",
      "loss\n",
      "0.0010589712765067816\n",
      "loss\n",
      "0.0019512200960889459\n",
      "loss\n",
      "0.002201516181230545\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0015368566382676363\n",
      "loss\n",
      "0.005285100545734167\n",
      "loss\n",
      "0.00283316383138299\n",
      "loss\n",
      "0.002679450437426567\n",
      "loss\n",
      "0.0008553183870390058\n",
      "loss\n",
      "0.005783846136182547\n",
      "loss\n",
      "0.0013325868640094995\n",
      "loss\n",
      "0.0020808966364711523\n",
      "loss\n",
      "0.0010180057724937797\n",
      "loss\n",
      "0.0015910121146589518\n",
      "loss\n",
      "0.002381110331043601\n",
      "loss\n",
      "0.0018386616138741374\n",
      "loss\n",
      "0.0021241975482553244\n",
      "loss\n",
      "0.0017482249531894922\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007312007946893573\n",
      "loss\n",
      "0.0012354368809610605\n",
      "loss\n",
      "0.0022005646023899317\n",
      "loss\n",
      "0.003978315275162458\n",
      "loss\n",
      "0.0013494918821379542\n",
      "loss\n",
      "0.002420830773189664\n",
      "loss\n",
      "0.003981996327638626\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.0021873614750802517\n",
      "loss\n",
      "0.001004667836241424\n",
      "loss\n",
      "0.0024986020289361477\n",
      "loss\n",
      "0.004319860599935055\n",
      "loss\n",
      "0.0005863377591595054\n",
      "loss\n",
      "0.0010850501712411642\n",
      "loss\n",
      "0.0016085079405456781\n",
      "loss\n",
      "0.0014430596493184566\n",
      "loss\n",
      "0.0023011888843029737\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.006295610684901476\n",
      "loss\n",
      "0.001369729870930314\n",
      "loss\n",
      "0.0011634016409516335\n",
      "loss\n",
      "0.0015768486773595214\n",
      "loss\n",
      "0.0007338214782066643\n",
      "loss\n",
      "0.001019553979858756\n",
      "loss\n",
      "0.0008622265886515379\n",
      "loss\n",
      "0.0012063853209838271\n",
      "loss\n",
      "0.0012644876260310411\n",
      "loss\n",
      "0.0010768335778266191\n",
      "loss\n",
      "0.0011632826644927263\n",
      "loss\n",
      "0.0032733690459281206\n",
      "loss\n",
      "0.001688008545897901\n",
      "loss\n",
      "0.0009617946925573051\n",
      "loss\n",
      "0.0008934320067055523\n",
      "loss\n",
      "0.0010639727115631104\n",
      "loss\n",
      "0.004857524763792753\n",
      "loss\n",
      "0.0022261380217969418\n",
      "loss\n",
      "0.003475104458630085\n",
      "loss\n",
      "0.0010546842822805047\n",
      "loss\n",
      "0.0016202905680984259\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001911718980409205\n",
      "loss\n",
      "0.0008288762182928622\n",
      "loss\n",
      "0.0038984029088169336\n",
      "loss\n",
      "0.0010794533882290125\n",
      "loss\n",
      "0.0025000290479511023\n",
      "loss\n",
      "0.06439148634672165\n",
      "loss\n",
      "0.0005739472107961774\n",
      "loss\n",
      "0.0867927223443985\n",
      "loss\n",
      "0.0008388814167119563\n",
      "loss\n",
      "0.0008179179858416319\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0008789013954810798\n",
      "loss\n",
      "0.001069450518116355\n",
      "loss\n",
      "0.0004976941272616386\n",
      "loss\n",
      "0.0016221948899328709\n",
      "loss\n",
      "0.00044431351125240326\n",
      "loss\n",
      "0.0019490785198286176\n",
      "loss\n",
      "0.0010580186499282718\n",
      "loss\n",
      "0.003044258337467909\n",
      "loss\n",
      "0.0017691688844934106\n",
      "loss\n",
      "0.0010022860951721668\n",
      "loss\n",
      "0.0004898302140645683\n",
      "loss\n",
      "0.000834117061458528\n",
      "loss\n",
      "0.0032225127797573805\n",
      "loss\n",
      "0.006845948286354542\n",
      "loss\n",
      "0.005816676188260317\n",
      "loss\n",
      "0.0025216706562787294\n",
      "loss\n",
      "0.0019045800436288118\n",
      "loss\n",
      "0.013794328086078167\n",
      "loss\n",
      "0.0015889888163655996\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.0013323486782610416\n",
      "loss\n",
      "0.0006561510381288826\n",
      "loss\n",
      "0.010727358050644398\n",
      "loss\n",
      "0.0008112476789392531\n",
      "loss\n",
      "0.0013940150383859873\n",
      "loss\n",
      "0.0067525296472013\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "1.133132815361023\n",
      "loss\n",
      "0.0005555993411689997\n",
      "loss\n",
      "0.000635183765552938\n",
      "loss\n",
      "0.003932482097297907\n",
      "loss\n",
      "0.0025434307754039764\n",
      "loss\n",
      "0.002568519674241543\n",
      "loss\n",
      "0.0027159492019563913\n",
      "loss\n",
      "0.5777732133865356\n",
      "loss\n",
      "0.0015739921946078539\n",
      "loss\n",
      "0.009132400155067444\n",
      "loss\n",
      "0.0021394239738583565\n",
      "loss\n",
      "0.0009253510506823659\n",
      "loss\n",
      "0.004545712377876043\n",
      "loss\n",
      "0.0010368215152993798\n",
      "loss\n",
      "0.0029525042045861483\n",
      "loss\n",
      "0.0005034133209846914\n",
      "loss\n",
      "0.00035565727739594877\n",
      "loss\n",
      "0.0013454442378133535\n",
      "loss\n",
      "0.0008417400531470776\n",
      "loss\n",
      "0.00573691027238965\n",
      "loss\n",
      "0.0008646087371744215\n",
      "loss\n",
      "0.004140377044677734\n",
      "loss\n",
      "0.002090056659653783\n",
      "loss\n",
      "0.0017508429009467363\n",
      "loss\n",
      "0.0005458295345306396\n",
      "loss\n",
      "0.007576303090900183\n",
      "loss\n",
      "0.002320099389180541\n",
      "loss\n",
      "0.006270022597163916\n",
      "loss\n",
      "0.0028695380315184593\n",
      "loss\n",
      "0.0013546108966693282\n",
      "loss\n",
      "0.0011938833631575108\n",
      "loss\n",
      "0.0004612335760612041\n",
      "loss\n",
      "0.0020207001361995935\n",
      "loss\n",
      "0.0028019000310450792\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0005999195855110884\n",
      "loss\n",
      "0.0004219118563923985\n",
      "loss\n",
      "0.0046008918434381485\n",
      "loss\n",
      "0.002042233245447278\n",
      "loss\n",
      "0.006754186935722828\n",
      "loss\n",
      "0.0007593132322654128\n",
      "loss\n",
      "0.00041547726141288877\n",
      "loss\n",
      "0.0008174415561370552\n",
      "loss\n",
      "0.0022442173212766647\n",
      "loss\n",
      "0.000581572181545198\n",
      "loss\n",
      "0.003397290362045169\n",
      "loss\n",
      "0.0006323245470412076\n",
      "loss\n",
      "0.0019155264599248767\n",
      "loss\n",
      "0.0005765683017671108\n",
      "loss\n",
      "0.0003532739356160164\n",
      "loss\n",
      "0.002211745595559478\n",
      "loss\n",
      "0.03702251985669136\n",
      "loss\n",
      "0.0009205871028825641\n",
      "loss\n",
      "0.008095311000943184\n",
      "loss\n",
      "0.0005775213940069079\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0011156531982123852\n",
      "loss\n",
      "0.0019378946162760258\n",
      "loss\n",
      "0.003487696871161461\n",
      "loss\n",
      "0.0017197832930833101\n",
      "loss\n",
      "0.0022155519109219313\n",
      "loss\n",
      "0.0010141950333490968\n",
      "loss\n",
      "0.07641874998807907\n",
      "loss\n",
      "0.0013409203384071589\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "0.0014016337227076292\n",
      "loss\n",
      "0.017542092129588127\n",
      "loss\n",
      "0.0032928551081568003\n",
      "loss\n",
      "0.0005334384622983634\n",
      "loss\n",
      "0.0008803306263871491\n",
      "loss\n",
      "0.0032828745897859335\n",
      "loss\n",
      "0.0023209319915622473\n",
      "loss\n",
      "0.0004161922261118889\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.0006065912893973291\n",
      "loss\n",
      "0.0072313714772462845\n",
      "loss\n",
      "0.0024629279505461454\n",
      "loss\n",
      "0.0021594080608338118\n",
      "loss\n",
      "0.0022205475252121687\n",
      "loss\n",
      "0.0010596857173368335\n",
      "loss\n",
      "0.001319729257375002\n",
      "loss\n",
      "0.009429323486983776\n",
      "loss\n",
      "0.002771110739558935\n",
      "loss\n",
      "0.001105888863094151\n",
      "loss\n",
      "0.006941134110093117\n",
      "loss\n",
      "0.0014033003244549036\n",
      "loss\n",
      "0.00024911639047786593\n",
      "loss\n",
      "0.00403613829985261\n",
      "loss\n",
      "0.0013616346986964345\n",
      "loss\n",
      "0.002226970624178648\n",
      "loss\n",
      "0.0029974314384162426\n",
      "loss\n",
      "0.0010115751065313816\n",
      "loss\n",
      "0.0009703694959171116\n",
      "loss\n",
      "0.0023328252136707306\n",
      "loss\n",
      "0.0012416280806064606\n",
      "loss\n",
      "0.008411219343543053\n",
      "loss\n",
      "0.0014676999999210238\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000969535845797509\n",
      "loss\n",
      "0.00026556302327662706\n",
      "loss\n",
      "0.00041261743172071874\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.001522930571809411\n",
      "loss\n",
      "0.0008607972995378077\n",
      "loss\n",
      "0.001702408422715962\n",
      "loss\n",
      "0.00075049843871966\n",
      "loss\n",
      "0.0002951186615973711\n",
      "loss\n",
      "0.0009332115878351033\n",
      "loss\n",
      "0.0014611531514674425\n",
      "loss\n",
      "0.0015103134792298079\n",
      "loss\n",
      "0.0012911563972011209\n",
      "loss\n",
      "0.000649956171400845\n",
      "loss\n",
      "0.002857889048755169\n",
      "loss\n",
      "0.001141611486673355\n",
      "loss\n",
      "0.0008865240379236639\n",
      "loss\n",
      "0.004283539019525051\n",
      "loss\n",
      "0.0007805161876603961\n",
      "loss\n",
      "0.0007681279676035047\n",
      "loss\n",
      "0.0013560395454987884\n",
      "loss\n",
      "0.0003937899600714445\n",
      "loss\n",
      "0.002218644367530942\n",
      "loss\n",
      "0.0014017528155818582\n",
      "loss\n",
      "0.0006868863711133599\n",
      "loss\n",
      "0.026876559481024742\n",
      "loss\n",
      "0.002644615015015006\n",
      "loss\n",
      "0.0009517907164990902\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.003852209774777293\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.00044764988706447184\n",
      "loss\n",
      "0.001259963377378881\n",
      "loss\n",
      "0.002960229991003871\n",
      "loss\n",
      "0.001384491566568613\n",
      "loss\n",
      "0.0019179059891030192\n",
      "loss\n",
      "0.0013440155889838934\n",
      "loss\n",
      "0.0005482124397531152\n",
      "loss\n",
      "0.005588384345173836\n",
      "loss\n",
      "0.0002812943421304226\n",
      "loss\n",
      "0.0008534126682206988\n",
      "loss\n",
      "0.0009110590908676386\n",
      "loss\n",
      "0.0007183355046436191\n",
      "loss\n",
      "0.0003687655262183398\n",
      "loss\n",
      "0.0008818790083751082\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.002041638595983386\n",
      "loss\n",
      "0.002261463785544038\n",
      "loss\n",
      "0.0010599239030852914\n",
      "loss\n",
      "0.0013654442736878991\n",
      "loss\n",
      "0.004441278520971537\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.010698935016989708\n",
      "loss\n",
      "0.000696654780767858\n",
      "loss\n",
      "0.0009991897968575358\n",
      "loss\n",
      "0.00774192251265049\n",
      "loss\n",
      "0.0011189873330295086\n",
      "loss\n",
      "0.0007273888913914561\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.00035232058144174516\n",
      "loss\n",
      "0.003114256775006652\n",
      "loss\n",
      "0.005950829479843378\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.00035398892941884696\n",
      "loss\n",
      "0.0009184433147311211\n",
      "loss\n",
      "0.00034850722295232117\n",
      "loss\n",
      "0.0016182672698050737\n",
      "loss\n",
      "0.0009114163694903255\n",
      "loss\n",
      "0.010211721062660217\n",
      "loss\n",
      "0.0008002892718650401\n",
      "loss\n",
      "0.0028330450877547264\n",
      "loss\n",
      "0.000977038755081594\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0009078433504328132\n",
      "loss\n",
      "0.0004047528200317174\n",
      "loss\n",
      "0.0012709167785942554\n",
      "loss\n",
      "0.0021843877620995045\n",
      "loss\n",
      "0.0014002051902934909\n",
      "loss\n",
      "0.001053255284205079\n",
      "loss\n",
      "0.0003840185818262398\n",
      "loss\n",
      "0.0006049233488738537\n",
      "loss\n",
      "0.0006123098428361118\n",
      "loss\n",
      "0.0007838514284230769\n",
      "loss\n",
      "0.005385533440858126\n",
      "loss\n",
      "0.0015470929211005569\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.002130145439878106\n",
      "loss\n",
      "0.000745018885936588\n",
      "loss\n",
      "0.0004693360242526978\n",
      "loss\n",
      "0.0018774517811834812\n",
      "loss\n",
      "0.0003051292151212692\n",
      "loss\n",
      "0.0018917298875749111\n",
      "loss\n",
      "0.0010977915953844786\n",
      "loss\n",
      "0.001210909802466631\n",
      "loss\n",
      "0.0024791003670543432\n",
      "loss\n",
      "0.002086844528093934\n",
      "loss\n",
      "0.0011520899133756757\n",
      "loss\n",
      "0.00046957432641647756\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "0.0006725909770466387\n",
      "loss\n",
      "0.0010973153403028846\n",
      "loss\n",
      "0.0013992529129609466\n",
      "loss\n",
      "0.0011435167398303747\n",
      "loss\n",
      "0.0009629856795072556\n",
      "loss\n",
      "0.009789552539587021\n",
      "loss\n",
      "0.00024577934527769685\n",
      "loss\n",
      "0.0004914983292110264\n",
      "loss\n",
      "0.00042906138696707785\n",
      "loss\n",
      "0.005800202023237944\n",
      "loss\n",
      "0.0008683010237291455\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0016599221853539348\n",
      "loss\n",
      "0.0010418231831863523\n",
      "loss\n",
      "0.0007952864980325103\n",
      "loss\n",
      "0.0007488307310268283\n",
      "loss\n",
      "0.00200130813755095\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.0005787128466181457\n",
      "loss\n",
      "0.0008704449282959104\n",
      "loss\n",
      "0.0008831891464069486\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0006168370018713176\n",
      "loss\n",
      "0.0010122895473614335\n",
      "loss\n",
      "0.000797192333266139\n",
      "loss\n",
      "0.001840565470047295\n",
      "loss\n",
      "0.0009284476400353014\n",
      "loss\n",
      "0.002931585069745779\n",
      "loss\n",
      "0.00023958197562023997\n",
      "loss\n",
      "0.0002543602604418993\n",
      "loss\n",
      "0.0008665143977850676\n",
      "loss\n",
      "0.0005872909096069634\n",
      "loss\n",
      "0.0009951406391337514\n",
      "loss\n",
      "0.0030088413041085005\n",
      "loss\n",
      "4.661959648132324\n",
      "loss\n",
      "0.012682733125984669\n",
      "loss\n",
      "0.0011266082292422652\n",
      "loss\n",
      "0.0011711412807926536\n",
      "loss\n",
      "0.0006866481271572411\n",
      "loss\n",
      "0.005445883143693209\n",
      "loss\n",
      "0.0044141001999378204\n",
      "loss\n",
      "0.0148676922544837\n",
      "loss\n",
      "0.0033791130408644676\n",
      "loss\n",
      "0.01798722706735134\n",
      "loss\n",
      "0.0019363479223102331\n",
      "loss\n",
      "0.0010003806091845036\n",
      "loss\n",
      "0.001105412608012557\n",
      "loss\n",
      "0.000695463502779603\n",
      "loss\n",
      "0.0007178590167313814\n",
      "loss\n",
      "0.058921705931425095\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.002141089178621769\n",
      "loss\n",
      "0.0011211306555196643\n",
      "loss\n",
      "0.06521012634038925\n",
      "loss\n",
      "0.0007843278581276536\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.0006530536338686943\n",
      "loss\n",
      "0.002709410386160016\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.007267585955560207\n",
      "loss\n",
      "0.005277866963297129\n",
      "loss\n",
      "0.0009411911014467478\n",
      "loss\n",
      "0.0005138983833603561\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.002880473854020238\n",
      "loss\n",
      "0.026496661826968193\n",
      "loss\n",
      "0.001375206047669053\n",
      "loss\n",
      "0.002098502591252327\n",
      "loss\n",
      "0.37235745787620544\n",
      "loss\n",
      "0.0003971264814026654\n",
      "loss\n",
      "0.0007009433466009796\n",
      "loss\n",
      "0.006032118573784828\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.001023483811877668\n",
      "loss\n",
      "0.00044550508027896285\n",
      "loss\n",
      "0.0014649622607976198\n",
      "loss\n",
      "0.0023430532310158014\n",
      "loss\n",
      "0.0051649706438183784\n",
      "loss\n",
      "0.0011328000109642744\n",
      "loss\n",
      "0.012277033179998398\n",
      "loss\n",
      "0.0009029601933434606\n",
      "loss\n",
      "0.0016807490028440952\n",
      "loss\n",
      "0.0008746135863475502\n",
      "loss\n",
      "0.0005560758872888982\n",
      "loss\n",
      "0.0037347583565860987\n",
      "loss\n",
      "0.0024947968777269125\n",
      "loss\n",
      "0.005204224959015846\n",
      "loss\n",
      "0.0009472650708630681\n",
      "loss\n",
      "0.000876757490914315\n",
      "loss\n",
      "0.0005915798828937113\n",
      "loss\n",
      "0.0004741021548397839\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007281036232598126\n",
      "loss\n",
      "0.001086598145775497\n",
      "loss\n",
      "0.0012180536286905408\n",
      "loss\n",
      "0.0010181248653680086\n",
      "loss\n",
      "0.0006835508393123746\n",
      "loss\n",
      "0.0008760428754612803\n",
      "loss\n",
      "0.001959786517545581\n",
      "loss\n",
      "0.0006316096987575293\n",
      "loss\n",
      "0.005911367479711771\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0007014198345132172\n",
      "loss\n",
      "0.001320443581789732\n",
      "loss\n",
      "0.0007500219508074224\n",
      "loss\n",
      "0.0008264940115623176\n",
      "loss\n",
      "0.0005551227368414402\n",
      "loss\n",
      "0.0007950482540763915\n",
      "loss\n",
      "0.002404181519523263\n",
      "loss\n",
      "0.0028921226039528847\n",
      "loss\n",
      "0.06963595002889633\n",
      "loss\n",
      "0.0018892312655225396\n",
      "loss\n",
      "0.002776341512799263\n",
      "loss\n",
      "1.097870945930481\n",
      "loss\n",
      "0.0005777596961706877\n",
      "loss\n",
      "0.0008172033121809363\n",
      "loss\n",
      "0.009694874286651611\n",
      "loss\n",
      "0.006388006266206503\n",
      "loss\n",
      "0.0017229963559657335\n",
      "loss\n",
      "0.001210671616718173\n",
      "loss\n",
      "0.0014313939027488232\n",
      "loss\n",
      "0.008590529672801495\n",
      "loss\n",
      "0.00047743841423653066\n",
      "loss\n",
      "0.0011382774682715535\n",
      "loss\n",
      "0.0026242840103805065\n",
      "loss\n",
      "0.0003644755925051868\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.00157803890760988\n",
      "loss\n",
      "0.0009129646932706237\n",
      "loss\n",
      "0.0011154150124639273\n",
      "loss\n",
      "0.001625765347853303\n",
      "loss\n",
      "2.8665642738342285\n",
      "loss\n",
      "0.012164924293756485\n",
      "loss\n",
      "0.0004950728034600616\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.005643506534397602\n",
      "loss\n",
      "0.00040451448876410723\n",
      "loss\n",
      "0.00030298411729745567\n",
      "loss\n",
      "0.04739883169531822\n",
      "loss\n",
      "0.036705754697322845\n",
      "loss\n",
      "0.00945920031517744\n",
      "loss\n",
      "0.00024720950750634074\n",
      "loss\n",
      "0.061474040150642395\n",
      "loss\n",
      "0.0006235085893422365\n",
      "loss\n",
      "0.007738137152045965\n",
      "loss\n",
      "0.00026246439665555954\n",
      "loss\n",
      "0.04900194704532623\n",
      "loss\n",
      "0.00022837892174720764\n",
      "loss\n",
      "0.021302001550793648\n",
      "loss\n",
      "0.07682917267084122\n",
      "loss\n",
      "0.0003003622987307608\n",
      "loss\n",
      "0.03357384353876114\n",
      "loss\n",
      "0.0002033503697020933\n",
      "loss\n",
      "0.00017331528943032026\n",
      "loss\n",
      "0.0002858230145648122\n",
      "loss\n",
      "0.012821382842957973\n",
      "loss\n",
      "0.016321489587426186\n",
      "loss\n",
      "0.08305186033248901\n",
      "loss\n",
      "0.0014668668154627085\n",
      "loss\n",
      "0.00033158526639454067\n",
      "loss\n",
      "0.018014157190918922\n",
      "loss\n",
      "0.00038211196078918874\n",
      "loss\n",
      "0.05664663016796112\n",
      "loss\n",
      "0.0001652104256208986\n",
      "loss\n",
      "0.0010937429033219814\n",
      "loss\n",
      "0.0409710630774498\n",
      "loss\n",
      "0.040607012808322906\n",
      "loss\n",
      "0.0007090438157320023\n",
      "loss\n",
      "0.005363598000258207\n",
      "loss\n",
      "0.00033968876232393086\n",
      "loss\n",
      "0.0037461596075445414\n",
      "loss\n",
      "0.000534868217073381\n",
      "loss\n",
      "0.00042715485324151814\n",
      "loss\n",
      "0.005437346640974283\n",
      "loss\n",
      "0.0005265279905870557\n",
      "loss\n",
      "0.018740614876151085\n",
      "loss\n",
      "0.005633667577058077\n",
      "loss\n",
      "0.002406084444373846\n",
      "loss\n",
      "0.0005832401220686734\n",
      "loss\n",
      "0.0005103239673189819\n",
      "loss\n",
      "0.0033570146188139915\n",
      "loss\n",
      "0.007710220292210579\n",
      "loss\n",
      "0.0002157455455744639\n",
      "loss\n",
      "0.004921702668070793\n",
      "loss\n",
      "0.00028391621890477836\n",
      "loss\n",
      "0.009648121893405914\n",
      "loss\n",
      "0.0002779574424494058\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.00031704644788987935\n",
      "loss\n",
      "0.002370406873524189\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.0037905762437731028\n",
      "loss\n",
      "0.003954449202865362\n",
      "loss\n",
      "0.00046695294440723956\n",
      "loss\n",
      "0.00443332688882947\n",
      "loss\n",
      "0.0001833270798670128\n",
      "loss\n",
      "0.0022333934903144836\n",
      "loss\n",
      "0.003699603257700801\n",
      "loss\n",
      "0.0019722788129001856\n",
      "loss\n",
      "0.010431881994009018\n",
      "loss\n",
      "0.0002743821241892874\n",
      "loss\n",
      "6.502380847930908\n",
      "loss\n",
      "0.0034477810841053724\n",
      "loss\n",
      "0.0003575639275368303\n",
      "loss\n",
      "0.0002674698771443218\n",
      "loss\n",
      "0.0030373651534318924\n",
      "loss\n",
      "0.0006556744920089841\n",
      "loss\n",
      "0.002501931507140398\n",
      "loss\n",
      "0.0031947072129696608\n",
      "loss\n",
      "0.0007147617870941758\n",
      "loss\n",
      "0.0033837463706731796\n",
      "loss\n",
      "0.001416394836269319\n",
      "loss\n",
      "0.0024562685284763575\n",
      "loss\n",
      "0.00471147894859314\n",
      "loss\n",
      "0.0022033003624528646\n",
      "loss\n",
      "0.002617269055917859\n",
      "loss\n",
      "0.00243831193074584\n",
      "loss\n",
      "0.0016720612766221166\n",
      "loss\n",
      "0.12599332630634308\n",
      "loss\n",
      "0.0017636949196457863\n",
      "loss\n",
      "0.0017951102927327156\n",
      "loss\n",
      "0.00887355301529169\n",
      "loss\n",
      "0.007288059685379267\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.0016561138909310102\n",
      "loss\n",
      "0.005817742552608252\n",
      "loss\n",
      "0.0010451575508341193\n",
      "loss\n",
      "0.0009803733555600047\n",
      "loss\n",
      "0.022383779287338257\n",
      "loss\n",
      "0.0032427129335701466\n",
      "loss\n",
      "0.0009266611887142062\n",
      "loss\n",
      "0.019285082817077637\n",
      "loss\n",
      "0.0009420248097740114\n",
      "loss\n",
      "0.0016465928638353944\n",
      "loss\n",
      "0.0023178397677838802\n",
      "loss\n",
      "0.002938360208645463\n",
      "loss\n",
      "0.08495984971523285\n",
      "loss\n",
      "0.0014054430648684502\n",
      "loss\n",
      "0.0009221353684552014\n",
      "loss\n",
      "0.00436816830188036\n",
      "loss\n",
      "0.0030544791370630264\n",
      "loss\n",
      "0.005048622377216816\n",
      "loss\n",
      "0.0060799880884587765\n",
      "loss\n",
      "0.016540072858333588\n",
      "loss\n",
      "0.2260076254606247\n",
      "loss\n",
      "0.007220128085464239\n",
      "loss\n",
      "0.004577396437525749\n",
      "loss\n",
      "0.0031294680666178465\n",
      "loss\n",
      "0.0011528043542057276\n",
      "loss\n",
      "0.008339227177202702\n",
      "loss\n",
      "0.0013830630341544747\n",
      "loss\n",
      "0.0027998790610581636\n",
      "loss\n",
      "0.001560185570269823\n",
      "loss\n",
      "0.0018848287872970104\n",
      "loss\n",
      "0.030961912125349045\n",
      "loss\n",
      "0.0022762122098356485\n",
      "loss\n",
      "0.003579401643946767\n",
      "loss\n",
      "0.0034686895087361336\n",
      "loss\n",
      "0.002000237349420786\n",
      "loss\n",
      "0.001800346071831882\n",
      "loss\n",
      "0.0035957936197519302\n",
      "loss\n",
      "0.014768090099096298\n",
      "loss\n",
      "0.0046996138989925385\n",
      "loss\n",
      "0.0067700534127652645\n",
      "loss\n",
      "0.0009396428358741105\n",
      "loss\n",
      "0.0017360866768285632\n",
      "loss\n",
      "0.001462581567466259\n",
      "loss\n",
      "0.0004752936656586826\n",
      "loss\n",
      "0.0007898071780800819\n",
      "loss\n",
      "0.0008774721063673496\n",
      "loss\n",
      "0.0011689979583024979\n",
      "loss\n",
      "0.004408997017890215\n",
      "loss\n",
      "0.01529852394014597\n",
      "loss\n",
      "0.0017256144201382995\n",
      "loss\n",
      "0.0005324853118509054\n",
      "loss\n",
      "0.00284944917075336\n",
      "loss\n",
      "0.3101171851158142\n",
      "loss\n",
      "0.0016018429305404425\n",
      "loss\n",
      "0.0008465044084005058\n",
      "loss\n",
      "0.00684417225420475\n",
      "loss\n",
      "0.0010065733222290874\n",
      "loss\n",
      "0.001503290724940598\n",
      "loss\n",
      "0.01349575724452734\n",
      "loss\n",
      "0.0005980133428238332\n",
      "loss\n",
      "0.005173865240067244\n",
      "loss\n",
      "0.0010562323732301593\n",
      "loss\n",
      "0.04587842524051666\n",
      "loss\n",
      "0.001095886342227459\n",
      "loss\n",
      "0.0011753087164834142\n",
      "loss\n",
      "0.000433112756581977\n",
      "loss\n",
      "0.0035584955476224422\n",
      "loss\n",
      "0.03456712141633034\n",
      "loss\n",
      "0.01112897228449583\n",
      "loss\n",
      "0.19578249752521515\n",
      "loss\n",
      "0.0008631794480606914\n",
      "loss\n",
      "0.0010082405060529709\n",
      "loss\n",
      "0.0028364923782646656\n",
      "loss\n",
      "0.0018275955226272345\n",
      "loss\n",
      "0.0023895539343357086\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.00041214076918549836\n",
      "loss\n",
      "0.0006101653561927378\n",
      "loss\n",
      "0.0008522216230630875\n",
      "loss\n",
      "0.0012179345358163118\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0016235039802268147\n",
      "loss\n",
      "0.0009777533123269677\n",
      "loss\n",
      "0.001444011926651001\n",
      "loss\n",
      "0.0035663354210555553\n",
      "loss\n",
      "0.01225253939628601\n",
      "loss\n",
      "0.000615407363511622\n",
      "loss\n",
      "0.002785851713269949\n",
      "loss\n",
      "0.0010449193650856614\n",
      "loss\n",
      "0.0018184330547228456\n",
      "loss\n",
      "0.0007669368060305715\n",
      "loss\n",
      "0.0027184458449482918\n",
      "loss\n",
      "0.001904699020087719\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "0.0010415849974378943\n",
      "loss\n",
      "0.000568228424526751\n",
      "loss\n",
      "0.0031387372873723507\n",
      "loss\n",
      "0.00719101307913661\n",
      "loss\n",
      "0.0006353028584271669\n",
      "loss\n",
      "0.000486970558995381\n",
      "loss\n",
      "0.0007688426994718611\n",
      "loss\n",
      "0.0007824220228940248\n",
      "loss\n",
      "0.0013804440386593342\n",
      "loss\n",
      "0.0009111781837418675\n",
      "loss\n",
      "0.0014972201315686107\n",
      "loss\n",
      "0.0006042085005901754\n",
      "loss\n",
      "0.0012735360069200397\n",
      "loss\n",
      "0.0013263961300253868\n",
      "loss\n",
      "0.0005981324939057231\n",
      "loss\n",
      "0.0006130246329121292\n",
      "loss\n",
      "0.0023889592848718166\n",
      "loss\n",
      "0.0011750705307349563\n",
      "loss\n",
      "0.0005891970940865576\n",
      "loss\n",
      "0.0004895919119007885\n",
      "loss\n",
      "0.0005679901223629713\n",
      "loss\n",
      "0.5016879439353943\n",
      "loss\n",
      "0.0017500099493190646\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.0005539313424378633\n",
      "loss\n",
      "0.003807914676144719\n",
      "loss\n",
      "0.028101855888962746\n",
      "loss\n",
      "0.0008391196606680751\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.00046433156239800155\n",
      "loss\n",
      "0.1355494260787964\n",
      "loss\n",
      "0.000809818331617862\n",
      "loss\n",
      "0.005297314375638962\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.0007186928996816278\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.0024447336327284575\n",
      "loss\n",
      "0.0013624681159853935\n",
      "loss\n",
      "0.000847933697514236\n",
      "loss\n",
      "0.0033197076991200447\n",
      "loss\n",
      "0.0002531684876885265\n",
      "loss\n",
      "0.0006457865820266306\n",
      "loss\n",
      "0.00021288513380568475\n",
      "loss\n",
      "0.0028309053741395473\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "5.774600982666016\n",
      "loss\n",
      "0.0007955246837809682\n",
      "loss\n",
      "0.0029955299105495214\n",
      "loss\n",
      "1.699110507965088\n",
      "loss\n",
      "0.07665915042161942\n",
      "loss\n",
      "0.0006833125371485949\n",
      "loss\n",
      "0.0008222059695981443\n",
      "loss\n",
      "0.001655756845138967\n",
      "loss\n",
      "0.000842692912556231\n",
      "loss\n",
      "0.001057184999808669\n",
      "loss\n",
      "0.0017527469899505377\n",
      "loss\n",
      "0.0015608996618539095\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001906721736304462\n",
      "loss\n",
      "0.0029006809927523136\n",
      "loss\n",
      "0.0015824426664039493\n",
      "loss\n",
      "0.006690955720841885\n",
      "loss\n",
      "0.026949666440486908\n",
      "loss\n",
      "0.005086457822471857\n",
      "loss\n",
      "0.0062572285532951355\n",
      "loss\n",
      "0.0013733012601733208\n",
      "loss\n",
      "0.015596204437315464\n",
      "loss\n",
      "0.004771632142364979\n",
      "loss\n",
      "0.010210777632892132\n",
      "loss\n",
      "0.0018556771101430058\n",
      "loss\n",
      "0.0012656782055273652\n",
      "loss\n",
      "0.001777260797098279\n",
      "loss\n",
      "0.0009158230968751013\n",
      "loss\n",
      "0.00970514491200447\n",
      "loss\n",
      "0.0015851801726967096\n",
      "loss\n",
      "0.007525546941906214\n",
      "loss\n",
      "0.0019395602867007256\n",
      "loss\n",
      "0.14365465939044952\n",
      "loss\n",
      "0.006396060809493065\n",
      "loss\n",
      "0.0021193204447627068\n",
      "loss\n",
      "0.007879721000790596\n",
      "loss\n",
      "0.03223589062690735\n",
      "loss\n",
      "0.0014585343888029456\n",
      "loss\n",
      "0.12759247422218323\n",
      "loss\n",
      "0.0017432268941774964\n",
      "loss\n",
      "0.0009039129945449531\n",
      "loss\n",
      "0.02098698727786541\n",
      "loss\n",
      "0.0011002921964973211\n",
      "loss\n",
      "0.015343839302659035\n",
      "loss\n",
      "0.0017241863533854485\n",
      "loss\n",
      "0.012848097831010818\n",
      "loss\n",
      "0.016196461394429207\n",
      "loss\n",
      "0.007988644763827324\n",
      "loss\n",
      "0.0008741371566429734\n",
      "loss\n",
      "0.0033289750572293997\n",
      "loss\n",
      "0.002584690460935235\n",
      "loss\n",
      "0.3376161456108093\n",
      "loss\n",
      "0.17622306942939758\n",
      "loss\n",
      "0.005285219289362431\n",
      "loss\n",
      "0.0030551922973245382\n",
      "loss\n",
      "0.0048184944316744804\n",
      "loss\n",
      "0.004540016409009695\n",
      "loss\n",
      "0.004049673210829496\n",
      "loss\n",
      "0.004293984733521938\n",
      "loss\n",
      "0.0020384264644235373\n",
      "loss\n",
      "0.6701012849807739\n",
      "loss\n",
      "0.006541150622069836\n",
      "loss\n",
      "0.0017422748496755958\n",
      "loss\n",
      "0.03246106579899788\n",
      "loss\n",
      "0.0052717006765306\n",
      "loss\n",
      "0.005346760619431734\n",
      "loss\n",
      "0.0010355116100981832\n",
      "loss\n",
      "0.0005392765742726624\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0026057357899844646\n",
      "loss\n",
      "0.003418674925342202\n",
      "loss\n",
      "0.0008293526479974389\n",
      "loss\n",
      "0.009370513260364532\n",
      "loss\n",
      "0.0006006343755871058\n",
      "loss\n",
      "0.003137667663395405\n",
      "loss\n",
      "0.0011179156135767698\n",
      "loss\n",
      "0.01869744248688221\n",
      "loss\n",
      "0.0004447901446837932\n",
      "loss\n",
      "0.001640761154703796\n",
      "loss\n",
      "0.0004435985756572336\n",
      "loss\n",
      "0.004437480587512255\n",
      "loss\n",
      "0.0004259632551111281\n",
      "loss\n",
      "0.02258472703397274\n",
      "loss\n",
      "0.0006771179032512009\n",
      "loss\n",
      "0.0015863704029470682\n",
      "loss\n",
      "0.0030540036968886852\n",
      "loss\n",
      "0.0007546676206402481\n",
      "loss\n",
      "0.002878334140405059\n",
      "loss\n",
      "0.0005322470096871257\n",
      "loss\n",
      "0.026510940864682198\n",
      "loss\n",
      "0.009304258041083813\n",
      "loss\n",
      "0.021776961162686348\n",
      "loss\n",
      "0.001404728856869042\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0008524598088115454\n",
      "loss\n",
      "0.0027340196538716555\n",
      "loss\n",
      "0.002498839981853962\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.00899465661495924\n",
      "loss\n",
      "0.0012816318776458502\n",
      "loss\n",
      "0.001966211013495922\n",
      "loss\n",
      "0.0018600797047838569\n",
      "loss\n",
      "0.0019061268540099263\n",
      "loss\n",
      "0.0014174662064760923\n",
      "loss\n",
      "0.003535688389092684\n",
      "loss\n",
      "0.0014075858052819967\n",
      "loss\n",
      "0.0005794276366941631\n",
      "loss\n",
      "0.0028871302492916584\n",
      "loss\n",
      "0.0061088986694812775\n",
      "loss\n",
      "0.00042572495294734836\n",
      "loss\n",
      "0.000428108120104298\n",
      "loss\n",
      "0.0028246049769222736\n",
      "loss\n",
      "0.007277645170688629\n",
      "loss\n",
      "0.0010798105504363775\n",
      "loss\n",
      "0.0030859727412462234\n",
      "loss\n",
      "0.0006139777251519263\n",
      "loss\n",
      "0.0005782362422905862\n",
      "loss\n",
      "0.010593726299703121\n",
      "loss\n",
      "0.0012159105390310287\n",
      "loss\n",
      "0.0005053196800872684\n",
      "loss\n",
      "0.001134705264121294\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000846266164444387\n",
      "loss\n",
      "0.0016955060418695211\n",
      "loss\n",
      "0.001279369811527431\n",
      "loss\n",
      "0.00779361417517066\n",
      "loss\n",
      "0.0016464737709611654\n",
      "loss\n",
      "0.003351786872372031\n",
      "loss\n",
      "0.005628214683383703\n",
      "loss\n",
      "0.0035672858357429504\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.0006885541952215135\n",
      "loss\n",
      "0.0004690977220889181\n",
      "loss\n",
      "0.0005258131423033774\n",
      "loss\n",
      "0.0017419178038835526\n",
      "loss\n",
      "0.001593511551618576\n",
      "loss\n",
      "0.0007316772826015949\n",
      "loss\n",
      "0.000707971747033298\n",
      "loss\n",
      "0.0006911749369464815\n",
      "loss\n",
      "0.002665896899998188\n",
      "loss\n",
      "0.0006254147156141698\n",
      "loss\n",
      "0.0015022194711491466\n",
      "loss\n",
      "0.009392124600708485\n",
      "loss\n",
      "0.012946008704602718\n",
      "loss\n",
      "0.0010157431242987514\n",
      "loss\n",
      "0.0014280608156695962\n",
      "loss\n",
      "0.003851853543892503\n",
      "loss\n",
      "0.000559173640795052\n",
      "loss\n",
      "0.0003511289251036942\n",
      "loss\n",
      "0.004013104364275932\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.030644262209534645\n",
      "loss\n",
      "0.0011582816950976849\n",
      "loss\n",
      "0.0014397265622392297\n",
      "loss\n",
      "0.0021531034726649523\n",
      "loss\n",
      "0.001191621064208448\n",
      "loss\n",
      "0.0016000575851649046\n",
      "loss\n",
      "0.0017477489309385419\n",
      "loss\n",
      "0.008792140521109104\n",
      "loss\n",
      "0.0028020190075039864\n",
      "loss\n",
      "0.0006766413571313024\n",
      "loss\n",
      "0.0018878034316003323\n",
      "loss\n",
      "0.0017471539322286844\n",
      "loss\n",
      "0.001256510615348816\n",
      "loss\n",
      "0.001255677198059857\n",
      "loss\n",
      "0.0038440159987658262\n",
      "loss\n",
      "0.0017959432443603873\n",
      "loss\n",
      "0.0016683719586580992\n",
      "loss\n",
      "0.0008314966107718647\n",
      "loss\n",
      "0.0022165034897625446\n",
      "loss\n",
      "0.0023671959061175585\n",
      "loss\n",
      "0.0002982171718031168\n",
      "loss\n",
      "0.014729914255440235\n",
      "loss\n",
      "0.0027272433508187532\n",
      "loss\n",
      "0.0008842610404826701\n",
      "loss\n",
      "0.0004936429904773831\n",
      "loss\n",
      "0.0007914748275652528\n",
      "loss\n",
      "0.0013392536202445626\n",
      "loss\n",
      "0.0038484097458422184\n",
      "loss\n",
      "0.0009913297835737467\n",
      "loss\n",
      "0.03474237769842148\n",
      "loss\n",
      "0.0005641775787808001\n",
      "loss\n",
      "0.0003849719068966806\n",
      "loss\n",
      "0.0013365155318751931\n",
      "loss\n",
      "0.0008220868767239153\n",
      "loss\n",
      "0.0014574630185961723\n",
      "loss\n",
      "0.0016183863626793027\n",
      "loss\n",
      "0.00040356122190132737\n",
      "loss\n",
      "0.0005911033367738128\n",
      "loss\n",
      "0.0027939353603869677\n",
      "loss\n",
      "0.00039641151670366526\n",
      "loss\n",
      "0.0027140469755977392\n",
      "loss\n",
      "0.0018346159486100078\n",
      "loss\n",
      "0.002303329762071371\n",
      "loss\n",
      "0.0006030171643942595\n",
      "loss\n",
      "0.009124013595283031\n",
      "loss\n",
      "0.0012161486083641648\n",
      "loss\n",
      "0.0027712297160178423\n",
      "loss\n",
      "0.0002739054325502366\n",
      "loss\n",
      "0.0006935574929229915\n",
      "loss\n",
      "0.0003129946126136929\n",
      "loss\n",
      "0.000476246903417632\n",
      "loss\n",
      "0.00044288364006206393\n",
      "loss\n",
      "0.0018682897789403796\n",
      "loss\n",
      "0.0003418338019400835\n",
      "loss\n",
      "0.0012497241841629148\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0023768290411680937\n",
      "loss\n",
      "0.02574142999947071\n",
      "loss\n",
      "0.0017401328077539802\n",
      "loss\n",
      "0.004134322516620159\n",
      "loss\n",
      "0.0008989107445813715\n",
      "loss\n",
      "0.0005607224884442985\n",
      "loss\n",
      "0.001046705641783774\n",
      "loss\n",
      "0.0007827793597243726\n",
      "loss\n",
      "0.0006280356901697814\n",
      "loss\n",
      "0.02786920592188835\n",
      "loss\n",
      "0.0008538890979252756\n",
      "loss\n",
      "0.0010851691477000713\n",
      "loss\n",
      "0.00240013818256557\n",
      "loss\n",
      "0.0021174170542508364\n",
      "loss\n",
      "0.002991488901898265\n",
      "loss\n",
      "0.0014588914345949888\n",
      "loss\n",
      "0.004241281189024448\n",
      "loss\n",
      "0.003736658487468958\n",
      "loss\n",
      "0.0022800182923674583\n",
      "loss\n",
      "0.0007152383332140744\n",
      "loss\n",
      "0.000924993772059679\n",
      "loss\n",
      "0.003994819708168507\n",
      "loss\n",
      "0.000894146622158587\n",
      "loss\n",
      "0.0027646913658827543\n",
      "loss\n",
      "0.0007594323833473027\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.0012949660886079073\n",
      "loss\n",
      "0.002144657773897052\n",
      "loss\n",
      "0.0016444505890831351\n",
      "loss\n",
      "0.0005864569102413952\n",
      "loss\n",
      "0.00039188333903439343\n",
      "loss\n",
      "0.0029439465142786503\n",
      "loss\n",
      "0.0022638426162302494\n",
      "loss\n",
      "0.0009483369067311287\n",
      "loss\n",
      "0.0116086695343256\n",
      "loss\n",
      "0.0008537700050510466\n",
      "loss\n",
      "0.001016814960166812\n",
      "loss\n",
      "0.0008986725588329136\n",
      "loss\n",
      "0.000701658078469336\n",
      "loss\n",
      "0.0017089537577703595\n",
      "loss\n",
      "0.0011794761521741748\n",
      "loss\n",
      "0.0004247716860845685\n",
      "loss\n",
      "0.0005683475756086409\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0010266992030665278\n",
      "loss\n",
      "0.0019143365789204836\n",
      "loss\n",
      "0.0005214046686887741\n",
      "loss\n",
      "0.001148994080722332\n",
      "loss\n",
      "0.004422408062964678\n",
      "loss\n",
      "0.0005122303264215589\n",
      "loss\n",
      "0.0011468507582321763\n",
      "loss\n",
      "0.00022957073815632612\n",
      "loss\n",
      "0.0027576773427426815\n",
      "loss\n",
      "0.000952267087996006\n",
      "loss\n",
      "0.016635630279779434\n",
      "loss\n",
      "0.0007250064518302679\n",
      "loss\n",
      "0.0006623458466492593\n",
      "loss\n",
      "0.0007408496458083391\n",
      "loss\n",
      "0.002483262214809656\n",
      "loss\n",
      "0.0021470370702445507\n",
      "loss\n",
      "0.0009137984015978873\n",
      "loss\n",
      "0.00023457636416424066\n",
      "loss\n",
      "0.0023837266489863396\n",
      "loss\n",
      "0.0011623300379142165\n",
      "loss\n",
      "0.0013184197014197707\n",
      "loss\n",
      "0.001476865611039102\n",
      "loss\n",
      "0.003979858942329884\n",
      "loss\n",
      "0.0005212855176068842\n",
      "loss\n",
      "0.0005154472892172635\n",
      "loss\n",
      "0.00042965717148035765\n",
      "loss\n",
      "0.000764792668633163\n",
      "loss\n",
      "0.0009112972766160965\n",
      "loss\n",
      "0.0008796160109341145\n",
      "loss\n",
      "0.0008474572678096592\n",
      "loss\n",
      "0.0005744237569160759\n",
      "loss\n",
      "0.0019026764202862978\n",
      "loss\n",
      "0.002003092784434557\n",
      "loss\n",
      "0.42928773164749146\n",
      "loss\n",
      "0.0008839037618599832\n",
      "loss\n",
      "0.001987983239814639\n",
      "loss\n",
      "0.014038268476724625\n",
      "loss\n",
      "0.0011518517276272178\n",
      "loss\n",
      "0.00032658010604791343\n",
      "loss\n",
      "0.0008891443139873445\n",
      "loss\n",
      "0.000750736624468118\n",
      "loss\n",
      "0.013025554828345776\n",
      "loss\n",
      "0.004036019556224346\n",
      "loss\n",
      "0.005081120412796736\n",
      "loss\n",
      "0.0002636561985127628\n",
      "loss\n",
      "0.003246871754527092\n",
      "loss\n",
      "0.00031418632715940475\n",
      "loss\n",
      "0.005515950731933117\n",
      "loss\n",
      "0.0016540905926376581\n",
      "loss\n",
      "0.00022194306075107306\n",
      "loss\n",
      "0.0026214304380118847\n",
      "loss\n",
      "0.016800694167613983\n",
      "loss\n",
      "0.004386090207844973\n",
      "loss\n",
      "0.0003225283289793879\n",
      "loss\n",
      "0.000758955895435065\n",
      "loss\n",
      "0.00035613393993116915\n",
      "loss\n",
      "0.00037448544753715396\n",
      "loss\n",
      "0.0017446548445150256\n",
      "loss\n",
      "0.00015090756642166525\n",
      "loss\n",
      "0.0002574589161667973\n",
      "loss\n",
      "0.0007256020326167345\n",
      "loss\n",
      "0.001990600721910596\n",
      "loss\n",
      "0.00012468514614738524\n",
      "loss\n",
      "0.0026844439562410116\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.000788258679676801\n",
      "loss\n",
      "0.00028796817059628665\n",
      "loss\n",
      "0.0013338964199647307\n",
      "loss\n",
      "0.00020621081057470292\n",
      "loss\n",
      "0.05171101912856102\n",
      "loss\n",
      "0.0030126445926725864\n",
      "loss\n",
      "0.005566809326410294\n",
      "loss\n",
      "0.00016234986833296716\n",
      "loss\n",
      "0.00017736769223120064\n",
      "loss\n",
      "0.0002821285743266344\n",
      "loss\n",
      "0.0006914132391102612\n",
      "loss\n",
      "0.0004648081958293915\n",
      "loss\n",
      "0.0008348317351192236\n",
      "loss\n",
      "0.0019875073339790106\n",
      "loss\n",
      "0.0002169373765354976\n",
      "loss\n",
      "0.0012260308722034097\n",
      "loss\n",
      "0.00018523407925385982\n",
      "loss\n",
      "0.00041571559268049896\n",
      "loss\n",
      "0.0006481691962108016\n",
      "loss\n",
      "0.0010397987207397819\n",
      "loss\n",
      "0.0006696127820760012\n",
      "loss\n",
      "0.011480340734124184\n",
      "loss\n",
      "0.0003594706067815423\n",
      "loss\n",
      "0.0024949158541858196\n",
      "loss\n",
      "0.011587812565267086\n",
      "loss\n",
      "0.0006804534932598472\n",
      "loss\n",
      "0.0003280101518612355\n",
      "loss\n",
      "0.0011869773734360933\n",
      "loss\n",
      "0.0009833505610004067\n",
      "loss\n",
      "0.00016330339713022113\n",
      "loss\n",
      "0.0008563903393223882\n",
      "loss\n",
      "0.0004122599493712187\n",
      "loss\n",
      "0.0005706112715415657\n",
      "loss\n",
      "0.003935806918889284\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0002915434306487441\n",
      "loss\n",
      "0.00020215852418914437\n",
      "loss\n",
      "0.00126008247025311\n",
      "loss\n",
      "0.0009531007381156087\n",
      "loss\n",
      "0.00030632095877081156\n",
      "loss\n",
      "0.0005411829333752394\n"
     ]
    }
   ],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "for i in range(100):\n",
    "    for seqs, seqs_len in train_dls[2]:\n",
    "\n",
    "        gating.train()\n",
    "\n",
    "        gating_optimizer.zero_grad()\n",
    "\n",
    "        outputs = gating(seqs, seqs_len)\n",
    "\n",
    "        #print(\"seq\")\n",
    "        #print(seqs)\n",
    "        #print(\"outputs\")\n",
    "        #print(outputs)\n",
    "\n",
    "        if seqs[1] == 7:\n",
    "            trgts = torch.tensor([0])\n",
    "        else:\n",
    "            trgts = torch.tensor([1])\n",
    "\n",
    "        loss = compute_loss(outputs, trgts, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        gating_optimizer.step()\n",
    "\n",
    "        print(\"loss\")\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ONMUcvfWeipD"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                        reduction=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcqtDHZcJah8",
    "outputId": "a48e44cc-67ab-474d-8e5b-5267ec501dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "gating, gating_optimizer = init_gating()\n",
    "model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "print(model.apply(init_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "UYduyRNYJolw"
   },
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARkJQvgNgU27",
    "outputId": "0012ec60-7122-4876-8fb5-5a863be5f25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 5, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 7, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 2])\n",
      "1 - tensor([1, 5, 3, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kO7XbXIIgjWN",
    "outputId": "67eb26e2-95b1-4844-e801-752078589da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 7, 4, 6, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 2])\n",
      "1 - tensor([1, 7, 3, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "XQStSnwP-bwf"
   },
   "outputs": [],
   "source": [
    "gating_optimizer = optim.Adam(model.gating.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xtz-JxQ--n-U",
    "outputId": "a4164e4b-10f8-4a69-acb9-d454881779b5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3362e-14,\n",
      "          0.0000e+00,  0.0000e+00, -1.0627e-14,  1.2231e-14,  0.0000e+00],\n",
      "        [ 9.5526e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5729e-09,\n",
      "         -8.0322e-09,  0.0000e+00, -1.3070e-08,  2.0716e-08,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.2082e-14, -4.8358e-12,  6.6748e-12, -6.2617e-12,\n",
      "         -5.3810e-12,  7.6332e-12, -2.3784e-12,  0.0000e+00, -7.3044e-12],\n",
      "        [-4.1353e-13, -4.9267e-13, -3.5623e-13,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  5.5728e-13,  0.0000e+00,  4.3358e-13, -5.3819e-13],\n",
      "        [ 0.0000e+00,  0.0000e+00, -8.9321e-11,  5.7395e-14,  0.0000e+00,\n",
      "         -1.0017e-10,  1.4144e-10,  0.0000e+00,  5.0259e-14, -6.3196e-14],\n",
      "        [ 0.0000e+00, -1.0079e-14,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -8.3384e-15,  1.0995e-14, -6.3370e-15,  0.0000e+00, -1.1091e-14]])\n",
      "rnn.weight_ih_l0\n",
      "tensor([[-1.2741e-11, -4.1343e-15, -1.2010e-12,  5.5348e-14, -6.1908e-12,\n",
      "         -2.3396e-11,  1.1962e-12,  1.5775e-12,  1.0668e-11, -6.6139e-14],\n",
      "        [-1.7986e-11, -6.1930e-15, -1.7422e-12,  8.5238e-14, -8.7462e-12,\n",
      "         -3.3065e-11,  1.7365e-12,  2.2192e-12,  1.5059e-11, -1.0169e-13],\n",
      "        [-1.7355e-11, -5.7870e-15, -1.6031e-12,  7.9681e-14, -8.4371e-12,\n",
      "         -3.1844e-11,  1.5982e-12,  2.1442e-12,  1.4531e-11, -9.5055e-14],\n",
      "        [-1.8238e-11, -6.5488e-15, -1.8412e-12,  8.8614e-14, -8.8711e-12,\n",
      "         -3.3589e-11,  1.8348e-12,  2.2480e-12,  1.5270e-11, -1.0583e-13],\n",
      "        [-9.1758e-12, -2.9753e-15, -9.0931e-13,  3.4315e-14, -4.4529e-12,\n",
      "         -1.6883e-11,  9.0369e-13,  1.1421e-12,  7.6826e-12, -4.1435e-14],\n",
      "        [-1.2727e-17, -2.0106e-19, -7.9940e-19,  1.0168e-20, -3.3603e-17,\n",
      "         -2.3203e-17,  1.0065e-18, -1.8089e-17,  2.2752e-17, -2.1822e-19],\n",
      "        [-2.1118e-11, -6.7207e-15, -1.9434e-12,  9.6933e-14, -1.0267e-11,\n",
      "         -3.8743e-11,  1.9376e-12,  2.6091e-12,  1.7681e-11, -1.1530e-13],\n",
      "        [-4.2235e-12, -1.5118e-15, -4.4800e-13,  1.6007e-14, -2.0497e-12,\n",
      "         -7.7942e-12,  4.4502e-13,  5.2544e-13,  3.5362e-12, -1.9453e-14],\n",
      "        [-1.2104e-11, -4.2045e-15, -1.2392e-12,  4.8817e-14, -5.8774e-12,\n",
      "         -2.2303e-11,  1.2321e-12,  1.5028e-12,  1.0134e-11, -5.8929e-14],\n",
      "        [-4.7408e-12, -2.2792e-15, -5.7470e-13,  2.1609e-14, -2.3041e-12,\n",
      "         -8.8055e-12,  5.7121e-13,  5.8587e-13,  3.9693e-12, -2.6509e-14],\n",
      "        [ 6.6286e-13,  7.3984e-16, -8.5020e-14, -1.5201e-15,  3.2065e-13,\n",
      "          1.0991e-12,  8.3078e-14, -8.3417e-14, -5.5514e-13,  2.3342e-15],\n",
      "        [ 8.7248e-13,  8.9356e-16, -1.1475e-13, -1.8182e-15,  4.2183e-13,\n",
      "          1.4442e-12,  1.1224e-13, -1.1005e-13, -7.3065e-13,  2.8197e-15],\n",
      "        [ 8.4736e-13,  8.5097e-16, -1.0343e-13, -1.5827e-15,  4.0949e-13,\n",
      "          1.4090e-12,  1.0117e-13, -1.0709e-13, -7.0961e-13,  2.5146e-15],\n",
      "        [ 1.0759e-12,  9.2496e-16, -1.3016e-13, -2.1993e-15,  5.2013e-13,\n",
      "          1.7902e-12,  1.2728e-13, -1.3580e-13, -9.0093e-13,  3.2712e-15],\n",
      "        [ 5.6700e-13,  8.6599e-16, -8.4782e-14, -2.0148e-15,  2.7489e-13,\n",
      "          9.3052e-13,  8.2685e-14, -7.0587e-14, -4.7490e-13,  2.9716e-15],\n",
      "        [ 3.2364e-17,  1.4174e-16,  3.2161e-17, -3.8775e-17,  2.1664e-16,\n",
      "          1.3156e-16, -1.3289e-16,  2.2662e-16, -1.2378e-16,  1.6023e-16],\n",
      "        [ 1.1006e-12,  8.1159e-16, -1.1776e-13, -1.7806e-15,  5.3170e-13,\n",
      "          1.8437e-12,  1.1522e-13, -1.3944e-13, -9.2167e-13,  2.7080e-15],\n",
      "        [ 3.1208e-13,  9.5127e-16, -5.4686e-14, -1.5326e-15,  1.5156e-13,\n",
      "          5.0549e-13,  5.3179e-14, -3.8384e-14, -2.6151e-13,  2.4646e-15],\n",
      "        [ 6.8898e-13,  1.1269e-15, -1.1249e-13, -2.6129e-15,  3.3411e-13,\n",
      "          1.1229e-12,  1.0977e-13, -8.5628e-14, -5.7703e-13,  3.8803e-15],\n",
      "        [ 4.0820e-13,  1.7776e-15, -8.1602e-14, -2.9815e-15,  1.9877e-13,\n",
      "          6.5251e-13,  7.9180e-14, -4.9381e-14, -3.4215e-13,  4.7378e-15],\n",
      "        [-1.1690e-10, -4.0569e-14, -9.9090e-12,  5.7459e-13, -5.6869e-11,\n",
      "         -2.1379e-10,  9.9007e-12,  1.4401e-11,  9.7877e-11, -6.8428e-13],\n",
      "        [ 1.5179e-10,  5.5814e-14,  1.3358e-11, -8.0338e-13,  7.3899e-11,\n",
      "          2.7800e-10, -1.3354e-11, -1.8637e-11, -1.2709e-10,  9.5584e-13],\n",
      "        [ 1.4691e-10,  5.1973e-14,  1.2321e-11, -7.5753e-13,  7.1503e-11,\n",
      "          2.6858e-10, -1.2322e-11, -1.8059e-11, -1.2300e-10,  9.0062e-13],\n",
      "        [ 1.5032e-10,  5.7241e-14,  1.3668e-11, -8.0311e-13,  7.3187e-11,\n",
      "          2.7565e-10, -1.3659e-11, -1.8447e-11, -1.2585e-10,  9.5701e-13],\n",
      "        [ 7.5015e-11,  2.6564e-14,  6.6023e-12, -3.2660e-13,  3.6450e-11,\n",
      "          1.3737e-10, -6.5820e-12, -9.2864e-12, -6.2807e-11,  3.9260e-13],\n",
      "        [-4.9563e-15, -4.4676e-18, -2.3274e-16,  6.1259e-18, -2.7919e-15,\n",
      "         -8.9162e-15,  2.3483e-16,  3.4348e-16,  4.3254e-15, -1.1415e-17],\n",
      "        [-1.7836e-10, -5.9934e-14, -1.4637e-11,  8.8339e-13, -8.6773e-11,\n",
      "         -3.2581e-10,  1.4634e-11,  2.1964e-11,  1.4933e-10, -1.0495e-12],\n",
      "        [-3.8141e-11, -1.4796e-14, -3.5249e-12,  1.7314e-13, -1.8539e-11,\n",
      "         -6.9977e-11,  3.5140e-12,  4.7135e-12,  3.1934e-11, -2.0884e-13],\n",
      "        [ 9.0623e-11,  3.4656e-14,  8.3105e-12, -4.2677e-13,  4.4064e-11,\n",
      "          1.6622e-10, -8.2893e-12, -1.1184e-11, -7.5875e-11,  5.1302e-13],\n",
      "        [-3.8155e-11, -2.0677e-14, -4.1315e-12,  2.1317e-13, -1.8584e-11,\n",
      "         -7.0482e-11,  4.1218e-12,  4.6718e-12,  3.1946e-11, -2.5969e-13]])\n",
      "rnn.weight_hh_l0\n",
      "tensor([[ 3.2406e-11, -3.2207e-11, -3.2240e-11, -3.2168e-11, -3.2604e-11,\n",
      "          3.2936e-11,  3.2080e-11,  3.2774e-11, -3.2534e-11,  3.2767e-11],\n",
      "        [ 4.5892e-11, -4.5609e-11, -4.5657e-11, -4.5555e-11, -4.6172e-11,\n",
      "          4.6641e-11,  4.5431e-11,  4.6412e-11, -4.6072e-11,  4.6402e-11],\n",
      "        [ 4.4040e-11, -4.3768e-11, -4.3814e-11, -4.3715e-11, -4.4309e-11,\n",
      "          4.4760e-11,  4.3596e-11,  4.4539e-11, -4.4213e-11,  4.4530e-11],\n",
      "        [ 4.6769e-11, -4.6481e-11, -4.6530e-11, -4.6426e-11, -4.7053e-11,\n",
      "          4.7530e-11,  4.6300e-11,  4.7297e-11, -4.6952e-11,  4.7287e-11],\n",
      "        [ 2.3477e-11, -2.3332e-11, -2.3357e-11, -2.3304e-11, -2.3620e-11,\n",
      "          2.3859e-11,  2.3241e-11,  2.3742e-11, -2.3569e-11,  2.3737e-11],\n",
      "        [ 3.1307e-17, -3.1110e-17, -3.1140e-17, -3.1068e-17, -3.1490e-17,\n",
      "          3.1876e-17,  3.0985e-17,  3.1657e-17, -3.1424e-17,  3.1647e-17],\n",
      "        [ 5.3566e-11, -5.3236e-11, -5.3291e-11, -5.3171e-11, -5.3893e-11,\n",
      "          5.4443e-11,  5.3027e-11,  5.4174e-11, -5.3777e-11,  5.4162e-11],\n",
      "        [ 1.0898e-11, -1.0831e-11, -1.0842e-11, -1.0818e-11, -1.0964e-11,\n",
      "          1.1075e-11,  1.0789e-11,  1.1021e-11, -1.0940e-11,  1.1018e-11],\n",
      "        [ 3.1093e-11, -3.0902e-11, -3.0934e-11, -3.0865e-11, -3.1282e-11,\n",
      "          3.1599e-11,  3.0781e-11,  3.1444e-11, -3.1215e-11,  3.1437e-11],\n",
      "        [ 1.2456e-11, -1.2380e-11, -1.2393e-11, -1.2366e-11, -1.2531e-11,\n",
      "          1.2657e-11,  1.2332e-11,  1.2595e-11, -1.2505e-11,  1.2593e-11],\n",
      "        [-1.2247e-12,  1.2160e-12,  1.2177e-12,  1.2142e-12,  1.2331e-12,\n",
      "         -1.2481e-12, -1.2107e-12, -1.2407e-12,  1.2299e-12, -1.2403e-12],\n",
      "        [-1.6030e-12,  1.5915e-12,  1.5938e-12,  1.5892e-12,  1.6139e-12,\n",
      "         -1.6337e-12, -1.5846e-12, -1.6239e-12,  1.6098e-12, -1.6235e-12],\n",
      "        [-1.5819e-12,  1.5707e-12,  1.5729e-12,  1.5684e-12,  1.5927e-12,\n",
      "         -1.6119e-12, -1.5639e-12, -1.6024e-12,  1.5886e-12, -1.6020e-12],\n",
      "        [-2.0123e-12,  1.9981e-12,  2.0009e-12,  1.9952e-12,  2.0260e-12,\n",
      "         -2.0505e-12, -1.9894e-12, -2.0384e-12,  2.0208e-12, -2.0378e-12],\n",
      "        [-1.0099e-12,  1.0026e-12,  1.0041e-12,  1.0011e-12,  1.0169e-12,\n",
      "         -1.0296e-12, -9.9817e-13, -1.0233e-12,  1.0142e-12, -1.0230e-12],\n",
      "        [-2.7163e-16,  2.6840e-16,  2.6669e-16,  2.6575e-16,  2.6862e-16,\n",
      "         -3.1099e-16, -2.6572e-16, -2.7043e-16,  2.6912e-16, -2.6736e-16],\n",
      "        [-2.1069e-12,  2.0921e-12,  2.0950e-12,  2.0891e-12,  2.1210e-12,\n",
      "         -2.1463e-12, -2.0831e-12, -2.1339e-12,  2.1157e-12, -2.1333e-12],\n",
      "        [-5.3080e-13,  5.2684e-13,  5.2764e-13,  5.2603e-13,  5.3453e-13,\n",
      "         -5.4151e-13, -5.2447e-13, -5.3800e-13,  5.3309e-13, -5.3782e-13],\n",
      "        [-1.1973e-12,  1.1885e-12,  1.1903e-12,  1.1867e-12,  1.2056e-12,\n",
      "         -1.2209e-12, -1.1832e-12, -1.2134e-12,  1.2024e-12, -1.2130e-12],\n",
      "        [-6.6248e-13,  6.5742e-13,  6.5846e-13,  6.5637e-13,  6.6724e-13,\n",
      "         -6.7629e-13, -6.5440e-13, -6.7170e-13,  6.6538e-13, -6.7146e-13],\n",
      "        [ 2.7702e-10, -2.7530e-10, -2.7560e-10, -2.7497e-10, -2.7872e-10,\n",
      "          2.8157e-10,  2.7422e-10,  2.8018e-10, -2.7811e-10,  2.8012e-10],\n",
      "        [-3.5715e-10,  3.5494e-10,  3.5531e-10,  3.5451e-10,  3.5933e-10,\n",
      "         -3.6301e-10, -3.5355e-10, -3.6121e-10,  3.5856e-10, -3.6113e-10],\n",
      "        [-3.4445e-10,  3.4232e-10,  3.4268e-10,  3.4190e-10,  3.4656e-10,\n",
      "         -3.5012e-10, -3.4097e-10, -3.4838e-10,  3.4581e-10, -3.4830e-10],\n",
      "        [-3.5395e-10,  3.5176e-10,  3.5213e-10,  3.5134e-10,  3.5611e-10,\n",
      "         -3.5975e-10, -3.5038e-10, -3.5797e-10,  3.5534e-10, -3.5789e-10],\n",
      "        [-1.7851e-10,  1.7741e-10,  1.7759e-10,  1.7719e-10,  1.7960e-10,\n",
      "         -1.8144e-10, -1.7671e-10, -1.8054e-10,  1.7921e-10, -1.8050e-10],\n",
      "        [ 1.1874e-14, -1.1799e-14, -1.1812e-14, -1.1784e-14, -1.1948e-14,\n",
      "          1.2074e-14,  1.1752e-14,  1.2012e-14, -1.1921e-14,  1.2009e-14],\n",
      "        [ 4.1527e-10, -4.1270e-10, -4.1313e-10, -4.1219e-10, -4.1782e-10,\n",
      "          4.2211e-10,  4.1107e-10,  4.2001e-10, -4.1691e-10,  4.1992e-10],\n",
      "        [ 9.2472e-11, -9.1901e-11, -9.1998e-11, -9.1790e-11, -9.3037e-11,\n",
      "          9.3986e-11,  9.1541e-11,  9.3522e-11, -9.2836e-11,  9.3501e-11],\n",
      "        [-2.1470e-10,  2.1337e-10,  2.1360e-10,  2.1312e-10,  2.1601e-10,\n",
      "         -2.1821e-10, -2.1254e-10, -2.1714e-10,  2.1554e-10, -2.1709e-10],\n",
      "        [ 9.3705e-11, -9.3130e-11, -9.3226e-11, -9.3019e-11, -9.4273e-11,\n",
      "          9.5226e-11,  9.2767e-11,  9.4760e-11, -9.4072e-11,  9.4740e-11]])\n",
      "rnn.bias_ih_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.9870e-10, -3.8939e-10, -3.7496e-10, -3.8699e-10, -1.9244e-10,\n",
      "         1.3200e-14,  4.5421e-10,  9.8371e-11, -2.3352e-10,  1.0031e-10])\n",
      "rnn.bias_hh_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.8158e-10, -3.6301e-10, -3.5012e-10, -3.5975e-10, -1.8144e-10,\n",
      "         1.2938e-14,  4.2211e-10,  9.3988e-11, -2.1822e-10,  9.5228e-11])\n",
      "rnn.weight_ih_l0_reverse\n",
      "tensor([[-8.4594e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.0747e-11,\n",
      "         -1.4896e-10,  0.0000e+00,  1.0874e-11,  7.0829e-11,  0.0000e+00],\n",
      "        [-2.9883e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4394e-11,\n",
      "         -5.2622e-11,  0.0000e+00,  3.8413e-12,  2.5020e-11,  0.0000e+00],\n",
      "        [-5.3683e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.5858e-12,\n",
      "         -9.4532e-12,  0.0000e+00,  6.9007e-13,  4.4948e-12,  0.0000e+00],\n",
      "        [-1.1488e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5334e-11,\n",
      "         -2.0229e-10,  0.0000e+00,  1.4767e-11,  9.6184e-11,  0.0000e+00],\n",
      "        [-1.3635e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.5675e-11,\n",
      "         -2.4010e-10,  0.0000e+00,  1.7527e-11,  1.1416e-10,  0.0000e+00],\n",
      "        [-1.1582e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5790e-12,\n",
      "         -2.0396e-11,  0.0000e+00,  1.4889e-12,  9.6977e-12,  0.0000e+00],\n",
      "        [-5.7277e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7589e-12,\n",
      "         -1.0086e-11,  0.0000e+00,  7.3627e-13,  4.7957e-12,  0.0000e+00],\n",
      "        [ 1.6456e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.9265e-12,\n",
      "          2.8978e-11,  0.0000e+00, -2.1153e-12, -1.3778e-11,  0.0000e+00],\n",
      "        [-9.6630e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.6545e-12,\n",
      "         -1.7016e-11,  0.0000e+00,  1.2421e-12,  8.0906e-12,  0.0000e+00],\n",
      "        [-4.0936e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9718e-11,\n",
      "         -7.2086e-11,  0.0000e+00,  5.2621e-12,  3.4275e-11,  0.0000e+00],\n",
      "        [ 8.6252e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1546e-10,\n",
      "          1.5188e-09,  0.0000e+00, -1.1087e-10, -7.2217e-10,  0.0000e+00],\n",
      "        [ 6.7716e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2618e-11,\n",
      "          1.1924e-10,  0.0000e+00, -8.7046e-12, -5.6697e-11,  0.0000e+00],\n",
      "        [ 3.7308e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7971e-10,\n",
      "          6.5697e-10,  0.0000e+00, -4.7958e-11, -3.1237e-10,  0.0000e+00],\n",
      "        [ 8.7162e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1984e-10,\n",
      "          1.5349e-09,  0.0000e+00, -1.1204e-10, -7.2979e-10,  0.0000e+00],\n",
      "        [ 8.1539e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9276e-10,\n",
      "          1.4359e-09,  0.0000e+00, -1.0482e-10, -6.8271e-10,  0.0000e+00],\n",
      "        [ 5.1821e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4961e-10,\n",
      "          9.1254e-10,  0.0000e+00, -6.6614e-11, -4.3389e-10,  0.0000e+00],\n",
      "        [-5.1632e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4870e-11,\n",
      "         -9.0921e-11,  0.0000e+00,  6.6371e-12,  4.3231e-11,  0.0000e+00],\n",
      "        [-1.8575e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.9473e-11,\n",
      "         -3.2710e-10,  0.0000e+00,  2.3878e-11,  1.5553e-10,  0.0000e+00],\n",
      "        [-2.9914e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4409e-11,\n",
      "         -5.2677e-11,  0.0000e+00,  3.8454e-12,  2.5047e-11,  0.0000e+00],\n",
      "        [ 1.2659e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.0975e-11,\n",
      "          2.2291e-10,  0.0000e+00, -1.6272e-11, -1.0599e-10,  0.0000e+00],\n",
      "        [ 8.4579e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0740e-10,\n",
      "          1.4894e-09,  0.0000e+00, -1.0872e-10, -7.0816e-10,  0.0000e+00],\n",
      "        [-1.3305e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.4088e-10,\n",
      "         -2.3429e-09,  0.0000e+00,  1.7103e-10,  1.1140e-09,  0.0000e+00],\n",
      "        [-1.0231e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9283e-11,\n",
      "         -1.8017e-10,  0.0000e+00,  1.3152e-11,  8.5665e-11,  0.0000e+00],\n",
      "        [ 1.1314e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4499e-10,\n",
      "          1.9924e-09,  0.0000e+00, -1.4544e-10, -9.4733e-10,  0.0000e+00],\n",
      "        [-1.4078e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.7809e-10,\n",
      "         -2.4790e-09,  0.0000e+00,  1.8096e-10,  1.1787e-09,  0.0000e+00],\n",
      "        [-1.6939e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.1590e-11,\n",
      "         -2.9828e-10,  0.0000e+00,  2.1774e-11,  1.4182e-10,  0.0000e+00],\n",
      "        [ 1.1274e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4306e-10,\n",
      "          1.9853e-09,  0.0000e+00, -1.4493e-10, -9.4398e-10,  0.0000e+00],\n",
      "        [-7.6532e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.6864e-10,\n",
      "         -1.3477e-09,  0.0000e+00,  9.8378e-11,  6.4078e-10,  0.0000e+00],\n",
      "        [ 1.0906e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2532e-10,\n",
      "          1.9205e-09,  0.0000e+00, -1.4019e-10, -9.1313e-10,  0.0000e+00],\n",
      "        [-1.3831e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.6623e-10,\n",
      "         -2.4356e-09,  0.0000e+00,  1.7780e-10,  1.1581e-09,  0.0000e+00]])\n",
      "rnn.weight_hh_l0_reverse\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "rnn.bias_ih_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.9353e-09,  3.0444e-09,  2.3411e-10, -2.5889e-09,  3.2212e-09,\n",
      "         3.8758e-10, -2.5797e-09,  1.7512e-09, -2.4954e-09,  3.1648e-09])\n",
      "rnn.bias_hh_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.4298e-09,  1.0445e-09,  2.2012e-10, -1.7819e-09,  2.0395e-09,\n",
      "         3.5252e-10, -7.8725e-10,  4.3729e-10, -7.7968e-10,  1.1508e-09])\n",
      "fc_out.weight\n",
      "tensor([[-9.4410e-09,  9.3946e-09,  9.4020e-09,  9.3900e-09,  9.4874e-09,\n",
      "         -9.5659e-09, -9.3705e-09, -9.5267e-09,  9.4705e-09, -9.5252e-09,\n",
      "          5.9604e-09, -2.1056e-10, -8.9559e-09,  5.1718e-09, -4.0032e-09,\n",
      "         -8.5015e-09, -1.5177e-10,  5.1033e-10, -8.8546e-11, -4.0043e-10],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.2325e-08,  1.2264e-08,  1.2274e-08,  1.2258e-08,  1.2385e-08,\n",
      "         -1.2488e-08, -1.2233e-08, -1.2437e-08,  1.2363e-08, -1.2435e-08,\n",
      "          7.7809e-09, -2.7487e-10, -1.1691e-08,  6.7515e-09, -5.2260e-09,\n",
      "         -1.1098e-08, -1.9813e-10,  6.6621e-10, -1.1559e-10, -5.2274e-10]])\n",
      "fc_out.bias\n",
      "tensor([-9.5659e-09,  0.0000e+00, -1.2488e-08])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.gating.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgyhIfAtNrto"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKTw6jmppP8y",
    "outputId": "6161027d-7195-48ea-952c-005a6461f81b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 238,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_criterion(torch.tensor([[0, 1.0000e+10, 0],\n",
    "        [0, 1.0000e+10, 0]]),\n",
    "        torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbNMxqrZp_S-",
    "outputId": "7d2a76f9-b577-4a74-bb77-4af9d2f93c01",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([2])\n",
      ">> Gating Loss\n",
      "tensor(1.0986, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[-2.3803e-05]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0837, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0650, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0255]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0419, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0131, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9796, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0922]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9351, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1283]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8878, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1678]], grad_fn=<SliceBackward>)\n",
      "0.82974424213171\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8212, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2259]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.7432, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2976]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.6585, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.4752, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5942]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.3468, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.7856]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.9747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.2153]], grad_fn=<SliceBackward>)\n",
      "0.9061286374926567\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.4879]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.7517]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.9076]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.1951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.4174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.5413]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.8154]], grad_fn=<SliceBackward>)\n",
      "0.9074996039271355\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0053, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.9616]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0032, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2070]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.4383]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3716]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6711]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6474]], grad_fn=<SliceBackward>)\n",
      "0.9339357018470764\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6148]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6527]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2055]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9430]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1320]], grad_fn=<SliceBackward>)\n",
      "0.8914671167731285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3627]], grad_fn=<SliceBackward>)\n",
      "0.9092058017849922\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2314]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4475]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4427]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4040]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.8662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4927]], grad_fn=<SliceBackward>)\n",
      "0.9086853638291359\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7027]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5852]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4688]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8051]], grad_fn=<SliceBackward>)\n",
      "0.9540341049432755\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5801]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5563]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3491]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5286]], grad_fn=<SliceBackward>)\n",
      "0.9324938133358955\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3866]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6850]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7848]], grad_fn=<SliceBackward>)\n",
      "0.9254426285624504\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7740]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8104e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2422]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2047]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3928]], grad_fn=<SliceBackward>)\n",
      "0.8661048337817192\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6382]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7866e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9646]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4085]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6154]], grad_fn=<SliceBackward>)\n",
      "0.8831527382135391\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9931]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7738]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4841]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6868]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4436]], grad_fn=<SliceBackward>)\n",
      "0.880535438656807\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4818]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0234]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7940]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8329]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4648e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9808]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5697]], grad_fn=<SliceBackward>)\n",
      "0.8675431832671165\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6845]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4355]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6103]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9230]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6864]], grad_fn=<SliceBackward>)\n",
      "0.9496602267026901\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4083]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7828]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7782]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3674]], grad_fn=<SliceBackward>)\n",
      "0.9153327122330666\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4941]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9431]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0011]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9210]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7318]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6719]], grad_fn=<SliceBackward>)\n",
      "0.9284641966223717\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2209]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7534]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6434]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5050]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4922]], grad_fn=<SliceBackward>)\n",
      "0.9609360843896866\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6555e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4290e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7235]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1793]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7433]], grad_fn=<SliceBackward>)\n",
      "0.9041190296411514\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3575e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9867]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6078e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6897]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6544]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7483]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8843]], grad_fn=<SliceBackward>)\n",
      "0.903328500688076\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8428]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7444]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4540]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2109]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7608]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7668]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3372]], grad_fn=<SliceBackward>)\n",
      "0.9555044546723366\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8617]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5408]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7394]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2313]], grad_fn=<SliceBackward>)\n",
      "0.9302210956811905\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2986]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3337e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9876]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5304]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4039e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0407]], grad_fn=<SliceBackward>)\n",
      "0.8923574611544609\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5188]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7251]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1046e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7108]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6158]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5218e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0967]], grad_fn=<SliceBackward>)\n",
      "0.9684783220291138\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8901e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1406]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4026e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9114]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8457]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1415]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2771]], grad_fn=<SliceBackward>)\n",
      "0.9525922313332558\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5614]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6529e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9403e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0997]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3187]], grad_fn=<SliceBackward>)\n",
      "0.8704231753945351\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5723]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0595e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0026]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7264]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5248]], grad_fn=<SliceBackward>)\n",
      "0.8824385702610016\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5601e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9758]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4923]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7087]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1972]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8807e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0128]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0074]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2956]], grad_fn=<SliceBackward>)\n",
      "0.9546280577778816\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3962]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3098e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0331e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1300]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7753]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6634]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9485]], grad_fn=<SliceBackward>)\n",
      "0.9295287430286407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3370]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9178]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1839]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2145e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9944]], grad_fn=<SliceBackward>)\n",
      "0.8718064799904823\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6463]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8458]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5933e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0913]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5339]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8529]], grad_fn=<SliceBackward>)\n",
      "0.8350213691592216\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3111]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8939e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1571]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1373]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0569e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1284]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6946]], grad_fn=<SliceBackward>)\n",
      "0.941309466958046\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6921]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7919]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3554]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1072e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8788]], grad_fn=<SliceBackward>)\n",
      "0.9300222471356392\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.6980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3648]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2035]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2370e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3549e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1079]], grad_fn=<SliceBackward>)\n",
      "0.9206623733043671\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2090]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2973]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8679]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6243]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7694]], grad_fn=<SliceBackward>)\n",
      "0.9505183324217796\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3803]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1271]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6526]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2224]], grad_fn=<SliceBackward>)\n",
      "0.8610163629055023\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8198e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0000]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8999]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8926e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5534]], grad_fn=<SliceBackward>)\n",
      "0.9559815302491188\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5469e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9510]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7809]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7747e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9638]], grad_fn=<SliceBackward>)\n",
      "0.982656717300415\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7947e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1464]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1478]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0726]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1562]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1804]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1315]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1703]], grad_fn=<SliceBackward>)\n",
      "0.9076273813843727\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7778]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7198]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1668e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4522]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0199e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0213]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9854e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0903]], grad_fn=<SliceBackward>)\n",
      "0.9130949825048447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6566]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7200]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9442]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.1178e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0568]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3338]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1155]], grad_fn=<SliceBackward>)\n",
      "0.9232373982667923\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2943]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5479]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3715]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9175]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4886e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3081]], grad_fn=<SliceBackward>)\n",
      "0.9844113141298294\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3553]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7245e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2013e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0521]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1420]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0777]], grad_fn=<SliceBackward>)\n",
      "0.906353659927845\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2683]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1621]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5701]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "0.9535688981413841\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3641]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0211]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4322]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4635e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0359]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2119e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1173]], grad_fn=<SliceBackward>)\n",
      "0.97125643491745\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9165e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0093]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5485]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6330]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1748]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5840e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9730]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1825]], grad_fn=<SliceBackward>)\n",
      "1.0099817141890526\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5900]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1847]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2959]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3417e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2336]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1123]], grad_fn=<SliceBackward>)\n",
      "0.9761429280042648\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7194]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1948e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5258]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6055]], grad_fn=<SliceBackward>)\n",
      "0.9192996472120285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0276]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3697]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2895]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2797]], grad_fn=<SliceBackward>)\n",
      "0.9878424927592278\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6423e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0256]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1844]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.2106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1912]], grad_fn=<SliceBackward>)\n",
      "0.9360367059707642\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2252]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3832]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3678]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2162]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3179]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9967]], grad_fn=<SliceBackward>)\n",
      "0.9009067863225937\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9006]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0940e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4516e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0364]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3992]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2383e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0727]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3520]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6362]], grad_fn=<SliceBackward>)\n",
      "0.9399682730436325\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5874]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3456e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9861]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4212]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3954]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0737]], grad_fn=<SliceBackward>)\n",
      "0.9046507701277733\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2467]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4278]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9299]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0054e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4131]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0379]], grad_fn=<SliceBackward>)\n",
      "0.8922909200191498\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3813e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9843]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2239]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4288]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2247]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9376]], grad_fn=<SliceBackward>)\n",
      "0.9415184482932091\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2638]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4342]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6291e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0882]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1603]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4346]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5226]], grad_fn=<SliceBackward>)\n",
      "0.9168690741062164\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7180]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0374]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9205]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2857]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8505]], grad_fn=<SliceBackward>)\n",
      "0.9186825603246689\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6391]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3396]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1430]], grad_fn=<SliceBackward>)\n",
      "1.0298770293593407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3640]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8706]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6598]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4381]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2565]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5695e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3518]], grad_fn=<SliceBackward>)\n",
      "0.9632200226187706\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3154]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2023]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4719]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3268]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8894]], grad_fn=<SliceBackward>)\n",
      "0.8725502789020538\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1332]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4833]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6266]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5859]], grad_fn=<SliceBackward>)\n",
      "0.9114488065242767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2089]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5034]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2518]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3826]], grad_fn=<SliceBackward>)\n",
      "0.997397854924202\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8003]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3097]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4366]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4170]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1381]], grad_fn=<SliceBackward>)\n",
      "0.8821030855178833\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3788e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1053]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9641e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0062]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5363e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9755]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4294]], grad_fn=<SliceBackward>)\n",
      "0.9253259226679802\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6894]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8625]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5305]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6994]], grad_fn=<SliceBackward>)\n",
      "0.826819896697998\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3774]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7934e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9637]], grad_fn=<SliceBackward>)\n",
      "0.9356672465801239\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5946e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0275]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1360]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6202]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2711]], grad_fn=<SliceBackward>)\n",
      "0.9173995330929756\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5826]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3270]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7982]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7323]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7734e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4181]], grad_fn=<SliceBackward>)\n",
      "0.9404812529683113\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2763]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6613]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5192e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2514]], grad_fn=<SliceBackward>)\n",
      "0.908241368830204\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5550e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5391e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1784]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6141]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2168]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6144]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4947]], grad_fn=<SliceBackward>)\n",
      "0.960708424448967\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9057]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9271e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0684]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3351]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6400]], grad_fn=<SliceBackward>)\n",
      "0.9716712683439255\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9403]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4854]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0977]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8236]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9031]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.3085e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6196]], grad_fn=<SliceBackward>)\n",
      "0.9475336000323296\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6060]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6597]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2132]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6303]], grad_fn=<SliceBackward>)\n",
      "0.9496114104986191\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3421]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2233]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9064]], grad_fn=<SliceBackward>)\n",
      "0.9413476660847664\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6966]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4809e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9695]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8241]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8081]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8603]], grad_fn=<SliceBackward>)\n",
      "0.8706775084137917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2751]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0582e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0599]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7073]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3231]], grad_fn=<SliceBackward>)\n",
      "0.9316414818167686\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0994e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5410]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6669]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7125]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2988]], grad_fn=<SliceBackward>)\n",
      "0.9206378161907196\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7277]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2903]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7470e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4822e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3551]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3291]], grad_fn=<SliceBackward>)\n",
      "0.8711266815662384\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0027e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7609]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5503]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3892]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3649]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6760]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3847]], grad_fn=<SliceBackward>)\n",
      "0.9439148530364037\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9087e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5720]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5509]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8880]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1769]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8602]], grad_fn=<SliceBackward>)\n",
      "0.9201693758368492\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1868e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6908]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5094]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8538]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3829]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5983]], grad_fn=<SliceBackward>)\n",
      "0.9166541695594788\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7206]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1399]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5801e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1612]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2305e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5201]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5670]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6542]], grad_fn=<SliceBackward>)\n",
      "0.9151469171047211\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0888e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4002]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9099]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5418e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3490]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5680]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7409]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4017]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1219e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "0.8847057297825813\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3875]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1602]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8996]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4501]], grad_fn=<SliceBackward>)\n",
      "0.9680259078741074\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5604]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9340]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7770]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0650e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4043]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7885]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8000e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8142]], grad_fn=<SliceBackward>)\n",
      "0.9062342792749405\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2459]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2098]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8056]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4743]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4388]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2013]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8037]], grad_fn=<SliceBackward>)\n",
      "0.8556824177503586\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6071]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7881e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2281]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6260]], grad_fn=<SliceBackward>)\n",
      "0.8968541398644447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4615]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9842]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4981]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8386]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6171]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6759]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6369]], grad_fn=<SliceBackward>)\n",
      "0.9463448226451874\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4136]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8466]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7012]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2749]], grad_fn=<SliceBackward>)\n",
      "0.9251849204301834\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6263]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8456]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9686]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6559e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4523]], grad_fn=<SliceBackward>)\n",
      "0.9190128594636917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2278e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0058]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4835e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2537]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1821]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4788]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9957]], grad_fn=<SliceBackward>)\n",
      "0.9369889944791794\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8579]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6681]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6682]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8702]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6698]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4175e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1353]], grad_fn=<SliceBackward>)\n",
      "0.8986769914627075\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2790]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5072]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5080]], grad_fn=<SliceBackward>)\n",
      "0.8923800513148308\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2570]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7044]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8404]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1791e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1504]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1086e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5827e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2201]], grad_fn=<SliceBackward>)\n",
      "0.8838892206549644\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4253]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3228]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8626]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9010]], grad_fn=<SliceBackward>)\n",
      "0.8662857860326767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3378]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5552]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5242]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8772]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5620]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0331]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6600]], grad_fn=<SliceBackward>)\n",
      "0.8832837343215942\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9028]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8647]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5368]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7733]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0967e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7280]], grad_fn=<SliceBackward>)\n",
      "0.9058996215462685\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1378e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2865]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0713]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9529]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7744]], grad_fn=<SliceBackward>)\n",
      "0.9235536307096481\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5516]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9810]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7508]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4873e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5926]], grad_fn=<SliceBackward>)\n",
      "0.8536463901400566\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3014]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9365]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "0.9422603696584702\n"
     ]
    }
   ],
   "source": [
    "for x in range(100):\n",
    "    print( train_dynamoe_gating(model, train_dls[1], gating_criterion,\n",
    "                                expert_criterion_unreduced,\n",
    "                                CLIP, verbose=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fcaN82bJ-f5",
    "outputId": "2c94f843-e801-4dfb-d79c-89ab704be8fd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630012683570385\n",
      "0.47744666039943695\n",
      "0.44646283984184265\n",
      "0.4113902524113655\n",
      "0.4127246364951134\n",
      "0.37290962785482407\n",
      "0.43906231224536896\n",
      "0.385328508913517\n",
      "0.37053677439689636\n",
      "0.3609902262687683\n",
      "0.3726344630122185\n",
      "0.3744181916117668\n",
      "0.36561162024736404\n",
      "0.3524796664714813\n",
      "0.39585812389850616\n",
      "0.32904189825057983\n",
      "0.3342840299010277\n",
      "0.3753737062215805\n",
      "0.40356797724962234\n",
      "0.3463684171438217\n",
      "0.35821671038866043\n",
      "0.36753934621810913\n",
      "0.31112828850746155\n",
      "0.3395487293601036\n",
      "0.32247451692819595\n",
      "0.31689079105854034\n",
      "0.3214147612452507\n",
      "0.3431061953306198\n",
      "0.32723822444677353\n",
      "0.3066282793879509\n",
      "0.2927127256989479\n",
      "0.3012479245662689\n",
      "0.2619609013199806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d7e8f4aa73e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_dynamoe_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgating_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_criterion_unreduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-00addaa6fd57>\u001b[0m in \u001b[0;36mtrain_dynamoe_both\u001b[0;34m(model, iterator, gating_criterion, expert_criterion, clip)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Train newly initialized model on new train examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mreduced_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mreduced_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(400):\n",
    "    print( train_dynamoe_both(model, train_dls[1], gating_criterion, expert_criterion_unreduced, CLIP) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAASoarxMs3k"
   },
   "source": [
    "## Transfer F: DynaMoE with weighting instead of choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaMoE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "ztDsiI2WM98Y"
   },
   "outputs": [],
   "source": [
    "class DynaMoE2(nn.Module):\n",
    "    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(DynaMoE2, self).__init__()\n",
    "\n",
    "        assert len(experts) == len(expert_optimizers)\n",
    "        \n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        self.n_active_experts = 1\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio = 0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gatings[:,:self.n_active_experts]\n",
    "\n",
    "        expert_outputs = torch.empty((self.n_active_experts, seq_len, batch_size, vocab_size))\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_output = self.experts[e_id](seqs, seqs_len, seqs, teacher_forcing_ratio)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                #expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b]\n",
    "            \n",
    "            # print(\"expert_out\")\n",
    "            # print(expert_output)\n",
    "            # print(\"gating_masked\")\n",
    "            # print(gating_masked)\n",
    "\n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "        return weighted_outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_dynamoe2_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dynamoe2_gating(model, iterator, criterion, clip, verbose=False):\n",
    "    assert isinstance(model, DynaMoE2)\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        vocab_size = model.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        expert_outputs = torch.empty((model.n_active_experts, seq_len,\n",
    "                                      batch_size, vocab_size))\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "\n",
    "            expert_output = model.experts[e_id](seqs, seqs_len, seqs)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "            \n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "        # Gating Loss just is total loss\n",
    "        gating_loss = compute_loss(weighted_outputs, seqs, expert_criterion,\n",
    "                            cutFirstInSequence=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        \n",
    "        epoch_loss += gating_loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_dynamoe2_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dynamoe2_both(model, iterator, criterion, clip):\n",
    "    assert isinstance(model, DynaMoE2)\n",
    "    \n",
    "    model.eval()\n",
    "    model.experts[model.n_active_experts - 1].train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        # model.gating_optimizer.zero_grad()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].zero_grad()\n",
    "        \n",
    "        outputs = model(seqs, seqs_len, seqs)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # model.gating_optimizer.step()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_dynamoe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dynamoe2(model, task_id, epochs, step_size_evaluation, clip,\n",
    "                case = \"train_gating_initialized_expert\" ):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    case : string\n",
    "        \"train_gating_uninitialized_expert\" | \"train_gating_train_expert\" | \n",
    "        \"train_gating_initialized_expert\"\n",
    "    \"\"\"\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    loss_tracker = torch.zeros((epochs,))\n",
    "\n",
    "    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        if case == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe2_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id], allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    # Switch to train the expert and gating\n",
    "\n",
    "                    case = \"train_gating_train_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if case == \"train_gating_train_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe2_both(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "        if case == \"train_gating_uninitialized_expert\":\n",
    "            assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe2_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    case = \"train_gating_initialized_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate(model, train_dls[other_id], expert_criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate(model, test_dls[other_id], expert_criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate(model, test_ugr_dls[other_id], expert_criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment DynaMoE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "iu9jVht1wbg7"
   },
   "outputs": [],
   "source": [
    "SAVE = N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "N_EPOCHS = SAVE + 200\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynaMoE2(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-F0\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.796 | Train PPL:   2.217\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.859 | Train PPL:   2.362\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.846 | Train PPL:   2.331\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.764 | Train PPL:   2.146\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.838 | Train PPL:   2.311\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.900 | Train PPL:   2.460\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.906 | Train PPL:   2.474\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.837 | Train PPL:   2.310\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.897 | Train PPL:   2.453\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.843 | Train PPL:   2.323\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.830 | Train PPL:   2.294\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.821 | Train PPL:   2.273\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.889 | Train PPL:   2.433\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.832 | Train PPL:   2.298\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.798 | Train PPL:   2.222\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.770 | Train PPL:   2.160\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.802 | Train PPL:   2.229\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.788 | Train PPL:   2.200\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.820 | Train PPL:   2.271\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.888 | Train PPL:   2.430\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.826 | Train PPL:   2.284\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.831 | Train PPL:   2.295\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.823 | Train PPL:   2.277\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.905 | Train PPL:   2.471\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.806 | Train PPL:   2.238\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.830 | Train PPL:   2.294\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.888 | Train PPL:   2.430\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.796 | Train PPL:   2.217\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.807 | Train PPL:   2.241\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.952 | Train PPL:   2.590\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.857 | Train PPL:   2.357\n",
      "\t Val. Loss: 0.831 |  Val. PPL:   2.295\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.671 | Train PPL:   1.957\n",
      "\t Val. Loss: 0.575 |  Val. PPL:   1.777\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.687\n",
      "\t Val. Loss: 0.502 |  Val. PPL:   1.651\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.460 | Train PPL:   1.585\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.522\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.514\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.484\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.373 |  Val. PPL:   1.452\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.441\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.418\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.411\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.338\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.229\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.076\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.067 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.068 | Train PPL:   1.070\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.072 |  Val. PPL:   1.075\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.073\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.070 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.076 | Train PPL:   1.079\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.074 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.078\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.076\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.077\n",
      "\t Val. Loss: 0.075 |  Val. PPL:   1.077\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.073 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.074 |  Val. PPL:   1.077\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.072 | Train PPL:   1.075\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.075\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.075 | Train PPL:   1.078\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.071\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.069 | Train PPL:   1.072\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.071 | Train PPL:   1.074\n",
      "\t Val. Loss: 0.073 |  Val. PPL:   1.076\n",
      "tr-AE-30-10-0.01-F1\n",
      "The model has 7341 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.024 | Train PPL:   2.785\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.602 | Train PPL:   1.825\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.462 | Train PPL:   1.587\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.567\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.610\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.639\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.590\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.590\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.460 | Train PPL:   1.584\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.594\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.620\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.575\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 1.115 | Train PPL:   3.050\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 1.030 | Train PPL:   2.801\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 1.093 | Train PPL:   2.985\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.928 | Train PPL:   2.528\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 1.012 | Train PPL:   2.752\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.665 | Train PPL:   1.944\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.900 | Train PPL:   2.459\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.920 | Train PPL:   2.510\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.911 | Train PPL:   2.488\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.913 | Train PPL:   2.492\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 1.142 | Train PPL:   3.133\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.917 | Train PPL:   2.501\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.914 | Train PPL:   2.494\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.892 | Train PPL:   2.439\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.914 | Train PPL:   2.495\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.999\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.904 | Train PPL:   2.468\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.917 | Train PPL:   2.502\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.959 | Train PPL:   2.610\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.932 | Train PPL:   2.541\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 1.224 | Train PPL:   3.402\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.996 | Train PPL:   2.709\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 1.192 | Train PPL:   3.292\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.910 | Train PPL:   2.485\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.904 | Train PPL:   2.470\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.901 | Train PPL:   2.462\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 1.006 | Train PPL:   2.734\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.961 | Train PPL:   2.614\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.998\n",
      "\t Val. Loss: 0.504 |  Val. PPL:   1.655\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.458 | Train PPL:   1.581\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.603\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.479\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.483\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.314 |  Val. PPL:   1.369\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.348\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.334\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.344\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.332\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.320\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.317\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.301\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.142\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.093\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "tr-AE-30-10-0.01-F2\n",
      "The model has 13219 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 1.418 | Train PPL:   4.130\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 1.214 | Train PPL:   3.367\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 1.135 | Train PPL:   3.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 201 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 202 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 203 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 204 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 205 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 206 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 207 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 208 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 209 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 210 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 211 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 212 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 213 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 214 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 215 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 216 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 217 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 218 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 219 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 220 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 221 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 222 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 223 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 224 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 225 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 226 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 227 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 228 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 229 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 230 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 231 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 232 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 233 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 234 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 235 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 236 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 237 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 238 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 239 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 240 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 241 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 242 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 243 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 244 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 245 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 246 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 247 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 248 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 249 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 250 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 251 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 252 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 253 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 254 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 255 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 256 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 257 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 258 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 259 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 260 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 261 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 262 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 263 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 264 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 265 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 266 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 267 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 268 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 269 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 270 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 271 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 272 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 273 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 274 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 275 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 276 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 277 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 278 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 279 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 280 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 281 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 282 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 283 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 284 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 285 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 286 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 287 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 288 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 289 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 290 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 291 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 292 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 293 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 294 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 295 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 296 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 297 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 298 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 299 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 300 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 301 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 302 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 303 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 304 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 305 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 306 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 307 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 308 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 309 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 310 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 311 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 312 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 313 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 314 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 315 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 316 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 317 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 318 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 319 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 320 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 321 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 322 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 323 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 324 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 325 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 326 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 327 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 328 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 329 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 330 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 331 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 332 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 333 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 334 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 335 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 336 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 337 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 338 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 339 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 340 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 341 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 342 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 343 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 344 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 345 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 346 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 347 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 348 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 349 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 350 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 351 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 352 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 353 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 354 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 355 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 356 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 357 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 358 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 359 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 360 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 361 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 362 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 363 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 364 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 365 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 366 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 367 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 368 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 369 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 370 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 371 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 372 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 373 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 374 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 375 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 376 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 377 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 378 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 379 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 380 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 381 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 382 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 383 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 384 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 385 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 386 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 387 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 388 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 389 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 390 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 391 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 392 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 393 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 394 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 395 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 396 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 397 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 398 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 399 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 400 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n"
     ]
    }
   ],
   "source": [
    "models_F = []\n",
    "hist_losses_F = []\n",
    "hist_hitsss_F = []\n",
    "\n",
    "expert, expert_optimizer = init_expert()\n",
    "gating, gating_optimizer = init_gating()\n",
    "model = DynaMoE2(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "\n",
    "# model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "\n",
    "for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"F{n_task}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    print(title)\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "    \n",
    "    if n_task == 0:\n",
    "        case = \"train_gating_initialized_expert\"\n",
    "    else:\n",
    "        case = \"train_gating_uninitialized_expert\"\n",
    "    hist_loss_temp, hist_hits_temp = fit_dynamoe2(model, n_task, N_EPOCHS,\n",
    "                                                 STEP_SIZE_EVALUATION, CLIP,\n",
    "                                                 case)\n",
    "    hist_losses_F.append(hist_loss_temp)\n",
    "    hist_hitsss_F.append(hist_hits_temp)\n",
    "    models_F.append(copy.deepcopy(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist_losses_F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-c83919370fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist_loss_F\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_losses_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhist_hits_F\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_hitsss_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplotResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_loss_F\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_hits_F\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist_losses_F' is not defined"
     ]
    }
   ],
   "source": [
    "hist_loss_F = torch.cat(hist_losses_F, dim=2)\n",
    "hist_hits_F = torch.cat(hist_hitsss_F, dim=2)\n",
    "\n",
    "plotResults(hist_loss_F, hist_hits_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show_expert2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_expert2(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_masked[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2000e+01,  1.7990e-01, -1.0471e-02],\n",
      "        [ 6.7971e+00,  6.8288e+00, -2.9813e-02]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[2]:\n",
    "    print(model.gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.7971, 6.8288]) - tensor([1, 6, 7, 5, 3, 4, 5, 3, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 7, 5, 4, 2, 0, 0, 0])\n",
      "tensor([12.0075,  0.1695]) - tensor([1, 6, 5, 7, 4, 7, 3, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 5, 7, 4, 7, 2, 0])\n",
      "tensor([6.7159, 6.7874]) - tensor([1, 6, 5, 3, 4, 5, 3, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 5, 4, 2, 0, 0, 0])\n",
      "tensor([10.8500,  0.2858]) - tensor([1, 6, 7, 3, 4, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 7, 4, 2, 0])\n",
      "tensor([6.7845, 6.8455]) - tensor([1, 6, 7, 5, 4, 5, 3, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 5, 4, 5, 3, 2, 0])\n",
      "tensor([11.9999,  0.1799]) - tensor([1, 6, 5, 7, 3, 4, 7, 3, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 7, 3, 4, 7, 3, 2, 0])\n",
      "tensor([11.0634,  0.2913]) - tensor([1, 6, 7, 4, 7, 3, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 5, 3, 4, 2, 0])\n",
      "tensor([6.7663, 6.9836]) - tensor([1, 6, 7, 5, 4, 5, 2])\n",
      "tensor([0.6660, 0.3219]) - tensor([1, 6, 5, 7, 4, 2, 0])\n"
     ]
    }
   ],
   "source": [
    "show_expert2(model, train_dls[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "TCvCl-WOjvYN"
   },
   "outputs": [],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "\n",
    "gating, gating_optimizer = init_gating()\n",
    "\n",
    "model2 = DynaMoE2(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "\n",
    "model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22869906201958656\n",
      "0.19848032295703888\n",
      "0.17827840894460678\n",
      "0.1478596329689026\n",
      "0.1328919641673565\n",
      "0.11753029003739357\n",
      "0.13586381450295448\n",
      "0.12771780043840408\n",
      "0.12642693519592285\n",
      "0.10890525579452515\n",
      "0.11053463444113731\n",
      "0.11512905731797218\n",
      "0.10789792984724045\n",
      "0.1396242491900921\n",
      "0.13007165119051933\n",
      "0.10671130195260048\n",
      "0.12281512469053268\n",
      "0.11282141506671906\n",
      "0.11576562002301216\n",
      "0.11426208913326263\n",
      "0.09666670113801956\n",
      "0.10877194255590439\n",
      "0.11849036067724228\n",
      "0.10577232390642166\n",
      "0.11337213218212128\n",
      "0.11631719395518303\n",
      "0.10239308699965477\n",
      "0.11547045782208443\n",
      "0.09947025775909424\n",
      "0.11925547197461128\n",
      "0.1172763966023922\n",
      "0.09785504266619682\n",
      "0.11256325617432594\n",
      "0.11795744299888611\n",
      "0.12875186279416084\n",
      "0.11906967684626579\n",
      "0.11535120382905006\n",
      "0.12183238565921783\n",
      "0.11042068153619766\n",
      "0.1258276328444481\n",
      "0.10611820220947266\n",
      "0.11356543004512787\n",
      "0.12004068866372108\n",
      "0.11736428737640381\n",
      "0.10979463905096054\n",
      "0.1302037090063095\n",
      "0.10663412138819695\n",
      "0.12038356065750122\n",
      "0.10787971690297127\n",
      "0.10383082553744316\n",
      "0.1187928281724453\n",
      "0.10345876961946487\n",
      "0.11615287512540817\n",
      "0.10248106718063354\n",
      "0.12555744498968124\n",
      "0.10940905660390854\n",
      "0.11270658299326897\n",
      "0.10263677313923836\n",
      "0.11481521651148796\n",
      "0.10700425878167152\n",
      "0.11103396862745285\n",
      "0.11774175614118576\n",
      "0.11040911078453064\n",
      "0.1000622883439064\n",
      "0.10484669730067253\n",
      "0.11479347571730614\n",
      "0.11351212114095688\n",
      "0.10314466059207916\n",
      "0.10699784010648727\n",
      "0.10784124210476875\n",
      "0.10584190860390663\n",
      "0.10274897143244743\n",
      "0.11306047439575195\n",
      "0.11217445507645607\n",
      "0.11249392107129097\n",
      "0.10664930939674377\n",
      "0.1226293072104454\n",
      "0.10691475123167038\n",
      "0.1127341240644455\n",
      "0.11603338271379471\n",
      "0.10185177251696587\n",
      "0.1077551431953907\n",
      "0.1284193992614746\n",
      "0.11325674876570702\n",
      "0.10813923925161362\n",
      "0.11353151500225067\n",
      "0.12413913756608963\n",
      "0.10799911990761757\n",
      "0.11498769372701645\n",
      "0.09973367676138878\n",
      "0.1055334061384201\n",
      "0.11444810032844543\n",
      "0.10445881634950638\n",
      "0.10906705632805824\n",
      "0.11393177136778831\n",
      "0.10584009811282158\n",
      "0.09891145303845406\n",
      "0.11946999654173851\n",
      "0.10650274157524109\n",
      "0.11207767203450203\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    print(train_dynamoe2_gating(model2, train_dls[2], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4835, 6.2265, 0.7313],\n",
      "        [0.6559, 0.2670, 0.2011]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[1]:\n",
    "    print(model2.gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0],\n",
      "        [6, 6],\n",
      "        [7, 5],\n",
      "        [5, 3],\n",
      "        [4, 4],\n",
      "        [5, 2]])\n",
      "tensor([[1, 1],\n",
      "        [6, 6],\n",
      "        [7, 5],\n",
      "        [5, 3],\n",
      "        [4, 4],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[1]:\n",
    "    print(model2.experts[1](seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print(seqs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16881714761257172\n",
      "0.15141833573579788\n",
      "0.15692844986915588\n",
      "0.15707916021347046\n",
      "0.15791257470846176\n",
      "0.18426920473575592\n",
      "0.15594960004091263\n",
      "0.2099481225013733\n",
      "0.19191593676805496\n",
      "0.16780265420675278\n",
      "0.16559413820505142\n",
      "0.14507802575826645\n",
      "0.14517706632614136\n",
      "0.15876172482967377\n",
      "0.18021392822265625\n",
      "0.14854232966899872\n",
      "0.17011234909296036\n",
      "0.19770054519176483\n",
      "0.15062608569860458\n",
      "0.16328270733356476\n",
      "0.1630496382713318\n",
      "0.15208538621664047\n",
      "0.1585157811641693\n",
      "0.1476331725716591\n",
      "0.19418861716985703\n",
      "0.14303864538669586\n",
      "0.15178367495536804\n",
      "0.13727358728647232\n",
      "0.14693152904510498\n",
      "0.16229414194822311\n",
      "0.16068880259990692\n",
      "0.13847041130065918\n",
      "0.17334415018558502\n",
      "0.15148300677537918\n",
      "0.16502273082733154\n",
      "0.14539223909378052\n",
      "0.12727899849414825\n",
      "0.1345638856291771\n",
      "0.14078588038682938\n",
      "0.16145791113376617\n",
      "0.1524231731891632\n",
      "0.1419893577694893\n",
      "0.12537699937820435\n",
      "0.14602867513895035\n",
      "0.14317089319229126\n",
      "0.16259494423866272\n",
      "0.17060024291276932\n",
      "0.1443256139755249\n",
      "0.13479702174663544\n",
      "0.15025698393583298\n",
      "0.14225276559591293\n",
      "0.16034740954637527\n",
      "0.12438227236270905\n",
      "0.12518545240163803\n",
      "0.14440368115901947\n",
      "0.13735894113779068\n",
      "0.19062084704637527\n",
      "0.13652129471302032\n",
      "0.17650730162858963\n",
      "0.14275554567575455\n",
      "0.16762355715036392\n",
      "0.14181502908468246\n",
      "0.1351715624332428\n",
      "0.12215232849121094\n",
      "0.1263386607170105\n",
      "0.11908707767724991\n",
      "0.12115544825792313\n",
      "0.1282174363732338\n",
      "0.1253407672047615\n",
      "0.12408994138240814\n",
      "0.14472167938947678\n",
      "0.128596231341362\n",
      "0.12068396806716919\n",
      "0.12849827855825424\n",
      "0.1212557777762413\n",
      "0.1384652704000473\n",
      "0.172116719186306\n",
      "0.11886386573314667\n",
      "0.11503040790557861\n",
      "0.1102377399802208\n",
      "0.11419399827718735\n",
      "0.13094515353441238\n",
      "0.11548421531915665\n",
      "0.12197885662317276\n",
      "0.11363319307565689\n",
      "0.11196141690015793\n",
      "0.11577392369508743\n",
      "0.11174105852842331\n",
      "0.1156032532453537\n",
      "0.1134130209684372\n",
      "0.12646576017141342\n",
      "0.12065742164850235\n",
      "0.11694453656673431\n",
      "0.13224681466817856\n",
      "0.10931586474180222\n",
      "0.13207688927650452\n",
      "0.123417429625988\n",
      "0.10937980562448502\n",
      "0.12781494110822678\n",
      "0.11629518866539001\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    print(train_dynamoe2_both(model2, train_dls[1], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "pJE4efjNcUCM"
   },
   "outputs": [],
   "source": [
    "model2.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVuFGE3vj6K-",
    "outputId": "80edc0df-4dcb-4f7c-f48f-6e818ad0a2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [4],\n",
      "        [6],\n",
      "        [7],\n",
      "        [2]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.6035e-04, -3.0651e-03, -3.0967e+00,  5.0364e+00, -2.0193e+00,\n",
      "           1.1399e+01, -1.7679e+00,  4.7675e+00]],\n",
      "\n",
      "        [[-1.7267e-02, -1.7725e-02, -2.5892e+00,  8.6882e+00, -3.9250e-02,\n",
      "           2.2423e+00,  2.9604e+00,  1.6000e+00]],\n",
      "\n",
      "        [[-6.3461e-03, -2.2191e-02, -3.4556e+00,  2.0296e-01,  1.4208e-01,\n",
      "          -6.5787e-02,  1.4856e+01,  2.9764e+00]],\n",
      "\n",
      "        [[-1.5827e-02, -6.2566e-03, -2.2252e+00,  1.3526e+00,  6.0539e+00,\n",
      "          -3.2105e-01,  1.5760e+00,  7.5781e+00]],\n",
      "\n",
      "        [[-1.3362e-02,  1.7968e-02,  3.8569e+00, -1.9823e+00,  8.1386e+00,\n",
      "           1.5963e+00, -1.1647e-02,  1.3524e+00]],\n",
      "\n",
      "        [[-5.1290e-03, -1.0336e-02,  3.7318e+00,  1.4661e+00, -1.7547e+00,\n",
      "          -1.2773e+00,  1.0091e+01,  2.9304e+00]],\n",
      "\n",
      "        [[-6.9810e-03,  1.6720e-02,  7.0399e+00, -3.2432e-01, -3.1907e+00,\n",
      "           1.1463e-01,  1.1009e+00,  1.0808e+01]],\n",
      "\n",
      "        [[-1.7517e-02,  9.3911e-03,  1.1679e+01, -1.5853e+00, -6.2468e-01,\n",
      "           1.4161e+00, -1.5484e+00,  6.6679e+00]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[0]:\n",
    "    print(seqs)\n",
    "    print(model2(seqs, seqs_len, seqs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hz95pA8kDHf"
   },
   "source": [
    "## Transfer G: DynaMoE with context vector\n",
    "\n",
    "1. Give DynaMoE additional artificial context onehot vector encoding\n",
    "2. Use Continual Learning technique to make gating network remember task\n",
    "3. In this case, replay (maybe later EWC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GatingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len, context):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        return outputs\n",
    "        return F.softmax(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DynaMoEContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaMoEContext(nn.Module):\n",
    "    def __init__(self, gating, gating_optimizer, experts, expert_optimizers):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(DynaMoE, self).__init__()\n",
    "\n",
    "        assert len(experts) == len(expert_optimizers)\n",
    "        \n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        self.n_active_experts = 1\n",
    "        # set mask\n",
    "        self.recompute_mask()\n",
    "    \n",
    "    def recompute_mask(self):\n",
    "        gating_mask = torch.zeros(self.gating.n_max_experts).to(device)\n",
    "\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            gating_mask[e_id] = 1\n",
    "\n",
    "        self.gating_mask = gating_mask\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        masked_gatings = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        # @TODO: Probabilistic vs argmax?\n",
    "        network_ids = torch.argmax(masked_gatings, dim=1)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,\n",
    "                                                     teacher_forcing_ratio))\n",
    "\n",
    "        outputs = torch.empty((seq_len, batch_size, vocab_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            network_id = network_ids[b]\n",
    "            outputs[:,b] = expert_outputs[network_id][:,b]\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1\n",
    "        # Recompute mask\n",
    "        self.recompute_mask()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "agl_autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "fa0c181e15994ab92b77e0a9eeb7815659805982e17e67ed50eb685ba3cdee26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
