{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnant-urt22Y"
   },
   "source": [
    "# AGL Autoencoder\n",
    "\n",
    "Based on https://github.com/bentrevett/pytorch-seq2seq/blob/master/4%20-%20Packed%20Padded%20Sequences%2C%20Masking%2C%20Inference%20and%20BLEU.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo collection\n",
    "\n",
    "DynaMoE:\n",
    "* Exploring vs exploiting, decreasing over time\n",
    "* New expert only learns from examples it was actually chosen for\n",
    "* consolidation of dynamoe and training gating code into one codeblock\n",
    "* visualize in graph switches of DynaMoE status\n",
    "\n",
    "General:\n",
    "- EWC\n",
    "- Replay\n",
    "- Run on 3 tasks\n",
    "- Test interleaved training: A -> C -> A, only test on A - what is performance of A?\n",
    "- Let's use a CNN and pictures for the input!\n",
    "\n",
    "Interleaved training:\n",
    "- set up so network can switch within single dataloader\n",
    "- optimize for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xKTgS0wMOZbb"
   },
   "outputs": [],
   "source": [
    "BASIS = \"../\" #\"drive/MyDrive/Colab_data/\"\n",
    "MODELFOLDER = BASIS + \"/models/\"\n",
    "PLOTSFOLDER = BASIS + \"/plots/\"\n",
    "MODELAUTOSAVE = MODELFOLDER + \"/autosave/\"\n",
    "PLOTSAUTOSAVE = PLOTSFOLDER + \"/autosave/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8BeTXi1OaMs",
    "outputId": "ca356d96-806e-46bb-9036-0f3c2d8c8139"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0NKkMOK5QRVp"
   },
   "outputs": [],
   "source": [
    "#!cp drive/MyDrive/Colab_data/data.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yjgT9Azct22a"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvhH1uQsvnV7"
   },
   "source": [
    "## Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HV39YqA5vnV8"
   },
   "outputs": [],
   "source": [
    "class GrammarGen():\n",
    "    \"\"\"\n",
    "    Generates Grammar sequences from grammars, and offers other functionalities\n",
    "    Grammars are dictionaries:\n",
    "    - always have START\n",
    "    - all paths lead eventually to END\n",
    "    - Entries starting with the same letter\n",
    "      have same output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, grammar=None):\n",
    "        if grammar is None:\n",
    "            self.grammar = data.g0()\n",
    "        else:\n",
    "            self.grammar = grammar\n",
    "\n",
    "        # find how many letters in grammar\n",
    "        self.len = len(set([token[0] for token in self.grammar if (token != 'START' and token != 'END')]))\n",
    "\n",
    "        # variable to check how many sequences have been generated for the grammaticality test\n",
    "        self.grammCheckMaxLen = -1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"Generates n tokens\"\"\"\n",
    "        ret = []\n",
    "        count = 0\n",
    "        hashtrack = set()\n",
    "        while count < n:\n",
    "            token = []\n",
    "            current = 'START'\n",
    "            while current != 'END':\n",
    "                # Append current\n",
    "                if current != 'START':\n",
    "                    token.append(current[0])\n",
    "                # Choose next\n",
    "                r = random.randint(0, len(self.grammar[current]) - 1)\n",
    "                current = self.grammar[current][r]\n",
    "            # Check if seq is already inside\n",
    "            tokenhash = ''.join([str(x) for x in token])\n",
    "            if tokenhash not in hashtrack:\n",
    "                hashtrack.add(tokenhash)\n",
    "                ret.append((token, ))\n",
    "                count += 1\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def generateAllGrammatical(self, maxlen=float('inf')):\n",
    "        \"\"\"Generates all grammatical sequences until length maxlen\"\"\"\n",
    "        def genAllHelp(seq, current):\n",
    "            if current == 'END':\n",
    "                return [seq]\n",
    "            if len(seq) >= maxlen:\n",
    "                return []\n",
    "            # Append Current\n",
    "            if current != 'START':\n",
    "                seq.append(current[0])\n",
    "            # Generate next possibilities\n",
    "            options = range(len(self.grammar[current]))\n",
    "            ret = [(genAllHelp(copy.copy(seq), self.grammar[current][i]))\n",
    "                   for i in options]\n",
    "            return itertools.chain(*ret)\n",
    "        return set([tuple(seq) for seq in genAllHelp([], 'START')])\n",
    "\n",
    "    def isGrammatical(self, seqs):\n",
    "        \"\"\"Check for grammaticality of sequences in seqs\"\"\"\n",
    "        maxlen = max([len(seq) for seq in seqs])\n",
    "        if self.grammCheckMaxLen < maxlen:\n",
    "            self.allGrammatical = self.generateAllGrammatical(maxlen)\n",
    "            self.grammCheckMaxLen = maxlen\n",
    "\n",
    "        return [tuple(seq) in self.allGrammatical for seq in seqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKt9wtL6t22s"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Deyfd3OByLSC"
   },
   "source": [
    "\n",
    "\n",
    "### Cosine Loss, Init_weights, count_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6X9xZXFQvnWF"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class CosineLoss():\n",
    "    def __init__(self, vocabsize, ignore_index, reduction=\"mean\"):\n",
    "        self.vocabsize = vocabsize\n",
    "        self.ignore_index = ignore_index\n",
    "        self.eye = torch.eye(self.vocabsize).to(device)\n",
    "        self.cosSim = nn.CosineSimilarity(dim=1)\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, outputs, labels):\n",
    "        maxlen = outputs.shape[0]\n",
    "        bs = outputs.shape[1]\n",
    "\n",
    "        # Deal with positions which should be ignored and make them the same as label\n",
    "        if self.ignore_index is not None:\n",
    "            ignore_positions = (labels == self.ignore_index).to(device)\n",
    "            outputs[ignore_positions] = self.eye[self.ignore_index]\n",
    "\n",
    "        # Convert labels to onehot\n",
    "        labels_onehot = torch.empty((maxlen, bs, self.vocabsize)).to(device)\n",
    "        for idx in range(maxlen):\n",
    "            labels_onehot[idx,:,:] = self.eye[labels[idx,:]]\n",
    "\n",
    "        # Put labels and output in correct form for torch cosSim function\n",
    "        batch_first_labels = labels_onehot.permute(1,0,2)\n",
    "        processed_labels = batch_first_labels.reshape(-1, maxlen * self.vocabsize)\n",
    "        batch_first_outputs = outputs.permute(1,0,2)\n",
    "        processed_outputs = batch_first_outputs.reshape(-1, maxlen * self.vocabsize)\n",
    "\n",
    "        # use cosSim function\n",
    "        res = (1 - self.cosSim(processed_labels, processed_outputs))\n",
    "\n",
    "        # Same interface as native los functions: reductions for the output\n",
    "        if self.reduction == \"none\":\n",
    "            return res\n",
    "        elif self.reduction == \"mean\":\n",
    "            return res.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return res.sum()\n",
    "        else:\n",
    "            print(\"error\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3d2C9Rvt22w"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Fut2CtQrt22w"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src, src_len = batch\n",
    "        trg = src\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, src_len, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        if isinstance(criterion, CosineLoss):\n",
    "            loss = criterion(output, trg)\n",
    "        else:\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VIT6a8uqt22w"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "            \n",
    "            #seq = [seq_len, batch_size]\n",
    "            #output = [seq_len, batch_size, output_dim]\n",
    "\n",
    "            if isinstance(criterion, CosineLoss):\n",
    "                loss = criterion(output, seq)\n",
    "            else:\n",
    "                output_dim = output.shape[-1]\n",
    "                \n",
    "                output = output[1:].view(-1, output_dim)\n",
    "                trg = seq[1:].view(-1)\n",
    "                \n",
    "                #trg = [(trg len - 1) * batch size]\n",
    "                #output = [(trg len - 1) * batch size, output dim]\n",
    "                \n",
    "                loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uOtABRKLvnWI"
   },
   "outputs": [],
   "source": [
    "def evaluate_extra(model, dataloader, loss_func):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for seqs, seqs_len in dataloader:\n",
    "\n",
    "            outputs = model(seqs, seqs_len, seqs, 0)\n",
    "\n",
    "            loss = loss_func(outputs, seqs) / seqs.shape[1]\n",
    "\n",
    "            loss_total += loss.item()\n",
    "        \n",
    "#        if loss_func == allOrNoneLoss:\n",
    "#            return loss_total\n",
    "\n",
    "        return loss_total / len(dataloader)\n",
    "\n",
    "def cutEndToken(seq):\n",
    "    ret = []\n",
    "    for stim in seq:\n",
    "        if stim == END_TOKEN:\n",
    "            break\n",
    "        ret.append(stim)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def allOrNoneLoss(output, trg):\n",
    "    bs = output.shape[1]\n",
    "    ret = 0\n",
    "    pred = output.argmax(-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    for b in range(bs):\n",
    "        p = cutEndToken(pred[:,b].tolist())\n",
    "        t = cutEndToken(trg[:,b].tolist())\n",
    "        ret += not p == t\n",
    "    return torch.tensor(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lW3r-pjXt22x"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUY7o5eGt22x"
   },
   "source": [
    "During Training in addition to collecting the train/validation loss, collect the amount of entirely correct predicted sequences on the train and test gr/ugr set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_tasks(model, total_loss_array, total_hits_array, n_tasks_total, idx, criterion):\n",
    "    for other_id in range(n_tasks_total):\n",
    "        total_loss_array[other_id,0,idx] = evaluate(model,\n",
    "                                                    train_dls[other_id],\n",
    "                                                    criterion)\n",
    "        total_loss_array[other_id,1,idx] = evaluate(model,\n",
    "                                                    test_dls[other_id],\n",
    "                                                    criterion)\n",
    "        total_loss_array[other_id,2,idx] = evaluate(model,\n",
    "                                                    test_ugr_dls[other_id],\n",
    "                                                    criterion)\n",
    "        total_hits_array[other_id,0,idx] = evaluate_extra(model,\n",
    "                                                          train_dls[other_id],\n",
    "                                                          allOrNoneLoss)\n",
    "        total_hits_array[other_id,1,idx] = evaluate_extra(model,\n",
    "                                                          test_dls[other_id],\n",
    "                                                          allOrNoneLoss)\n",
    "        total_hits_array[other_id,2,idx] = evaluate_extra(model,\n",
    "                                                          test_ugr_dls[other_id],\n",
    "                                                          allOrNoneLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "JAVqiZ3kvnWJ"
   },
   "outputs": [],
   "source": [
    "def fit(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id,\n",
    "    n_task_epochs,\n",
    "    step_size_evaluation,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    clip=1,\n",
    "    repetition=None\n",
    "):\n",
    "        \n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(n_task_epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, 0, criterion)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train(model, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        #if valid_loss < best_valid_loss:\n",
    "        #    best_valid_loss = valid_loss\n",
    "        #    torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        # Log performance AFTER training\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0 and epoch != 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, idx, criterion)\n",
    "\n",
    "        \n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbtZwmf8egUu"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9S4PXbSe06C",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "P57X-q51t22n"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "                \n",
    "        #need to explicitly put lengths on cpu!\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'))\n",
    "                \n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "                                 \n",
    "        #packed_outputs is a packed sequence containing all hidden states\n",
    "        #hidden is now from the final non-padded element in the batch\n",
    "            \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs) \n",
    "            \n",
    "        #outputs is now a non-packed sequence, all hidden states obtained\n",
    "        #  when the input is a pad token are all zeros\n",
    "            \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le1kHQOIt22o"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "P_t5icjNt22o"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention = [batch size, src len]\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v371q1Tkt22q"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ECjR4jZot22q"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        # assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toSHWJmEt22q"
   },
   "source": [
    "### Seq2Seq\n",
    "\n",
    "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
    "\n",
    "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
    "\n",
    "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
    "\n",
    "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
    "\n",
    "The attention at each time-step is stored in the `attentions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OkcTBfr-t22s"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.src_pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #src_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "                    \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        mask = self.create_mask(src)\n",
    "\n",
    "        #mask = [batch size, src len]\n",
    "                \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state, all encoder hidden states \n",
    "            #  and mask\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs, mask)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLVj3zdDt22d"
   },
   "source": [
    "## Data\n",
    "\n",
    "First, get the training and test sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH6UqaslvnV_"
   },
   "source": [
    "### Sequencedataset\n",
    "Define a Dataset for Sequences:\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rBmNkT78t22d"
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Sequences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seqs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (int): amount of sequences generated\n",
    "        \"\"\"\n",
    "        self.seqs = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5x7ocJD0vnWB"
   },
   "source": [
    "### collate_batch\n",
    "Define collate_batch for the Dataloader: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "\n",
    "Sequences are padded and their non-padded lengths are returned.\n",
    "Since pack_padded_sequences requires sequences to be sorted, they are sorted too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tOBuBfPPt22e"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    seq_lens = []\n",
    "    processed_seqs = []\n",
    "    # Sort in descending order\n",
    "    batch.sort(reverse=True, key=(lambda x: len(x)))\n",
    "    # append start and end token\n",
    "    for seq in batch:\n",
    "        seq = [START_TOKEN] + seq + [END_TOKEN]\n",
    "        seq_lens.append(len(seq))\n",
    "        processed_seqs.append(torch.tensor(seq))\n",
    "    # pad\n",
    "    padded_seqs = pad_sequence(processed_seqs)\n",
    "    seq_lens = torch.tensor(seq_lens)\n",
    "    return padded_seqs.to(device), seq_lens.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiskh9dalsvW"
   },
   "source": [
    "### Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4UgG2QIFlsg3"
   },
   "outputs": [],
   "source": [
    "N_TASKS = 2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "PAD_TOKEN = 0\n",
    "START_TOKEN = 1\n",
    "END_TOKEN = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2cXtZIVe5_q"
   },
   "source": [
    "### Task loading\n",
    "For reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "HREfj6IuvnWJ"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHhSjYo6vnWK"
   },
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jWwIlfPcvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs = data.g0_train()\n",
    "valid_seqs = data.g0_train()\n",
    "test_seqs = data.g0_test_gr()\n",
    "test_ugr_seqs = data.g0_test_ugr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5I5lDz8vnWL"
   },
   "source": [
    "Sort for better perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ocjMQF-RvnWL"
   },
   "outputs": [],
   "source": [
    "train_seqs.sort(key=(lambda x: len(x)))\n",
    "valid_seqs.sort(key=(lambda x: len(x)))\n",
    "test_seqs.sort(key=(lambda x: len(x)))\n",
    "test_ugr_seqs.sort(key=(lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zMSM652MvnWL"
   },
   "outputs": [],
   "source": [
    "def buildVocab(letterset):\n",
    "    vocab = {'<pad>': PAD_TOKEN, '<sos>': START_TOKEN, '<eos>': END_TOKEN}\n",
    "    counter = len(vocab)\n",
    "    for letter in letterset:\n",
    "        vocab[letter] = counter\n",
    "        counter +=1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "yBCWnEM1vnWM"
   },
   "outputs": [],
   "source": [
    "letters = set()\n",
    "for seq in train_seqs:\n",
    "    [letters.add(letter) for letter in seq]\n",
    "letters = list(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPjDWlVPvnWM"
   },
   "source": [
    "Make all tasks, creates an additional task with all sequences from all tasks mashed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "YtuaqeKNvnWM"
   },
   "outputs": [],
   "source": [
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idxs = []\n",
    "idx2lets = []\n",
    "\n",
    "usedpermutations = set()\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        temp_letters = copy.copy(letters)\n",
    "\n",
    "        while str(temp_letters) in usedpermutations:\n",
    "            random.shuffle(temp_letters)\n",
    " \n",
    "        usedpermutations.add(str(temp_letters))\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Conversion Functions\n",
    "        let2idxs.append(lambda seq: [vocabs[-1][let] for let in seq])\n",
    "        idx2lets.append(lambda seq: [rvocabs[-1][let] for let in seq])\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idxs[-1](seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idxs[-1](seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idxs[-1](seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idxs[-1](seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-R9Nw9XB8Iw"
   },
   "source": [
    "### Task loading v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "E2Sn2FU9CAlD"
   },
   "outputs": [],
   "source": [
    "SWAP1 = 2\n",
    "SWAP2 = 4\n",
    "train_dls = []\n",
    "valid_dls = []\n",
    "test_dls = []\n",
    "test_ugr_dls = []\n",
    "vocabs = []\n",
    "rvocabs = []\n",
    "let2idx = lambda task_id, seq: [vocabs[task_id][let] for let in seq]\n",
    "idx2let = lambda task_id, seq: [rvocabs[task_id][let] for let in seq]\n",
    "\n",
    "usedpermutations = set()\n",
    "listUsedPermutations = []\n",
    "train_convs = []\n",
    "valid_convs = []\n",
    "test_convs = []\n",
    "test_ugr_convs = []\n",
    "alternate = 0\n",
    "for t in range(N_TASKS + 1):\n",
    "    # Create normal tasks\n",
    "    if t != N_TASKS:\n",
    "        if alternate == 0:\n",
    "            temp_letters = copy.copy(letters)\n",
    "\n",
    "            while str(temp_letters) in usedpermutations:\n",
    "                random.shuffle(temp_letters)\n",
    "    \n",
    "            usedpermutations.add(str(temp_letters))\n",
    "            listUsedPermutations.append(temp_letters)\n",
    "\n",
    "            alternate = 1\n",
    "        else:\n",
    "            temp_letters = listUsedPermutations[-1]\n",
    "\n",
    "            #Swap 5th and 6th indice\n",
    "            temp_letters[SWAP1], temp_letters[SWAP2] = temp_letters[SWAP2], temp_letters[SWAP1]\n",
    "\n",
    "            usedpermutations.add(str(temp_letters))\n",
    "\n",
    "            alternate = 0\n",
    "\n",
    "        # Vocab\n",
    "        vocabs.append(buildVocab(temp_letters))\n",
    "        rvocabs.append({v: k for k, v in vocabs[-1].items()})\n",
    "\n",
    "        # Convert to indices\n",
    "        train_conv = [let2idx(t, seq) for seq in train_seqs]\n",
    "        valid_conv = [let2idx(t, seq) for seq in valid_seqs]\n",
    "        test_conv = [let2idx(t, seq) for seq in test_seqs]\n",
    "        test_ugr_conv = [let2idx(t, seq) for seq in test_ugr_seqs]\n",
    "\n",
    "        # Add conv seq to sequence collection\n",
    "        train_convs.extend(train_conv)\n",
    "        valid_convs.extend(valid_conv)\n",
    "        test_convs.extend(test_conv)\n",
    "        test_ugr_convs.extend(test_ugr_conv)\n",
    "\n",
    "    # Create joint task\n",
    "    else:\n",
    "        train_conv = train_convs\n",
    "        valid_conv = valid_convs\n",
    "        test_conv = test_convs\n",
    "        test_ugr_conv = test_ugr_convs\n",
    "\n",
    "    # Datasets\n",
    "    train_ds = SequenceDataset(train_conv)\n",
    "    valid_ds = SequenceDataset(valid_conv)\n",
    "    test_ds = SequenceDataset(test_conv)\n",
    "    test_ugr_ds = SequenceDataset(test_ugr_conv)\n",
    "    \n",
    "    # Dataloader\n",
    "    train_dls.append(\n",
    "        DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=True, collate_fn=collate_batch))\n",
    "    valid_dls.append(\n",
    "        DataLoader(valid_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_dls.append(\n",
    "        DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))\n",
    "    test_ugr_dls.append(\n",
    "        DataLoader(test_ugr_ds, batch_size=BATCH_SIZE,\n",
    "                   shuffle=False, collate_fn=collate_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVaRvD2svnWN",
    "outputId": "ddaa8495-e224-4457-af03-78c62225b990"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'D': 3, 'F': 4, 'G': 5, 'C': 6, 'A': 7}\n",
      "{'<pad>': 0, '<sos>': 1, '<eos>': 2, 'D': 3, 'F': 4, 'A': 5, 'C': 6, 'G': 7}\n",
      "\n",
      "First Batch of Task 0:\n",
      "tensor([[1],\n",
      "        [7],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2]])\n",
      "\n",
      "First Batch of Task 1:\n",
      "tensor([[1],\n",
      "        [5],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "print(vocabs[0])\n",
    "print(vocabs[1])\n",
    "for i in range(len(valid_dls) - 1):\n",
    "    for seqs, _ in valid_dls[i]:\n",
    "        print(f\"\\nFirst Batch of Task {i}:\")\n",
    "        print(seqs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFM4eGVOs2lZ",
    "outputId": "ef66cff7-2e19-4a9e-8b0e-9fb932381c00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(task_id, seq)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzTfiG5qPpgS",
    "outputId": "be70246f-611c-4bd8-98e7-d46f7b1dceca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 4] - [5, 6, 4]\n",
      "[7, 6, 5, 4] - [5, 6, 7, 4]\n",
      "[7, 3, 6, 4] - [5, 3, 6, 4]\n",
      "[7, 6, 4, 6, 5] - [5, 6, 4, 6, 7]\n",
      "[7, 3, 6, 4, 6] - [5, 3, 6, 4, 6]\n",
      "[7, 6, 5, 4, 6, 5] - [5, 6, 7, 4, 6, 7]\n",
      "[7, 3, 6, 4, 6, 5] - [5, 3, 6, 4, 6, 7]\n",
      "[7, 3, 6, 5, 4, 6, 5] - [5, 3, 6, 7, 4, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "convTrainSeqs = lambda task_id: [let2idx(task_id, seq) for seq in train_seqs]\n",
    "convTestSeqs = lambda task_id: [let2idx(task_id, seq) for seq in test_ugr_seqs]\n",
    "tr1 = convTrainSeqs(0)\n",
    "tr2 = convTrainSeqs(1)\n",
    "te1 = convTestSeqs(0)\n",
    "te2 = convTestSeqs(1)\n",
    "\n",
    "for seq in tr1:\n",
    "    if seq in tr2:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "for se in tr2:\n",
    "    if se in te1:\n",
    "        print(se)\n",
    "        print(\"hi\")\n",
    "\n",
    "\n",
    "for i in range(len(tr1)):\n",
    "    print(f\"{tr1[i]} - {tr2[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpwRQ7xLvnWR"
   },
   "source": [
    "## Plotting & Evaluation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhO-beIdl9SC"
   },
   "source": [
    "### plotTranser, plotResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "y6vb8He6vnWR"
   },
   "outputs": [],
   "source": [
    "def plotTransfer(data, title):\n",
    "    # data = [n_methods, n_tasks, combinedepochs]\n",
    "    n_methods, n_tasks, n_combinedepochs = data.shape\n",
    "    fig, axs = plt.subplots(n_tasks, 1)\n",
    "    colors = ['blue','green','orange','red','yellow','violett']\n",
    "    \n",
    "    xvals = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    for task_idx in range(n_tasks):\n",
    "        for method_idx in range(n_methods):\n",
    "            axs[task_idx].plot(\n",
    "                xvals,\n",
    "                data[method_idx, task_idx],\n",
    "                color=colors[method_idx]\n",
    "            )\n",
    "            axs[task_idx].set_ylim(0,1.1)\n",
    "            if task_idx != n_tasks - 1:\n",
    "                axs[task_idx].tick_params(\n",
    "                    axis='x',\n",
    "                    which='both',\n",
    "                    labelbottom=False\n",
    "                )\n",
    "        x_lines = range(0, n_combinedepochs * STEP_SIZE_EVALUATION, N_EPOCHS)\n",
    "        for xpos in x_lines:\n",
    "            axs[task_idx].axvline(xpos, color=\"grey\")\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "def plotResults(hist_loss, hist_hits, plotLoss=False):\n",
    "    if torch.is_tensor(hist_loss):\n",
    "        hist_loss = hist_loss.numpy()\n",
    "    if torch.is_tensor(hist_hits):\n",
    "        hist_hits = hist_hits.numpy()\n",
    "    if plotLoss:\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,0,:], 0), \"Train Loss\")\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,1,:], 0), \"Test Gr Loss\")\n",
    "        plotTransfer( np.expand_dims(hist_loss[:,2,:], 0), \"Test Ugr Loss\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,0,:], 0), \"Train Hits\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,1,:], 0), \"Test Gr Hits\")\n",
    "    plotTransfer( np.expand_dims(hist_hits[:,2,:], 0), \"Test Ugr Hits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotAverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAverages(hist_all, schedule, step_size_evaluation, datasets=(0,1,2), figsize=(15,9), title=\"\", save=False, path=None):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets: tuple (int)\n",
    "        0 for train, 1 for test gr, 2 for test ugr\n",
    "    title: string\n",
    "        plot title\n",
    "    \"\"\"\n",
    "    # Handle case where only one dataset is displayed\n",
    "    if isinstance(datasets, int):\n",
    "        hist_all = np.expand_dims(hist_all[:,:,datasets], axis=-2)\n",
    "    elif len(datasets) == 1:\n",
    "        hist_all = np.expand_dims(hist_all[:,:,datasets[0]], axis=-2)\n",
    "    else:\n",
    "        hist_all = hist_all[:,:,datasets]\n",
    "    datasets = np.array(datasets)\n",
    "\n",
    "    # Consistent plot colors for train test gr and ugr\n",
    "    colors = np.array(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n",
    "    chosen_colors = colors[datasets]\n",
    "    \n",
    "    # hist_all = [repetitions, n_trainings, n_datasets, n_evalsteps]\n",
    "    n_repetitions, n_trainings, n_datasets, n_evalsteps_total = hist_all.shape\n",
    "    \n",
    "    # Invert Data\n",
    "    hist_all = 1 - hist_all\n",
    "    # averages\n",
    "    hist_all_avg = np.average(hist_all, axis=0)\n",
    "    #hist_all_avg_concat = np.concatenate(hist_all_avg, axis=2)\n",
    "    # standard derivations\n",
    "    hist_all_std = np.std(hist_all, axis=0)\n",
    "    #hist_all_std_concat = np.concatenate(hist_all_std, axis=2)\n",
    "    hist_all_min = np.fmax(hist_all_avg - hist_all_std, 0)\n",
    "    hist_all_max = np.fmin(hist_all_avg + hist_all_std, 1)\n",
    "    \n",
    "    n_tasks = len(set([x[0] for x in schedule]))\n",
    "    n_totalepochs = sum([x[1] for x in schedule])\n",
    "    list_epochs_passed = [0] + list(np.cumsum([x[1] for x in schedule]))\n",
    "    list_evalsteps_passed = [0] + list(np.cumsum([x[1]//step_size_evaluation for x in schedule]))\n",
    "    \n",
    "    fig, axs = plt.subplots(n_tasks, 1, figsize=figsize)\n",
    "    \n",
    "    for dataset_id in range(n_datasets):\n",
    "        for task_id, n_epochs in schedule:\n",
    "            # Right y limits\n",
    "            axs[task_id].set_ylim(-0.05, 1.05)\n",
    "            \n",
    "            # Hide parts of the graph without training\n",
    "            xvals = range(list_epochs_passed[task_id], n_totalepochs, step_size_evaluation)\n",
    "\n",
    "            # Average\n",
    "            axs[task_id].plot(xvals,\n",
    "                              hist_all_avg[task_id,dataset_id,list_evalsteps_passed[task_id]:],\n",
    "                              color=chosen_colors[dataset_id])\n",
    "            # Standard deviation\n",
    "            axs[task_id].fill_between(xvals,\n",
    "                                      hist_all_min[task_id,dataset_id,list_evalsteps_passed[task_id]:],\n",
    "                                      hist_all_max[task_id,dataset_id,list_evalsteps_passed[task_id]:],\n",
    "                                      color=chosen_colors[dataset_id],\n",
    "                                      alpha=0.2)\n",
    "\n",
    "            # prevent ticks in upper subplots\n",
    "            if task_id != n_tasks - 1:\n",
    "                axs[task_id].tick_params(axis=\"x\", \n",
    "                                         which=\"both\",\n",
    "                                         labelbottom=False)\n",
    "            # Epoch label\n",
    "            else:\n",
    "                axs[task_id].set_xlabel(\"Epochs\")\n",
    "\n",
    "            # Task Label\n",
    "            if task_id == 0:\n",
    "                axs[task_id].set_ylabel(f\"Replication percentage average N={n_repetitions}\")\n",
    "\n",
    "            # Vertical lines\n",
    "            if dataset_id == n_datasets - 1:\n",
    "                axs[task_id].vlines(list_epochs_passed, 0, 1,\n",
    "                                    transform=axs[task_id].get_xaxis_transform(),\n",
    "                                    colors=\"grey\")\n",
    "            \n",
    "    legends = np.array([\"Train\", \"Test Grammatical\", \"Test Ungrammatical\"])\n",
    "    plt.legend(legends[datasets])\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotSpecificTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSpecificTask(hist_all, train_id, task_id, figsize=(13,7), title=\"\", save=False):\n",
    "    # hist_all = [repetitions, n_train_runs, n_tasks, n_datasets, n_evalsteps]\n",
    "    n_repetitions, _, _, n_datasets, n_evalsteps = hist_all.shape\n",
    "    # Invert data\n",
    "    hist_all = 1 - hist_all\n",
    "    titles = (\"Train\", \"Test Grammatical\", \"Test Ungrammatical\")\n",
    "    # average\n",
    "    hist_all_avg = np.average(hist_all, axis=0)\n",
    "    hist_specific = hist_all_avg[train_id, task_id]\n",
    "    \n",
    "    # standard deviation\n",
    "    hist_all_std = np.std(hist_all, axis=0)\n",
    "    hist_std = hist_all_std[train_id, task_id]\n",
    "    hist_max = np.fmin(hist_specific + hist_std, 1)\n",
    "    hist_min = np.fmax(hist_specific - hist_std, 0)\n",
    "    \n",
    "    xvals = range(0, n_evalsteps * STEP_SIZE_EVALUATION, STEP_SIZE_EVALUATION)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    for dataset_id in range(n_datasets):\n",
    "        plt.plot(xvals, hist_specific[dataset_id])\n",
    "        plt.fill_between(xvals, hist_min[dataset_id], hist_max[dataset_id], alpha=0.2)\n",
    "    \n",
    "    plt.ylim(-0.05,1.05)\n",
    "    plt.legend(titles)\n",
    "    plt.title(title)\n",
    "    plt.ylabel(f\"Replication percentage average N={n_repetitions}\")\n",
    "    plt.xlabel(\"Epochs Trained\")\n",
    "    if save:\n",
    "        plt.savefig(PLOTSAVE, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMGZLw18mBw9"
   },
   "source": [
    "### visual_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "kQWqbao6vnWR"
   },
   "outputs": [],
   "source": [
    "def visual_eval(model, test_dl, cutEndToken=True):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    errors = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(test_dl):\n",
    "\n",
    "            src, src_len = batch\n",
    "            trg = src\n",
    "\n",
    "            output = model(src, src_len, trg, 0, verbose=True) #turn off teacher forcing\n",
    "            show_batch(output, trg, cutEndToken)\n",
    "\n",
    "\n",
    "def show_batch(output, trg, cutEndToken=True):\n",
    "    bs = output.shape[1]\n",
    "    pred = output.argmax(axis=-1)[1:]\n",
    "    trg = trg[1:]\n",
    "    if cutEndToken: \n",
    "        for b in range(bs):\n",
    "            p = cutEndToken(pred[:,b].tolist())\n",
    "            t = cutEndToken(trg[:,b].tolist())\n",
    "            status = \"same\" if p == t else \"different\"\n",
    "            print(f\"pred = {p} - {status} \\ntrg  = {t}\\n-\")\n",
    "    else:\n",
    "        for b in range(bs):\n",
    "            p = pred[:,b].tolist()\n",
    "            t = trg[:,b].tolist()\n",
    "            print(f\"pred = {p} \\ntrg  = {t}\\n-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6xnLRCNmDu1"
   },
   "source": [
    "### accuracy, accuracyAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "nPn1QguwvnWR"
   },
   "outputs": [],
   "source": [
    "def accuracy(model, iterator=None):\n",
    "    if iterator is None:\n",
    "        iterator = test_dls\n",
    "    for task_id in range(N_TASKS + 1):\n",
    "        gr_not_hits = evaluate_extra(model, iterator[task_id], allOrNoneLoss)\n",
    "        ugr_not_hits = evaluate_extra(model, test_ugr_dls[task_id], allOrNoneLoss)\n",
    "        gr_hits = 1 - gr_not_hits\n",
    "        ugr_hits = 1 - ugr_not_hits\n",
    "        total_acc = (gr_hits + ugr_not_hits) / 2\n",
    "        print(f\"Task {task_id}: Acc {total_acc:2.2}% | Gr acc {gr_hits:2.2} | Ugr acc {ugr_not_hits:2.2}\")\n",
    "        \n",
    "def accuracyAll(models):\n",
    "    for model_id in range(len(models)):\n",
    "        print(f\"\\nModel {model_id}\")\n",
    "        accuracy(models[model_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xoKXlA1vnWQ"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jc0kqVE-vnWQ"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = max(vocabs[0].values()) + 1\n",
    "OUTPUT_DIM = max(vocabs[0].values()) + 1\n",
    "ENC_EMB_DIM = 19\n",
    "DEC_EMB_DIM = 19\n",
    "ENC_HID_DIM = 9\n",
    "DEC_HID_DIM = 9\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.01\n",
    "LEARNING_RATE_GATING = 0.01\n",
    "SRC_PAD_IDX = PAD_TOKEN\n",
    "TRG_PAD_IDX = PAD_TOKEN\n",
    "PREFIX = \"pres\"\n",
    "CLIP = 1\n",
    "STEP_SIZE_EVALUATION = 10\n",
    "\n",
    "TEST_ALL_TASKS = 1\n",
    "N_EPOCHS = 200\n",
    "N_EPOCHS_TOTAL = 1200\n",
    "# N_EPOCHS_TOTAL = N_EPOCHS * (N_TASKS + TEST_ALL_TASKS)\n",
    "\n",
    "N_REPETITIONS = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(\n",
    "    experiment_name,\n",
    "    n_repetitions,\n",
    "    schedule,       # List of tuples: [(task_id, n_task_epochs),...]\n",
    "    init_func,      # Returns tuple: model, pass_on_variable which is given to repeat_func\n",
    "    repeat_func,    # Returns triple: hist_loss, hist_hits, model\n",
    "    step_size_evaluation=10\n",
    "):\n",
    "    n_epochs_total = sum([x[1] for x in schedule])\n",
    "    n_tasks_total = len(set([x[0] for x in schedule]))\n",
    "    n_evaluations = sum([x[1] // step_size_evaluation for x in schedule])\n",
    "    n_datasets = 3\n",
    "    \n",
    "    hist_all_losses = np.empty((n_repetitions, n_tasks_total, n_datasets,\n",
    "                                n_evaluations))\n",
    "    hist_all_hitsss = np.empty((n_repetitions, n_tasks_total, n_datasets,\n",
    "                                n_evaluations))\n",
    "\n",
    "    for repetition in range(n_repetitions):\n",
    "        print(f\"@@@@@@@@@ Repetition {repetition:3} @@@@@@@@@\")\n",
    "        if repetition == 0:\n",
    "            models = []\n",
    "        \n",
    "        # Call task specific init function\n",
    "        pass_on_variables = init_func()\n",
    "        \n",
    "        n_evalsteps_passed = 0\n",
    "        for i, (task_id, n_task_epochs) in enumerate(schedule):\n",
    "            tag = f\"{experiment_name}.s{i}.t{task_id}.e{n_task_epochs}\"\n",
    "            print(f\"\\nSCHEDULE: {tag}\")\n",
    "            \n",
    "            # Call task specific repeat function\n",
    "            hist_loss, hist_hits, model = repeat_func(n_tasks_total,\n",
    "                                                      n_task_epochs,\n",
    "                                                      task_id,\n",
    "                                                      step_size_evaluation,\n",
    "                                                      repetition,\n",
    "                                                      pass_on_variables)\n",
    "            \n",
    "            start_idx = n_evalsteps_passed\n",
    "            end_idx = n_evalsteps_passed + n_task_epochs // step_size_evaluation\n",
    "            hist_all_losses[repetition,:,:,start_idx:end_idx] = hist_loss\n",
    "            hist_all_hitsss[repetition,:,:,start_idx:end_idx] = hist_hits\n",
    "            \n",
    "            if repetition == 0 and model is not None:\n",
    "                models.append(copy.deepcopy(model))\n",
    "                \n",
    "            n_evalsteps_passed = end_idx\n",
    "    \n",
    "    return hist_all_losses, hist_all_hitsss, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schedule(n_epochs_total, n_total_tasks, n_task_epochs):\n",
    "    # Creates a training schedule:\n",
    "    schedule = []\n",
    "    epoch_counter = n_epochs_total\n",
    "\n",
    "    for task_id in itertools.cycle(range(n_total_tasks)):\n",
    "\n",
    "        if epoch_counter - n_task_epochs <=0:\n",
    "            schedule.append((task_id, epoch_counter))\n",
    "            break\n",
    "        else:\n",
    "            schedule.append((task_id, n_task_epochs))\n",
    "            epoch_counter -= n_task_epochs\n",
    "    return schedule\n",
    "\n",
    "def schedule_repetitions(x):\n",
    "    \"\"\"x amount of repetitions each task gets\"\"\"\n",
    "    assert x != 0\n",
    "    n_task_epochs = N_EPOCHS_TOTAL // ((N_TASKS + TEST_ALL_TASKS) * x)\n",
    "    return create_schedule(N_EPOCHS_TOTAL, N_TASKS + TEST_ALL_TASKS, n_task_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_not_interleaved = create_schedule(150,3,50)\n",
    "dbg_medium_interleaved = create_schedule(150,3,25)\n",
    "dbg_strong_interleaved = create_schedule(150,3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_interleaved = schedule_repetitions(1)\n",
    "medium_interleaved = schedule_repetitions(4)\n",
    "strong_interleaved = schedule_repetitions(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline A: Individual Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_individual():\n",
    "    return None\n",
    "\n",
    "def repeat_individual(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "    print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "    hist_loss, hist_hits= fit(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        CLIP,\n",
    "        repetition=repetition      \n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4QnSSsYav-F"
   },
   "source": [
    "### Experiment Individual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE = not_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "I9vjyK31vnWT"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "\n",
      "SCHEDULE: Individual-1.s0.t0.e400\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(19, 9, bidirectional=True)\n",
      "    (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "      (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(37, 9)\n",
      "    (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 4028 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 0.505 |  Val. PPL:   1.657\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.436 | Train PPL:   1.547\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.514\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.361\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.401\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.438\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.252\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c5bc0cc5c0f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_repetitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m hist_all_losses_A, hist_all_hitsss_A, models_A = experiment(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"Individual-1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_repetitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSCHEDULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-4fb69d5f2fa2>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(experiment_name, n_repetitions, schedule, init_func, repeat_func, step_size_evaluation)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Call task specific repeat function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             hist_loss, hist_hits, model = repeat_func(n_tasks_total,\n\u001b[0m\u001b[1;32m     34\u001b[0m                                                       \u001b[0mn_task_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                       \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-558ed593cd84>\u001b[0m in \u001b[0;36mrepeat_individual\u001b[0;34m(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The model has {count_parameters(model)} trainable parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     hist_loss, hist_hits= fit(\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mn_tasks_total\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-abf148d09830>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(n_tasks_total, model, task_id, n_task_epochs, step_size_evaluation, optimizer, criterion, clip, repetition)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-35e15fd6387a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_A, hist_all_hitsss_A, models_A = experiment(\n",
    "    \"Individual-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_individual,\n",
    "    repeat_individual,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAverages(hist_all_hitsss_A, SCHEDULE, STEP_SIZE_EVALUATION, figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlY2ObLqvnWV",
    "outputId": "6476a3b7-512d-4bc0-9dd2-dc47319cbef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.81% | Gr acc 0.62 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yD_rpqtSvnWV"
   },
   "source": [
    "## Baseline B: Keep Training same model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_standard():\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "    return model\n",
    "\n",
    "def repeat_standard(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model = pass_on_variables\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "    hist_loss, hist_hits= fit(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        CLIP,\n",
    "        repetition=repetition      \n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqEL860-a9t9"
   },
   "source": [
    "### Experiment Keep Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "TRGPK1CjvnWV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE = not_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9zXm7oSvnWW",
    "outputId": "b2d8e43a-6773-4c10-e7d7-15fe7a662fb2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(19, 9, bidirectional=True)\n",
      "    (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "      (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(37, 9)\n",
      "    (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: B-standard-.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.662 | Train PPL:   1.938\n",
      "\t Val. Loss: 0.503 |  Val. PPL:   1.654\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.458 | Train PPL:   1.581\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.512\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.430 | Train PPL:   1.537\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.466 |  Val. PPL:   1.594\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.466\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.485\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.401\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.312\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.226\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.182\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.143\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B-standard-.s1.t1.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.307 |  Val. PPL:   1.359\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.209\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B-standard-.s2.t2.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.181\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_B, hist_all_hitsss_B, models_B = experiment(\n",
    "    \"B-standard-\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_standard,\n",
    "    repeat_standard,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_B, SCHEDULE, STEP_SIZE_EVALUATION, figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotSpecificTask(hist_all_hitsss_B, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZD9r3RykvnWX",
    "outputId": "e5330dc5-23ab-4038-af99-0fb47c16ec07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.94% | Gr acc 0.88 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaved Keep Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "TRGPK1CjvnWV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEDULE = medium_interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9zXm7oSvnWW",
    "outputId": "b2d8e43a-6773-4c10-e7d7-15fe7a662fb2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(19, 9, bidirectional=True)\n",
      "    (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "      (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 19)\n",
      "    (rnn): GRU(37, 9)\n",
      "    (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: B2-standard-.s0.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.701 | Train PPL:   2.015\n",
      "\t Val. Loss: 0.550 |  Val. PPL:   1.733\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.572\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.482\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.571\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.517\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.518\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.328\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.188\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.164\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.168\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.160\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.145\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "\n",
      "SCHEDULE: B2-standard-.s1.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.511\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.363\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.248\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.175\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "\n",
      "SCHEDULE: B2-standard-.s2.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.195\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.172\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "\n",
      "SCHEDULE: B2-standard-.s3.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s4.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s5.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s6.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B2-standard-.s7.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B2-standard-.s8.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: B2-standard-.s9.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: B2-standard-.s10.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "\n",
      "SCHEDULE: B2-standard-.s11.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.077 | Train PPL:   1.080\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_B2, hist_all_hitsss_B2, models_B2 = experiment(\n",
    "    \"B2-standard-\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_standard,\n",
    "    repeat_standard,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_B2, SCHEDULE, STEP_SIZE_EVALUATION, figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUleAoTVvnWY"
   },
   "source": [
    "## Baseline C: Freeze Parameters (not updated)\n",
    "\n",
    "1. Define functions\n",
    "2. Train model, freeze core weights in between tasks\n",
    "3. Look at performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbcfqNFlbGjA"
   },
   "source": [
    "### onTaskUpdate, applyOnParameters, freezeParameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "EiFSSGphvnWY"
   },
   "outputs": [],
   "source": [
    "def applyOnParameters(model, conditions, apply_function):\n",
    "    for name, param in model.named_parameters():\n",
    "        # Check every condition\n",
    "        for condition in conditions:\n",
    "            # check every keyword\n",
    "            allincluded = True\n",
    "            for keyword in condition:\n",
    "                if keyword not in name:\n",
    "                    allincluded = False\n",
    "                    break\n",
    "            if allincluded:\n",
    "                apply_function(param)\n",
    "\n",
    "def freezeParameters(model, conditions):\n",
    "    def freeze(param):\n",
    "        param.requires_grad = False\n",
    "    applyOnParameters(model, conditions, freeze)\n",
    "\n",
    "def unfreezeParameters(model, conditions):\n",
    "    def unfreeze(param):\n",
    "        param.requires_grad = True\n",
    "    applyOnParameters(model, conditions, unfreeze)\n",
    "\n",
    "def showModelParameters(model, requires_grad=False):\n",
    "    for name, param in model.named_parameters():\n",
    "        if requires_grad:\n",
    "            if param.requires_grad:\n",
    "                print(name)\n",
    "        else:\n",
    "            print(name)\n",
    "            \n",
    "def onTaskUpdate(model):\n",
    "    # Freeze core weights\n",
    "    freezeParameters(model, ((\"\"),))    # Freeze everything\n",
    "    unfreezeParameters(model, ((\"encoder\",\"embedding\"), (\"decoder\",\"fc_out\"), (\"attention\",))) # Unfreeze relevant stuff\n",
    "    \n",
    "    # Reinitialize\n",
    "    to_constant = lambda param: nn.init.constant_(param.data, 0)\n",
    "    applyOnParameters(model, ((\"decoder\",\"fc_out\",\"bias\"),(\"attn\",\"bias\")), to_constant)\n",
    "    to_normal = lambda param: nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "    applyOnParameters(model, ((\"encoder\",\"embedding\"),(\"decoder\",\"fc_out\",\"weight\"),(\"attention\",\"weight\")), to_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJlVK1CCvnWZ"
   },
   "source": [
    "### Experiment Freeze Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "9_UJzijZvnWZ"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H83P-2BXvnWZ",
    "outputId": "8c4cbe78-90a0-46bc-cc0f-8a9328d47640",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "tr-AE-30-10-0.01-C0\n",
      "The model has 5878 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.613 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.603\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.463\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.477\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.398 |  Val. PPL:   1.489\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.397\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.299 |  Val. PPL:   1.349\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.326\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.307\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.184\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.134\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.128\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.121\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.122\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.126 |  Val. PPL:   1.135\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "tr-AE-30-10-0.01-C1\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.642 | Train PPL:   1.900\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.402 | Train PPL:   1.495\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.288 |  Val. PPL:   1.333\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.318\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.244\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.228\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.211\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.226\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.262\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.193\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.137 |  Val. PPL:   1.147\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.168 |  Val. PPL:   1.183\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.204\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.151\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.144\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.180\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.172\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.147\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "tr-AE-30-10-0.01-C2\n",
      "The model has 1048 trainable parameters\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.652\n",
      "\t Val. Loss: 0.428 |  Val. PPL:   1.534\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.491\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.448\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.453\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.334 |  Val. PPL:   1.396\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.397\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.400\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.381\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.306 |  Val. PPL:   1.358\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.342\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.336\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.290 |  Val. PPL:   1.337\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.325\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.283 |  Val. PPL:   1.327\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.348\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.305\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.313\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = N_REPETITIONS\n",
    "hist_all_losses_C = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "hist_all_hitsss_C = np.empty((n_repetitions, N_TASKS + TEST_ALL_TASKS,\n",
    "                              N_TASKS + TEST_ALL_TASKS, 3,\n",
    "                              N_EPOCHS // STEP_SIZE_EVALUATION))\n",
    "for repetition in range(n_repetitions):\n",
    "    print(f\"\\n\\n\\n\\n\\n\\n------ REPETITION {repetition:3} ------\")\n",
    "    # To have single copy\n",
    "    if repetition == n_repetitions - 1:\n",
    "        models_F = []\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    for n_task in range(N_TASKS + TEST_ALL_TASKS):\n",
    "        SUFFIX = f\"C{n_task}\"\n",
    "        title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "        LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "        PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "        criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "        print(title)\n",
    "        print(f'The model has {count_parameters(model)} trainable parameters')\n",
    "\n",
    "        hist_loss_temp, hist_hits_temp = fit(model, n_task, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "        hist_all_losses_C[repetition,n_task] = hist_hits_temp\n",
    "        hist_all_hitsss_C[repetition,n_task] = hist_hits_temp\n",
    "        if repetition == n_repetitions - 1:\n",
    "            models_C.append(copy.deepcopy(model))\n",
    "\n",
    "        # Freeze, reinitialize\n",
    "        onTaskUpdate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "x_07dWu8vnWa",
    "outputId": "22e6837e-a1ee-44e7-cc6c-1ccfd78122e3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEdCAYAAABZtfMGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtz0lEQVR4nO3deZhU1Z3/8feXZo0bCooICNhBlMTEpUEdHYMxUZBEjMERYsQdaTWj+WUmMTFjljEzWX0So3aLirhFjEuURBS3aBLHhcUNNETAhe5SQVQQRNbv749TFYru6qrbtXTdrvq8nqef7rr39rnn0E19+px77rnm7oiIiMRNl3JXQEREJBMFlIiIxJICSkREYkkBJSIisaSAEhGRWFJAiYhILCmgRDoBM1trZvuUux4iHUkBJRUh+Qae+thqZuvTXp+aR3mPm9k5OY7pbmaXmdliM1tnZs1m9oCZHdvOc7mZfbLFth+a2a2p1+6+o7svS+6bYWaXt+ccIp1R13JXQKQY3H3H1Ndm9jpwjrs/UuLT3gUMACYDzyW3fR4YBzzU8mAz6+rum0tcJ5GKoR6UVDQz62Jml5jZUjNbZWa/N7Pdkvt6mtmtye0fmNlcM+tnZj8B/hW4KtkDuypDuV8AvgiMd/dn3H1j8uNBd78o7bjXzew7ZvYisM7M8vqjMNXLMrMpwKnAt5N1+2Ny/3eSPbgPkz26Y/I5j0icqAclle7fgROBzwErgSuBq4FJwOnALsAgYANwILDe3S81syOAW939+jbK/QLwjLs3RajDJEKv6t1Ce1DuPs3M/gVocvfvA5jZcOBCYKS7J8xsCFBTyHlE4kABJZXuPODCVJCY2Q+BN83sNGAT0Af4pLu/CMxvR7l9gbdTL5K9smWAAT3cvWfasVe6+/Ic5S0ws61pr3sShhCj2AL0AEaY2Up3fz3i94nEmob4pNINBv6QHML7AHiF8IbeD7gFmAPMNLOEmf3czLpFLHcV0D/1wt3fc/fewCGEsEiXK5wADnb33qkP4KcR64G7LwEuBn4IrDCzmWa2V9TvF4krBZRUuuXA2PQ3f3fv6e7N7r7J3X/k7iOAfwG+RJjwAJBrmf9HgZFmNjBCHYr9yIBW5bn779z9SEIgO/CzIp9TpMMpoKTSNQI/MbPBAGa2u5mNT359tJkdYGY1wBrCkN+W5Pe9A7R535G7PwT8GbjXzA5NTjnvBhxWwrakbFc3MxtuZp83sx7Ax8B6trVDpNNSQEml+w0wC3jIzD4EngYOTe7bk3CdZw1h6O8J4Na075tgZu+b2ZVtlH0S8Kfk93wAvEaYYTem+M3Yzg2E600fmNm9hCHFnwLvEq6L7QF8r8R1ECk50wMLRUQkjtSDEhGRWFJAiYhILCmgREQklhRQIiISSwooERGJJQWUiIjEkgJKRERiSQElIiKxpIASEZFYUkCJiEgsKaBERCSWFFAiIhJLCigREYklBZSIiMSSAkpERGJJASUiIrGkgBIRkVjqWq4T9+3b14cMGVJQGatWrQKgT58+RaiRxIl+tpVNP19JN3/+/HfdffeW28sWUEOGDGHevHkFlTFjxgwAzjjjjMIrJLGin21l089X0pnZG5m2a4hPRERiKWdAmdl0M1thZgvb2G9mdqWZLTGzF83s4OJXU0REqk2UHtQMYEyW/WOBYcmPKUBD4dUSEZFql/MalLv/xcyGZDlkPHCzuzvwtJn1NrP+7v5WsSrZloULYetW+OIXox3fpQtceikcdVTuYzdtgosugqlT4TOfyX38hg1wwQXw7W/DvvtGq4+07f33Yfny6D9b6VwOOCB81s+3czvwQPjFL0pXfjEmSQwAlqe9bkpuaxVQZjaF0Mti7733LvjEW7fCli3w0UfRjn/5Zfiv/4Innsh97KxZ0NAAvXrBr36V+/hnnoEbbgjBdtNN0eojbVuxAj74IPrPVjqXLVvCZ/18O7ePPy5t+cUIKMuwzTMd6O7TgGkAdXV1GY9pj1TP5je/iXb8L34RejiLFsGnPpX92GuuCZ/nzo1Wduq4O+6AK64AzZ4tzIYNsOOO8OST5a6JlEJyEl/k/7tSnYoxi68JGJT2eiCQKEK5RXfmmdCjBzQ2Zj9u8WJ47DHYaSdYsGDbX3vZzJsX3lA3bIAbbyxOfavZxo3QvXu5ayEi5VSMgJoFTE7O5jsMWN0R15/y0bcvnHwy3HwzrF3b9nGNjdCtWxgOXLcO/v733GXPnQvHHQdHHhm+f+vW4tW7Gm3YEP6YEJHqFWWa+e3AU8BwM2sys7PNbKqZTU0eMhtYBiwBrgPOL1lti6C+Htasgdtvz7z/o4/C8MNJJ8G4cWFbrvuJ338fli6FurpQ/tKl8MgjRa12VdmwATZvVg9KpNrlDCh3n+Tu/d29m7sPdPcb3L3R3RuT+93dL3D3Wnc/wN0LWx6ixA4/PFy7amgAz3AV7I47wsX5+noYPjwM2+W6DpUKsJEj4atfhd13D+VLfhLJAWL1oESqW9WtJGEWwue55+DZZ1vvb2iAESPCVPSaGjj44Nw9qNT+Qw4Jb6pnnRVmATY1Fb/+1SAVUOpBiVS3qgsogFNPDT2jlr2c+fNDb2nq1BBkEHpFzz8fLtq3Ze5cGDYMevcOr887L/TOrruuFLWvfOpBiQhUaUDttBOcdhrMnAnJRZWBEFif+ARMnrxtW11duCayaFHb5c2bF45LGToUxowJAbVpU/HrX+nUgxIRqNKAgjDMt2HDtvsxPvgAfvc7+NrXYJddth03cmT43NZ1qHfeCSsepI5LL/+tt+C++4pd88rX3Bx6sN26lbsmIlJOVRtQBxwARxyxbUr4zTfD+vUhWNLtsw/sumvb16FS29N7UADHHw97763JEvlIJDS8JyJVHFAQwmjJkjAlvKEBRo0KkyLSmYXwaasHNXduWOPvoIO2315TA1OmhBt+Fy8uTf0rVSKh4T0RqfKAmjAh3Lx7/vnhZtyWvaeUkSPDwrTr17feN28e7L9/mHTR0tlnQ9euuVeukO2pByUiUOUBlZoSvnRpGMY75ZTMx9XVhRtHX3hh++3uoQfV8vpTyp57hht+Z8wIMwSff377j8WLM9+LVe2am9WDEpEqDygIU8K7dAlB1atX5mNSAdTyOlRTU1h1u+X1p3QXXBAmYNTVhWHA9I/99tOKEy19+GFYhko9KBEpxmrmndo++4QFYbM9w2nAAOjXr/V1qNTrtnpQEG74ffzxsBxSuvXrw4zBl17SM3HSaYq5iKRUfUABfPaz2febhRBq2YOaNy9cY8r1QMPPfS7z9vPPD8OLso1u0hWRlKof4ouqrg5eeSUMQaXMnRvCqWfP/MqsrVVAtdTcHD6rByUiCqiIRo4MExqeey68dm+9gkR7KaBaUw9KRFIUUBGlgih13Wnp0jD5Idv1p1xqa+H118MMQQkSibAUVU1NuWsiIuWmgIpojz3CyhCp61BtrSDRHrW1IZzefLPw+lWKRAL22qvctRCROFBAtUP6ihJz54ZrT5/6VP7l1daGzxrm26a5WQElIoECqh1Gjgxh8t57IaAOPLCwBU0VUK2pByUiKQqodkgN5z37bLh3qpDrTxDur+rRQwGV4h4CasCActdEROJAAdUOqYC69VZYt66w608QVrAYOlQBlfLee+HBkOpBiQgooNqld+/w5Nw77wyvC+1Bgaaap0vdA6WAEhGIGFBmNsbMFpvZEjO7JMP+0Wa22syeT35cVvyqxkNdXfgrf8cdsy+PFFUqoLRo7LZ7oDTEJyIQIaDMrAa4GhgLjAAmmdmIDIf+1d0PTH78uMj1jI1Ur+mQQ4pzr05tbRguXLGi8LI6u1RAqQclIhCtBzUKWOLuy9x9IzATGF/aasVX6rpTodefUjSTb5vUEF///uWth4jEQ5SAGgAsT3vdlNzW0uFm9oKZPWBmGe8OMrMpZjbPzOatXLkyj+qWX10djBnT9rOj2ksBtU0iAX36aJkjEQmirGZuGba1vGKyABjs7mvN7HjgXmBYq29ynwZMA6irq+uUV1169YIHHiheeUOHhtXSFVCaYi4i24vSg2oCBqW9Hggk0g9w9zXuvjb59Wygm5n1LVotK1iPHjBwoAIKdJOuiGwvSkDNBYaZ2VAz6w5MBGalH2Bme5qZJb8elSx3VbErW6k01TzQMkciki7nEJ+7bzazC4E5QA0w3d0XmdnU5P5GYAJQb2abgfXARHdNnI6qthb++Mdy16K8Nm+Gd97REJ+IbBPpibrJYbvZLbY1pn19FXBVcatWPWprwzTzDz8Mj5qoRitWwNat6kGJyDZaSSIGUjP5li0rbz3KSfdAiUhLCqgY0FRzLXMkIq0poGJAAaVljkSkNQVUDPTuDbvtpoDq0iU8uVhEBBRQsVHtU80TCdhzz+KsbygilUEBFRPVHlC6B0pEWlJAxURtLbz5JmzaVO6alIeWORKRlhRQMVFbC1u2wBtvFK/MDRuKV1ZU7uF5We2lZY5EpCUFVEwUeybfnDmwyy7w0kvFKS+qq64Kj8t4773o37NhA6xapYASke0poGKi2AF1xRXhjf+qDlzfY8uWcN733oMZM6J/n6aYi0gmCqiY6N8fevYsTkAtWQIPPRSWTbrtNlizpvAyo5gzB15/PZy3sTEsXRSFVpEQkUwUUDHRpQvss09xAuraa8N07VtuCY+Tv+WWwsuMoqEB+vWDX/8aXn0VHnss2vcpoEQkEwVUjBRjqvnHH8ONN8KJJ8L48XDIISE4Sr22/BtvwP33wznnwNe+Fp6M29AQ7XsVUCKSiQIqRlIBVUiY3HlnmHBQXx9e19fDokXwt78Vp45tmTYtPBl4ypQwVHnWWXDffdvW2MumuTk8uHG33UpbRxHpXBRQMVJbC+vXw1tv5V9GQwPsuy98/vPh9cSJYTZf1N5MPjZuhOuvh3HjYO+9w7bzzguTJq6/Pvf3p6aYh0deiogECqgYKXQm3wsvwFNPwdSp297sd9gBTj8d7rorPHOpFP7wh1B2qtcGoS3HHQfXXRceRpiN7oESkUwUUDFSaEA1NIThtdNP33771KlhhYrp0wurX7bzDh0aAildfX0Yvsv1tGCtIiEimSigYmTIkDCbL5+AWrMGbr01DOm1vJaz//4wenSY3bdlSzFqus3LL8MTT4QhvS4tfpvGjYOBA3MPL2odPhHJRAEVI927w6BB+QXUrbeGKeXpw2zp6uvDPUpz5hRUxVYaG0O9zzqr9b6uXcOkiYcfDtPOM/nwQ1i7VgElIq0poGImn6nm7qGXcvDBMHJk5mNOPDHco1TMyRLr1sFNN8GECbD77pmPOeecEFTXXpt5v6aYi0hbIgWUmY0xs8VmtsTMLsmw38zsyuT+F83s4OJXtTrkE1BPPgkLF4ZeUlsz4bp3D2Fx//3FW5D29tvD0GJbvTYIK2SceGK4N2v9+tb7tcyRiLQlZ0CZWQ1wNTAWGAFMMrMRLQ4bCwxLfkwBSjipubLV1ob7mFavjv49DQ1hKvmkSdmPmzIlBNi0aYXVEbb12j79aTjiiOzH1teH9fnuvLP1vtR9UupBiUhLXSMcMwpY4u7LAMxsJjAeeDntmPHAze7uwNNm1tvM+rt7AXf0VKdPfjJ8vvjiaDeuuocp5OedF6aUZ7P33vClL4WA+vjjwuq5di0sWABXX537/qWjj4bhw+Hyy8NU+HQLFoTP/fsXVh8RqTzmOZYtMLMJwBh3Pyf5+jTgUHe/MO2YPwE/dfe/JV8/CnzH3ee1KGsKoYcFMBxYXIQ29AXeLUI5nUU1tbea2gpqbyWrprZC+9s72N1bXcmO0oPK9Pdxy1SLcgzuPg0owgBT2onN5rl7XTHLjLNqam81tRXU3kpWTW2F4rU3yiSJJmBQ2uuBQCKPY0RERCKLElBzgWFmNtTMugMTgVktjpkFTE7O5jsMWK3rTyIiUoicQ3zuvtnMLgTmADXAdHdfZGZTk/sbgdnA8cAS4CPgzNJVuZWiDhl2AtXU3mpqK6i9laya2gpFam/OSRIiIiLloJUkREQklhRQIiISSwooERGJJQWUiIjEkgJKRERiSQElIiKxpIASEZFYUkCJiEgsKaBERCSWFFAiIhJLCigREYmlKM+DKom+ffv6kCFDCipj1apVAPTp06cINZI40c+2sunnK+nmz5//br4PLCyJIUOGMG/evNwHZjFjxgwAzjjjjMIrJLGin21l089X0pnZG5m2a4hPRERiKWdAmdl0M1thZgvb2G9mdqWZLTGzF83s4OJXU0REqk2UHtQMYEyW/WOBYcmPKUBD4dUSEZFqF+WJun8xsyFZDhkP3OzhyYdPm1lvM+uvR75LId5+G5YuhW9+s/W+ujp46CEw6/h6SeGamuCZZ2Dz5sw/30x69YKHH4ZPfSr3sQsWwLhx8PHHrff17g1//SsMHJi7nIcfhlNPhU2bWu/r3x+eegp22SV3OTNnwgUXwNatuY/N5cAD4dFHoUuErsVPfgK//GXh58zmX/8VZs0qXfnFmCQxAFie9ropua1VQJnZFEIvi7333rsIp5ZK9cEH4A6TJ2+/vakJ7rkHnngCRo8uR82kUI2NITwGDGj9823L9dfDb34D0yI8SPyKK2DdOjjzzO23b9kC11wD114L//3fucv5+c+hpgYmTdp++0cfhfrcemsInmzcQ1D07g1f+lLuc2bT3Ax33w2PPQZf+EL2Y9etg1/8Aj75STjiiMLOm82wYaUrG4oTUJn+js34HHl3n0byWfV1dXV61ry0acMG2GGH8KaUbv368MbW0KCA6ow2bgxv7l//enjzjDqJb906uO228KabrdeyciXceSdMmdL6dwfg9dfD+S+7DLp1a7ucf/wDHnkELr8cLr209f7nnw+/g+efn70n/+STsHBhOOfZZ7d9XBQffwyPPx7OmyugZs6E1avh17+GI48s7LzlVIxZfE3AoLTXA4FEEcqVKrZxI3Tv3np7r17hL+N77gnDgNK53HsvvPMO7LVX+76vvj70XG6+OftxN94YfnemTm27nLffDvXIprERunZtO1Tq62HRojBcmE1DQwjUiROzHxdFz55w1llw332hN9UW99BT/PSnS9t76gjFCKhZwOTkbL7DgNW6/iSF2rABevTIvG/q1HD94vrrO7ZOUrhrroGhQ2G33dr3fYccAiNHhjd8b2PsZevWECxHHdX2taoxY2Dw4FBOW9avhxkz4KSTYM89Mx8zcWIYtstWzooVcNddcPrpYTSgGM47LwxVZvvdnzs3XIerr+/812mjTDO/HXgKGG5mTWZ2tplNNbPU3yizgWXAEuA64PyS1Vaqwtq14T9hph4UhHHvL3whXI/YsqVj6yb5e/nlcO3wvPPy+/76enjlFfjLXzLvnzMHXnstHNeWmppw/j//OZSVyR13wPvvZy/nE58IwXP33aFHmMn06dl7c/morYXjjgu/+5kmb0AIzR12CMOonV3OgHL3Se7e3927uftAd7/B3RvdvTG53939AnevdfcD3L2w5SGk6iWSA8Rt9aAgvHksXw73398xdZLCNTaGPzrOOiu/7z/llOy9loYG2GOP0PPJ5uyzw/Wnxsa2y9l/f/jc57KXM3VqCInp01vv27IlTMYYPTqUVUz19eH/yB//2Hrfe++F609f/zrsvHNxz1sOWklCYicVUG31oABOOCFcx8g2xCLxsW4d3HQTTJgAu7dacS2aT3wiTKq4557WvZY33wx/rJx9dvbfGwghNmFCqM+6ddvvW7AAnn02hE+u4bH99oOjjw5B1LInP2dOmJCRrReWr3HjwjT5TL/7N90UJlOU4rzloICS2InSg+raFc49N7wRLFvWMfWS/N1+O6xZU/gbZ6rXcsMN22+fNi1cm5oyJVo59fVhltvMmdtvb2gIQRh1+nt9PbzxBjz4YOty+vWDE0+MVk57dO0a2vnII/Dqq9u2u4de4eGHw2c/W/zzloMCSmInNUMpW0BBCKguXcJfsBJf7uENuxizyoYPh89/fvteS2rq+vHHQ9QHJBx5ZJhIkd4LWb0afve7cN9T797RyjnxxDCRIr2cN94Ivblzzsndm8vXOeeEoEofpnzssTA9vlJ6T6CAkhhKJELw1NRkP27AgDDUN316mPUn8VTsWWX19WFI74EHwuvU1PX2vDGbhePnzw/1gzCF/aOP2ldOt24hLGbPDkN6EHpzZtF7c/no3z+E4403hlmHEEKyTx84+eTSnbejKaAkdhKJ3L2nlPp6ePfdMJ1X4qnYs8rGjw9v0Ndcs638wYPDFPL2OO20UK/U1PWGhjCV/ZBD2lfOlCkhkK69dltvbtw4KPViOfX1Ybbh738fRh3uvTfcI9izZ2nP25EUUBI77QmoY44JKxJoskQ8lWJWWarX8uCDoefy+ONh6niuHndLO+8c1tqbOTO8ub/ySn7DY4MGhWWMbrghTFFfsaJjhtmOPjoMeTY0hFDcsiX/KfxxpYCS2Glujj5236VLuHD+5JPw0kulrZe0X6lmlZ17bui1TJoUAivfZYTq68MQ2RlnhOtOp5ySfzkrV8KFF4YbkY87Lr9y2sMs/O4/80xYf/DYY8Mfa5WkbE/UFcnEvX09KAhvLpdeCt/7XmlmTRXb0UfDPvtEO/bpp8OSOoXq0iX8lR91iveDD2ZfTieqq68uzayyQYPgy18Oy/5MnBimjufjwAPhsMPCv/PFF4cZfPk49tjwM122LPweRlltvBhOPz2crxgzJONIASWx8v77YcJDe2Y/9ekTpgVfdx386U+lq1uxHHpoeEPMZfXqMIT50UfFOe/kyaFHk8vChTB2bHHOCWE171K4+OIwxPfv/15YOd/6VrgeVcgbfJcu4dEh3/9+/jci52PXXcP5Hnqo8NXS40gBJbES5R6oTBoa4L/+q/j1KbZbbw1/8T73HBx0UPZjU7PKHn44XGsoxA9/GFYDv+KKEOjZNDSEf/8FC2CnnQo7b/fu4X6gUhg9OoR4r16FlTNhQpjUUGg5F1wQhhoLLae9fvObsDZl1wp8N6/AJklnlhpWau/9IzU1Ydgn7urrwyMcGhqyP9soNats1Kjcj1aI4uKLw3T8GTNCj6Eta9fCLbfAv/0bjBhR+HlLrVhhUIxyzDo+nCD87rd3gkhnoUkSEiv59qA6i969w4X9224Lf/235Ykn8p9VlskBB4SbZBsbsz/Z9bbb4MMPK/N6hnQ+CiiJlSjr8HV2qWcb3XJL28c0NITrC/nOKmvrvEuWhEeGZ5LqtX32s2HigEi5KaAkVhKJ8KygjpoFVQ65nm309tthQdQzzijukNGECdC3b9v3jD39NLzwQmU8R0gqQwW/DUhn1Nzc/qetdkb19eH5SJmeyHrDDeGidzGfIwRh2PSss2DWrMxTyBsawqSIU08t7nlF8qWAklhJJKojoNp6ttGWLWHyxDHHwL77Fv+8550XrkFdd93221etCkvmnHYa7Lhj8c8rkg8FlMRKIhEWga10qWcbtXwi6+zZYSHUUk1S2GefsMrBdddt/0TWG28M959pcoTEiQJKYmPLlnD9pRp6UJD5iawNDWEh1BNOKN15Wz6RdevWMLvvyCPDIzFE4kIBJbGxYkUIqWoJqJbPNnrttbDE0LnnhvXlSmXcuHDPWGp48ZFHYOlS9Z4kfhRQEhupKebVElCw/RNZr702zF4899zSnrOmZvsnsjY0hDX6vvrV0p5XpL0iBZSZjTGzxWa2xMwuybB/tJmtNrPnkx+XFb+qUulSAVUN16BSUs82+vWvw+y9L38ZBg4s/XlTT2T9/vfDrL6zzqrcm6Ol88oZUGZWA1wNjAVGAJPMLNMiKH919wOTHz8ucj2lClRjDyr1bKNHHgkPXuyoYbY994SvfCXM3HOvvOcISWWI0oMaBSxx92XuvhGYCYwvbbWkGjU3hyGuUi0uGlfnnhvaXVtbnHX3okqF4Zgx4RlGInETZbHYAcDytNdNwKEZjjvczF4AEsB/uHurp9iY2RRgCsDepX4esnQ6iUQIp0pclTmbQYPgt78ND5vryBU0Ro+Gyy6Dk07quHOKtEeUt4JMi560XKBlATDY3dea2fHAvcCwVt/kPg2YBlBXV5dhkRepZtVyk24m55/f8ec0gx/9qOPPKxJVlL/XmoD0BxkMJPSS/snd17j72uTXs4FuZta3aLWUqlAtyxyJSDRRAmouMMzMhppZd2AiMCv9ADPb0ywsL2lmo5Llrip2ZaWyVXMPSkRayznE5+6bzexCYA5QA0x390VmNjW5vxGYANSb2WZgPTDRPdM6zSKZbdgQZrFV0xRzEcku0uXo5LDd7BbbGtO+vgq4qrhVk2ry9tvhs3pQIpKilSQkFlKPf1BAiUiKAkpioRpv0hWR7BRQEgvVuMyRiGSngJJYSCTCsj99+pS7JiISFwooiYXUPVCW6bZwEalKCiiJhWp5kq6IRKeAkljQTboi0pICSmJBASUiLSmgpOzWroU1axRQIrI9BZSUnaaYi0gmCigpO92kKyKZKKCk7BRQIpKJAkrKTuvwiUgmCigpu0QCdtwRdt653DURkThRQEnZaYq5iGSigJKyU0CJSCYKKCm75mZNMReR1hRQUlbu6kGJSGYKKCmr99+HDRsUUCLSmgJKykr3QIlIWyIFlJmNMbPFZrbEzC7JsN/M7Mrk/hfN7ODiV1UqUeoeKF2DEpGWcgaUmdUAVwNjgRHAJDMb0eKwscCw5McUoKHI9ZQKpR6UiLSla4RjRgFL3H0ZgJnNBMYDL6cdMx642d0deNrMeptZf3d/q+g1TvPSS7BlC4weXcqzSCktXx4+9+9f3nqISPxECagBwPK0103AoRGOGQBsF1BmNoXQwwJYa2aL21XbzPrCme8WoZzOoi9Qce3t1Svj5r5nnqmfbQWrpp9v1f1saV97B2faGCWgLMM2z+MY3H0aMC3COSMzs3nuXlfMMuOsmtpbTW0FtbeSVVNboXjtjTJJogkYlPZ6IJDI4xgREZHIogTUXGCYmQ01s+7ARGBWi2NmAZOTs/kOA1aX+vqTiIhUtpxDfO6+2cwuBOYANcB0d19kZlOT+xuB2cDxwBLgI+DM0lW5laIOGXYC1dTeamorqL2VrJraCkVqr4WJdyIiIvGilSRERCSWFFAiIhJLCigREYklBZSIiMSSAkpERGJJASUiIrGkgBIRkVhSQImISCwpoEREJJYUUCIiEksKKBERiaUoz4Mqib59+/qQIUMKKmPVqlUA9OnTpwg1kjjRz7ay6ecr6ebPn/+uu+/ecnvZAmrIkCHMmzevoDJmzJgBwBlnnFF4hSRW9LOtbPr5SjozeyPT9pxDfGY23cxWmNnCNvabmV1pZkvM7EUzO7jQyoqIiES5BjUDGJNl/1hgWPJjCtBQeLVERKTaRXlg4V/MbEiWQ8YDN3t4sNTTZtbbzPrriboi0pZNm2DLFli2LNrxO+wA/fpFL//tt+Gjj1pv33ln6Ns3ejnNzbBhQ+vtu+0GvXtHK8Mdli+HzZujn7ct/fqFf4sotm6F118v/JzZ9OwJe+1VuvKLcQ1qALA87XVTcpsCSkRaefNN+L//C1/X10f7HjO47jo4++zcx159NVx4YeZ9XbvCXXfB+PG5y/nBD+DHP868r2dPeOQROOKI7GW4w9SpMK1Iz9PdYw94+mkYOjT7cZs2wfHHhzqW0tFHw2OPla78YgSUZdiW8TG9ZjaFMAzI3nvvXYRTi0hns3hx+Dx4MNx0U7TvuemmEGbDh8ORR7Z93KOPwkUXwZgxMGlS6/1XXglf/zo89RR8+tNtl3PHHSGcTjklvNGnc4ef/AROOgnmzoVsb2W//W0IpylTcodZLhs3wn/+J5xwQgj4nXZq+9iLLw7hdNllUFtb2Hmz2XPP0pUNxQmoJmBQ2uuBQCLTge4+jeSz6uvq6vSseZEqlEi+O/TrB5MnR/ueL38ZDj00hMK8eZlDYelSOPlk2G8/+P3vM7+BH3MM1NWFN/m5cyHTLPf58+HMM0MQ3nwzdO/e+phRo+Cww0JP7G9/yzzs9vDD8M1vhmMaGqBLEe46HTwYxo4N/2533525zMZGuOaaEGY/+lHh5yynYtyoOwuYnJzNdxiwWtefRKQtqYDq0SP69+y6K8yaFa4HnXACrFu3/f41a0KImYXj2updDBgA994b6nDyyWEoLN3bb8OJJ4brVHffnTmcAPbfH26/HV54IYSZt/hz+9VXQ+9rxAi45ZbihBPAF78IV1wR2vCDH7Te//jj8I1vhBD73/8tzjnLKco089uBp4DhZtZkZmeb2VQzm5o8ZDawDFgCXAecX7Laikin19wcrgW19017v/1g5kx48UU4/fQwCQDCZIuvfQ3+8Y9wfWmffbKXc+ihYdjtz38OPZyUDRvgK1+B994LIbfHHtnLOf54+NnP4M474fLLt21fvTr0mrp0yR6W+frGN8K1uMsvD0ORKa+9BhMmhCG922+HmprinrccosziyzCSu91+By4oWo1EpKIlEtC/f37fO3Ys/PznYfjq8svDNZbvfx/uvz9Mjjj66GjlTJ4ML70Ev/wlHHBAuEY0dWqYgPD738OBB0Yr5z/+IwTmZZeFa1onnACnnhp6UA8/nHsyQz7MQlv//vfQexs2LHyccEII61mzYJddin/ecijbShIiUp0SCShklbNvfSuEwg9+AE1NYXbfeedFnxGY8tOfwsKFYcbfs8/CjBkhaE4+OXoZqdmF//gHnHZaGB68//5wDWj06PbVpz169AhDkCNHht7aZz4DL78MDzwA++5buvN2NC0WKyIdKpFo3/WnlszCEN2oUSEcjjoqzM6zTPOJs6ipCUNhtbUwfXoY3st0XSeXnj3hD38I91jddlvoibU3LPPRrx/cdx+sWgWzZ8OvfgXHHlv683Yk9aBEpMNs3QpvvdX25IOoevYMEwV++9twHSnf8nr3Dj2e6dPhu9/NfzLDXnvBgw/CPffA976XXxn5OOgg+OMfw3DlRRd13Hk7igJKRDrMypVhRYVCAwrCdaz/+Z/Cy6mtDfc1FeoznwkfHe2YY8JHJdIQn4h0mHymmEv1UkCJSIdpbg6fFVAShQJKRDpMqgdVjCE+qXwKKBHpMIlEmG2ngJIoFFAi0mESibBCQ3unhEt1UkCJSIdpbg7r4YlEoYASkQ6TSJT2AXdSWRRQItJhFFDSHgooEekQmzbBihUa4pPoFFAi0iHeSj4lTj0oiUoBJSIdInUPlAJKolJAiUiHUEBJeymgRKRDpJY50jUoiUoBJSIdIpGAbt2gT59y10Q6CwWUiHSI1KPe833mklQf/aqISIfQPVDSXgooEekQWuZI2itSQJnZGDNbbGZLzOySDPtHm9lqM3s++XFZ8asqIp2ZelDSXjkf+W5mNcDVwBeBJmCumc1y95dbHPpXd/9SCeooIp3cunWwerUCStonSg9qFLDE3Ze5+0ZgJjC+tNUSkUqSWkVCQ3zSHlECagCwPO11U3JbS4eb2Qtm9oCZfSpTQWY2xczmmdm8lStX5lFdEemMUvdAqQcl7REloDI9WsxbvF4ADHb3zwK/Be7NVJC7T3P3Onev23333dtVURHpvLSKhOQjSkA1AYPSXg8EEukHuPsad1+b/Ho20M3M+hatliLSqSmgJB9RAmouMMzMhppZd2AiMCv9ADPb0yw8xNnMRiXLXVXsyopI59TcDDvsADvvXO6aSGeScxafu282swuBOUANMN3dF5nZ1OT+RmACUG9mm4H1wER3bzkMKCJVKjXF3DJdMBBpQ86Agn8O281usa0x7eurgKuKWzURqRS6B0ryoZUkRKTkFFCSDwWUiJSUu5Y5kvwooESkpD74AD7+WD0oaT8FlIiUlKaYS74UUCJSUqmA0hCftJcCSkRKSsscSb4UUCJSUqkeVP/+5a2HdD4KKBEpqUQCdt0VevUqd02ks1FAiUhJaYq55EsBJSIlpZt0JV8KKBEpKQWU5EsBJSIls3VreJquAkryoYASkZJZsQK2bNE1KMmPAkpESkarSEghFFAiUjIKKCmEAkpESia1ioSG+CQfCigRKZlEIjxFt1+/ctdEOiMFlIiUTCIRwqlrpGd3i2xPASUiJaN7oKQQCigRKRktcySFiBRQZjbGzBab2RIzuyTDfjOzK5P7XzSzg4tfVRHpbNSDkkLkDCgzqwGuBsYCI4BJZjaixWFjgWHJjylAQ5HrKSKdzMaNsHKlAkryF+XS5ShgibsvAzCzmcB44OW0Y8YDN7u7A0+bWW8z6+/ubxW9xmmeeircpf6Nb5TyLFIOp5wSPutn23m5h88a4pN8mad+i9o6wGwCMMbdz0m+Pg041N0vTDvmT8BP3f1vydePAt9x93ktyppC6GEBDAcWF6ENfYF3i1BOZ1FN7a2mtoLaW8mqqa3Q/vYOdvfdW26M0oOyDNtaplqUY3D3acC0COeMzMzmuXtdMcuMs2pqbzW1FdTeSlZNbYXitTfKJIkmYFDa64FAIo9jREREIosSUHOBYWY21My6AxOBWS2OmQVMTs7mOwxYXerrTyIiUtlyDvG5+2YzuxCYA9QA0919kZlNTe5vBGYDxwNLgI+AM0tX5VaKOmTYCVRTe6upraD2VrJqaisUqb05J0mIiIiUg1aSEBGRWFJAiYhILHXagMq1/FJnZGbTzWyFmS1M27abmT1sZq8mP++atu+7yfYvNrPjylPr/JjZIDP7s5m9YmaLzOyi5PZKbW9PM3vWzF5ItvdHye0V2V4Iq9CY2XPJ+yQruq0AZva6mb1kZs+b2bzktopsc3IxhrvM7O/J/8OHl6St7t7pPgiTNZYC+wDdgReAEeWuVxHadRRwMLAwbdvPgUuSX18C/Cz59Yhku3sAQ5P/HjXlbkM72tofODj59U7AP5JtqtT2GrBj8utuwDPAYZXa3mQb/h/wO+BPydcV29ZkO14H+rbYVpFtBm4Czkl+3R3oXYq2dtYe1D+XX3L3jUBq+aVOzd3/ArzXYvN4wi8Dyc8npm2f6e4b3P01wgzKUR1Rz2Jw97fcfUHy6w+BV4ABVG573d3XJl92S344FdpeMxsIjAOuT9tckW3NoeLabGY7E/6YvgHA3Te6+weUoK2dNaAGAMvTXjclt1Wifp68pyz5eY/k9or5NzCzIcBBhF5FxbY3OeT1PLACeNjdK7m9vwa+DWxN21apbU1x4CEzm59c1g0qs837ACuBG5NDuNeb2Q6UoK2dNaAiLa1U4Sri38DMdgTuBi529zXZDs2wrVO11923uPuBhJVWRpnZp7Mc3mnba2ZfAla4+/yo35JhW6doawtHuPvBhKc7XGBmR2U5tjO3uSvhUkSDux8ErCMM6bUl77Z21oCqpqWV3jGz/gDJzyuS2zv9v4GZdSOE023ufk9yc8W2NyU5HPI4MIbKbO8RwAlm9jph+P3zZnYrldnWf3L3RPLzCuAPhGGsSmxzE9CUHAEAuIsQWEVva2cNqCjLL1WKWcDpya9PB+5L2z7RzHqY2VDCs7ieLUP98mJmRhjDfsXdr0jbVant3d3Meie/7gV8Afg7Fdhed/+uuw909yGE/5uPufvXqcC2ppjZDma2U+pr4FhgIRXYZnd/G1huZsOTm44hPH6p+G0t92yQAmaRHE+Y+bUUuLTc9SlSm24H3gI2Ef7qOBvoAzwKvJr8vFva8Zcm278YGFvu+rezrUcSuvkvAs8nP46v4PZ+Bngu2d6FwGXJ7RXZ3rQ2jGbbLL6KbSvhuswLyY9FqfekSm0zcCAwL/n7fC+waynaqqWOREQkljrrEJ+IiFQ4BZSIiMSSAkpERGJJASUiIrGkgBIRkVhSQImISCwpoEREJJb+PyBfxASFEYWCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_C = torch.cat(hist_losses_C, dim=2)\n",
    "hist_hits_C = torch.cat(hist_hitsss_C, dim=2)\n",
    "\n",
    "plotResults(hist_loss_C, hist_hits_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6K7g1tH3vnWa",
    "outputId": "25d75c5a-f339-4071-8aad-0f4df6f05ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkGxECJqvnWb"
   },
   "source": [
    "## Transfer D: EWC (unfunctional)\n",
    "\n",
    "1. Create Fisher functions\n",
    "2. Train model on tasks\n",
    "3. Compare results\n",
    "\n",
    "Based on: https://github.com/ContinualAI/colab/blob/master/notebooks/intro_to_continual_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNS4W04AvnWc"
   },
   "source": [
    "Compute optimal parameters and fisher information after training on tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI4QzSeavnWc"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_ewc(model, task_id, train_dl, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #accumulate Gradient\n",
    "    for it in range(100):\n",
    "        for seq, seq_len in train_dl:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0)\n",
    "\n",
    "            if criterion == F.cross_entropy:\n",
    "              output_dim = output.shape[-1]\n",
    "                \n",
    "              output = output[1:].view(-1, output_dim)\n",
    "\n",
    "              trg = seq[1:].view(-1)\n",
    "\n",
    "              loss = criterion(output, trg)\n",
    "            else:\n",
    "              loss = criterion(output, seq)\n",
    "            #print(loss)\n",
    "\n",
    "            loss.backward()\n",
    "        \n",
    "    fishers.append({})\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        fishers[task_id][name] = param.grad.data.clone().pow(2)\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJu-KM7mvnWc"
   },
   "source": [
    "Adapt evaluation and training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRW-TlhwvnWd"
   },
   "outputs": [],
   "source": [
    "def train_ewc(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(\"-\\n\", loss)\n",
    "        \n",
    "        # EWC Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                fisher = fishers[other_task_id][name]\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                #print(ewc_lambda)\n",
    "                loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                #print((fisher * (optParam - param).pow(2)).sum())\n",
    "                #print((optParam - param).pow(2).sum())\n",
    "                #loss += ewc_lambda * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        if task_id > 0:\n",
    "            print(loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnQbtl1yvnWe"
   },
   "outputs": [],
   "source": [
    "def evaluate_ewc(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # EWC Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    fisher = fishers[other_task_id][name]\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += (ewc_lambda / 2) * (fisher * (optParam - param).pow(2)).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgGMwiHcvnWe"
   },
   "outputs": [],
   "source": [
    "def fit_ewc(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_ewc(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_ewc(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_ewc(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_ewc(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_ewc(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NI--0ZFvnWe"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bB0VOa5lvnWf",
    "outputId": "cfd946be-3342-47d9-a885-90fb65f024f9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1112, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1282, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1422, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1009, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1829, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2079, grad_fn=<AddBackward0>)\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0826, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1142, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1186, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1145, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1275, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1264, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1391, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1517, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1385, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1511, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1339, grad_fn=<AddBackward0>)\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.2965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1228, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1162, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1303, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1152, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1298, grad_fn=<AddBackward0>)\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1306, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1457, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2151, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2300, grad_fn=<AddBackward0>)\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1167, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1315, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1146, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2175, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1107, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1256, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1189, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3172, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3321, grad_fn=<AddBackward0>)\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1111, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2133, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1113, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1131, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1285, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1082, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1157, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1317, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1101, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1240, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3278, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3444, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1205, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1088, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1260, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1217, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1146, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1379, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1557, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1368, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1546, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1193, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1245, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2326, grad_fn=<AddBackward0>)\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2543, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2703, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1196, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1361, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1592, grad_fn=<AddBackward0>)\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1610, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1140, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0984, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1218, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1472, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1120, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1283, grad_fn=<AddBackward0>)\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.1032, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1194, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1286, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1150, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1199, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1310, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1318, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1477, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1253, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1411, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1187, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1345, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1036, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2618, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2772, grad_fn=<AddBackward0>)\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1200, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1105, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1043, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1284, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1431, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1154, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1065, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1314, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1461, grad_fn=<AddBackward0>)\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1154, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1300, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1524, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1669, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1488, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1632, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1057, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1153, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1014, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.1235, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1379, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1386, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1148, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0945, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1082, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1210, grad_fn=<AddBackward0>)\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1168, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2639, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2778, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0972, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1286, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1419, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1130, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0715, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.1074, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1198, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1144, grad_fn=<AddBackward0>)\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0710, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0826, grad_fn=<AddBackward0>)\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0965, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1187, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1155, grad_fn=<AddBackward0>)\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0963, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1083, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0697, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0868, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0952, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0856, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1047, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0906, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1019, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1067, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1066, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2002, grad_fn=<AddBackward0>)\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1006, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0858, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3459, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1389, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1514, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1031, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0938, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1178, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1060, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1110, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1248, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1234, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0923, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1073, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1090, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1013, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1158, grad_fn=<AddBackward0>)\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1089, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0802, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1117, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1258, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1239, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1122, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1255, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1353, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1478, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1041, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1079, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1024, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1091, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1215, grad_fn=<AddBackward0>)\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1179, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1299, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0920, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1072, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0830, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1879, grad_fn=<AddBackward0>)\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.1426, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1551, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2227, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1103, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1241, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1212, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1272, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1221, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1364, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1007, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0992, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1323, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1464, grad_fn=<AddBackward0>)\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1513, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1658, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1164, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1316, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1349, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1100, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1384, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1367, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0810, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1294, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1449, grad_fn=<AddBackward0>)\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1254, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1564, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1716, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1096, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1463, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1618, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "-\n",
      " tensor(0.2395, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1288, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1223, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1237, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0939, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1160, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1093, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1265, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1131, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1156, grad_fn=<AddBackward0>)\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "-\n",
      " tensor(0.1188, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1102, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1297, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1470, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1308, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1482, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1238, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1127, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1308, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1166, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0959, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1072, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0968, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0811, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1008, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0967, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1022, grad_fn=<AddBackward0>)\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1251, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1132, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1103, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0891, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1046, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1062, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0768, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0976, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1115, grad_fn=<AddBackward0>)\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1088, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1149, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1282, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0914, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1126, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1261, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1196, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0864, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1061, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1014, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0909, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1059, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0969, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0957, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1083, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0998, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1121, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1048, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1167, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0689, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0836, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0892, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0990, grad_fn=<AddBackward0>)\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0747, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0723, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0823, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0938, grad_fn=<AddBackward0>)\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0851, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1142, grad_fn=<AddBackward0>)\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0681, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1129, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0885, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0782, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0963, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0695, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0807, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0903, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1039, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0882, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0969, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0843, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0847, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0862, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0895, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0860, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0970, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0824, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0949, grad_fn=<AddBackward0>)\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0988, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0692, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0725, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1095, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0667, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0985, grad_fn=<AddBackward0>)\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1044, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0954, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0837, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0999, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0840, grad_fn=<AddBackward0>)\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0746, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0854, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0820, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2095, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0727, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0842, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0942, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0841, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1405, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1522, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1144, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1259, grad_fn=<AddBackward0>)\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1058, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1366, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1205, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1337, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1173, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1202, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1113, grad_fn=<AddBackward0>)\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1027, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1184, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1024, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1092, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1249, grad_fn=<AddBackward0>)\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1253, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0876, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0879, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1181, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1323, grad_fn=<AddBackward0>)\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0915, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1124, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1263, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0987, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1124, grad_fn=<AddBackward0>)\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0997, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0884, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0781, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0868, grad_fn=<AddBackward0>)\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1033, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0835, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0852, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0903, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0943, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0688, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0743, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0866, grad_fn=<AddBackward0>)\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0830, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0855, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0696, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0815, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0752, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0870, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0852, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0827, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0685, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0802, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0822, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1011, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0803, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0693, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0808, grad_fn=<AddBackward0>)\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0813, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0864, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0872, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0869, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0709, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0823, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0845, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0872, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0712, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0859, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0822, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1009, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0749, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0902, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0989, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0960, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1075, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0957, grad_fn=<AddBackward0>)\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0925, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0811, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0898, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0954, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0774, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0703, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0750, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0863, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0680, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0791, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1026, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0684, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0889, grad_fn=<AddBackward0>)\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0832, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0772, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0838, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0729, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0839, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0719, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0828, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1052, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0736, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0846, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0871, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0795, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0924, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0842, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0828, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0686, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0795, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0796, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0887, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0738, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0978, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0757, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0869, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0861, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0817, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0831, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0739, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0850, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1232, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1343, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1888, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0876, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0809, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1075, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1195, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1035, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1284, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1070, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1207, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0779, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0917, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1059, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1079, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0992, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1163, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1189, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1330, grad_fn=<AddBackward0>)\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0791, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1068, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1208, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1006, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2473, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0940, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1076, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0956, grad_fn=<AddBackward0>)\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0812, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0953, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2180, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2321, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0978, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.4974, grad_fn=<MeanBackward0>)\n",
      "tensor(0.5117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1094, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1097, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0986, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0956, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1117, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2505, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1074, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1071, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0849, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1010, grad_fn=<AddBackward0>)\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1374, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1534, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0986, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0900, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1060, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0836, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0912, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1073, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1225, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1385, grad_fn=<AddBackward0>)\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.0946, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1105, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0953, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1108, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0914, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0933, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0855, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0919, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1226, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1370, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0898, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1031, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0770, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1003, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1145, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1035, grad_fn=<AddBackward0>)\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0718, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0861, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1172, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0787, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0907, grad_fn=<AddBackward0>)\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0827, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0974, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1190, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0909, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0735, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0759, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2361, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2506, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0737, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0819, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0964, grad_fn=<AddBackward0>)\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0960, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0775, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1036, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1487, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1135, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0815, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0916, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1037, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0901, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0983, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0993, grad_fn=<AddBackward0>)\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0939, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1050, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2765, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0951, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.2020, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2165, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1015, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0722, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0865, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1091, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0965, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1109, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1160, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1102, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1247, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2069, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1134, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0951, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1094, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1244, grad_fn=<AddBackward0>)\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1433, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1584, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0848, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1055, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1219, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1483, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1649, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0961, grad_fn=<AddBackward0>)\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0793, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0962, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1048, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1061, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1233, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0882, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1053, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1004, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1943, grad_fn=<AddBackward0>)\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-\n",
      " tensor(0.1054, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1222, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1318, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1008, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2344, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2513, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1002, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0839, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1011, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1442, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1615, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1928, grad_fn=<AddBackward0>)\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0983, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1536, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1715, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1547, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0927, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1076, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1262, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1081, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1269, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-\n",
      " tensor(0.1030, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1220, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0923, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1114, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2137, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2329, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1077, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0853, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1045, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1217, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1408, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1325, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1038, grad_fn=<AddBackward0>)\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1460, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0977, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1192, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1374, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1052, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1175, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3141, grad_fn=<AddBackward0>)\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1047, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1111, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1040, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1340, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0941, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1127, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1038, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1224, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1397, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1585, grad_fn=<AddBackward0>)\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1122, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1140, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0846, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1293, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1350, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1543, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1126, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0950, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0994, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1183, grad_fn=<AddBackward0>)\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0911, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1101, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1086, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1277, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1116, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1139, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0896, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2191, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2381, grad_fn=<AddBackward0>)\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1098, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1042, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1232, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1200, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1389, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2116, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2304, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1080, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1085, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "-\n",
      " tensor(0.1044, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1229, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0982, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1169, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1173, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1360, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1928, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2113, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0988, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1170, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1118, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1373, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1554, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2714, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1112, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0816, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1409, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1601, grad_fn=<AddBackward0>)\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.2625, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2818, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1331, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1071, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1248, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1141, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1125, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1335, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1578, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1656, grad_fn=<AddBackward0>)\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1214, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2545, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2758, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1275, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1488, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1216, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1498, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1712, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1203, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1418, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1530, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1746, grad_fn=<AddBackward0>)\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-\n",
      " tensor(0.1233, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1450, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1077, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1570, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1786, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0985, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1201, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1202, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1416, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2362, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0922, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1133, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0913, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1125, grad_fn=<AddBackward0>)\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.0997, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1211, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1943, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2157, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1023, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1236, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1250, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1462, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.3139, grad_fn=<MeanBackward0>)\n",
      "tensor(0.3351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2391, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1224, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1436, grad_fn=<AddBackward0>)\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "-\n",
      " tensor(0.1698, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1910, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1399, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1611, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1151, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1207, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1424, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1704, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1922, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1583, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1018, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1237, grad_fn=<AddBackward0>)\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "-\n",
      " tensor(0.1396, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1616, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1235, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1359, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1581, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1875, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1063, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1197, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1093, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1104, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1327, grad_fn=<AddBackward0>)\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1188, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1378, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1143, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1365, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1081, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1161, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0843, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1066, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1159, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1104, grad_fn=<AddBackward0>)\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1213, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1099, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1199, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1092, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1138, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1084, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1295, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0976, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0821, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1017, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1121, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1313, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0753, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0944, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1193, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1383, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1149, grad_fn=<AddBackward0>)\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0950, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0881, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1065, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0820, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0835, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1018, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0936, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1020, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0979, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1017, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0958, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0975, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0716, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0857, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-\n",
      " tensor(0.1001, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1171, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1120, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0814, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0980, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1158, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0889, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0990, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1152, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1034, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0886, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0734, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0891, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0760, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0916, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0931, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0897, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1049, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1086, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0908, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1056, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1279, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1427, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.2341, grad_fn=<MeanBackward0>)\n",
      "tensor(0.2489, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0700, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0848, grad_fn=<AddBackward0>)\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0880, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0850, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0993, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1143, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1026, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0873, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1027, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1032, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1307, grad_fn=<AddBackward0>)\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0994, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1192, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0858, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1007, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1162, grad_fn=<AddBackward0>)\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1136, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1033, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0874, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1029, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1252, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0844, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0996, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0841, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0989, grad_fn=<AddBackward0>)\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0946, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0754, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0902, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0674, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0821, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1012, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1040, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0924, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0834, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0981, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0980, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1128, grad_fn=<AddBackward0>)\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0958, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1110, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0908, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0832, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0984, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0921, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0847, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0995, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1080, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0767, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0844, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0763, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0856, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0999, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0901, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1043, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0966, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0875, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1016, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0732, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0874, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0776, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0918, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1050, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1191, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1123, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0786, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0897, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0906, grad_fn=<AddBackward0>)\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-\n",
      " tensor(0.0966, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1107, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0865, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1005, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0806, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0945, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0711, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0740, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0877, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0941, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0713, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0849, grad_fn=<AddBackward0>)\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0794, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0837, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0973, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0806, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0900, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0866, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-\n",
      " tensor(0.0962, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1097, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0867, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1003, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0721, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0857, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0991, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0937, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0840, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0741, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0878, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0790, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0927, grad_fn=<AddBackward0>)\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0731, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0867, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0751, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1215, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1351, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0894, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0724, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0860, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0877, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1013, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0796, grad_fn=<AddBackward0>)\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-\n",
      " tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0853, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0748, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0883, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0825, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1002, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1137, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0778, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0912, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0935, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0799, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0934, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1223, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1358, grad_fn=<AddBackward0>)\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-\n",
      " tensor(0.0652, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1444, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1579, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1068, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0669, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0803, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0968, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1029, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1164, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1256, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1392, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0744, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0881, grad_fn=<AddBackward0>)\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "-\n",
      " tensor(0.0800, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0833, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0975, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1178, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1322, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0972, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0851, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1000, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0870, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1021, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0801, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0955, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0773, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0928, grad_fn=<AddBackward0>)\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.1264, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1421, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0892, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1022, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1181, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0926, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1087, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0824, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0987, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1016, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1180, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0890, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1057, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0886, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "-\n",
      " tensor(0.0949, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1119, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1005, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1176, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1270, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.1322, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1493, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0871, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1041, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1090, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0758, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0792, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0959, grad_fn=<AddBackward0>)\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-\n",
      " tensor(0.1037, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1204, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0885, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1051, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0926, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0807, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0971, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0728, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0890, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0862, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1023, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0817, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0977, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0919, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1078, grad_fn=<AddBackward0>)\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0911, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0884, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1039, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0789, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0942, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0777, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0930, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0904, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1055, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0878, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1028, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0932, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0818, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0967, grad_fn=<AddBackward0>)\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-\n",
      " tensor(0.1577, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1725, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0726, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0873, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0733, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0879, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0854, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0998, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0838, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0982, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0905, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0785, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0929, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0887, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1030, grad_fn=<AddBackward0>)\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-\n",
      " tensor(0.0804, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0745, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0888, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0797, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0940, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0859, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1001, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0883, grad_fn=<MeanBackward0>)\n",
      "tensor(0.1025, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0771, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0913, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0755, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0896, grad_fn=<AddBackward0>)\n",
      "-\n",
      " tensor(0.0762, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0904, grad_fn=<AddBackward0>)\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n"
     ]
    }
   ],
   "source": [
    "fishers = []\n",
    "optParams = []\n",
    "ewc_lambda = 10\n",
    "\n",
    "models_D = []\n",
    "hist_losses_D = []\n",
    "hist_hitsss_D = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_ewc(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D.append(hist_loss_temp)\n",
    "    hist_hitsss_D.append(hist_hits_temp)\n",
    "    models_D.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_ewc(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYEBTL-bo_Tr"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QY_YzfCoVkI"
   },
   "source": [
    "L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TVqno83vnWh"
   },
   "outputs": [],
   "source": [
    "hist_loss_D = torch.cat(hist_losses_D, dim=2)\n",
    "hist_hits_D = torch.cat(hist_hitsss_D, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D, hist_hits_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0k0Kp66cvnWh"
   },
   "outputs": [],
   "source": [
    "accuracyAll(models_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17U8oYfQTlma"
   },
   "outputs": [],
   "source": [
    "torch.max(fishers[0]['encoder.embedding.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DnwXlOTXsbM"
   },
   "source": [
    "## Transfer D2: L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iatypwWYytb"
   },
   "source": [
    "### onTaskUpdate_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wzf8QezfX4rt"
   },
   "outputs": [],
   "source": [
    "def onTaskUpdate_l2reg(model, task_id, train_dl, criterion):\n",
    "    # Save optimal parameters for each task\n",
    "    optParams.append({})\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        optParams[task_id][name] = param.data.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-GTTipSYufm"
   },
   "source": [
    "### train_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfX9ClLaYNpv"
   },
   "outputs": [],
   "source": [
    "def train_l2reg(model, task_id, dataloader, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seq, seq_len in dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(seq, seq_len, seq)\n",
    "        loss = criterion(output, seq)\n",
    "        \n",
    "        # L2 Training penalty\n",
    "        for other_task_id in range(task_id):\n",
    "            for name, param in model.named_parameters():\n",
    "                optParam = optParams[other_task_id][name]\n",
    "                loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGgyR2yZYDfS"
   },
   "source": [
    "### eval_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RsqsxlvY86I"
   },
   "outputs": [],
   "source": [
    "def evaluate_l2reg(model, task_id, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for seq, seq_len in dataloader:\n",
    "\n",
    "            output = model(seq, seq_len, seq, 0) #turn off teacher forcing\n",
    "\n",
    "            loss = criterion(output, seq).type(torch.float)\n",
    "            \n",
    "            # L2 Training penalty\n",
    "            for other_task_id in range(task_id):\n",
    "                for name, param in model.named_parameters():\n",
    "                    optParam = optParams[other_task_id][name]\n",
    "                    loss += LAMBDA_L2REG * (optParam - param).pow(2).sum()\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfwySP1EZNc_"
   },
   "source": [
    "### fit_l2reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TIQ9dV7WZPX6"
   },
   "outputs": [],
   "source": [
    "def fit_l2reg(model, task_id, epochs, step_size_evaluation, clip ):\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((N_TASKS + 1, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        train_loss = train_l2reg(model, task_id, train_dls[task_id], optimizer, criterion, clip)\n",
    "        valid_loss = evaluate_l2reg(model, task_id, valid_dls[task_id], criterion)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            for other_id in range(task_id + 1):\n",
    "                total_loss[other_id,0,idx] = evaluate_l2reg(model, task_id, train_dls[other_id], criterion)\n",
    "                total_loss[other_id,1,idx] = evaluate_l2reg(model, task_id, test_dls[other_id], criterion)\n",
    "                total_loss[other_id,2,idx] = evaluate_l2reg(model, task_id, test_ugr_dls[other_id], criterion)\n",
    "                total_hits[other_id,0,idx] = evaluate_extra(model, train_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,1,idx] = evaluate_extra(model, test_dls[other_id], allOrNoneLoss)\n",
    "                total_hits[other_id,2,idx] = evaluate_extra(model, test_ugr_dls[other_id], allOrNoneLoss)\n",
    "\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWITsrZDZz1z"
   },
   "source": [
    "### Experiment L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bki7JTypZT9E"
   },
   "outputs": [],
   "source": [
    "LAMBDA_L2REG = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08h_J3oUZ5-x"
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O7MObEkiZ6pa",
    "outputId": "ab9a9f0e-bb75-472d-ea4b-7dbb17755167",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(30, 10, bidirectional=True)\n",
      "    (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "      (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(8, 30)\n",
      "    (rnn): GRU(50, 10)\n",
      "    (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "    (dropout): Dropout(p=0.7, inplace=False)\n",
      "  )\n",
      ")\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.796\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.452 |  Val. PPL:   1.571\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.507\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.415 |  Val. PPL:   1.515\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.533\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.503\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.450\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.352\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.348\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.266\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.218\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.215\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.138\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.148 |  Val. PPL:   1.159\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.117\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.116\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.158 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.596 | Train PPL:   1.814\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.613\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.581\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.478 | Train PPL:   1.612\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.460 |  Val. PPL:   1.583\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.577\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.469 | Train PPL:   1.598\n",
      "\t Val. Loss: 0.501 |  Val. PPL:   1.651\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.478 |  Val. PPL:   1.613\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.569\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.492 |  Val. PPL:   1.635\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.604\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.487\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.577\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.579\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.618\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.545\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.463 |  Val. PPL:   1.589\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.521 |  Val. PPL:   1.683\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.467 |  Val. PPL:   1.595\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.530\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.483 |  Val. PPL:   1.621\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.596\n",
      "\t Val. Loss: 0.498 |  Val. PPL:   1.646\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.596\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.479 |  Val. PPL:   1.614\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.444 | Train PPL:   1.559\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.509 |  Val. PPL:   1.663\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.457 |  Val. PPL:   1.580\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.476 |  Val. PPL:   1.610\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.605\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.497 |  Val. PPL:   1.644\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.450 | Train PPL:   1.568\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.568\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.561\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.614\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.588\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.561\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.504\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.548\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.550\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.541\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.563\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.476 | Train PPL:   1.609\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.556\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.558\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.538\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.499 |  Val. PPL:   1.647\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.607\n",
      "\t Val. Loss: 0.456 |  Val. PPL:   1.578\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.450 |  Val. PPL:   1.569\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.563\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.558\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.515\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.508\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.518\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.486 |  Val. PPL:   1.626\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.472 | Train PPL:   1.604\n",
      "\t Val. Loss: 0.455 |  Val. PPL:   1.577\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.435 |  Val. PPL:   1.545\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.435 | Train PPL:   1.544\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.541\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.479\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.563\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.482\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.442 | Train PPL:   1.556\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.508\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.463 | Train PPL:   1.589\n",
      "\t Val. Loss: 0.505 |  Val. PPL:   1.657\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.461 | Train PPL:   1.586\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.408 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.552\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.550\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.449 | Train PPL:   1.566\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.535\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.530\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.447 | Train PPL:   1.564\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.547\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.448 | Train PPL:   1.565\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.467 | Train PPL:   1.595\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.524\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.440 |  Val. PPL:   1.553\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.434 | Train PPL:   1.543\n",
      "\t Val. Loss: 0.441 |  Val. PPL:   1.554\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.451 | Train PPL:   1.570\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.391 |  Val. PPL:   1.478\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.528\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.521\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.532\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.496\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.524\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.611\n",
      "\t Val. Loss: 0.410 |  Val. PPL:   1.506\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.426 | Train PPL:   1.531\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.419 |  Val. PPL:   1.520\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.575\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.411 | Train PPL:   1.508\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
      "\t Val. Loss: 0.434 |  Val. PPL:   1.543\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.429 | Train PPL:   1.536\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.425\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.437 |  Val. PPL:   1.548\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.528\n",
      "\t Val. Loss: 0.413 |  Val. PPL:   1.511\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.525\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.506\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.479\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.386 | Train PPL:   1.471\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.526\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.521\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.482 |  Val. PPL:   1.619\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.580\n",
      "\t Val. Loss: 0.494 |  Val. PPL:   1.639\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.487 | Train PPL:   1.628\n",
      "\t Val. Loss: 0.491 |  Val. PPL:   1.635\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.562\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.546\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.393 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.469 |  Val. PPL:   1.599\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.453 |  Val. PPL:   1.573\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.422 | Train PPL:   1.524\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.566\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.392 | Train PPL:   1.480\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.489 |  Val. PPL:   1.631\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.470 | Train PPL:   1.600\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.575\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.472 |  Val. PPL:   1.603\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.417 | Train PPL:   1.517\n",
      "\t Val. Loss: 0.454 |  Val. PPL:   1.574\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.455 | Train PPL:   1.576\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.582\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.481 |  Val. PPL:   1.618\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.597\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.432 | Train PPL:   1.540\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.443 |  Val. PPL:   1.557\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.442 |  Val. PPL:   1.555\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.543\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.416 | Train PPL:   1.516\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.464 |  Val. PPL:   1.590\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.501\n",
      "\t Val. Loss: 0.432 |  Val. PPL:   1.540\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.532\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.582\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.456\n",
      "\t Val. Loss: 0.420 |  Val. PPL:   1.522\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.425 |  Val. PPL:   1.529\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.493 |  Val. PPL:   1.637\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.440 | Train PPL:   1.553\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.570\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.447 |  Val. PPL:   1.564\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.559\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.451 |  Val. PPL:   1.571\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.554\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.541\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.537\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train PPL:   1.579\n",
      "\t Val. Loss: 0.399 |  Val. PPL:   1.490\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
      "\t Val. Loss: 0.438 |  Val. PPL:   1.549\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.489\n",
      "\t Val. Loss: 0.395 |  Val. PPL:   1.484\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.503\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.424 | Train PPL:   1.527\n",
      "\t Val. Loss: 0.421 |  Val. PPL:   1.523\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.484\n",
      "\t Val. Loss: 0.439 |  Val. PPL:   1.551\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.462 |  Val. PPL:   1.587\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.528 |  Val. PPL:   1.696\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.427 | Train PPL:   1.532\n",
      "\t Val. Loss: 0.458 |  Val. PPL:   1.580\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.512\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.446 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.444 |  Val. PPL:   1.560\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 01 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 02 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 03 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 04 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 05 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 06 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 07 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 08 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 09 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 10 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 11 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 12 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 13 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 14 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 15 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 16 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 17 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 18 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 19 | Time: 0m 0s\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 20 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 21 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 22 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.389\n",
      "Epoch: 23 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 24 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 25 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 26 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 27 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 28 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.424 |  Val. PPL:   1.529\n",
      "Epoch: 29 | Time: 0m 0s\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 30 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 31 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 32 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 33 | Time: 0m 0s\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.482\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 34 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 35 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 36 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.435\n",
      "Epoch: 37 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 38 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 39 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 40 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 41 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 42 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 43 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 44 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 45 | Time: 0m 0s\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 46 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 47 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.472\n",
      "Epoch: 48 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 49 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.329 |  Val. PPL:   1.390\n",
      "Epoch: 50 | Time: 0m 0s\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 51 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 52 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 53 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.407\n",
      "Epoch: 54 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 55 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 56 | Time: 0m 0s\n",
      "\tTrain Loss: 0.307 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.421\n",
      "Epoch: 57 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 58 | Time: 0m 0s\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 59 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 60 | Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 61 | Time: 0m 0s\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 62 | Time: 0m 0s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 63 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 64 | Time: 0m 0s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.427\n",
      "Epoch: 65 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 66 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 67 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 68 | Time: 0m 0s\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 69 | Time: 0m 0s\n",
      "\tTrain Loss: 0.425 | Train PPL:   1.529\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 70 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.415\n",
      "Epoch: 71 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 72 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 73 | Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 74 | Time: 0m 0s\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.413\n",
      "Epoch: 75 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 76 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 77 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 78 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 79 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 80 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 81 | Time: 0m 0s\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 82 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 83 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.352 |  Val. PPL:   1.422\n",
      "Epoch: 84 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 85 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 86 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 87 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 88 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 89 | Time: 0m 0s\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 90 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 91 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 92 | Time: 0m 0s\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 93 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 94 | Time: 0m 0s\n",
      "\tTrain Loss: 0.336 | Train PPL:   1.399\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 95 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 96 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 97 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "Epoch: 98 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 99 | Time: 0m 0s\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 100 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 101 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 102 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 103 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 104 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 105 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 106 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.407\n",
      "Epoch: 107 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 108 | Time: 0m 0s\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 109 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 110 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 111 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 112 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.336 |  Val. PPL:   1.400\n",
      "Epoch: 113 | Time: 0m 0s\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 114 | Time: 0m 0s\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 115 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 116 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 117 | Time: 0m 0s\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 118 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 119 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 120 | Time: 0m 0s\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 121 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 122 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 123 | Time: 0m 0s\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "Epoch: 124 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 125 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 126 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.418\n",
      "Epoch: 127 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.411\n",
      "Epoch: 128 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 129 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 130 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 131 | Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.430\n",
      "Epoch: 132 | Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 133 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 134 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 135 | Time: 0m 0s\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.403\n",
      "Epoch: 136 | Time: 0m 0s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 137 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 138 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 139 | Time: 0m 0s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 140 | Time: 0m 0s\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 141 | Time: 0m 0s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.434\n",
      "Epoch: 142 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 143 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 144 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 145 | Time: 0m 0s\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.388\n",
      "Epoch: 146 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 147 | Time: 0m 0s\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 148 | Time: 0m 0s\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n",
      "Epoch: 149 | Time: 0m 0s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 150 | Time: 0m 0s\n",
      "\tTrain Loss: 0.360 | Train PPL:   1.433\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 151 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.414\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 152 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.338 |  Val. PPL:   1.402\n",
      "Epoch: 153 | Time: 0m 0s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.417\n",
      "Epoch: 154 | Time: 0m 0s\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 155 | Time: 0m 0s\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 156 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 157 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 158 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 159 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 160 | Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.410\n",
      "Epoch: 161 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.393\n",
      "Epoch: 162 | Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.363 |  Val. PPL:   1.437\n",
      "Epoch: 163 | Time: 0m 0s\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.442\n",
      "Epoch: 164 | Time: 0m 0s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395\n",
      "\t Val. Loss: 0.371 |  Val. PPL:   1.449\n",
      "Epoch: 165 | Time: 0m 0s\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.343 |  Val. PPL:   1.409\n",
      "Epoch: 166 | Time: 0m 0s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.326 |  Val. PPL:   1.386\n",
      "Epoch: 167 | Time: 0m 0s\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.337 |  Val. PPL:   1.401\n",
      "Epoch: 168 | Time: 0m 0s\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 169 | Time: 0m 0s\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 170 | Time: 0m 0s\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.406\n",
      "Epoch: 171 | Time: 0m 0s\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.424\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 172 | Time: 0m 0s\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.429\n",
      "\t Val. Loss: 0.377 |  Val. PPL:   1.458\n",
      "Epoch: 173 | Time: 0m 0s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.456\n",
      "Epoch: 174 | Time: 0m 0s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 175 | Time: 0m 0s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 176 | Time: 0m 0s\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 177 | Time: 0m 0s\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469\n",
      "Epoch: 178 | Time: 0m 0s\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 179 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 180 | Time: 0m 0s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 181 | Time: 0m 0s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.423\n",
      "Epoch: 182 | Time: 0m 0s\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 183 | Time: 0m 0s\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.474\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.431\n",
      "Epoch: 184 | Time: 0m 0s\n",
      "\tTrain Loss: 0.340 | Train PPL:   1.405\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 185 | Time: 0m 0s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.358 |  Val. PPL:   1.430\n",
      "Epoch: 186 | Time: 0m 0s\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.413\n",
      "Epoch: 187 | Time: 0m 0s\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.361 |  Val. PPL:   1.434\n",
      "Epoch: 188 | Time: 0m 0s\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 189 | Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.420\n",
      "Epoch: 190 | Time: 0m 0s\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.382 |  Val. PPL:   1.465\n",
      "Epoch: 191 | Time: 0m 0s\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 192 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 193 | Time: 0m 0s\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.355 |  Val. PPL:   1.426\n",
      "Epoch: 194 | Time: 0m 0s\n",
      "\tTrain Loss: 0.379 | Train PPL:   1.461\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 195 | Time: 0m 0s\n",
      "\tTrain Loss: 0.384 | Train PPL:   1.468\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 196 | Time: 0m 0s\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.365 |  Val. PPL:   1.440\n",
      "Epoch: 197 | Time: 0m 0s\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.420\n",
      "Epoch: 198 | Time: 0m 0s\n",
      "\tTrain Loss: 0.346 | Train PPL:   1.413\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 199 | Time: 0m 0s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.349 |  Val. PPL:   1.418\n",
      "Epoch: 200 | Time: 0m 0s\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.406\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.441\n"
     ]
    }
   ],
   "source": [
    "optParams = []\n",
    "\n",
    "models_D2 = []\n",
    "hist_losses_D2 = []\n",
    "hist_hitsss_D2 = []\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "\n",
    "print(model.apply(init_weights))\n",
    "\n",
    "for task_id in range(N_TASKS + TEST_ALL_TASKS):\n",
    "    SUFFIX = f\"D2.{task_id}\"\n",
    "    title = f\"{PREFIX}-AE-{ENC_EMB_DIM}-{ENC_HID_DIM}-{LEARNING_RATE}-{SUFFIX}\"\n",
    "    LOADNAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    SAVENAME = MODELAUTOSAVE + title + \".pt\"\n",
    "    PLOTSAVE = PLOTSAUTOSAVE + title + \".png\"\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    hist_loss_temp, hist_hits_temp = fit_l2reg(model, task_id, N_EPOCHS, STEP_SIZE_EVALUATION, CLIP)\n",
    "    hist_losses_D2.append(hist_loss_temp)\n",
    "    hist_hitsss_D2.append(hist_hits_temp)\n",
    "    models_D2.append(copy.deepcopy(model))\n",
    "    onTaskUpdate_l2reg(model, task_id, train_dls[task_id], F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "id": "ITGjKqf6c-Bt",
    "outputId": "d49eb2d3-0c6b-4b66-fc1a-744da9729248"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist_loss_D2 = torch.cat(hist_losses_D2, dim=2)\n",
    "hist_hits_D2 = torch.cat(hist_hitsss_D2, dim=2)\n",
    "\n",
    "plotResults(hist_loss_D2, hist_hits_D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YEnIhlRbdPUs",
    "outputId": "36ab6809-9d23-4399-c9b8-c03a0bb6a3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.56% | Gr acc 0.12 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwQE8R-QvnWj"
   },
   "source": [
    "## Transfer E: DynaMoE\n",
    "\n",
    "1. Create DynaMoe network functions:\n",
    "2. Gating\n",
    "3. Run experiment\n",
    "\n",
    "Todos:\n",
    "- Training by computing loss of every expert is cheaty?\n",
    "- Training by feeding all new inputs to new expert is cheaty?\n",
    "- Experts only train on what gating gives them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiVkxi06Nu8y"
   },
   "source": [
    "### Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "JFB0iAdSvnWk"
   },
   "outputs": [],
   "source": [
    "class Gating(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pL7WzzmNqBp"
   },
   "source": [
    "### DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ysf64peLvnWj"
   },
   "outputs": [],
   "source": [
    "class DynaMoE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gating=None,\n",
    "        gating_optimizer=None,\n",
    "        experts=None,\n",
    "        expert_optimizers=None,\n",
    "        status=\"train_gating_initialized_expert\",\n",
    "        expert_decay=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        status : string\n",
    "            \"train_gating_uninitialized_expert\" | \"train_gating_train_expert\" | \n",
    "            \"train_gating_initialized_expert\"\n",
    "        \"\"\"\n",
    "        super(DynaMoE, self).__init__()\n",
    "\n",
    "        assert (gating is None) == (gating_optimizer is None), \"gating needs gating_optimizer, and vice versa\"\n",
    "        \n",
    "        if gating is None:\n",
    "            gating, gating_optimizer = init_gating()\n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        \n",
    "        if experts is None:\n",
    "            expert, expert_optimizer = init_expert()\n",
    "            experts = [expert,]\n",
    "            expert_optimizers = [expert_optimizer,]\n",
    "        \n",
    "        assert len(experts) == len(expert_optimizers), \"unequal amount of experts and expert_optimizers\"\n",
    "\n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "            \n",
    "        self.n_train_on_experts = [0 for _ in experts]   # List how often expert has been trained\n",
    "        self.n_active_experts = len(experts)\n",
    "        self.n_max_experts = gating.n_max_experts\n",
    "        self.status = status\n",
    "        \n",
    "        self.expert_decay = expert_decay\n",
    "        if self.expert_decay:\n",
    "            # Initialize Scheduler for expert decay\n",
    "            initScheduler = lambda opt: optim.lr_scheduler.StepLR(opt,\n",
    "                                                                  step_size=STEP_SIZE_DECAY,\n",
    "                                                                  gamma=GAMMA_DECAY)\n",
    "            self.expert_schedulers = [initScheduler(opt) for opt in expert_optimizers]\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        masked_gatings = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        # @TODO: Probabilistic vs argmax?\n",
    "        network_ids = torch.argmax(masked_gatings, dim=1)\n",
    "\n",
    "        expert_outputs = []\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_outputs.append(self.experts[e_id](seqs, seqs_len, seqs,\n",
    "                                                     teacher_forcing_ratio))\n",
    "\n",
    "        outputs = torch.empty((seq_len, batch_size, vocab_size))\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            network_id = network_ids[b]\n",
    "            outputs[:,b] = expert_outputs[network_id][:,b]\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1\n",
    "        self.n_train_on_experts.append(0)\n",
    "        if self.expert_decay:\n",
    "            self.expert_schedulers.append(\n",
    "                optim.lr_scheduler.StepLR(expert_optimizer, \n",
    "                                          step_size=STEP_SIZE_DECAY,\n",
    "                                          gamma=GAMMA_DECAY)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITHSpNoENze8"
   },
   "source": [
    "### compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "9Opa6EZlw8g-"
   },
   "outputs": [],
   "source": [
    "def compute_loss(outputs, targets, criterion, cutFirstInSequence=True):\n",
    "    if isinstance(criterion, CosineLoss):\n",
    "        return criterion(outputs, targets)\n",
    "    elif criterion == allOrNoneLoss:\n",
    "        return criterion(outputs, targets) / targets.shape[1]\n",
    "    else:\n",
    "        outputs_dim = outputs.shape[-1]\n",
    "        \n",
    "        if cutFirstInSequence:\n",
    "            outputs = outputs[1:].view(-1, outputs_dim)\n",
    "            #outputs = [batch size, output dim]\n",
    "            targets = targets[1:].view(-1)\n",
    "            #targets = [batch size]\n",
    "            # print(\"hi\")\n",
    "        else:\n",
    "            outputs = outputs.view(-1, outputs_dim)\n",
    "            targets = targets.view(-1)\n",
    "        \n",
    "        # print(\"######\")\n",
    "        # print(outputs)\n",
    "        # print(targets)\n",
    "        # print(\"######\")\n",
    "        \n",
    "        return criterion(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K75xVqKJN2Km"
   },
   "source": [
    "### train_dynamoe_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "icuVPvGqYI75"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_gating(\n",
    "    model,\n",
    "    iterator,\n",
    "    gating_criterion,\n",
    "    expert_criterion_unreduced,\n",
    "    clip,\n",
    "    verbose=False,\n",
    "    returnGatingChoices=False\n",
    "):\n",
    "    \n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    gating_choices_tracker = []\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                     expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                    expert_criterion_unreduced,\n",
    "                                    cutFirstInSequence=True)\n",
    "            \n",
    "            # Log loss to train gating\n",
    "            loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Indices of correct experts to have chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Target\")\n",
    "            print(gating_trgs)\n",
    "            print(\"Output\")\n",
    "            print(gating_outputs.argmax(dim=1))\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs,\n",
    "                                   gating_criterion,\n",
    "                                   cutFirstInSequence=False)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "        \n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "        # save gating choices\n",
    "        if returnGatingChoices:\n",
    "            gating_choices_tracker.append(gating_choices)\n",
    "    \n",
    "    ret_loss = epoch_loss / len(iterator)\n",
    "    \n",
    "    if returnGatingChoices:\n",
    "        return ret_loss, np.concatenate(gating_choices_tracker)\n",
    "    return ret_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbbZ_pYyN48v"
   },
   "source": [
    "### train_dynamoe_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "MaaIrZLCE1JG"
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_both(model, iterator, gating_criterion,\n",
    "                       expert_criterion_unreduced, clip):\n",
    "\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        # loss_experts = [batch_size, n_active_experts]\n",
    "        expert_trgs = seqs\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            train_model = e_id == model.n_active_experts - 1\n",
    "\n",
    "            if train_model:\n",
    "                model.experts[e_id].train()\n",
    "                model.expert_optimizers[e_id].zero_grad()\n",
    "                \n",
    "                # Get model prediction\n",
    "                expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                     expert_trgs)\n",
    "\n",
    "                loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                    expert_criterion_unreduced,\n",
    "                                    cutFirstInSequence=True)\n",
    "\n",
    "                # Log loss to train gating\n",
    "                loss_experts[:,e_id] = loss\n",
    "                \n",
    "                # Train newly initialized model on new train examples\n",
    "                reduced_loss = loss.mean()\n",
    "                reduced_loss.backward()\n",
    "                model.expert_optimizers[e_id].step()\n",
    "                \n",
    "            else:\n",
    "                model.experts[e_id].eval()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "\n",
    "                    # Get model prediction\n",
    "                    expert_outputs = model.experts[e_id](seqs, seqs_len,\n",
    "                                                         expert_trgs)\n",
    "\n",
    "                    loss = compute_loss(expert_outputs, expert_trgs,\n",
    "                                        expert_criterion_unreduced,\n",
    "                                        cutFirstInSequence=True)\n",
    "\n",
    "                    # Log loss to train gating\n",
    "                    loss_experts[:,e_id] = loss\n",
    "\n",
    "        # Compute expert which should have been chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        # Get loss of model gating chose\n",
    "        gating_masked = gating_outputs.squeeze(0)[:,:model.n_active_experts]\n",
    "        gating_choices = gating_masked.argmax(dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_dynamoe_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_dynamoe_expert(model, expert_id, iterator, expert_criterion, clip):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "\n",
    "        model.experts[expert_id].train()\n",
    "        model.expert_optimizers[expert_id].zero_grad()\n",
    "\n",
    "        # Get model prediction\n",
    "        expert_outputs = model.experts[expert_id](seqs, seqs_len,\n",
    "                                                  seqs)\n",
    "\n",
    "        loss = compute_loss(expert_outputs, seqs,\n",
    "                            expert_criterion,\n",
    "                            cutFirstInSequence=True)\n",
    "\n",
    "        # Train newly initialized model on new train examples\n",
    "        loss.backward()\n",
    "        model.expert_optimizers[expert_id].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQrjb4QEODx2",
    "tags": []
   },
   "source": [
    "### fit_dynamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "tuFUOzDwjZgI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_dynamoe(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id, \n",
    "    epochs,\n",
    "    step_size_evaluation,\n",
    "    gating_criterion,\n",
    "    expert_criterion,\n",
    "    expert_criterion_unreduced,\n",
    "    clip=1,\n",
    "    repetition=None\n",
    "):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.empty((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.empty((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    loss_tracker = torch.zeros((epochs,))\n",
    "\n",
    "    allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, epoch, expert_criterion)\n",
    "        \n",
    "        # Train model depending on its status\n",
    "        if model.status == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id],\n",
    "                                                 allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD_START)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    \n",
    "                    # Switch to train the expert and gating\n",
    "                    model.status = \"train_gating_train_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    \n",
    "                    model.last_loss = loss_tracker[epoch]\n",
    "                    \n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if model.status == \"train_gating_train_expert\":\n",
    "            switch_status = False\n",
    "            # Lookahead check whether data from another task is coming in\n",
    "            loss_ahead = evaluate_extra(model, valid_dls[task_id],\n",
    "                                        allOrNoneLoss)\n",
    "            if loss_ahead - PERFORMANCE_DIFFERENCE_NEW_TASK > model.last_loss:\n",
    "                switch_status = True\n",
    "            else:\n",
    "                start_time = time.time()\n",
    "\n",
    "                # First train gating\n",
    "                train_loss, gatingChoices = train_dynamoe_gating(model,\n",
    "                                                                 train_dls[task_id],\n",
    "                                                                 gating_criterion,\n",
    "                                                                 expert_criterion_unreduced,\n",
    "                                                                 clip,\n",
    "                                                                 returnGatingChoices=True)\n",
    "                \n",
    "                # Check whether gating chose something different\n",
    "                if (not all(gatingChoices == model.n_active_experts - 1) and\n",
    "                    epoch > allowed_until_check):\n",
    "                    # Abort, we have a sequence from a different task!\n",
    "                    switch_status = True\n",
    "                else:\n",
    "                    expert_id = model.n_active_experts - 1\n",
    "                    train_loss = train_dynamoe_expert(model, expert_id, train_dls[task_id], expert_criterion, clip)\n",
    "                    valid_loss = evaluate(model, valid_dls[task_id],\n",
    "                                          expert_criterion)\n",
    "\n",
    "                    end_time = time.time()\n",
    "\n",
    "                    model.last_loss = evaluate_extra(model, valid_dls[task_id],\n",
    "                                                     allOrNoneLoss)\n",
    "            if switch_status:\n",
    "                # Switch mode and \"consolidate\" expert\n",
    "                model.status = \"train_gating_uninitialized_expert\"\n",
    "                allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                \n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"-- Fix Expert, Train Gating only --\")\n",
    "                print(\"-----------------------------------\")\n",
    "                # (this runs into the next if and executes that case)\n",
    "\n",
    "        if model.status == \"train_gating_uninitialized_expert\":\n",
    "            # assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_dynamoe_gating(model, train_dls[task_id],\n",
    "                                              gating_criterion,\n",
    "                                              expert_criterion_unreduced, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], expert_criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD_START)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    model.status = \"train_gating_initialized_expert\"\n",
    "                    allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    \n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "#         if valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = valid_loss\n",
    "#             torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        # Log performance AFTER training\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0 and epoch != 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, idx, expert_criterion)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCiyq9ODN9fG"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Y4hPup6qpRWp"
   },
   "outputs": [],
   "source": [
    "def init_expert():\n",
    "    attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n",
    "    new_expert = Seq2Seq(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "    new_expert.apply(init_weights)\n",
    "    expert_optimizer = optim.Adam(new_expert.parameters(), lr=LEARNING_RATE)\n",
    "    return new_expert, expert_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "id": "-gPFPMm_JiO4"
   },
   "outputs": [],
   "source": [
    "def init_gating():\n",
    "    gating = Gating(INPUT_DIM, N_GATING_EMBED_DIM, N_GATING_HIDDEN_DIM,\n",
    "                    N_EXPERTS_START, N_MAX_EXPERTS, GATE_DROPOUT)\n",
    "    gating.to(device)\n",
    "    gating_optimizer = optim.Adam(gating.parameters(), lr=LEARNING_RATE_GATING)\n",
    "    return gating, gating_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouWPz-2MdMs1"
   },
   "source": [
    "### show expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "jFW0tm8tdTAl"
   },
   "outputs": [],
   "source": [
    "def show_expert(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            gating_choices = gating_masked.argmax(dim=1)\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_choices[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dynamoe():\n",
    "    # Initialize DynaMoE\n",
    "    model = DynaMoE()\n",
    "    print(model.apply(init_weights))\n",
    "\n",
    "    # gating_criterion = CosineLoss(N_MAX_EXPERTS, ignore_index=None)\n",
    "    # Cosine loss is inpractical for Gating because result vectors are low dimensional\n",
    "    # Cosine loss works better for small datasets, thus used for experts\n",
    "    gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "    expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                            reduction=\"none\")\n",
    "    \n",
    "    return (model, gating_criterion, expert_criterion, expert_criterion_unreduced)\n",
    "\n",
    "def repeat_dynamoe(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model, gating_criterion, expert_criterion, expert_criterion_unreduced = pass_on_variables\n",
    "    hist_loss, hist_hits = fit_dynamoe(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        gating_criterion,\n",
    "        expert_criterion,\n",
    "        expert_criterion_unreduced,\n",
    "        repetition=repetition\n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlpvLkEVfZhR"
   },
   "source": [
    "### Experiment DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "SCHEDULE = not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD_START = 0.3\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: DynaMoE-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.109 | Train PPL:   3.032\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.941\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.885\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.910\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.934\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.999\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.914\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.849\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.102 | Train PPL:   3.011\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.119 | Train PPL:   3.061\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.921\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.850\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.948\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.066\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.105 | Train PPL:   3.020\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.075 | Train PPL:   2.931\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.826\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.078 | Train PPL:   2.938\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.919\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.942\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.869\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.925\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.002\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.955\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.107 | Train PPL:   3.025\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.885\n",
      "\t Val. Loss: 0.531 |  Val. PPL:   1.701\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.443\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.216\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.333\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.132 |  Val. PPL:   1.141\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.136\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.127 |  Val. PPL:   1.135\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.151\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.129\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.130\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.138 | Train PPL:   1.148\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.108\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "\n",
      "SCHEDULE: DynaMoE-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.592 | Train PPL:   1.808\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.592 | Train PPL:   1.807\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.610 | Train PPL:   1.840\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.759\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.858\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.757\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.577 | Train PPL:   1.780\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.702\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.596 | Train PPL:   1.816\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.573 | Train PPL:   1.773\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.589 | Train PPL:   1.803\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.605 | Train PPL:   1.832\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.591 | Train PPL:   1.805\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.582 | Train PPL:   1.789\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.720\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.615 | Train PPL:   1.850\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.600 | Train PPL:   1.822\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.818\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.635 | Train PPL:   1.886\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.702\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.513 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.593 | Train PPL:   1.809\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.598 | Train PPL:   1.819\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.567 | Train PPL:   1.762\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.611 | Train PPL:   1.842\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.603 | Train PPL:   1.828\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.579 | Train PPL:   1.784\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.787\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.591 | Train PPL:   1.805\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.762\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.612 | Train PPL:   1.845\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.578 | Train PPL:   1.783\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.696\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.599 | Train PPL:   1.820\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.542 | Train PPL:   1.719\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.633 | Train PPL:   1.882\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.588 | Train PPL:   1.800\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.859\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.698\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.787\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.606 | Train PPL:   1.833\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.599 | Train PPL:   1.821\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.608 | Train PPL:   1.837\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.628 | Train PPL:   1.874\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.580 | Train PPL:   1.786\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.575 | Train PPL:   1.777\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.585 | Train PPL:   1.795\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.587 | Train PPL:   1.799\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.589 | Train PPL:   1.802\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.727\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.559 | Train PPL:   1.749\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.617 | Train PPL:   1.853\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.680 | Train PPL:   1.974\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.481 | Train PPL:   1.617\n",
      "\t Val. Loss: 0.606 |  Val. PPL:   1.833\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.561\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.511\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.536\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.426 |  Val. PPL:   1.531\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.431 |  Val. PPL:   1.539\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.518\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.459\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.403 |  Val. PPL:   1.496\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.423 |  Val. PPL:   1.527\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.375\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.409 | Train PPL:   1.505\n",
      "\t Val. Loss: 0.317 |  Val. PPL:   1.373\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.447\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.394\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.472\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.316\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.179\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.170\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.161\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.144 |  Val. PPL:   1.155\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.159\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.179\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.161 |  Val. PPL:   1.175\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.155 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.145\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.136\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.127\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.120\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.118\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.129 |  Val. PPL:   1.137\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.128 | Train PPL:   1.137\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.133 |  Val. PPL:   1.142\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.121 |  Val. PPL:   1.129\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.095\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.084\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.084\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.081 |  Val. PPL:   1.085\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.083\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.084 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.086\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.083 | Train PPL:   1.087\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.082 | Train PPL:   1.085\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.079 |  Val. PPL:   1.082\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.080 |  Val. PPL:   1.083\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.080 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.081 | Train PPL:   1.084\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.082\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.083\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.079 | Train PPL:   1.082\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.077 |  Val. PPL:   1.080\n",
      "\n",
      "SCHEDULE: DynaMoE-1.s2.t2.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.258\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.119\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.078 | Train PPL:   1.081\n",
      "\t Val. Loss: 0.078 |  Val. PPL:   1.081\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_E, hist_all_hitsss_E, models_E = experiment(\n",
    "    \"DynaMoE-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_dynamoe,\n",
    "    repeat_dynamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIGCAYAAABeTr5SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAB44UlEQVR4nO39eZxkZXn//7+uqt73vXtWZoCBmYFZgAFRo4KgQkxAExXUxCUxhLjEmE+iaD5J/MQsRM03n5io/Igxxk9UIgiKiqKJgomKMMMyC8MyDDDTM9PLzPT0Vr1V9fX741TN9PT0UtVdp6ur+v18PPpx6pxz1znXcOjlqvu+r9vcHREREREREUlfJNcBiIiIiIiI5BslUiIiIiIiIhlSIiUiIiIiIpIhJVIiIiIiIiIZUiIlIiIiIiKSISVSIiIiIiIiGVIiJSIiIiIikiElUiIiIiIiIhmaUyJlZn+W7UBERERERETyhbl75m8yO+Duq0OIR0REREREZNErmu6EmfVNdwooDyccERERERGRxW/aRAo4AVzq7p2TT5jZwdAiEhERERERWeRmmiP1ZeCsac59NYRYRERERERE8sKc5kiJiIiIiIgsZWlV7TOzmolbERERERGRpSzd8ucPTNqKiIiIiIgsWZmuI2WhRCEiIiIiIpJH5rQgr4iIiIiIyFKmREpERERERCRDmSZSKvEnIiIiIiJLXrqJlE3aioiIiIiILFlprSNlZue5+zOp7QLEJSIiIiIismhpQV4REREREZEMFU13wsx+zPRzotzdrwonJBERERERkcVt2h4pM7tkisOXAx8Gutz90jADExERERERWazSnSP1KuBPgVLgr939e2EHJiIiIiIislhNO7QPwMxeR5BADQN/5e4/XpCoREREREREFrGZhvY9AjQDnwJ+Pvm8uz8abmhTa2pq8jVr1uTi1lPq6OgAoK2tLceRiMhc6ftYJP/p+1gk/y3G7+MdO3Ycdffmqc7N1CM1CAwAbwJ+ndPXkHLg1VmLMANr1qxh+/btubj1lG699VYAbrnllhxHIiJzpe9jkfyn72OR/LcYv4/N7MXpzk2bSLn7FaFEIyIiIiIikuciYV3YzL5oZl1mtnua82ZmnzGzfWa208wuDisWERERERGRbAotkQK+BFwzw/lrgXXJr5uAz4cYi4iIiIiISNaElki5+0+A4zM0uR74sgceAurMbFlY8YiIiIiIiGTLjOXPIRiCB7wdONvd/8LMVgNt7v7wPO+9Ajg4Yb89eezIPK8rIfrb7z9FZ+8w/98NWxf0vtt/dA8lP/kr3jn+Z4xSsqD3lsI3OroRgP/3Z9/PcSSnvO0lq/mT12/MdRgiee9d//owjzw/0+e6IpILb7xoBX/5xk25DmNeZk2kgM8B4wRV+v4C6Ae+AVw6z3vbFMemrMVuZjcRDP9j9erV87ytzFXf8Bj/+tPnGR4b5z2vPJuNy2oW7N4vPPpD3sSzvHNlJ89Xb1uw+8rS8NzeJwA4Z8OWHEcS+Olzx3hov/7wE8mGX+w/zoq6ci5YvnC/s0RkdsvrynMdwrylk0i9xN0vNrPHANy9x8yy0SXQDqyasL8SODxVQ3e/HbgdYNu2bVMvfCWh++7OIwyPjQPw7z9/kb/+tYX5FOHAsRijJ45AEXzw7CPY1RctyH1l6bj11vsBuOWt78ptIEm/9aVHOHA8xvi4E4lM9ZmTiKRjfNwZGktw6Zp6/ubXN+c6HBEpMOnMkRozsyjJ3iIzaybooZqve4F3JKv3XQ70uruG9S1id+1oZ1VDBVtX1fFfT3USj2fjf4M07vtoOy12AgA79uyC3FMklypKogyNJhgbX5jvMZFCFRtLAFBWHM1xJCJSiNJJpD4D3AO0mNlfAf8D/PVsbzKzrwE/B843s3Yz+20zu9nMbk42uQ/YD+wD/hl471z+AbIwnuseYMeLPVy9voV3vPQsOvtG+N6ejtDvOz7ufGNHO2eVDgQHju0L/Z4iuVZZUsTwWIJ4Qh3wIvMRG4kDUF6iREpEsm/WoX3u/hUz2wFcRTCv6Q3uvjeN9711lvMOvC/dQCW37trRTsTgyvNbuHRtA//7m7u559FD/OqW5aHe9+f7j3HoxBBttb0QB47vh6FeKK8N9b4iuVRRGlUiJZIFg6PqkRKR8MzaI2VmDUAX8DXgq0CnmRWHHZgsHolx5+5H27nkrHouXlNPeUmUX9m8nJ8+d5SjAyOh3vvO7QepLIlQNXYMyhsgPgxHHgv1niK5VllSxNBYgtFEItehiOS1wWSPlBIpEQlDOkP7HgW6gWeAZ5OvnzezR83skjCDk8XhJ89209k3wtUbWqkqDToxb7h0FSPxcb76iwOh3bdveIzv7e7gmnPKsPE4rLosONG+PbR7iiwGFaVRxh1io0qkROYj9T1UrkRKREKQTiL1feCX3b3J3RuBa4GvE8xp+lyYwcnicNeOdqrLirhyfcvJYxevruOshgq+vzu8eVLf3XmEkfg4rzsrWbVs+UUQKdY8KSl4lSXBBxZ9Q2M5jkQkvw2OpuZIpfPnjohIZtL5ybLN3e9P7bj7D4BXuvtDQGlokcmicCI2yg/2dHDFec2c3VR58riZccOlq3jySB+PH+gJ5d53bj/IqoYKtjWOBgeqWqD5fCVSUvAqkhPjB5LDkkRkbmIjQY9UVVk6q72IiGQmnUTquJl9xMzOSn59GOhJlkRXbd4Cd+8ThxlLOFdvaKUoevr/Lr9+yUoiBl99+GDW77uva4BHD5zg6vUt1I8nE7XKVli2BY4+C2PDWb+nyGKRGkKrREpkflI9UtWlmtotItmXTiL1NoLFcr8JfAtYnTwWBd4SWmSyKNy5vZ2zmyp52blNZ5xrrSnjZec08aOnOhnL8ppSE6sE2mBXcLB2RZBIDZ+A7qeyej+RxaQimUj1DyuREpmPVPnz1IcTIiLZNGsi5e5H3f0D7n6Ru2919/e7e7e7j7q7xlgVsKc6+th1qJerNrTQXD31KM4bL1vF0YFR7n3icNbumxh37nnsVJVABjohWgKVzdC2KWjU/kjW7iey2FQmh/YNjqjYhMh8pMqfV2ton4iEIJ3y581m9ikzu8/MfpT6WojgJLfu2t5OUcS44vyWadtcvaGV6rIivvV49hKpM6oEDnRBRQOUVEDrhUGjrlmXMhPJWxXJYhNDo+qREpmPwZE40YidnHcoIpJN6Qzt+wrwFLAW+D/AC4C6AwrcWGKcex47xKVrGrhodd207cqKo1y/ZTkP7T9GV1925i3dtb2dmolVAvs7gjWkiiugrAbqzlLBCSlolaXBH31DY5qGKjIfsdEEZcURiqOq2ici2ZfOT5ZGd/8XYMzdH3T33wIuDzkuybEfP9XFscFRrt7QevLT8enccOlqRhPjfPnnL877vidio/zgyQ6uOL/lVJXAgc6gR6ooObxw2eYgkRrXsCcpTKnvueEx/T8uMh+DI3HKi6NnFEsSEcmGdAYNpxYyOWJmrwcOExSfkALwh//xOP/5VCf46ceHx8apqyjmivVnFpmY7MIVNZzTXMkPnuzgj153flr3vePhA/zN957C/fQbx8edsYRz1fqWU7/4BjqDsucpy7bA3m/DiYPQsCat+4nkk1SPlBIpkfkJeqSiFEUs16GISAFKJ5H6SzOrBf4X8I9ADfChUKOSBXHweIy7HzvE5pW1rG6oOOP8JavrWdNYNet1zIy3Xraav/zuXh55/hiXrm2csb27c9uDz1FZEuXis+rPON9aU8arzm8OduKjMNQTDO1LadscbNsfUSIlBamsKIoBQ0qkROZlcDQeJFJRJVIikn0zJlLJtaLWuft3gF7gygWJShbENx5tx4APvnodV21snde13nDRCv7me0/xtYcPzppI7XixhxeOxfjgq9fxodeeN/OFB7uDbcXERCpZua9jJ2x+8zyiFlmcIhGjvCSqHimReYqNJCgvjlIc0dA+Ecm+GX+yuHsCuG6BYpEFND7u3LWjnc0ra9m2tmH2N8yiqaqUV53XzI+f7mJklj/+7tzeTllxhFdvmL4a4EkDHcF2Yo9U9bJgXwUnpIBVlERVbEJkngZHgzlSEQ3tE5EQpPMRzc/M7J/M7BVmdnHqK/TIJFS/eP447T1DXL2hldry7Kz4fuOlq+iJjXH3Y+3TtomNxvn2zsO8/Jwm1i+rnv2iA8nFeCf2SJkFvVLH9sGkOVYihaKipEg9UiLzNDASDO0TEQlDOnOkXpbc/sWEYw68OvvhyEK5c8dBKkqiXLm+OWvXvHJ9C3XlxXz7iSO89bKzpmzz/d0dxEYTXLWhldKiNH65DXQG2+oVpx9ftgVe/BnEjkPlzEMJRfJRVakSKZH5io0mKC/WsD4RCcesiZS7a15UgRkYifO9XR28cl0T57XWZO26xdEIb7x4Bf/v5y9yqCfGivozC1jcub2dtpoyXnXe7NUAg2CTPVI1y08/vmwLjI/Boe1w3uvmGbnI4lNZGlWxCZF5iqlHSkRCNOvHNGbWamb/YmbfS+5vNLPfTufiZnaNmT1tZvvM7JYpztea2bfN7Akz22Nm7878nyCZ+u7OwwyNJbh6QyslRdn9pO6GS1cRH/cp15Q6eDzGz/cf46oNLSyvK0/vggOdUFoD5bWnH08VnDj82DwjFlmcKpM9UuPjGr4qMhfuHpQ/L1EiJSLhSOev6C8B9wOpLoFngD+Y7U3Jin+fBa4FNgJvNbONk5q9D3jS3bcAVwB/Z2Yl6QQuc3fXjnZW1JXzS+vS7BXKwPq2GjYsq+aHT3ZOeV8DXr2+BbM0J/72dwTzo4onJV6N50JRGRx9dv5BiyxClSVFDI0miCuREpmTobEEDpSrR0pEQpJOItXk7l8HxgHcPQ6kM97kMmCfu+9391HgDuD6SW0cqLbgr+oq4DgQTzd4ydzzRwd55IUertrQQlttWSj3uPHS1ew/Osj/PHv05LHxcecbjyarBK7JoErgQFdQoa940jDBSBRaNqhynxSsVNW++Lgq94nMxeBI8KeKhvaJSFjSSaQGzayRIOnBzC4nWFNqNiuAgxP225PHJvonYANwGNgFfNDdz/irwcxuMrPtZra9u7s7jVvLdO7acZCIwavPz6BXKEPXb11OUcT4+vZTj/+h54/NrUrgQLJHKjLFL8JlW4NEajQ2/6BFFpnU0L6xhHqkROYiNhp8LqtiEyISlnR+uvwv4F7gHDP7KfBl4ANpvG+qv9In/0XwOuBxgmGDW4F/MrMzqh+4++3uvs3dtzU3Z6/K3FKTGHfufvQQF62u55I19aHdp66ihKs3tPLA013ERoJfZHdtb8+8SqB70CNVMU0PVtsmGB2Ajt1ZiFpkcalILsgbT6hHSmQu1CMlImGbNZFy9x3AqwjKoP8ucIG770zj2u3Aqgn7Kwl6niZ6N3C3B/YBzwPr0wlcMvfTfUc50jvM1RtaqS7LztpR07nhslX0Dce5c0c7/cNj3Lf7CK9Y15xZlcCRfogPn74Y70TLtgTb9ofnH7DIIlNZWkR83FW5T2SOUj1SSqREJCzpVO17AvgwMOzuu919LM1rPwKsM7O1yQISNxL0bE10ALgqeZ9W4Hxgf7rBS2bu3NFOVWkRV54ffq/eK9c101RVwnd3HuG+XUcYHhvn6g0tmVUJTK0hNV2PVMtGsAgcfWb+AYssMhXJSmN9Q+n+yBWRiQZHgw8hVGxCRMKSzoK81wE3AF83s3HgP4Cvu/uBmd7k7nEzez9Bxb8o8EV332NmNyfP3wZ8AviSme0iGAr4EXc/Ou1FZc56Y2Pcv6eD12xo5ZyWqtDvF40Yv37xSv75v/fTExsNqgSem2GVwFQiNV2PVEkFNJytghNSkCpLgh/P/cOqvyMyF6mh5VVl6fypIyKSuXSG9r3o7p9090uAtwGbCYbgzcrd73P389z9HHf/q+Sx25JJFO5+2N1f6+6b3P1Cd//3efxbZAbf3nmY0fg4V29opTi6MBNv33LpKsYdnu0a4OoNrZlXCUwlUpUt07dZtiVIpBL6Y1MKS0Vp8Cn6wIj+3xaZi1SPVHWZeqREJBxpfUxjZmuAtxD0TCUIhvpJHrnnsUOsaazgZeemWXr8B/8bHv/q1OfW/wpc95lZL3FOcxWbV9ay+1AvV57fnHmVwIGuYFszudjjBG2bYfc34NPnBsP8RObgA8NDwYtP3p7bQCa44Ky3AK9QIiUyR6k5Upv++31wz44cRyMiZ9j0Zrj2b3MdxbzMmkiZ2S+AYuBO4M3urjlMecbd2Xukj6vWt9BaUz77GwB23Qml1acKOqQcehSe/SHER6Fo9rWT//INF/KjvV1cujaDtaNSBjohUgTVrdO32XwDdOyCcc0jkbl7+qkXAbh4zVk5jiTpxZ/S2PnfwCsY0NA+kTlJVe2rOfQg1K2G1o05jkhETlPRmOsI5i2dHql3uvtToUcioTkRGyM2mqC1Js2hdQPd0N8Bl78Xrvmb08/d/yfw8D8HZceLZk+ONq+sY/PKusyDhuRivPVQUjl9m5pl8KZ/mdv1RZJ+cOutAFz8lltyHEnSHW+n5PAeAGKjqtonMheDI3HKbJRIYgTOfhW8/u9yHZKIFJhZEyl3f8rMXg9cAJRNOP4XYQYm2XOwJ1iwtiXdRKpzV7BtPPfMc1WtkBiBwWPTV9PLlv7kYrzFafaiiRSKslqK4gMADKv8ucicDI7GaS4eCXZKwi+yJCJLTzrlz28jmBv1AYLKem8GFsn4F0lHe08w/6OtpjS9NxxJLhPWeuGZ56qSw+x6D2YhslkMdAYV+4oyLFIhku/KaomMDgJoHSmROYqNJGgqGg52SpVIiUj2pTM7/2Xu/g6gx93/D/BSTl9oVxa59mSP1JrGGYbITdSxK0iYGs4+81xVsoJe3+S1lUMw0BmMn820SIVIviurIxKPESWhHimRORocjdNclCwkU1Kd22BEpCClk0glfwoRM7PlwBiwNryQJNvae4aoLI2mX368Y2cwrK+8/sxzqR6pwa7sBTiV8QTEFmD4oMhiVFYLQDUxhsfGcxyMSH6KjSZoiCb/hFGPlIiEIJ1E6jtmVgd8CngUeAH4WogxSZYdPB6jtbqMytI0aouMDsLRZ4NEKjpF++q2YBs7lt0gJxs8Cj4+/WK8IoUsmUg1Fw1paJ/IHA2OxKmPBCMypvxgUERkntIpNvGJ5MtvmNl3gDJ37w03LMmm9p4hWmvKqChJY1HCrr2AQ9MUhSYAyuqCkuRDx7MZ4pkGOoKteqRkKUomUi1FMQ3tE5mjwdE49ZFkj5Q+lBOREGS0gqm7jyiJyi/uzsGeGC3VpektiHvkiWDbdP7U5yMRqGyGWNiJVHLooH75yVJ0skcqph4pkTkaHElQm+qR0odyIhKCjBIpyT/HB0cZHhtPv/R5x66gTGzLBdO3qWpZgB6pzmBbvSzc+4gsRslEqjGqHimRuYqNxqkhBpFiFZsQkVAokSpwBzMtfZ4qNFE5w2rT1W1Bj9R4iJPgU4lUzfLw7iGyWCUTqYaIik2IzFVsJEE1g0GhiaI0fweKiGQgnXWkzMx+w8z+LLm/2swuCz80yYZU6fPWdHqkEnHo3BMkUjMtglvVFhSbiA9N32a+BrqguDIofy6y1JTXAdAQjTE0msDdcxuPSJ5xd2KjCaoYDEZZFJXkOiQRKUDp9Eh9jmDtqLcm9/uBz4YWkWRVajHeNU1prCF1/DmID0PTupnbVbXCcC+MDGQhwmn0dwRj2mdK6EQKVUkVWIQ6C+ZIjSWUSIlkYiQ+TsKdqvGBoEcqqh4pEcm+dBKpl7j7+4BhAHfvAfTRTp5o74lRXVpESzpD+47sDLaN01TsS6lqCUqTh7ko70CXEilZusygtIZqBhkeS5AYVyIlkonBkTgAFeMDwQcT0eIcRyQihSidRGrMzKKAA5hZM6BB+3ni4PGg9HlVOmtIdewMJuW2bZ65XWpR3t72+Qc4nYGOoGKffvnJUlVWQzXJHqkw5yOKFKDYaFCkpSyRTKTSqVorIpKhdBKpzwD3AC1m9lfA/wB/HWpUkjXtPTFaakopL05jDamOXdCw9tSiu9NJJVL9C9AjJbJUldVR6UGPVFxD+0QyMjga9EiVJfqDoX0iIiGYNZFy968AHwb+BjgCvMHd70zn4mZ2jZk9bWb7zOyWadpcYWaPm9keM3swk+BlZu5Oe88QLdVls68h5X6qYl/pLGViq5OJVOxYdgKdbHQQRgeUSMnSVlZHhQ8ylnCGk38Uikh6BkcSgFMaT/ZIiYiEYNbxXmbWAHQBX5twrNjdx2Z5X5SgKMVrgHbgETO7192fnNCmjqCYxTXufsDMWub0r5ApHR0YZSQ+Tms686P6jwSJUeO5sw+BqEw+prAW5U2VPtdivLKUlddSPv4CAH0jcbQQgEj6YqNxShkj6mPqkRKR0KQztO9RoBt4Bng2+fp5M3vUzC6Z4X2XAfvcfb+7jwJ3ANdPavM24G53PwDg7l2Z/gNkegczKX2eKjTRNEuhCYCSiuATvrAW5R1I/m+gHilZyspqKU0MAtA/POPnViIyyeBIghqC7x/1SIlIWNJJpL4P/LK7N7l7I3At8HXgvQS9SdNZARycsN+ePDbReUC9mT1gZjvM7B1TXcjMbjKz7Wa2vbu7O42QBU6VPk+rR6pjV7CdrdBESlVL+D1Slc3hXF8kH5TVURIPlhjoH07kOBiR/BIbjVNjwYeJSqREJCzpJFLb3P3+1I67/wB4pbs/BMz0F/pU48Mmz5guAi4BXg+8DvhTMzvvjDe53+7u29x9W3Oz/rhOV2ox3rXprCHVsRNqVkDt6vQuXtUafo9UtQYzyRJWVkfR+AjFxE+WchaR9AyOJqhN9UjNNu9XRGSO0kmkjpvZR8zsrOTXh4Ge5ByomWrytgOrJuyvBCaXeWsHvu/ug+5+FPgJsCWD+GUG7T1D1JQV0VydxtC+VKGJ8rr0Ll69LJhTlQhhyNFAJ1gkuIfIUlVWC0A1MQ3tE8lQbCROjaUSKfVIiUg40kmk3kaQBH0T+BawOnksCrxlhvc9Aqwzs7VmVgLcCNw7qc23gFeYWZGZVQAvAfZm9C+QaR08HktvDanhXuh5IUikImmUSYegRyp2HMZi847zDP0dUFanTxFlaUsmUjU2yNCohvaJZGJwJE4tyd9PZZpvKyLhmLVqX7Kn6APTnN43w/viZvZ+4H6CpOuL7r7HzG5Onr/N3fea2feBnQS9W19w992Z/iNkau09QyyvK6eseJZ8uSP5n7xpXfoXr2oJkqjYsZN/8GVNag2p4vLsXlckn6QSKWIMjWlBXpFMDI4maCwK5gmrcJGIhCWd8ufNBOtIXQCcHCPm7q+e7b3ufh9w36Rjt03a/xTwqTTjlTSNjzuHeoa4aFXd7GtIpQpNtGxI/wapRXl7D0HD2XMLcjoDncEvvqI0hiSKFKqTPVIxhsfUIyWSidhonOXRoWBmdnl9rsMRkQKVztC+rwBPAWuB/wO8QDBsTxaxowMjjCbGaUmn9HnHruAXTeMZdT6ml0qk+g7NLcCZDHRCeSNE0vnfU6RAneyRGmRIiZRIRgZHEtRHhvBoCZTV5DocESlQ6fyl2uju/wKMufuD7v5bwOUhxyXzdGoNqXRKnz8RzI+qbEz/BtXJRKq/Yw7RzWB8HAa7NRRDJFn4pdYG1SMlkqHYaJw6iwWlz6MluQ5HRApUOolUqlzUETN7vZldRFB8Qhax1BpSbbWz9EjFR6HrqSCRKkoj6UpJ9UhluwT6UA+Mx5VIiSR7pBoiGtonkqnBkQS1SqREJGSzzpEC/tLMaoH/BfwjUAP8QZhByfylEqmzZ1tDqvspGB8LEqlMVDQGJcqzvSjvQLKHq1yJlCxxxRUQKaKBGM+r2IRIRgZH41TbIFZWrURKREKTTiLV4+69QC9wJYCZvTzUqGTe2nti1FUU01g1Sy9TqtBEU4aJVCQaJFPZ7pEa6Ay26pGSpc4MSmuoH4kxrPLnIhkZHIlT7YNQUg/R4lyHIyIFKp2hff+Y5jFZRA4eH6K1uozKklly5Y5dQXW8tjmsg5xaSyqbBrqS127L7nVF8lFZDXUWU7EJkQwNjiao8oFgaN9slWtFROZo2r+yzeylwMuAZjP7wwmnagjWhZJF7GBPjNUNFZSXzPKoOnYG5csrmzO/SXUbnDgA7tn7RZUqXlGzLDvXE8lnZXXU9muOlEimYiNxKiKDUFqV61BEpIDN1CNVAlQRJFvVE776gDeFH5rMVWoNqZbqWQpNuAeJVOO5c/tlU9UWDO2LD88t0KkcfSYoxa4eKREoq6OaoGqfu+c6GpG8ERuNUzGe7JESEQnJtD1S7v4g8KCZfcndX1zAmGSeuvpHiI/77KXPT7wII/3QtG5uN6pqgVgPjA5CcfncrjHZkWQpdi2gKALltVSxj6HROPFxpziqIUoisxmJJygeHyZKQomUiIQqnWITpWZ2O7BmYnt3f3VYQcn8nFxDarYeqSM7g22mFftSqlrBE9B/BCqb5naNieKj0P00bH4zFKnKkghldVR5sCBvPOEUa1C1yKxiIwlqCH4PamifiIQpnUTqTuA24AuABurngfZkIrWsbpZEqmNXUMK8bfPcblTVEmx7D0HbprldY6K5lmIXKVRltZSPDzKcSBAfH0fTU0VmNzgap8aSiZR6pEQkROkkUnF3/3zokUjWtB8P1pBaO9saUh07oW411Cyf242qk/OY+g7N7f1TxQPQeF52rieS78pqKfZRxseGiSc0R0okHbHRBDUMBjul1bkNRkQKWjrlz79tZu81s2Vm1pD6Cj0ymbODPTEaKkqor5xleFyq0ERZ3dxuVNUabAePzu39Z8STKsV+YXauJ5LvymoBKE3EGI5rQIBIOgZH4tRaMpFSj5SIhCidHql3Jrd/POGYA2dnPxzJhvaeIVpqSqkqneHxDh6DvsOw4TqIpJNPTyE1tC9bi/Ie2QmN55y6rshSl/yQo8YGGRiOQ21uwxHJB0GPVHJoX7k+9xWR8MyaSLn72oUIRLLnYE+MtY1VlM00M71jnoUmIPikr6g8O4vyjo8HMZ17NZTMMiRRZKlI9kjVEKN/OJ7jYETyw+BInJpUj1RlY26DEZGCNmtXhJlVmNn/Tlbuw8zWmdmvhB+azEVi3DlyYnj20ucdu4Jt6zyG0ZlBVXN2eqROvAijAyo0ITJRKpGyQfqGx3IcjEh+OL1HSktpiEh40hnT9a/AKPCy5H478JfpXNzMrjGzp81sn5ndMkO7S80sYWZa6HeeOvqGk2tIpVGxr7I5GEo3H1VtEDs2v2tAdnrIRApNeR0AtQwyMKIeKZF0pKr2JaKlmiMlIqFKJ5E6x90/CYwBuPsQMOuqkGYWBT4LXAtsBN5qZhunafe3wP0ZxC3TaD8efArXUj1bj9TO7Cx8W90WDO1LzPOPvFQp9mVzLMUuUohO9kjFGFQiJZKWYB2pQby4CqLFuQ5HRApYOonUqJmVExSYwMzOAUbSeN9lwD533+/uo8AdwPVTtPsA8A2gK72QZSbtPUHp82V15dM3GhuCo88EidR8f8lUtQZD++JD87tOx675lWIXKUQT5kgNjqhqn0g6BkaCHikvqYSiWT5UFBGZh3QSqT8Hvg+sMrOvAP8FfDiN960ADk7Yb08eO8nMVgBvJFjwV7KgvWcIA9Y2zlCwofNJ8HFoWjf/G1a1wkg/DJ2Y33WOPAGN6+Zeil2kEBWVMR4ppsYGGR5TIiWSjthonDqLBWtIRWdZBkREZB5mTaTc/YfArwHvAr4GbHP3B9K49lTD/yavKPl/gY+4+4x/IZjZTWa23cy2d3d3p3HrpetgT4yGyhLqK2foaUrNR2o6f/43TJUq722f+zUGj0L/EWg6NyhgISIBM7y0hhpiDCmREknL4GiCushgMD9KQ/tEJETpVO17IxB39++6+3eAuJm9IY1rtwOrJuyvBA5ParMNuMPMXgDeBHxuqmu7++3uvs3dtzU3N6dx66WrvSdGS00ZlTOtIdWxC4orofWC+d+wui3Y9h2a+zVSFQRVaELkTGU16pESyUBsJE4NMaxUhSZEJFxpDe1z997UjrufIBjuN5tHgHVmttbMSoAbgXsnNnD3te6+xt3XAHcB73X3b6YZu0zh4PEhWqtLZ19DqulcqGya/w1TPVL9R+Z+jVQPWeum+ccjUmCsrC7ZIzWe61BE8sLgaIJqBpVIiUjo0kmkpmqTzkK+ceD9BNX49gJfd/c9Znazmd2cWZiSjoGROIdODLGyfoZCE+MJ6Nwd9P4Uz9AuXVWtwXY+i/J27ILKFmjQ2s8ik0Uq6qm1GMOjqtonko7YyBhVxLDS6lyHIiIFbtaECNhuZv8fQSlzJ6iytyOdi7v7fcB9k45NWVjC3d+VzjVlenuP9AFwdvMMn8Id3x9U7cvWMLrK5FDL+SzKeyRLpdhFClFZHXW2l+FRLcgrko7E8ABRxkE9UiISsnR6pD5AsCDvfwBfB4aA94UZlMzNk4eDRGpd6wy/PI48EWyzlUhFi4MEaK49UqMxOPZsUEFQk4JFzlRWS416pETSFhlJzkZQIiUiIZuxRyq5WO633P3qBYpH5uHJw33UlhezrmWGXx4duyBSBMu2Zu/GVW0QOza393btDUqxq9CEyNTKaqliUD1SImmKjAYfKlKiREpEwjVjj1SyLHnMzGoXKB6Zhz2He1nbVEl95QzrZnTshPo1UN2avRtXJxfl9cnV7dPQkewhazove/GIFJKyWkqIMz4ay3UkInmheLQ/eKFESkRCls4cqWFgl5n9EBhMHXT33w8tKsnYWGKcpzv7+dXNyyktmqZin3swH2nlNiityd7Nq9qCnqX4CBSXZfbejl1QUgmtF2YvHpFCUhZ8jhVJ/XEoIjMqjfcFf92U6TNgEQlXOonUd5Nfsog91z3AWMJnLjQx0Amxo8F8pGwufFvVEvRIjcXmlkg1nguVjdmLR6SQJP8YLBrrI54YpyiaztRWkaUp4VAxPhDsqICRiIQsnTLm/2Zm5cBqd396AWKSOdhzKFmxr6ly+kZHkus1ZXs+UlUrJMZgoAsqGtJ/X6oU+/mvz04pdpFCVFYHQEl8gPi4M12Hs4hA3CPUWHIYbHkGv49EROZg1o82zexXgceB7yf3t5rZvTO+SRbck0f6KCmKsHH5DEP2Ti58uzm7N69uC7a9BzN737HnglLsTSo0ITKt8joAShMDjCW0KK/ITMaIUkMykcrkgz0RkTlIZ4zIx4HLgBMA7v44oJVTF5k9h3tZ21hJc3Xp9I06dkH1cqhfnd2bV7UE277Dmb2vI6QeMpFCkhzaV5oYJDE+h4IuIkvImEeosUFGI2VQXJHrcESkwKWTSMXdvXfSMf02X0TcnScP97G2qZLK0hlGa3aEtPBtVbIC4GB3Zu/r2AmRYmjbkt14RApJMpEqG48xElePlMhMxjxCDTHGopVQNEMFWxGRLEgnkdptZm8Doma2zsz+EfhZyHFJBg6dGKJvOM7ZzTPMjxrph+P7g2F0kSxPskj1SGW6KG/HruyXYhcpNMkKmzUMMjCitaREZjKWnCMVL6qEqBIpEQlXOonUB4ALgBHga0Af8AchxiQZ2nM4VWhipoV4dwfbMIbRldUFv7AyWZQ3VYq98dzslmIXKTTFZcQjJdRYjP6heK6jEVnUxohQwyCJ4iqIzjDUXUQkC9Kp2hcD/sTM/jbYdS1mssg8ebiPiMHG5dXTN+rYFWybN2Y/ADOobA5KoKervyOcUuwiBSheXE3N6CD9w0qkRGYy5hFqbRAvaYVoOiu8iIjMXTpV+y41s13AToKFeZ8ws0vCD03StedwHyvqylleN0MJ8Y6dwVyL5vPDCaKqNbOhfanEToUmRGYVL66hxmIMjCiREplJao4UpTOM0BARyZJ0Pq75F+C97v7fAGb2S8C/AlmuoS1z9eSRXs5trqamrBjuuRkO/uLMRn2HofWC8Ba+rW6DfT+Ef9iaXg/TcLJ+SdumcOIRKSDjpbXUEKNTiZTIjFJV+1yJlIgsgHQSqf5UEgXg7v9jZhret0iciI1y+MQwr93YRmRsAJ74GjSdB7WrTm9YvxbOvxaKQhozftlNEB/J7D0NZwdfIjIjL6uj1g7wnIb2icwojlHNEINlSqREJHzpJFIPm9n/j6DQhAM3AA+Y2cUA7v5oiPHJLJ48WWiiEjr3BAcveRe89H0LG8g5VwZfIpJ1Vl5LDYOMjGb4YYXIElPkY0TMiSqREpEFkE4itTW5/fNJx19GkFi9OpsBSWaePBIkUucvq4aOHwYHmzfkMCIRybZIeR01FmN4ZDTXoYgsaqU+AgaUzlB8SUQkS9Kp2jfnbgYzuwb4ByAKfMHdb510/u3AR5K7A8DvufsTc73fUvTk4T4aKktY21QJO58ISomHVVBCRHIiWlFHDUqkRGZT5sMAmHqkRGQBpLOO1JyYWRT4LHAtsBF4q5lNrr39PPAqd98MfAK4Pax4CtWew72c3VRJfUVJUAmvaR1UNuU6LBHJouLKeootQWJE01NFZlJGkEhFS9QjJSLhCy2RAi4D9rn7fncfBe4Arp/YwN1/5u49yd2HgJUhxlNwhscS7Osa5OzmKopJQNfeoJx4WAUlRCQnohV1ANjwiZzGIbLYlScTKUt+z4iIhCnMRGoFcHDCfnvy2HR+G/jeVCfM7CYz225m27u7u7MYYn57prOfhHtQaOLoM5AY0bpMIgXIymoBiAz35TgSkcWt3IcAKKpsyHEkIrIUpLXst5m9DFgzsb27f3m2t01xzKe5/pUEidQvTXXe3W8nOexv27ZtU15jKUpV7DunpRI6HgkONq3LYUQiEoryOgCio0qkRGZSkeqRKlciJSLhmzWRMrP/B5wDPA4kkocdmC2RagcmLma0Ejg8xfU3A18ArnX3Y7OHLClPHumjvDjKhmU18IudEC2BNq2TLFJwkj1S0THNkRKZSYUNJV8okRKR8KXTI7UN2OjumfYEPQKsM7O1wCHgRuBtExuY2WrgbuA33f2ZDK+/5O053MfZzZU0VpZCx05oOAeqWnIdlohkW1kdAMXxwdzGIbLIVfoQQ1ZOeXF5rkMRkSUgnTlSu4G2TC/s7nHg/cD9wF7g6+6+x8xuNrObk83+DGgEPmdmj5vZ9kzvs1SNjztPHuljbVMl5cWRoGJf47laO0OkECV7pIoTSqREZlJlQwxHKoMRGiIiIUunR6oJeNLMHgZGUgfd/brZ3uju9wH3TTp224TX7wHek3a0ctKLx2MMjSaCQhO9B2H4BDSp0IRIQSqtCTaJQRLjTjQy1RRUkaVt3KGaGCNRJVIisjDSSaQ+HnYQkrk9h3sBOLu5CjoeDw6qYp9IYSoqYcRKKR0fZCwxTjQSzXVEIovOGBFqLMZotFLLgIjIgph1aJ+7Pwg8BVQnv/Ymj0kOPXm4j2jEuGB5DRzZCRi0bcl1WCISkuFoFeXjMeLjKlwqMpUxj1DLIGNFVaAPG0RkAcyaSJnZW4CHgTcDbwF+YWZvCjswmdmew32sqi+npaYsmB9VuxJqZ1qmS0Ty2UhRDZXjA8QT47kORWRRinuEaosRL6rMdSgiskSkM7TvT4BL3b0LwMyagf8E7gozMIF4Ypzf+fJ2DhyPnXHuwPEYr1jXTE1ZUVCxr2ndycpeIlJ4xoqrqfQYY0qkRKY05hFqGKSvRImUiCyMdBKpSCqJSjpGetX+ZJ5+8mw3P366m62r6qgsPf1RLa8r55cvXIYN9QTFJs6/FiJ6LCKFKl5SQ421Mzg8RnN1Wa7DEVl04m7U2BBerERKRBZGOonU983sfuBryf0bmFSJT8Jx5/Z2asuL+fSbNnNu6zRlzZ//SbBtWrdwgYnIgkuU1FBDjN7BGDRrmQORyYp8LHhRUpXbQERkyZg1kXL3PzazXwdeDhhwu7vfE3pkS9zxwVF+uLeT11+4jDVNM3y61rEr2DZvXJjARCQnvLSWGotxaGg416GILErFHqzQEi1TIiUiCyOdHinc/RvAN0KORSb41uOHiCecqza0UhSdYcjekZ1Q0ageKZFCV1ZLDYMMDp45Z1JETiVSRWXqsRWRhTFtImVm/+Puv2Rm/cDEersGuLvXhB7dEnbn9nbOaa7kpec0ztywYyc0roOKhoUJTERywsrriJozMtiT61BEFqVSgkSqpFw9UiKyMKbt6nD3X0puq929ZsJXtZKocO053MuTR/q4ekMrzdUzLCo4NgzdTwcL8UaLFy5AEVlwkYpaAEb7j+U4EpHFqZRg2GtpZV1uAxGRJSOddaT+XzrHJHvu2tFOUcS4cn3zzA2794InoOnchQlMRHKmKNXrPNyb20BEFqkyDxKpkupZRnKIiGRJOvWyL5i4Y2ZFwCXhhCOj8XG++dghXnJ2I1tW1s/cOFVooum88AMTkZwqrkz+PBhSIiUylbLkHCmrVCIlIgtj2kTKzD6anB+12cz6kl/9QCfwrQWLcIn50VOd9MTGuHpDC+Ul0ZkbH9kJxRXQeuHCBCciOVNSFSRSNtqX40hEFqdyhhl3I6I5wyKyQGaaI/U37l4NfGrS/KhGd//oAsa4pNy1o52GihJedd4sw/og6JFqPAcq02grInmtvDr44zAyNpjjSEQWpwqGGKCcolItWC0iCyOddaQ+amb1wDqgbMLxn4QZ2FLU1T/Mj5/u5g1bV3BW4ywrs4+PQ+cuWPdaKKlYmABFJGdKkz1S0bGBHEcisjhVMEw/5TQXK5ESkYUxayJlZu8BPgisBB4HLgd+Drw61MiWoG8+dojEuHP1hhaiEZu5cc/zMDoYVOwTkYJnZUHVvqK41pESmUolQwxSwfKSGardiohkUTrFJj4IXAq86O5XAhcB3aFGtQS5O3dub2d9WzUvOTuNibIdO4OtFuIVWRqiRQxSRlFcPVIiU6kkxqCXYUVKpERkYaSTSA27BzVFzazU3Z8Czk/n4mZ2jZk9bWb7zOyWKc6bmX0meX6nmV2cWfiF44n2Xp7tGuDqDa00VJbM/oYjO8Gi0LYl/OBEZFEYsCpKE5ojJTKVamIMUAGRWQo1iYhkyaxD+4B2M6sDvgn80Mx6gMOzvcnMosBngdcA7cAjZnavuz85odm1BHOv1gEvAT6f3C45d+04SElRZPa1o1I6dkH9GqhuCzUuEVk8YpFKJVIi06iyIYZc86NEZOGkU2zijcmXHzezHwO1wPfTuPZlwD533w9gZncA1wMTE6nrgS+7uwMPmVmdmS1z9yOZ/CNyZd8TP+XXY18G4Pm/+Oq8rvWbiXF+tzzKim+XwSzTowA4/jycfQUk502ISOEbilRzwehOnv+LzbkORWTRWU0PQyiREpGFk06xicuBPe7e7+4Pmlk1wTypX8zy1hXAwQn77ZzZ2zRVmxXAaYmUmd0E3ASwevXq2UJeMMVllbxIMw6UF6UxHG8GVgR1tVEilcXpvaGqFTZeB5ZO1iUihWDoot9iz+N35DoMkUXpxXgD+6KaNywiCyedoX2fBybOXRqc4thUpvoL3+fQBne/HbgdYNu2bWecz5Wzzt/K1yquBeCWW86YAiYiklWX/PJvwy//dq7DEFmUbr311lyHICJLTDrFJiw59A4Adx8nzblVwKoJ+ys5c25VOm1EREREREQWlXQSqf1m9vtmVpz8+iCwP433PQKsM7O1ZlYC3AjcO6nNvcA7ktX7Lgd682V+lIiIiIiILF3pJFI3Ay8DDnFqntNNs73J3ePA+4H7gb3A1919j5ndbGY3J5vdR5CU7QP+GXhvxv8CERERERGRBZZO1b4ugt6kjLn7fQTJ0sRjt0147cD75nJtERERERGRXLEJ059OP2H2YXf/pJn9I1MXgPj9sIObipl1Ay/m4t4zaAKO5joICZWeceHTMy58esaFT8+48OkZF77F9ozPcvcpF3qdqUdqb3K7PfvxzN10/5BcMrPt7r4t13FIePSMC5+eceHTMy58esaFT8+48OXTM542kXL3bye3/7Zw4YiIiIiIiCx+0yZSZvZtphjSl+Lu14USkYiIiIiIyCI309C+Ty9YFPnv9lwHIKHTMy58esaFT8+48OkZFz4948KXN8942mITpzUK1oFaT9BD9bS7j4YdmIiIiIiIyGI1ayJlZq8HbgOeAwxYC/yuu38v/PBEREREREQWn3QSqaeAX3H3fcn9c4Dvuvv6BYhPRERERERk0Ymk0aYrlUQl7Qe6QopHRERERERk0UunR+rzwFnA1wnmSL0ZeBr4KYC73x1yjCIiIiIiIotKOonUv85w2t39t7IbkoiIiIiIyOKWVtU+EREREREROWXWOVJmdp6Z/ZeZ7U7ubzaz/x1+aCIiIiIiIotTOsUm/hn4KDAG4O47gRvDDEpERERERGQxSyeRqnD3hycdi4cRjIiIiIiISD4oSqPN0eTaUQ5gZm8CjoQa1Qyampp8zZo1ubr9GTo6OgBoa2vLcSQiMlf6PhbJf/o+Fsl/i/H7eMeOHUfdvXmqc+kkUu8DbgfWm9kh4Hng7VmMLyNr1qxh+/btubr9GW699VYAbrnllhxHIiJzpe9jkfyn72OR/LcYv4/N7MXpzs2aSLn7fuBqM6skGAo4BNwATHtRERERERGRQjbtHCkzqzGzj5rZP5nZa4AY8E5gH/CW2S5sZl80s65Utb8pzpuZfcbM9pnZTjO7eK7/CBERERERkYU0U7GJ/wecD+wCfgf4AfBm4A3ufn0a1/4ScM0M568F1iW/bgI+n8Y1RUREREREcm6moX1nu/smADP7AnAUWO3u/elc2N1/YmZrZmhyPfBlD1YEfsjM6sxsmbvnrJCFiIjM34FjMT7/4HP82a9spLwkOmv7YwMj/O9v7mZoLLEA0Uk2VCdO8GsnvsQd9b/LSKQ81+EA8NzAGgCe+tfJhYZFZDF61XnNvPvla3MdxrzMlEiNpV64e8LMnk83iUrTCuDghP325LEzEikzu4mg14rVq1dnMQQREcm22//7Ob728AHWNFbwu686Z9b2X3v4AN/b3cG5LVXYAsQn8/fLoz/myuHv8uDYBfy09JdyHQ4Aw+NB0n6oZyjHkYhIOvZ3D+Y6hHmbKZHaYmZ9ydcGlCf3DXB3r5nnvaf6felTNXT32wkqB7Jt27Yp24iISO4NjyW49/HDAHxn55FZEyl3564d7Vy4vIYv//ZLqClLp5is5Frkvm/CDvjTC7rw178i1+EA8OlPfRKAP/rgr+c4EhFJR8Ty/6OzaX9jufvs4zHmpx1YNWF/JXA45HuKiEiIfvBkJ33DcbasrOWJ9l72HOrlghW107Z/5IUeXjgW4w+uWkdDZckCRirz0rkLgOixZyE603TrhVe0yOIRkcKVy5829wLvSFbvuxzo1fwoEZH8duf2gzRXl/Knv7IRA/79oZlXyrhrx0HKiiO8en3LwgQo8zeegM5kQd5j+yARz208IiI5EtoYCjP7GnAF0GRm7cCfA8UA7n4bcB/wywTl1GPAu8OKRUREwnekd4j/efYob7l0FRevruel5zTyX091EY+PU1R05ud2sdE439l5hF86t4nzl1XnIGKZk+P7YWwI6lbDiQPQ8wI0nZvrqEREFlxoPVLu/lZ3X+buxe6+0t3/xd1vSyZReOB97n6Ou29y9+1hxSIiIuG7+9FDOHDV+hYiEeOGS1fR1T/Cd3dPPdjgvl0dxEYTXL2hldKisEeTS9YceSLYnpdc4eTgL3IXi4hIDmkgsYiIzJu7c+f2g1ywvIbL1jYA8LoL2qgsjXLPY4emfM+d2w+yrLaMV6xrWshQZb46dkGkCC54Y7DfuSe38YiI5IgSKRERmbftLwZFI67e0EpdRVA0oqw4ynVblvOz545xtH/ktPYHjsX4xfPHuWpDK8vrFsc6RJKmjl1QvwZaL4DKlmCelIjIEqRESkRE5u2u7e1TFo244dLVjMbH+fdfnF504q5H2zHg1ee3YAVQAnfJcIeOndB4LpTWQNsmOPZscFxEZIlRIiUiIvMSG43z7Z2Hefk5TayfVDRiy8pa1jRWcP/ujpPHxsedb+xoZ8uqOratrV/ocGU+BjphsBua1oEZLN8KPS8Gx0VElhglUiIiMi/fm6FohJlx42Wr2dvRz2Mv9gDw8/3HOHRiiKs3tFJTVpyLkGWujuwMto3JKn1tm8AT0P5I7mISEckRJVIiIjIvd+4Iika88rypi0b82sUriBh89eEDANy1o53KkihXnt+8kGFKNnQkE6nWzcG2LblNVfITEVlClEiJiMicHTgW46H9x7lqfcu0RSNaqst4xbpmfvRUF8cHR/ne7iO8Yl0z57Vp7ai807ELqpdD/epgv34tFFeo4ISILElKpEREZM6+kSwaceX6mYtG3HjpKo4NjvKh/3ic4bFxrt7QSnFUv4LyTqrQRHlyblskElTvO/psbuMSEckB/RYTEZE5GR937koVjVjTMGPbqza0UlNexIPPdLOqvpxfWte4QFFK1oz0w/H90HQuRCbMhVt+ERx/LjgvIrKEKJESEZE5eShZNOKq9S3Uls9cNKKkKMIbt64A4OoNrbTWlC1EiJJNqYV3U4UmUto2wdjQqUIUIiJLhBIpERGZkzt3tFNREj1j7ajp/NYvreXSNfW89oI2rR2Vj1KJUvOG04+3bQq2qtwnIktMUa4DEBGR/NM/PMb3dh/hVee1sK41vaIRZzVW8vXffamSqHzVsRPKaqF5/enHmzdApEjzpERkyVGPlIiIZOy7O48ki0a0UFKU/q8SJVF5LFVoonLS/LbisuD4MSVSIrK0KJESEZGM3bmjnZX15fzSuVOvHSUFJjEGXXuDhKmo9Mzzy7YEJdDjowsfm4hIjiiREhGRjOzvHmDHiz1cvaGVtloVjVgSjj4DiVFoWjf1+WVbIHZMw/tEZElRIiUiIhm5a0c7EYMrzmvWUL2lIlVoYnLFvpRUwYmDv1iYeEREFoFQEykzu8bMnjazfWZ2yxTna83s22b2hJntMbN3hxmPiIjMT2LcufvRQ1y8up5L1tTnOhxZKB27IFoKbVumPp9KpLr3LlxMIiI5FloiZWZR4LPAtcBG4K1mtnFSs/cBT7r7FuAK4O/MrCSsmEREZH7++9luOvqGuXpDK9VlM68dJQWkYyc0nA1VzVOfL6+HmhXBPCkRkSUizB6py4B97r7f3UeBO4DrJ7VxoNqCsSFVwHEgHmJMIiIyD3ftaKe6tIgrzp/mD2opPO6nKvaVzlDqvm1zMEdqfHzhYhMRyaEwE6kVwMEJ++3JYxP9E7ABOAzsAj7o7voJLCKyCPXGxvjBk5286vxmzm2pynU4slB6D8JwLzRNMz8qZflW6G2HvsMLEpaISK6FmUhNNQPZJ+2/DngcWA5sBf7JzGrOuJDZTWa23cy2d3d3ZztOERFJw71PHGI0Ps5V61spiqpW0ZIxW6GJlLZNgMPBh0MPSURkMQjzN2E7sGrC/kqCnqeJ3g3c7YF9wPPApCXTwd1vd/dt7r6tuVnDSUREcuHOHe2saazgZec25DoUWUgdu8Ai0xeaSGnbHGw7d4Yfk4jIIhBmIvUIsM7M1iYLSNwI3DupzQHgKgAzawXOB/aHGJOIiMzB0x397Gzv5eoNrbTWlOc6HFlIHbugdiXUTh6dP0ntSiitgaMqOCEiS0NRWBd297iZvR+4H4gCX3T3PWZ2c/L8bcAngC+Z2S6CoYAfcfejYcUkIiJzc9eOg0QjxpXrW3Idiiy0jieg6Twoq5u5nVkwvO+YFuUVkaUhtEQKwN3vA+6bdOy2Ca8PA68NMwYREZmfscQ4dz96iEvX1HPR6rpchyMLKXY8KCBx/ushksYgluUXwcO3w1BPUBJdRKSAhZpIiYjI4rPjxeM88kIPN7/qnLTaP/h0N8cGR/m9Da1UlEz4tdHzAuz4Erz6TyESnXtAQyfgex+B0YG5X0PCMdwbbGer2JfStgkSo3DH2xc8kXrj6DPBizt2Leh9RWSOzrkSLn1PrqOYFyVSIiJLzB0PH+TOHe289OxGtqyqm7X9nTsOUldezBXnTSr28+S34H/+Hta+Es559dwDOvAQ7LwjmGMT1Zrsi86yLbBiW3ptz74CWi+A/iPB1wKq9/7gRdeTC3pfEZmjyqZcRzBvSqRERJaY7oERAP79oRdnTaSODYzwX3u7+JXNy1nbPGntqIGuYHvgofklUgOdwfZX/gHWvmLu15HwFJWm1666DW7+adArtcC++Om/B+CW935owe8tInMQyf80JP//BSIikpHu/iCR+tFTXYzFxykumn7uyzcfP0x83Ll6QwvRyKTlAVMJ0LF5VmlLJWR1Z6X/B7ssXma5fY76f0hEFohWVBQRWWK6+keoqyjm2OAo33ri0Ixt79rRzrktVVx+TuOZJ7OWSHVCaTWU187vOiIiIgtIiZSIyBKSGHeOD4zy6vNbqCkr4luPT14n/ZTdh3rZe6SPqze00lQ1xaf8/RMSqdHY3IMa6ITyBijW+lQiIpI/lEiJiCwhxwdHSbjTUl3K9VtX8ND+Y3T0Dk/Z9q4d7RRHjSvPb57yPAMdUFQGo4PQsXPuQQ10QkUjFFfM/RoiIiILTImUiMgSkpofVVdRwg2XrmIs4Xz55y+c0W4knuCbjx3i8rMb2byy7swLxUeC0tgrLgn227fPPaj+DqhomF8JdRERkQWmREpEZAlJVexrqCzhguU1nNtSxQ+f7Dyj3Y/2dnFiaIyr1rdSXjJFgpMqELHiErAIHH16bgG5w2BXMLRPREQkjyiREhFZQlI9Ui01pZgZN166ime7Bnj4+WOntbtzRzuNlSVcMe2wvmQiVbMMGs6Go3MsODE6AGNDQY+UiIhIHlEiJSKyhKQSqbaaMgDeeNEKohHjq784eLJNV98wDzzdxavXt7C6YZp5S6mKfeUNsGxrUHAiMZZ5QKmETD1SIiKSZ5RIiYgsIV39w5QXR2moLAGgsaqUK85r5oGnuxgeSwBw92OHGHe4an0rkclrR6UMdATbmhWwbHMwPO/4/swD6k9eRz1SIiKSZ5RIiYgsId39I9RXFFM2Yd7TjZet5sTQGHc/2o67c9eOdjYsq+ElZ8+Q3Ax0AQY1y6Ftc3Ds4MOZB5Tq2aqcZgihiIjIIqVESkRkCenuH6G+soSyolOJ1BXnN1NfUcy3nzjC4wdPsK9rgKvWt1Cf7LWa0kAnlNVCaQ20bQqOde7OPKCTc61WZP5eERGRHFIiJSKyhAQ9UiWUFJ368V8cjfBrF6/k4ReO8/kHnqOkKMKr17fMfKGBrmA4XnE5VDZBVWswTypTA51gUahuy/y9IiIiOaRESkRkCUkN7ZvsLdtWkRh3fvBkJy8/p5GNy2tmvtBAZ1Agorg82F+2JUikxsczC2igCyrqoaQys/eJiIjkmBIpEZE89+Az3dzzWPus7YbHEvSPxKmvOHPI3vlt1WxcFiRPV29opax4lsVxJy+iu2wLnDhwas5TugY6oKLxVEImIiKSJ0JNpMzsGjN72sz2mdkt07S5wsweN7M9ZvZgmPGIiBSif/mf5/nr+55iLDFzb1Cq9PlUiRTAh15zHq9Y18QV62cp/OB+qkcqpW0z+Di0P5JR7CevU1SW2ftERERyrCisC5tZFPgs8BqgHXjEzO519ycntKkDPgdc4+4HzGyWQfkiIjJZd/8wxwZGOD44SmvN9AlJVzKRqqs8c2gfwGs2tnL1hhbMpil5njLcC4nR00uWpwpOHHkCNl6XfvD9nbBqNcx2TxERkUUmzB6py4B97r7f3UeBO4DrJ7V5G3C3ux8AcPeuEOMRESlIXX0jjDu8cHRwxnapHqmGGarxzZpEwalKexWNp47VnQUlVXDs2dnfnzKegNhRLcYrIiJ5KcxEagVwcMJ+e/LYROcB9Wb2gJntMLN3THUhM7vJzLab2fbu7u6QwhURyT/xxDjHB0cBeP7YLInUQJBILaud53yk1DyoiT1SkQi0XphZ5b7YsWA44MSETEREJE+EmUhN9bGmT9ovAi4BXg+8DvhTMzvvjDe53+7u29x9W3OzFm0UEUk5Pjh68gfr4Z6hGdt29w0TMWirned8pOkW0V2+FY49B0O96V2nvyPYVqhHSkRE8k+YiVQ7sGrC/krg8BRtvu/ug+5+FPgJsCXEmERECkpq3hNA54TXU+keGKGmvJjKklkq8s0mlUjVrDz9eNtmiA/DkcfTvE5yiKCG9omISB4KM5F6BFhnZmvNrAS4Ebh3UptvAa8wsyIzqwBeAuwNMSYRkYKSGq4H0Nk3PHPb5GK85dlIpKLFUDWpPlCq4MShHelfB7QYr4iI5KXQqva5e9zM3g/cD0SBL7r7HjO7OXn+Nnffa2bfB3YC48AX3H13WDGJiBSa7r4gkWqpLqWrb+Yeqa7kYrxlRfNNpLpOX4w3pXk9RIrg6DNpXifVszV5+qyIiMjiF1oiBeDu9wH3TTp226T9TwGfCjMOEZFCleqRunBFLTvbTzA8lph2Md3u/hE2LqshEplnqfGBzmBeU3HF6ceLSqDp/PQLTgx0QXGlik2IiEheCnVBXhERCVd3/wiVpVHObq7k2MAovUNjU7Zz95ND++bt5CK6pWeeW74lSKTio+ldp6IBirUYr4iI5B8lUiIieSyVHK1prMCB/d0DU7Y7ERsjPu7UT7MYb0b6O4IEaKo1p9q2wFAPdD81+3Wm69kSERHJA0qkRETy2MlEqqkSgOenWZQ3NQRw3j1SiTGIHZ++0l6q4MTBh2e/Vn9HcJ1oFpI7ERGRBaZESkQkj3X2D1NfUcyaxiCROnxi6sp93cnS6HXzTaQGjwI+/dpPbRcmb5hGj9Rgl9aQEhGRvKVESkQkj3X3j1BXUUJbTRlRs9PWlZrcDqC5ep6JVKrS3nQFIspqoXY1HHt25uuMxmCkX4mUiIjkLSVSIiJ5anAkTmw0QUNlCUXRCG21ZdOuJdXVHxxfXls+5fm0pRbRnSkBWrY5KDgxnpi+zaAW4xURkfymREpEJE8dTc572jz6KOy6i1X15XRNk0h1949QUhShsWq+PVIdwbZ62fRtlm2F3vbgazr9qZ4tJVIiIpKflEiJiOSp1HC9lxz5Kvzgf7OqvozO/hGGx87sCepOLsZbXjzP5QPTWUQ3VXCifYaCEyeHCDbNLx4REZEcUSIlIpKnUvOhqsaOwVAPK6sjHB8cpWfwzDWcugeC6n5lJfP8sT/QBSVVUF4/fZtUItWxa4brJBOp6uXzi0dERCRHlEiJiOSpVI9UxehRiA+ztiIY1re/+8wS6F19QSJVWhSd301Taz8VzbCIbs3yINE6OkPBiYEusEjQVkREJA8pkRIRyVPd/SMU2ThFIz0ArIkeBaZeS6qrf4T6ynnOj4IgAaponHkRXbOgV+rYPnCf5jqdUFYHpdXzj0lERCQHlEiJiOSp7v4Rzi4fwnwcgOUWJFJHek8vODEST9A7NEZ9RRYWvj25iO4sc62WbYWe54PFe6eS6tkqnqFnS0REZBFTIiUikqe6B0ZYWzZwcr8hcZSiiJ0sdZ5ybCCYM1U/38V44VQCNJu2zZAYg8OPznydonmWYxcREckRJVIiInmqq3+Y1SX9J/cjwz0sm2ItqdRcqnn3SI0MwFgsvbWfUgUnDk2TSPV3BteJ6NeQiIjkJ/0GExHJU119IywvOpVIETvOqoYKOvtGTmuXSqTq5tsjNZDB2k9N64KCFMemKDgxPg6D3cFcKxERkTylREpEJA+NjzvHBkZpi/YGB0prgkSqvoKu/uHT1pLqTi7c21ZbOr+bDnQF23QSqUgUmtcHBScmGz4B42NajFdERPJaqImUmV1jZk+b2T4zu2WGdpeaWcLM3hRmPCIihaInNkrCnWZ6oKQSalfB0DFW1pfTExvj6MCpXqmuZA/Vstp5zkc62SPVnF775VuDRGo0dvrx/o5gm84QQRERkUUqtETKzKLAZ4FrgY3AW81s4zTt/ha4P6xYREQKTaqXqX78RJCQVLcle6SCKnj7uwcmtB2muqyImvJ5zpFKJVK1K9Jr37YZRvqhc/fU11GPlIiI5LEwe6QuA/a5+353HwXuAK6fot0HgG8AXSHGIiJSUFLznqrjx4KEpGYZxI6zsjpYcPf5o7HT2tZXlFBWnIXFeC0SJG3paNscbNu3T7pO8sd9Vev84hEREcmhMBOpFcDBCfvtyWMnmdkK4I3AbSHGISJScFLD9arGjgVFG6qXwfAJVlYEa0pNrNwXJFLFlGcjkSpvgJKq9Nq3XhAkXt1PnXkdgJo0e7ZEREQWoTATKZvi2OQl7v8v8BF3T0zR9tSFzG4ys+1mtr27uztb8YmI5K3U0L7SkaNBclPVCj5Oi3dRHLXTKvd1JXukopGpfixnYKArufZTmovollRA/dozC04MdAbXqGiaXzwiIiI5FGYi1Q6smrC/Ejg8qc024A4zewF4E/A5M3vD5Au5++3uvs3dtzU3pznJWUSkgHX3j9BQPEZ0bDBIbpLD5CJ9h1hWW36yR8rd6e4fmX/pczi1iG5xBkUrlm0JEqlEfMJ1upI9WxXzj0lERCRHwkykHgHWmdlaMysBbgTundjA3de6+xp3XwPcBbzX3b8ZYkwiIgWhu3+Es8sHg50JiRT9R1jVUE5Xf5BI9Y/EGYmP01A5z0ITEFTbK28Ay6Bna9mWIAE7/vypYwMdmSdkIiIii0xoiZS7x4H3E1Tj2wt83d33mNnNZnZzWPcVEVkKuvtHWFOarMxX3gBVLcHr2FFWN1TQ1TfC0GjiZFGK+vn2SI0nYPBo5pX22jYF2/aHTx3rT861KprnulYiIiI5VBTmxd39PuC+ScemLCzh7u8KMxYRkULS2T/MS4r7gp3K5lM9UkPHWVlfwYmhMbr7R7KXSMWOgycyX/spVblvYgn0gU5o2TC/eERERHIs1AV5RUQkHN39IyyPJhOpmhXBfKOSqqAEen0wZG5/98DJRKqxap6J1FzXfqpqDnrLUgUn4iMwfEJrSImISN4LtUdKRESyb3gsQf9wnJZIb1BePNUbVdWSTKSCIg4vHB8kEVRDp602zUp705nPIrptm4NEyh0Gu+d+HRERkUVEPVIiInnmaLL0eaP3QHk9lFYHJ6paYeg4q5I9Uh29wdC+oojRWjPP+UgnF9Fdlvl7l22FnheCZKw/mZBlOkRQRERkkVGPlIhInkkN16sb7wkSklT1u+o2aN9OU3mEkmiEzr5hImbUVZRQUTLPH/cDHcG2Znnm723bBD4O7Y+AJRcFVo+UiIjkOSVSIiJ5JpVIVY0dh5rGUwvkVrVC7DiRxBDL68ro6hsmGo1QX1FMWXF0fjcd6AoStso5LKK7LFlw4sjjULMyeJ3aioiI5CklUiIieaYrmUhVjh6FirMgkhylXdUCY4MQ62FVQwVHeocpjhr1FSWUFs1zJPdA5+m9X5moWwMllXB0H0SSRS+q5zBEUEREZBHRHCkRkTzT3T9ChHGKh4+dPteoqi3Y9h5kVUMFXX3DdPaNUF9RjGWyiO5UBrqC4XhFc0ikIhFouSAoODHQCWW1wZeIiEgeUyIlIpJnugdGOKtsGPP46XONUtX7+g6xqr6CvuE4xwdHqaucZ+lzgP6O5CK6c7zW8ovg+HPQe3DuPVsiIiKLiBIpEZE8090/wprygWDntESqJdgOdJ5cSwqgYb6L8SavSUXj3N/ftgnGhoKCExVKpEREJP8pkRIRyTPd/SOcVTJVIpXskZqwKC9AfUXx/G44NgQjffOrtJcqODHUEyRkkXkWvxAREckxJVIiInmmq2+YFUX9wU5qXhQEFfUsAkOnFuUFqJ9vj1RqDan5rP3UvB4iRfO/joiIyCKhREpEJI+4O90DI7RF+4IDNStOnYxEg96e2DGaqk5V6mupzdJivPPpkSoqhcZ187+OiIjIIqFESkQkj/QNxRlLOM3WE6wfNXneUlULxI5jZqyoC4b3La+Z53ykgc5gO585UgDLtyavo0RKRETyn9aREhHJI90DwwA0jPdMXbShqg362sGdVQ0VdPYNU1s5aY5U+w7Y/kXA07vpseeC7XwX0W3bDE98TUP7RESkICiREhHJI6nFeGsSx4MeouKK0xtUt0HHTogP8/pNy6gsLaKyZNKP+p//E+y9N7MepmVboXbFrM1mdN7r4ImvQsvG+V1HRERkEVAiJSKSR7qTiVTV2DGoXXXmuk5VLUFlvNFB3nLpKn7t4hUURSeN4u7YCasvhxu+kkH1PIPSqvkF33gO3PRgcC0REZE8p0RKRCSPpBKpspGjULHlzAZVrTAeh/5OqGw6M4kaGQiG6q15BZTXhR/wZCp7LiIiBSLUYhNmdo2ZPW1m+8zslinOv93Mdia/fmZmU/xVICIiKd39I1RF4xSN9k091yi1llTvwakv0PUk4NB4bmgxioiILAWhJVJmFgU+C1wLbATeamaTB8Y/D7zK3TcDnwBuDyseEZFC0N0/wtnlg8HOVNXvUolU3+GpL3DkiWDbfH72gxMREVlCwuyRugzY5+773X0UuAO4fmIDd/+Zu/ckdx8C5lkSSkSksHUPjLCmNJVITVEsIpVIxbqnvkDHLiitUcEHERGReQozkVoBTBxb0p48Np3fBr4XYjwiInmvq3+EVSX9wc6UiVRLsI0dn/oCHbuCYX3zXRNKRERkiQszkZqqLNOUi5aY2ZUEidRHpjl/k5ltN7Pt3d3TfMoqIrIEdPUNs7yoL9ipWX5mg9LqYKHeoSkSqUQcuvZA07lQXBZuoCIiIgUuzESqHVg1YX8lcMagfTPbDHwBuN7dj011IXe/3d23ufu25ubmUIIVEVnsxhLj9MTGaI30AgbVy85sZBb0Sk3VI3XsWYiPqNCEiIhIFoSZSD0CrDOztWZWAtwI3DuxgZmtBu4GftPdnwkxFhGRvHdsYBSAJu+BstpgrtNUqlqnTqSO7Ay2SqRERETmLbR1pNw9bmbvB+4HosAX3X2Pmd2cPH8b8GdAI/A5MwOIu/u2sGISEclnqTWk6sZ7gjlOxeVTN6xuC+ZCJeIQnfBjvmMnRIthmVaaEBERma9QF+R19/uA+yYdu23C6/cA7wkzBhGRQtE9MAxAdfw4VDVMn0hVtULsQYgPQbT61PGOXdBwNlS2LEC0IiIihS3UBXlFRCR7uvqCHqnK0aPBYryR6NQNq1phpA+GTpw65h70SDWeGxSkEBERkXlRIiUikieCoX1OycjRqRfjTUmVQO89dOpY3yEY6oHGdUFBChEREZkXJVIiInmie2CEFaUjRBKjQY/UdKragm3fhESqY1ewVaEJERGRrFAiJSKSJ7r7R1hbPhjspNMj1X/k1LEjOwGDNhWaEBERyQYlUiIieSCeGGf7iz2sr4oFB2ZMpFqD7cRFeTt2Qu1KqFsZXpAiIiJLiBIpEZE88OAz3XT3j/CylkRwoLJ1+saVyYXLJ64l1bErGNZXVhdajCIiIkuJEikRkTxw1452asuL2VQXVO6jdsX0jYtKoLweYseC/aETcOJFaDoXIvqxLyIikg36jSoissgdHxzlh3s7ueK8Zhr9eLCobqrXaTpVraeG9nXuDrYqNCEiIpI1SqRERBa5bz1+iHjCuXpDK5HBbqhonH4x3pSq1lND+47sDLYtF4QbqIiIyBKiREpEZJG7c3s75zZX8dJzG2GgMyh9Xlwx85uq24IeqbHhYH5UeQM0rVuYgEVERJYAJVIiIovYnsO9PHmkj6s2tNBUVRokUhUNUFQ68xurWoIeqbFYULGv6dyZ154SERGRjCiREhFZxO7a0U5RxHj1+uTaUKkeKbOZ31jVConRYFHe7qegcV1QhEJERESyQomUiMgiNRof55uPHeIlZzeyeWUdJMaCXqaZ1pBKSa0l9fxPYDyuQhMiIiJZpkRKRGSR+tFTnfTExrh6QwvlJVEY7AY8KDYxm4mJFAQ9UiIiIpI1SqRERBapO7e301BZwpXnTxjWB+nNdUolUgd/AUVlsGxzOEGKiIgsUUqkREQWoa7+YR54ppsrz29hVUOyQt9AV7BNa2hfMvka6gmG9VU2hROoiIjIEqVESkRkEfrmY4dIjDtXb2ghGkkWlkj1SFUvn/0C5fUQKQpeN54LJZXhBCoiIrJEhZpImdk1Zva0me0zs1umOG9m9pnk+Z1mdnGY8YiI5AN3587t7axvq+YlZ0+YD5VKpGrSSKTMoDLZK9WkQhMiIiLZFloiZWZR4LPAtcBG4K1mtnFSs2uBdcmvm4DPhxWPiEi+eKK9l2e7Brh6QysNlRNKlvd3QmkNlNeld6Hq5DwpVewTERHJuqIQr30ZsM/d9wOY2R3A9cCTE9pcD3zZ3R14yMzqzGyZux8JMS4RkZN6e46yaei/AfjFP+7NcTSB44Oj/HXJKK853grfmbDw7v4HkovxlqV3oeo2sAi0bQklThERkaUszERqBXBwwn478JI02qwATkukzOwmgh4rVq9enfVARWTpGhuJ8RJ2AmBHcxzMBCVFRtWLEZi87u6610BxRXoXWfc6cKB2RbbDExERWfLCTKQm//qH4Fd6pm1w99uB2wG2bdt2xnkRkblqalvNreXvw915782/k+twTiqKRrDiKUZfR4ogmuaP7m3vhq1vh6KS2duKiIhIRsJMpNqBVRP2VwKH59BGRCR0ZkZtfQGWCFcSJSIiEoowq/Y9Aqwzs7VmVgLcCNw7qc29wDuS1fsuB3o1P0pERERERBa70Hqk3D1uZu8H7geiwBfdfY+Z3Zw8fxtwH/DLwD4gBrw7rHhERERERESyJcyhfbj7fQTJ0sRjt0147cD7woxBREREREQk20JdkFdERERERKQQWdAplD/MrBt4MddxTNIELKLCyRICPePCp2dc+PSMC5+eceHTMy58i+0Zn+XuzVOdyLtEajEys+3uvi3XcUh49IwLn55x4dMzLnx6xoVPz7jw5dMz1tA+ERERERGRDCmREhERERERyZASqey4PdcBSOj0jAufnnHh0zMufHrGhU/PuPDlzTPWHCkREREREZEMqUdKREREREQkQ0qkREREREREMqRESkREREREJENKpERERERERDKkREpERERERCRDSqREREREREQypERKREREREQkQ0qkREREREREMlSU6wAy1dTU5GvWrMl1GCd1dHQA0NbWluNIRGSu9H0skv/0fSyS/xbj9/GOHTuOunvzVOfyLpFas2YN27dvz3UYJ916660A3HLLLTmORETmSt/HIvlP38ci+W8xfh+b2YvTndPQPhERERERkQyFlkiZ2RfNrMvMdk9z3szsM2a2z8x2mtnFYcUiIiIiIiKSTWH2SH0JuGaG89cC65JfNwGfDzEWERERERGRrAltjpS7/8TM1szQ5Hrgy+7uwENmVmdmy9z9SKb3Ghsbo729neHh4bmGO2dXXHEFAHv37l3wey81ZWVlrFy5kuLi4lyHIiIieeJHT3Wy90h/rsMQkUk2LqvhyvUtuQ5jXnJZbGIFcHDCfnvy2BmJlJndRNBrxerVq8+4UHt7O9XV1axZswYzCyfaaRw5EoS7bNmyBb3vUuPuHDt2jPb2dtauXZvrcEREJE/8/tceZ2AknuswRGSS121sVSI1D1NlPD5VQ3e/HbgdYNu2bWe0GR4ezkkSJQvHzGhsbKS7uzvXoYiISJ4YHIkzMBLnNy4/i5teqQ/hRBaTypK8Kx5+hlz+C9qBVRP2VwKH53oxJVGFT89YREQycXRgBICmyhJWN1TmOBoRKTS5LH9+L/COZPW+y4HeucyPWgyOHz/O1q1b2bp1K21tbaxYseLk/ujo6Izv3b59O7//+7+/QJGKiIgsHd39QSJVX1GS40hEpBCF1iNlZl8DrgCazKwd+HOgGMDdbwPuA34Z2AfEgHeHFUvYGhoaePzxxwH4+Mc/TlVVFX/0R3908nw8HqeoaOr/1Nu2bWPbtm0LEaaIiMiScjKRqlSRIhHJvjCr9r11lvMOvC+s++fau971LhoaGnjssce4+OKLueGGG/iDP/gDhoaGKC8v51//9V85//zzeeCBB/j0pz/Nd77zHT7+8Y9z4MAB9u/fz4EDB/iDP/gD9VaJiIjMUVcykVpeX57jSESkEOX/LK9J/s+39/Dk4b6sXnPj8hr+/FcvyPh9zzzzDP/5n/9JNBqlr6+Pn/zkJxQVFfGf//mffOxjH+Mb3/jGGe956qmn+PGPf0x/fz/nn38+v/d7v6dy3yIiInPQ3T9CxKCluizXoYhIASq4RGoxefOb30w0GgWgt7eXd77znTz77LOYGWNjY1O+5/Wvfz2lpaWUlpbS0tJCZ2cnK1euXMiwRURECkJ3/wh15SVUlerPHRHJvoL7yTKXnqOwVFaeqhD0p3/6p1x55ZXcc889vPDCCycX8p2stLT05OtoNEo8rrUvRERE5qJ7YIS6ymLKi6O5DkVEClAuq/YtKb29vaxYsQKAL33pS7kNRkREZAno7h+mvqKE0iL9uSMi2aefLAvkwx/+MB/96Ed5+ctfTiKRyHU4IiIiBa+zb4SGihIiEa1DKCLZV3BD+3Lt4x//+JTHX/rSl/LMM8+c3P/EJz4BwBVXXHFymN/k9+7evTuMEEVERAre+LhzbGCUugoVbBKRcKhHSkRERApOT2yUhLsW4xWR0CiREhERkYLTPZBajFeJlIiEQ4mUiIiIFJzu5GK89RraJyIhUSIlIiIiBSeVSLXUlM7SUkRkbpRIiYiISMHpSiZSy2rLchyJiBQqJVIiIiJScLr7RygrjtBQqR4pEQmHEqksOH78OFu3bmXr1q20tbWxYsWKk/ujo6Ozvv+BBx7gZz/72bTnv//973PZZZexfv16tm7dyg033MCBAwey+U/IihMnTvC5z33u5P7hw4d505veNKdrvetd7+Kuu+7KVmgiIrLEdPePUF9RQllxNNehiEiB0jpSWdDQ0MDjjz8OBGtBVVVV8Ud/9Edpv/+BBx6gqqqKl73sZWec2717Nx/4wAe499572bBhAwD33nsvL7zwAqtXrz6tbTwep6god480lUi9973vBWD58uVKhkREJCe6+0eoqyihXImUiIREPVIh2bFjB6961au45JJLeN3rXseRI0cA+MxnPsPGjRvZvHkzN954Iy+88AK33XYbf//3f8/WrVv57//+79Ou87d/+7d87GMfO5lEAVx33XW88pWvBIIFfT/2sY/xqle9in/4h3/g29/+Ni95yUu46KKLuPrqq+ns7ASCBO+d73wnr33ta1mzZg133303H/7wh9m0aRPXXHMNY2NjAKxZs4aPfexjvPSlL2Xbtm08+uijvO51r+Occ87htttuA2BgYICrrrqKiy++mE2bNvGtb30LgFtuuYXnnnuOrVu38sd//Me88MILXHjhhQAkEgn+6I/+iE2bNrF582b+8R//EYC/+Iu/4NJLL+XCCy/kpptuwt3DeiQiIrKEdPUPU19RTEmR/tQRkXAUXo/U926Bjl3ZvWbbJrj21rSbuzsf+MAH+Na3vkVzczP/8R//wZ/8yZ/wxS9+kVtvvZXnn3+e0tJSTpw4QV1dHTfffPO0vVh79uyZtXfrxIkTPPjggwD09PTw0EMPYWZ84Qtf4JOf/CR/93d/B8Bzzz3Hj3/8Y5588kle+tKX8o1vfINPfvKTvPGNb+S73/0ub3jDGwBYtWoVP//5z/nQhz7Eu971Ln76058yPDzMBRdcwM0330xZWRn33HMPNTU1HD16lMsvv5zrrruOW2+9ld27d5/snXvhhRdOxnj77bfz/PPP89hjj1FUVMTx48cBeP/738+f/dmfAfCbv/mbfOc73+FXf/VX0/5vLSIiMpWu/hE2tNXkOgwRKWChJlJmdg3wD0AU+IK73zrpfC3w78DqZCyfdvd/DTOmhTAyMsLu3bt5zWteAwS9McuWLQNg8+bNvP3tb+cNb3jDycQlXceOHeOqq64iFotx0003nUywbrjhhpNt2tvbueGGGzhy5Aijo6OsXbv25Llrr72W4uJiNm3aRCKR4JprrgFg06ZNpyU911133cnjAwMDVFdXU11dTVlZGSdOnKCyspKPfexj/OQnPyESiXDo0KGTPV/T+c///E9uvvnmk0MPGxoaAPjxj3/MJz/5SWKxGMePH+eCCy5QIiUiIvMyPJagfzhOnRbjFZEQhZZImVkU+CzwGqAdeMTM7nX3Jyc0ex/wpLv/qpk1A0+b2VfcffYKDdPJoOcoLO7OBRdcwM9//vMzzn33u9/lJz/5Cffeey+f+MQn2LNnz4zXuuCCC3j00UfZsmULjY2NPP7443z6059mYGDgZJvKysqTrz/wgQ/wh3/4h1x33XU88MADfPzjHz95rrQ0qFwUiUQoLi7GzE7ux+PxKdulXk9s95WvfIXu7m527NhBcXExa9asYXh4eNb/Jqn7pQwPD/Pe976X7du3s2rVKj7+8Y/Peh0REZHZHB3QYrwiEr4wBw5fBuxz9/3JxOgO4PpJbRyotuAv7CrgOBAnz5WWltLd3X0ykRobG2PPnj2Mj49z8OBBrrzySj75yU9y4sSJkz0+/f39U17rwx/+MH/1V3/F3r17Tx6LxWLT3ru3t5cVK1YA8G//9m9Z/Fedfo+WlhaKi4v58Y9/zIsvvggw47/jta99LbfddtvJhO348eMnk6ampiYGBgZUmEJERLIitRhvfYV6pEQkPGEmUiuAgxP225PHJvonYANwGNgFfNDdx0OMaUFEIhHuuusuPvKRj7Blyxa2bt3Kz372MxKJBL/xG7/Bpk2buOiii/jQhz5EXV0dv/qrv8o999wzZbGJTZs28Q//8A+84x3vYP369bz85S9n7969vO1tb5vy3h//+Md585vfzCte8QqamppC+fe9/e1vZ/v27Wzbto2vfOUrrF+/HoDGxkZe/vKXc+GFF/LHf/zHp73nPe95D6tXr2bz5s1s2bKFr371q9TV1fE7v/M7bNq0iTe84Q1ceumlocQrIiJLSyqRatDQPhEJkYVVJc3M3gy8zt3fk9z/TeAyd//AhDZvAl4O/CFwDvBDYIu790261k3ATQCrV6++JNUDkrJ3797TqtotpFQ1vtQcKAlXLp+1FK5bbw2GBN9yyy05jkRE5mri9/FXfvEif3LPbu646SVcfnY4HyqKSPYtxt/HZrbD3bdNdS7MHql2YNWE/ZUEPU8TvRu42wP7gOeB9ZMv5O63u/s2d9/W3NwcWsAiIiKS/7r6RjCgraYs16GISAELM5F6BFhnZmvNrAS4Ebh3UpsDwFUAZtYKnA/sDzEmERERKXDdAyPUlBdTVaZiEyISntCq9rl73MzeD9xPUP78i+6+x8xuTp6/DfgE8CUz2wUY8BF3PxpWTCIiIlL4uvtHqK8oprw4mutQRKSAhbqOlLvfB9w36dhtE14fBl6bpXudUV5bCktY8/lERKSwdPePUFdRQpkSKREJUaiJ1EIpKyvj2LFjNDY2KpkqUO7OsWPHKCvTeHcREZlZV/8w61triEYMnvsRdD45+5tEZGG1boRzXp3rKOalIBKplStX0t7eTnd394Lfu7e3F4ATJ04s+L2XmrKyMlauXJnrMEREZBFzd7r7R3jp2cXgDv/xmzA6MPsbRWRhrf8VJVKLQXFxMWvXrs3JvRdjmUYREZGlqm8ozljCqasogdjxIIm67Ca45N25Dk1EJiqry3UE81YQiZSIiIgIQPfAMAD1FSVwIrnuZN3qYBiRiEgWhVn+XERERGRBdfWPAFBfUXwqkapelsOIRKRQKZESERGRgtGdTKSaqkvhxIHgYOO5OYxIRAqVEikREREpGKlEanltWZBIlVarR0pEQqFESkRERApGd/8IxVGjqaoUel6EqjYoqcx1WCJSgJRIiYiISMHo7h+hvqKEipKiYI5UtRIpEQmHEikREREpGN0DQSJVWmTB0L7qZWCW67BEpAApkRIREZGC0dU/Ql1FMaWjxyE+HPRIiYiEQImUiIiIFIzu/hEaKkuwVMU+JVIiEhIlUiIiIlIQEg7HB0dPX4xXiZSIhESJlIiIiBSEIS8CoK6ieMIaUutyGJGIFDIlUiIiIlIQhsaDRCrokToApTVQ1ZrjqESkUCmREhERkYIQ82IgmUj1JEufl1blOCoRKVRKpERERKQgxJI9Um21pcnS521QXJHjqESkUIWaSJnZNWb2tJntM7NbpmlzhZk9bmZ7zOzBMOMRERGRwhVLzpFaVlMGvVpDSkTCVRTWhc0sCnwWeA3QDjxiZve6+5MT2tQBnwOucfcDZtYSVjwiIiJS2GLjxVSVFlHrPRAfUcU+EQlVmD1SlwH73H2/u48CdwDXT2rzNuBudz8A4O5dIcYjIiIiBWzIi6ivKKZi8FBwQImUiIQozERqBXBwwn578thE5wH1ZvaAme0ws3eEGI+IiIgUsNh4EfUVJZQOJP/8qJn8Z4eISPaENrQPmGpQsk9x/0uAq4By4Odm9pC7P3PahcxuAm4CWL16dQihioiISL6LeRF1FSVE+5KJVMM5uQ1IRApamD1S7cCqCfsrgcNTtPm+uw+6+1HgJ8CWyRdy99vdfZu7b2tubg4tYBEREclfsfEiGiqTi/GW1UGVpl6LSHjCTKQeAdaZ2VozKwFuBO6d1OZbwCvMrMjMKoCXAHtDjElEREQK0KhHiBM9fQ2pEq0hJSLhCW1on7vHzez9wP1AFPiiu+8xs5uT529z971m9n1gJzAOfMHdd4cVk4iIiBSm1BpSdRUl8MIBqFsNxeU5jkpEClmYc6Rw9/uA+yYdu23S/qeAT4UZh4iIiBS2oeQaUvXlUeg9CKsu0xpSIhKqUBfkFREREVkIqR6p5UV9kBhV6XMRCZ0SKREREcl7MS8GYLV1BweqlEiJSLiUSImIiEjei40XYTiNiY7ggNaQEpGQKZESERGRvBfzIipsjIrBQ8GBJq0hJSLhUiIlIiIieS82Xky5xSnuPwjl9VChdSdFJFxKpERERCTvDXkR5ZE40b6DQaGJUq0hJSLhUiIlIiIieS82XkSFxbETB6B6mdaQEpHQKZESERGRvJYYd4a8iCobgd52lT4XkQUR6oK8IiIiImGLGLyr9kmqx/tgbEylz0VkQahHSkRERPKamVFq47RYT3BAPVIisgCUSImIiEhBqPUTyRcrcxqHiCwNSqRERESkINR6b/CiQWtIiUj4lEiJiIhIQajzXqhohMqmXIciIkuAEikREREpCLXeG5Q+L9EaUiISPiVSIiIiUhCCRKoNistyHYqILAFKpERERCTvmY9T7X0qfS4iC0aJlIiIiOS9avqJMq7S5yKyYEJNpMzsGjN72sz2mdktM7S71MwSZvamMOMRERGRwnSyYp8SKRFZIKElUmYWBT4LXAtsBN5qZhunafe3wP1hxSIiIiKF7dQaUqtyGoeILB1h9khdBuxz9/3uPgrcAVw/RbsPAN8AukKMRURERApYrffiAPVrcx2KiCwRYSZSK4CDE/bbk8dOMrMVwBuB22a6kJndZGbbzWx7d3d31gMVERGR/FY33ks/VVDZmOtQRGSJCDORsimO+aT9/wt8xN0TM13I3W93923uvq25uTlb8YmIiEiBqPVeeq1Oa0iJyIIpCvHa7cDEgcorgcOT2mwD7jAzgCbgl80s7u7fDDEuERERKTC13svByApWaQ0pEVkgYSZSjwDrzGwtcAi4EXjbxAbufnIgs5l9CfiOkigRERHJSCJONX30nlnTSkQkNKElUu4eN7P3E1TjiwJfdPc9ZnZz8vyM86JERERE0hKJ8pnSDwLOy3Mdi4gsGWH2SOHu9wH3TTo2ZQLl7u8KMxYREREpUGYMW3muoxCRJSbUBXlFREREREQKkRIpERERERGRDCmREhERERERyZASKRERERERkQwpkRIREREREcmQEikREREREZEMKZESERERERHJkBIpERERERGRDCmREhERERERyZASKRERERERkQwpkRIREREREcmQEikREREREZEMKZESERERERHJkBIpERERERGRDCmREhERERERyZASKRERERERkQyFmkiZ2TVm9rSZ7TOzW6Y4/3Yz25n8+pmZbQkzHhERERERkWwILZEysyjwWeBaYCPwVjPbOKnZ88Cr3H0z8Ang9rDiERERERERyZYwe6QuA/a5+353HwXuAK6f2MDdf+buPcndh4CVIcYjIiIiIiKSFWEmUiuAgxP225PHpvPbwPdCjEdERERERCQrikK8tk1xzKdsaHYlQSL1S9Ocvwm4CWD16tXZik9ERERERGROwuyRagdWTdhfCRye3MjMNgNfAK5392NTXcjdb3f3be6+rbm5OZRgRURERERE0hVmIvUIsM7M1ppZCXAjcO/EBma2Grgb+E13fybEWERERERERLImtKF97h43s/cD9wNR4IvuvsfMbk6evw34M6AR+JyZAcTdfVtYMYmIiIiIiGRDmHOkcPf7gPsmHbttwuv3AO8JMwYREREREZFsC3VBXhERERERkUKkREpERERERCRDSqREREREREQypERKREREREQkQ0qkREREREREMqRESkREREREJENKpERERERERDKkREpERERERCRDSqREREREREQypERKREREREQkQ0qkREREREREMqRESkREREREJENKpERERERERDKkREpERERERCRDSqREREREREQypERKREREREQkQ0qkREREREREMhRqImVm15jZ02a2z8xumeK8mdlnkud3mtnFYcYjIiIiIiKSDaElUmYWBT4LXAtsBN5qZhsnNbsWWJf8ugn4fFjxiIiIiIiIZEuYPVKXAfvcfb+7jwJ3ANdPanM98GUPPATUmdmyEGMSERERERGZtzATqRXAwQn77cljmbbBzG4ys+1mtr27uzvrgYqIiIiIiGQizETKpjjmc2iDu9/u7tvcfVtzc3NWghMREREREZmrMBOpdmDVhP2VwOE5tBEREREREVlUwkykHgHWmdlaMysBbgTundTmXuAdyep9lwO97n4kxJhERERERETmrSisC7t73MzeD9wPRIEvuvseM7s5ef424D7gl4F9QAx4d1jxiIiIiIiIZEtoiRSAu99HkCxNPHbbhNcOvC/MGERERERERLIt1AV5RURERERECpEFnUL5w8y6gRdzHcckTcDRXAchodIzLnx6xoVPz7jw6RkXPj3jwrfYnvFZ7j5l2fC8S6QWIzPb7u7bch2HhEfPuPDpGRc+PePCp2dc+PSMC18+PWMN7RMREREREcmQEikREREREZEMKZHKjttzHYCETs+48OkZFz4948KnZ1z49IwLX948Y82REhERERERyZB6pERERERERDKkRGoezOwaM3vazPaZ2S25jkfmxsxWmdmPzWyvme0xsw8mjzeY2Q/N7Nnktn7Cez6afO5Pm9nrche9ZMLMomb2mJl9J7mvZ1xAzKzOzO4ys6eS388v1TMuLGb2oeTP6d1m9jUzK9Mzzm9m9kUz6zKz3ROOZfxMzewSM9uVPPcZM7OF/rfI1KZ5xp9K/qzeaWb3mFndhHN584yVSM2RmUWBzwLXAhuBt5rZxtxGJXMUB/6Xu28ALgfel3yWtwD/5e7rgP9K7pM8dyNwAXAN8Lnk/w+y+H0Q2DthX8+4sPwD8H13Xw9sIXjWesYFwsxWAL8PbHP3C4EowTPUM85vXyJ4PhPN5Zl+HrgJWJf8mnxNyZ0vcebz+CFwobtvBp4BPgr594yVSM3dZcA+d9/v7qPAHcD1OY5J5sDdj7j7o8nX/QR/fK0geJ7/lmz2b8Abkq+vB+5w9xF3fx7YR/D/gyxiZrYSeD3whQmH9YwLhJnVAK8E/gXA3Ufd/QR6xoWmCCg3syKgAjiMnnFec/efAMcnHc7omZrZMqDG3X/uweT/L094j+TYVM/Y3X/g7vHk7kPAyuTrvHrGSqTmbgVwcMJ+e/KY5DEzWwNcBPwCaHX3IxAkW0BLspmefX76v8CHgfEJx/SMC8fZQDfwr8nhm18ws0r0jAuGux8CPg0cAI4Ave7+A/SMC1Gmz3RF8vXk45Iffgv4XvJ1Xj1jJVJzN9W4TJVAzGNmVgV8A/gDd++bqekUx/TsFzEz+xWgy913pPuWKY7pGS9uRcDFwOfd/SJgkORwoGnoGeeZ5DyZ64G1wHKg0sx+Y6a3THFMzzi/TfdM9azzlJn9CcEUi6+kDk3RbNE+YyVSc9cOrJqwv5JgiIHkITMrJkiivuLudycPdya7kkluu5LH9ezzz8uB68zsBYJhuK82s39Hz7iQtAPt7v6L5P5dBImVnnHhuBp43t273X0MuBt4GXrGhSjTZ9rOqaFhE4/LImZm7wR+BXi7n1qPKa+esRKpuXsEWGdma82shGBi3L05jknmIFn15V+Ave7+/004dS/wzuTrdwLfmnD8RjMrNbO1BBMeH16oeCVz7v5Rd1/p7msIvld/5O6/gZ5xwXD3DuCgmZ2fPHQV8CR6xoXkAHC5mVUkf25fRTCnVc+48GT0TJPD//rN7PLk/xvvmPAeWYTM7BrgI8B17h6bcCqvnnFRrgPIV+4eN7P3A/cTVA76orvvyXFYMjcvB34T2GVmjyePfQy4Ffi6mf02wS/wNwO4+x4z+zrBH2lx4H3unljwqCUb9IwLyweAryQ/3NoPvJvgA0M94wLg7r8ws7uARwme2WPA7UAVesZ5y8y+BlwBNJlZO/DnzO1n8+8RVIcrJ5hv8z1kUZjmGX8UKAV+mKxi/pC735xvz9hO9aSJiIiIiIhIOjS0T0REREREJENKpERERERERDKkREpERERERCRDSqREREREREQypERKREREREQkQ0qkRERk0TOzhJk9PuHrlixee42Z7c7W9UREZGnQOlIiIpIPhtx9a66DEBERSVGPlIiI5C0ze8HM/tbMHk5+nZs8fpaZ/ZeZ7UxuVyePt5rZPWb2RPLrZclLRc3sn81sj5n9wMzKk+1/38yeTF7njhz9M0VEZBFSIiUiIvmgfNLQvhsmnOtz98uAfwL+b/LYPwFfdvfNwFeAzySPfwZ40N23ABcDe5LH1wGfdfcLgBPAryeP3wJclLzOzeH800REJB+Zu+c6BhERkRmZ2YC7V01x/AXg1e6+38yKgQ53bzSzo8Aydx9LHj/i7k1m1g2sdPeRCddYA/zQ3dcl9z8CFLv7X5rZ94EB4JvAN919IOR/qoiI5An1SImISL7zaV5P12YqIxNeJzg1h/j1wGeBS4AdZqa5xSIiAiiREhGR/HfDhO3Pk69/BtyYfP124H+Sr/8L+D0AM4uaWc10FzWzCLDK3X8MfBioA87oFRMRkaVJn6yJiEg+KDezxyfsf9/dUyXQS83sFwQfDr41eez3gS+a2R8D3cC7k8c/CNxuZr9N0PP0e8CRae4ZBf7dzGoBA/7e3U9k6d8jIiJ5TnOkREQkbyXnSG1z96O5jkVERJYWDe0TERERERHJkHqkREREREREMqQeKRERERERkQwpkRIREREREcmQEikREREREZEMKZESERERERHJkBIpERERERGRDCmREhERERERydD/HwdVcvsOE6gBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_E, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interleaved training DynaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "SCHEDULE = medium_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD_START = 0.3\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s0.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.109 | Train PPL:   3.032\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.941\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.885\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.910\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.934\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.999\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.914\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.849\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.102 | Train PPL:   3.011\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.119 | Train PPL:   3.061\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.921\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.850\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.948\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.066\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.105 | Train PPL:   3.020\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.075 | Train PPL:   2.931\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.826\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.078 | Train PPL:   2.938\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.919\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.942\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.869\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.925\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.002\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.955\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.107 | Train PPL:   3.025\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.885\n",
      "\t Val. Loss: 0.531 |  Val. PPL:   1.701\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.310 |  Val. PPL:   1.364\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.382\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.366 |  Val. PPL:   1.443\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.317\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.234\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.306\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.240\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.236\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.215 |  Val. PPL:   1.239\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.185\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.188\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s1.t1.e100\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.755\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.590 | Train PPL:   1.804\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.571 | Train PPL:   1.770\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.682\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.771\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.630\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.484 | Train PPL:   1.623\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.736\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.525 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.560 | Train PPL:   1.751\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.567 | Train PPL:   1.764\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.747\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.577 | Train PPL:   1.781\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.549 | Train PPL:   1.731\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.712\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.725\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.586 | Train PPL:   1.796\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.537 | Train PPL:   1.711\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.565 | Train PPL:   1.760\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.553 | Train PPL:   1.738\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.514 | Train PPL:   1.673\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.572 | Train PPL:   1.773\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.631\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.688\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.556 | Train PPL:   1.744\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.532 | Train PPL:   1.703\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.544 | Train PPL:   1.723\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.500 | Train PPL:   1.649\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.545 | Train PPL:   1.724\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.506 | Train PPL:   1.659\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.551 | Train PPL:   1.735\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.516 | Train PPL:   1.675\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.558 | Train PPL:   1.748\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.524 | Train PPL:   1.690\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.546 | Train PPL:   1.726\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.581 | Train PPL:   1.788\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.663\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.499 | Train PPL:   1.647\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.555 | Train PPL:   1.742\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.717\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.501 | Train PPL:   1.650\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.543 | Train PPL:   1.722\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.564 | Train PPL:   1.758\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.643 | Train PPL:   1.902\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.616\n",
      "\t Val. Loss: 0.518 |  Val. PPL:   1.679\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.437 | Train PPL:   1.549\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.418 |  Val. PPL:   1.519\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.472\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.381 | Train PPL:   1.464\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.495\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.505\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.410 | Train PPL:   1.507\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.390 |  Val. PPL:   1.476\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.446\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.342 |  Val. PPL:   1.408\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.470\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.414 |  Val. PPL:   1.513\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.506\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.378 | Train PPL:   1.460\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.394 |  Val. PPL:   1.484\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.367\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.379\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.350\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.393\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.325\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.274\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.417\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.262\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.429\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s2.t2.e100\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.428\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.479\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s3.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s4.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s5.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s6.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s7.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s8.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s9.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s10.t1.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "\n",
      "SCHEDULE: DynaMoE-2.s11.t2.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_E2, hist_all_hitsss_E2, models_E2 = experiment(\n",
    "    \"DynaMoE-2\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_dynamoe,\n",
    "    repeat_dynamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_E2, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5kSqVvW6e7M"
   },
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_trgts = []\n",
    "gating_trgts.append( [torch.tensor([1,0,0]) for _ in range(len(train_dls[0]))])\n",
    "gating_trgts.append( [torch.tensor([0,1,0]) for _ in range(len(train_dls[1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dls[1]:\n",
    "    if x[1] == 7:\n",
    "        print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0990233421325684\n",
      "loss\n",
      "1.0725314617156982\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.06223726272583\n",
      "loss\n",
      "1.1580368280410767\n",
      "loss\n",
      "0.994653046131134\n",
      "loss\n",
      "1.0136317014694214\n",
      "loss\n",
      "1.1363170146942139\n",
      "loss\n",
      "1.0377978086471558\n",
      "loss\n",
      "1.273862361907959\n",
      "loss\n",
      "1.1935420036315918\n",
      "loss\n",
      "1.2392826080322266\n",
      "loss\n",
      "0.843536376953125\n",
      "loss\n",
      "1.0642420053482056\n",
      "loss\n",
      "0.8814082145690918\n",
      "loss\n",
      "1.1417649984359741\n",
      "loss\n",
      "1.2824432849884033\n",
      "loss\n",
      "0.8938198089599609\n",
      "loss\n",
      "0.9051280617713928\n",
      "loss\n",
      "0.9723299145698547\n",
      "loss\n",
      "1.2795649766921997\n",
      "loss\n",
      "1.1691962480545044\n",
      "loss\n",
      "1.2110319137573242\n",
      "loss\n",
      "0.7188755869865417\n",
      "loss\n",
      "1.3590319156646729\n",
      "loss\n",
      "0.9704457521438599\n",
      "loss\n",
      "0.9680742025375366\n",
      "loss\n",
      "1.385023593902588\n",
      "loss\n",
      "0.7331454157829285\n",
      "loss\n",
      "0.7141051888465881\n",
      "loss\n",
      "1.245851993560791\n",
      "loss\n",
      "0.7735995054244995\n",
      "loss\n",
      "1.3736876249313354\n",
      "loss\n",
      "0.8231424689292908\n",
      "loss\n",
      "1.3750696182250977\n",
      "loss\n",
      "1.51996910572052\n",
      "loss\n",
      "1.2661633491516113\n",
      "loss\n",
      "1.3473231792449951\n",
      "loss\n",
      "1.233702301979065\n",
      "loss\n",
      "0.7255655527114868\n",
      "loss\n",
      "0.6757468581199646\n",
      "loss\n",
      "0.6531469225883484\n",
      "loss\n",
      "0.745249330997467\n",
      "loss\n",
      "0.7534412145614624\n",
      "loss\n",
      "1.4418754577636719\n",
      "loss\n",
      "0.7937403321266174\n",
      "loss\n",
      "0.6716989278793335\n",
      "loss\n",
      "1.2280333042144775\n",
      "loss\n",
      "0.6496949195861816\n",
      "loss\n",
      "1.5460278987884521\n",
      "loss\n",
      "1.5514756441116333\n",
      "loss\n",
      "1.1636765003204346\n",
      "loss\n",
      "1.3178153038024902\n",
      "loss\n",
      "0.5455304384231567\n",
      "loss\n",
      "0.5333239436149597\n",
      "loss\n",
      "1.4688210487365723\n",
      "loss\n",
      "1.375108003616333\n",
      "loss\n",
      "0.5335070490837097\n",
      "loss\n",
      "1.430232048034668\n",
      "loss\n",
      "1.1741782426834106\n",
      "loss\n",
      "1.2446054220199585\n",
      "loss\n",
      "0.6195405721664429\n",
      "loss\n",
      "0.49828198552131653\n",
      "loss\n",
      "0.7075855135917664\n",
      "loss\n",
      "0.6252626776695251\n",
      "loss\n",
      "1.3688387870788574\n",
      "loss\n",
      "1.3391385078430176\n",
      "loss\n",
      "1.1697885990142822\n",
      "loss\n",
      "0.37812450528144836\n",
      "loss\n",
      "1.1592947244644165\n",
      "loss\n",
      "0.6913880109786987\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.6907056570053101\n",
      "loss\n",
      "0.4162207841873169\n",
      "loss\n",
      "0.5808714032173157\n",
      "loss\n",
      "1.1500698328018188\n",
      "loss\n",
      "0.352736234664917\n",
      "loss\n",
      "0.39313051104545593\n",
      "loss\n",
      "0.41192883253097534\n",
      "loss\n",
      "0.5116475820541382\n",
      "loss\n",
      "1.1172572374343872\n",
      "loss\n",
      "1.4131906032562256\n",
      "loss\n",
      "0.33028650283813477\n",
      "loss\n",
      "1.489363193511963\n",
      "loss\n",
      "1.2112787961959839\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.4293568730354309\n",
      "loss\n",
      "0.4328417181968689\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.47999051213264465\n",
      "loss\n",
      "0.19354216754436493\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.1361033916473389\n",
      "loss\n",
      "0.1798943281173706\n",
      "loss\n",
      "0.12139580398797989\n",
      "loss\n",
      "0.21990478038787842\n",
      "loss\n",
      "0.2244793325662613\n",
      "loss\n",
      "0.1716667264699936\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.5843265056610107\n",
      "loss\n",
      "0.11034240573644638\n",
      "loss\n",
      "0.3461153209209442\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.14620114862918854\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.13816894590854645\n",
      "loss\n",
      "0.15988250076770782\n",
      "loss\n",
      "0.2077673375606537\n",
      "loss\n",
      "0.13088884949684143\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.3269919157028198\n",
      "loss\n",
      "0.12710054218769073\n",
      "loss\n",
      "1.4896562099456787\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.10144913196563721\n",
      "loss\n",
      "0.07071235775947571\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.40675270557403564\n",
      "loss\n",
      "0.06046057492494583\n",
      "loss\n",
      "0.04141584411263466\n",
      "loss\n",
      "1.0897676944732666\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.07548543810844421\n",
      "loss\n",
      "1.0544909238815308\n",
      "loss\n",
      "1.0986123085021973\n",
      "loss\n",
      "0.059570763260126114\n",
      "loss\n",
      "0.06236014887690544\n",
      "loss\n",
      "0.04088432341814041\n",
      "loss\n",
      "0.6670161485671997\n",
      "loss\n",
      "0.09925808757543564\n",
      "loss\n",
      "0.06430821120738983\n",
      "loss\n",
      "0.5717656016349792\n",
      "loss\n",
      "0.5106241106987\n",
      "loss\n",
      "0.6109991073608398\n",
      "loss\n",
      "0.4585453271865845\n",
      "loss\n",
      "0.40513986349105835\n",
      "loss\n",
      "0.028454262763261795\n",
      "loss\n",
      "0.10292236506938934\n",
      "loss\n",
      "0.14290152490139008\n",
      "loss\n",
      "0.08874966204166412\n",
      "loss\n",
      "0.1695457398891449\n",
      "loss\n",
      "0.06828229129314423\n",
      "loss\n",
      "0.03192349523305893\n",
      "loss\n",
      "0.10639900714159012\n",
      "loss\n",
      "0.13714158535003662\n",
      "loss\n",
      "0.04405619949102402\n",
      "loss\n",
      "0.020778479054570198\n",
      "loss\n",
      "0.04888411983847618\n",
      "loss\n",
      "0.02315611019730568\n",
      "loss\n",
      "0.03884715959429741\n",
      "loss\n",
      "0.0185548085719347\n",
      "loss\n",
      "0.026514191180467606\n",
      "loss\n",
      "0.015334682539105415\n",
      "loss\n",
      "0.16082657873630524\n",
      "loss\n",
      "0.16628775000572205\n",
      "loss\n",
      "0.03266807645559311\n",
      "loss\n",
      "0.024600008502602577\n",
      "loss\n",
      "0.030816741287708282\n",
      "loss\n",
      "0.021857907995581627\n",
      "loss\n",
      "0.009852115996181965\n",
      "loss\n",
      "0.0058709559962153435\n",
      "loss\n",
      "0.017790623009204865\n",
      "loss\n",
      "0.015698540955781937\n",
      "loss\n",
      "0.08955177664756775\n",
      "loss\n",
      "0.020790155977010727\n",
      "loss\n",
      "0.037340905517339706\n",
      "loss\n",
      "0.030650736764073372\n",
      "loss\n",
      "0.024169908836483955\n",
      "loss\n",
      "0.011669941246509552\n",
      "loss\n",
      "0.8218876719474792\n",
      "loss\n",
      "0.0061847250908613205\n",
      "loss\n",
      "0.04579256847500801\n",
      "loss\n",
      "0.014900343492627144\n",
      "loss\n",
      "0.021984562277793884\n",
      "loss\n",
      "0.008054869249463081\n",
      "loss\n",
      "0.017524754628539085\n",
      "loss\n",
      "0.008446445688605309\n",
      "loss\n",
      "0.005841090343892574\n",
      "loss\n",
      "0.02205546200275421\n",
      "loss\n",
      "0.01886228285729885\n",
      "loss\n",
      "0.012258898466825485\n",
      "loss\n",
      "0.00998903438448906\n",
      "loss\n",
      "0.007056789472699165\n",
      "loss\n",
      "0.010330774821341038\n",
      "loss\n",
      "0.00817713513970375\n",
      "loss\n",
      "0.006179037969559431\n",
      "loss\n",
      "0.0073410761542618275\n",
      "loss\n",
      "0.010899768210947514\n",
      "loss\n",
      "0.026390664279460907\n",
      "loss\n",
      "0.016640907153487206\n",
      "loss\n",
      "0.00597974332049489\n",
      "loss\n",
      "0.009672206826508045\n",
      "loss\n",
      "0.005973581690341234\n",
      "loss\n",
      "3.6694374084472656\n",
      "loss\n",
      "0.00877902377396822\n",
      "loss\n",
      "4.256886959075928\n",
      "loss\n",
      "0.017297949641942978\n",
      "loss\n",
      "0.011129443533718586\n",
      "loss\n",
      "0.015219627879559994\n",
      "loss\n",
      "0.0030019478872418404\n",
      "loss\n",
      "0.021291498094797134\n",
      "loss\n",
      "0.018529647961258888\n",
      "loss\n",
      "0.009522965177893639\n",
      "loss\n",
      "0.012272440828382969\n",
      "loss\n",
      "0.008545498363673687\n",
      "loss\n",
      "0.008923650719225407\n",
      "loss\n",
      "0.0023492376785725355\n",
      "loss\n",
      "0.08297166228294373\n",
      "loss\n",
      "0.006391085684299469\n",
      "loss\n",
      "0.2621001899242401\n",
      "loss\n",
      "0.0033736478071659803\n",
      "loss\n",
      "0.06227020546793938\n",
      "loss\n",
      "0.10268152505159378\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.001490197260864079\n",
      "loss\n",
      "0.0031985098030418158\n",
      "loss\n",
      "0.19377416372299194\n",
      "loss\n",
      "0.43188416957855225\n",
      "loss\n",
      "0.010465031489729881\n",
      "loss\n",
      "0.313730925321579\n",
      "loss\n",
      "0.052673447877168655\n",
      "loss\n",
      "0.010588418692350388\n",
      "loss\n",
      "0.16984111070632935\n",
      "loss\n",
      "0.003559921169653535\n",
      "loss\n",
      "0.05207546055316925\n",
      "loss\n",
      "0.02877179905772209\n",
      "loss\n",
      "0.09439631551504135\n",
      "loss\n",
      "0.003439465072005987\n",
      "loss\n",
      "0.01418695505708456\n",
      "loss\n",
      "0.005863133817911148\n",
      "loss\n",
      "0.010725353844463825\n",
      "loss\n",
      "0.01660596765577793\n",
      "loss\n",
      "0.008103824220597744\n",
      "loss\n",
      "0.020882155746221542\n",
      "loss\n",
      "0.015934040769934654\n",
      "loss\n",
      "0.009476440958678722\n",
      "loss\n",
      "0.011041258461773396\n",
      "loss\n",
      "0.0038036394398659468\n",
      "loss\n",
      "0.015622024424374104\n",
      "loss\n",
      "0.004314637742936611\n",
      "loss\n",
      "0.002865258837118745\n",
      "loss\n",
      "0.003520245896652341\n",
      "loss\n",
      "0.00621197372674942\n",
      "loss\n",
      "0.3402809202671051\n",
      "loss\n",
      "1.1181387901306152\n",
      "loss\n",
      "0.016513574868440628\n",
      "loss\n",
      "0.005002601072192192\n",
      "loss\n",
      "0.005188333801925182\n",
      "loss\n",
      "0.0484754703938961\n",
      "loss\n",
      "0.011777392588555813\n",
      "loss\n",
      "0.00955106783658266\n",
      "loss\n",
      "0.0032876271288841963\n",
      "loss\n",
      "2.739438056945801\n",
      "loss\n",
      "0.08617750555276871\n",
      "loss\n",
      "0.0030841901898384094\n",
      "loss\n",
      "0.005740703083574772\n",
      "loss\n",
      "0.022282473742961884\n",
      "loss\n",
      "0.049073684960603714\n",
      "loss\n",
      "0.010887740179896355\n",
      "loss\n",
      "0.0028799984138458967\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.020307835191488266\n",
      "loss\n",
      "0.007002928759902716\n",
      "loss\n",
      "0.004172904882580042\n",
      "loss\n",
      "0.011328539811074734\n",
      "loss\n",
      "0.002724746707826853\n",
      "loss\n",
      "0.011057999916374683\n",
      "loss\n",
      "0.007479520980268717\n",
      "loss\n",
      "0.007599018048495054\n",
      "loss\n",
      "0.019894583150744438\n",
      "loss\n",
      "0.011453117243945599\n",
      "loss\n",
      "0.006259953137487173\n",
      "loss\n",
      "0.005067836493253708\n",
      "loss\n",
      "0.07912437617778778\n",
      "loss\n",
      "0.018125612288713455\n",
      "loss\n",
      "0.00830671563744545\n",
      "loss\n",
      "0.003499813610687852\n",
      "loss\n",
      "0.009177522733807564\n",
      "loss\n",
      "0.007959552109241486\n",
      "loss\n",
      "0.0048975031822919846\n",
      "loss\n",
      "0.00563117815181613\n",
      "loss\n",
      "0.011946087703108788\n",
      "loss\n",
      "0.12055058032274246\n",
      "loss\n",
      "0.0014368696138262749\n",
      "loss\n",
      "0.0018726922571659088\n",
      "loss\n",
      "0.0020918408408761024\n",
      "loss\n",
      "0.01692248322069645\n",
      "loss\n",
      "0.05113114416599274\n",
      "loss\n",
      "0.0038523285184055567\n",
      "loss\n",
      "0.0021715410985052586\n",
      "loss\n",
      "0.0018234307644888759\n",
      "loss\n",
      "0.03622610867023468\n",
      "loss\n",
      "0.004267395939677954\n",
      "loss\n",
      "0.004066176246851683\n",
      "loss\n",
      "1.5602083206176758\n",
      "loss\n",
      "0.004498837050050497\n",
      "loss\n",
      "0.006695810705423355\n",
      "loss\n",
      "0.023411402478814125\n",
      "loss\n",
      "0.011220568791031837\n",
      "loss\n",
      "0.005698625463992357\n",
      "loss\n",
      "0.2322503626346588\n",
      "loss\n",
      "0.00617643166333437\n",
      "loss\n",
      "0.001796538126654923\n",
      "loss\n",
      "0.005663539282977581\n",
      "loss\n",
      "0.015758037567138672\n",
      "loss\n",
      "0.0018020119750872254\n",
      "loss\n",
      "0.005692461505532265\n",
      "loss\n",
      "0.016637155786156654\n",
      "loss\n",
      "0.004839136730879545\n",
      "loss\n",
      "0.010831727646291256\n",
      "loss\n",
      "0.0023832509759813547\n",
      "loss\n",
      "0.004562444519251585\n",
      "loss\n",
      "0.009627577848732471\n",
      "loss\n",
      "0.024295106530189514\n",
      "loss\n",
      "0.011150899343192577\n",
      "loss\n",
      "0.009857428260147572\n",
      "loss\n",
      "0.00823152344673872\n",
      "loss\n",
      "0.006388835143297911\n",
      "loss\n",
      "0.006659812293946743\n",
      "loss\n",
      "0.005407468415796757\n",
      "loss\n",
      "0.006669285707175732\n",
      "loss\n",
      "0.007078569382429123\n",
      "loss\n",
      "0.00319589558057487\n",
      "loss\n",
      "0.004212554078549147\n",
      "loss\n",
      "0.00865777675062418\n",
      "loss\n",
      "0.0035727499052882195\n",
      "loss\n",
      "0.0016208856832236052\n",
      "loss\n",
      "0.006474115885794163\n",
      "loss\n",
      "0.011410925537347794\n",
      "loss\n",
      "0.01145629957318306\n",
      "loss\n",
      "0.007725598756223917\n",
      "loss\n",
      "0.0021824846044182777\n",
      "loss\n",
      "0.0015101945027709007\n",
      "loss\n",
      "0.01465426292270422\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.006637786515057087\n",
      "loss\n",
      "0.002001427114009857\n",
      "loss\n",
      "0.0010892179561778903\n",
      "loss\n",
      "0.005189519841223955\n",
      "loss\n",
      "0.005124291870743036\n",
      "loss\n",
      "0.016665644943714142\n",
      "loss\n",
      "0.005322452634572983\n",
      "loss\n",
      "0.0194013062864542\n",
      "loss\n",
      "0.0016494491137564182\n",
      "loss\n",
      "0.01149224303662777\n",
      "loss\n",
      "0.03479050472378731\n",
      "loss\n",
      "0.006425435654819012\n",
      "loss\n",
      "0.005977491848170757\n",
      "loss\n",
      "0.00912188645452261\n",
      "loss\n",
      "2.2605345249176025\n",
      "loss\n",
      "0.006742346566170454\n",
      "loss\n",
      "0.003994344733655453\n",
      "loss\n",
      "0.02096504159271717\n",
      "loss\n",
      "0.0022115076426416636\n",
      "loss\n",
      "0.011889898218214512\n",
      "loss\n",
      "0.0265306755900383\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.00421172333881259\n",
      "loss\n",
      "0.015857893973588943\n",
      "loss\n",
      "0.1096530631184578\n",
      "loss\n",
      "0.018577391281723976\n",
      "loss\n",
      "0.0031585826072841883\n",
      "loss\n",
      "0.007590973284095526\n",
      "loss\n",
      "0.01869463361799717\n",
      "loss\n",
      "0.0005868143052794039\n",
      "loss\n",
      "0.0009062950266525149\n",
      "loss\n",
      "0.0009920443408191204\n",
      "loss\n",
      "0.022964363917708397\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "1.710546851158142\n",
      "loss\n",
      "0.020992590114474297\n",
      "loss\n",
      "0.0007769426447339356\n",
      "loss\n",
      "0.0007874249131418765\n",
      "loss\n",
      "0.002684087259694934\n",
      "loss\n",
      "0.0014432977186515927\n",
      "loss\n",
      "0.014400719664990902\n",
      "loss\n",
      "0.01647898182272911\n",
      "loss\n",
      "0.0025668551679700613\n",
      "loss\n",
      "0.02713252790272236\n",
      "loss\n",
      "0.0029743739869445562\n",
      "loss\n",
      "0.006955931894481182\n",
      "loss\n",
      "0.02879623882472515\n",
      "loss\n",
      "0.009999656118452549\n",
      "loss\n",
      "0.002252900041639805\n",
      "loss\n",
      "0.0019014865392819047\n",
      "loss\n",
      "0.004394398536533117\n",
      "loss\n",
      "0.0068917665630578995\n",
      "loss\n",
      "0.0008858094224706292\n",
      "loss\n",
      "0.009932027198374271\n",
      "loss\n",
      "0.014354773797094822\n",
      "loss\n",
      "0.003590567270293832\n",
      "loss\n",
      "0.007239537313580513\n",
      "loss\n",
      "0.010349651798605919\n",
      "loss\n",
      "0.0017913023475557566\n",
      "loss\n",
      "0.005552583374083042\n",
      "loss\n",
      "0.005063566844910383\n",
      "loss\n",
      "0.00753796985372901\n",
      "loss\n",
      "0.005215846933424473\n",
      "loss\n",
      "0.019643884152173996\n",
      "loss\n",
      "0.010137026198208332\n",
      "loss\n",
      "0.009279691614210606\n",
      "loss\n",
      "0.24630846083164215\n",
      "loss\n",
      "0.20919343829154968\n",
      "loss\n",
      "0.011986136436462402\n",
      "loss\n",
      "0.03142803534865379\n",
      "loss\n",
      "0.0016289787599816918\n",
      "loss\n",
      "0.007744170259684324\n",
      "loss\n",
      "0.0020404488313943148\n",
      "loss\n",
      "0.0019769188947975636\n",
      "loss\n",
      "0.0036828566808253527\n",
      "loss\n",
      "0.001430917764082551\n",
      "loss\n",
      "0.0032458023633807898\n",
      "loss\n",
      "0.9635264873504639\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.0015397133538499475\n",
      "loss\n",
      "0.01138865016400814\n",
      "loss\n",
      "0.006255806889384985\n",
      "loss\n",
      "0.0012344843707978725\n",
      "loss\n",
      "0.012749706394970417\n",
      "loss\n",
      "0.016048895195126534\n",
      "loss\n",
      "0.016542652621865273\n",
      "loss\n",
      "0.009411019273102283\n",
      "loss\n",
      "0.016764706000685692\n",
      "loss\n",
      "0.0016001766780391335\n",
      "loss\n",
      "0.024112075567245483\n",
      "loss\n",
      "0.0008298290777020156\n",
      "loss\n",
      "0.0038938906509429216\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0008660380262881517\n",
      "loss\n",
      "0.005714982748031616\n",
      "loss\n",
      "0.005052180495113134\n",
      "loss\n",
      "0.006232587620615959\n",
      "loss\n",
      "0.01502776425331831\n",
      "loss\n",
      "0.005707752425223589\n",
      "loss\n",
      "0.002473630243912339\n",
      "loss\n",
      "0.009997768327593803\n",
      "loss\n",
      "0.012457652017474174\n",
      "loss\n",
      "0.011125553399324417\n",
      "loss\n",
      "0.0012016226537525654\n",
      "loss\n",
      "0.0020697140134871006\n",
      "loss\n",
      "0.0006506709614768624\n",
      "loss\n",
      "0.014135828241705894\n",
      "loss\n",
      "0.004716699477285147\n",
      "loss\n",
      "0.0012061471352353692\n",
      "loss\n",
      "1.7048331499099731\n",
      "loss\n",
      "0.0016477829776704311\n",
      "loss\n",
      "0.007360128220170736\n",
      "loss\n",
      "0.0018893502419814467\n",
      "loss\n",
      "0.0011075560469180346\n",
      "loss\n",
      "0.0037392713129520416\n",
      "loss\n",
      "0.001839375589042902\n",
      "loss\n",
      "0.003224414074793458\n",
      "loss\n",
      "0.0066930875182151794\n",
      "loss\n",
      "0.002837918698787689\n",
      "loss\n",
      "0.012988959439098835\n",
      "loss\n",
      "0.005118361674249172\n",
      "loss\n",
      "0.002372785471379757\n",
      "loss\n",
      "0.002551397541537881\n",
      "loss\n",
      "0.002693360671401024\n",
      "loss\n",
      "0.005424067843705416\n",
      "loss\n",
      "0.029661312699317932\n",
      "loss\n",
      "0.004786224570125341\n",
      "loss\n",
      "0.002189621329307556\n",
      "loss\n",
      "0.003100590081885457\n",
      "loss\n",
      "0.0031093843281269073\n",
      "loss\n",
      "0.009124367497861385\n",
      "loss\n",
      "0.003724069334566593\n",
      "loss\n",
      "0.02314271405339241\n",
      "loss\n",
      "0.0025100174825638533\n",
      "loss\n",
      "0.0035209585912525654\n",
      "loss\n",
      "0.002444852376356721\n",
      "loss\n",
      "0.005885177291929722\n",
      "loss\n",
      "0.013309091329574585\n",
      "loss\n",
      "0.10153012722730637\n",
      "loss\n",
      "0.004611808806657791\n",
      "loss\n",
      "0.0028217521030455828\n",
      "loss\n",
      "0.015542215667665005\n",
      "loss\n",
      "0.002033786615356803\n",
      "loss\n",
      "0.0014035383937880397\n",
      "loss\n",
      "0.0029042467940598726\n",
      "loss\n",
      "0.0021230080164968967\n",
      "loss\n",
      "0.003525234991684556\n",
      "loss\n",
      "0.0052865236066281796\n",
      "loss\n",
      "0.05563344433903694\n",
      "loss\n",
      "1.2306580543518066\n",
      "loss\n",
      "0.002805347554385662\n",
      "loss\n",
      "0.008296666666865349\n",
      "loss\n",
      "0.0025618611834943295\n",
      "loss\n",
      "0.0029922020621597767\n",
      "loss\n",
      "0.001327824778854847\n",
      "loss\n",
      "0.004977691452950239\n",
      "loss\n",
      "0.008818963542580605\n",
      "loss\n",
      "0.001708239782601595\n",
      "loss\n",
      "0.7690757513046265\n",
      "loss\n",
      "0.015876196324825287\n",
      "loss\n",
      "0.003787725931033492\n",
      "loss\n",
      "0.032406941056251526\n",
      "loss\n",
      "0.0013309201458469033\n",
      "loss\n",
      "0.0024917051196098328\n",
      "loss\n",
      "0.001479484373703599\n",
      "loss\n",
      "0.003427822608500719\n",
      "loss\n",
      "0.003249604720622301\n",
      "loss\n",
      "0.004893707111477852\n",
      "loss\n",
      "0.011703873984515667\n",
      "loss\n",
      "0.005132000893354416\n",
      "loss\n",
      "0.018576689064502716\n",
      "loss\n",
      "0.009199729189276695\n",
      "loss\n",
      "0.005308579187840223\n",
      "loss\n",
      "0.007215393707156181\n",
      "loss\n",
      "0.0033021229319274426\n",
      "loss\n",
      "0.025185471400618553\n",
      "loss\n",
      "0.001508289948105812\n",
      "loss\n",
      "0.007013701368123293\n",
      "loss\n",
      "0.05054481700062752\n",
      "loss\n",
      "0.0034468306694179773\n",
      "loss\n",
      "0.006219674367457628\n",
      "loss\n",
      "0.0019921474158763885\n",
      "loss\n",
      "0.0024717275518924\n",
      "loss\n",
      "0.0010458719916641712\n",
      "loss\n",
      "0.0013863962376490235\n",
      "loss\n",
      "0.00099871342536062\n",
      "loss\n",
      "0.003458235412836075\n",
      "loss\n",
      "0.0014266322832554579\n",
      "loss\n",
      "0.0684390440583229\n",
      "loss\n",
      "0.0028136686887592077\n",
      "loss\n",
      "0.0014552014181390405\n",
      "loss\n",
      "0.0019932182040065527\n",
      "loss\n",
      "0.0014911495381966233\n",
      "loss\n",
      "0.07449249923229218\n",
      "loss\n",
      "0.009254061616957188\n",
      "loss\n",
      "0.0026251161471009254\n",
      "loss\n",
      "0.0013711584033444524\n",
      "loss\n",
      "0.005050757434219122\n",
      "loss\n",
      "0.00182819040492177\n",
      "loss\n",
      "0.002353043295443058\n",
      "loss\n",
      "0.0033620046451687813\n",
      "loss\n",
      "0.0028780964203178883\n",
      "loss\n",
      "0.001177094760350883\n",
      "loss\n",
      "0.0012528197839856148\n",
      "loss\n",
      "0.005170782096683979\n",
      "loss\n",
      "0.006078684702515602\n",
      "loss\n",
      "0.0027772923931479454\n",
      "loss\n",
      "0.0025754161179065704\n",
      "loss\n",
      "0.0015905360924080014\n",
      "loss\n",
      "0.06839874386787415\n",
      "loss\n",
      "0.0038691910449415445\n",
      "loss\n",
      "0.00432888139039278\n",
      "loss\n",
      "0.0014771036803722382\n",
      "loss\n",
      "0.0012421043356880546\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0018992258701473475\n",
      "loss\n",
      "0.0033263610675930977\n",
      "loss\n",
      "0.0013111574808135629\n",
      "loss\n",
      "0.0015920833684504032\n",
      "loss\n",
      "0.0006268443539738655\n",
      "loss\n",
      "0.001416633022017777\n",
      "loss\n",
      "0.0014332984574139118\n",
      "loss\n",
      "0.004780292976647615\n",
      "loss\n",
      "0.1666361540555954\n",
      "loss\n",
      "0.0019267105963081121\n",
      "loss\n",
      "0.001359730027616024\n",
      "loss\n",
      "0.007523654028773308\n",
      "loss\n",
      "0.0007602662080898881\n",
      "loss\n",
      "0.0053190141916275024\n",
      "loss\n",
      "0.0015573289711028337\n",
      "loss\n",
      "0.0013621109537780285\n",
      "loss\n",
      "0.0012852036161348224\n",
      "loss\n",
      "0.0010589712765067816\n",
      "loss\n",
      "0.0019512200960889459\n",
      "loss\n",
      "0.002201516181230545\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0015368566382676363\n",
      "loss\n",
      "0.005285100545734167\n",
      "loss\n",
      "0.00283316383138299\n",
      "loss\n",
      "0.002679450437426567\n",
      "loss\n",
      "0.0008553183870390058\n",
      "loss\n",
      "0.005783846136182547\n",
      "loss\n",
      "0.0013325868640094995\n",
      "loss\n",
      "0.0020808966364711523\n",
      "loss\n",
      "0.0010180057724937797\n",
      "loss\n",
      "0.0015910121146589518\n",
      "loss\n",
      "0.002381110331043601\n",
      "loss\n",
      "0.0018386616138741374\n",
      "loss\n",
      "0.0021241975482553244\n",
      "loss\n",
      "0.0017482249531894922\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007312007946893573\n",
      "loss\n",
      "0.0012354368809610605\n",
      "loss\n",
      "0.0022005646023899317\n",
      "loss\n",
      "0.003978315275162458\n",
      "loss\n",
      "0.0013494918821379542\n",
      "loss\n",
      "0.002420830773189664\n",
      "loss\n",
      "0.003981996327638626\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.0021873614750802517\n",
      "loss\n",
      "0.001004667836241424\n",
      "loss\n",
      "0.0024986020289361477\n",
      "loss\n",
      "0.004319860599935055\n",
      "loss\n",
      "0.0005863377591595054\n",
      "loss\n",
      "0.0010850501712411642\n",
      "loss\n",
      "0.0016085079405456781\n",
      "loss\n",
      "0.0014430596493184566\n",
      "loss\n",
      "0.0023011888843029737\n",
      "loss\n",
      "0.0010885033989325166\n",
      "loss\n",
      "0.006295610684901476\n",
      "loss\n",
      "0.001369729870930314\n",
      "loss\n",
      "0.0011634016409516335\n",
      "loss\n",
      "0.0015768486773595214\n",
      "loss\n",
      "0.0007338214782066643\n",
      "loss\n",
      "0.001019553979858756\n",
      "loss\n",
      "0.0008622265886515379\n",
      "loss\n",
      "0.0012063853209838271\n",
      "loss\n",
      "0.0012644876260310411\n",
      "loss\n",
      "0.0010768335778266191\n",
      "loss\n",
      "0.0011632826644927263\n",
      "loss\n",
      "0.0032733690459281206\n",
      "loss\n",
      "0.001688008545897901\n",
      "loss\n",
      "0.0009617946925573051\n",
      "loss\n",
      "0.0008934320067055523\n",
      "loss\n",
      "0.0010639727115631104\n",
      "loss\n",
      "0.004857524763792753\n",
      "loss\n",
      "0.0022261380217969418\n",
      "loss\n",
      "0.003475104458630085\n",
      "loss\n",
      "0.0010546842822805047\n",
      "loss\n",
      "0.0016202905680984259\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001911718980409205\n",
      "loss\n",
      "0.0008288762182928622\n",
      "loss\n",
      "0.0038984029088169336\n",
      "loss\n",
      "0.0010794533882290125\n",
      "loss\n",
      "0.0025000290479511023\n",
      "loss\n",
      "0.06439148634672165\n",
      "loss\n",
      "0.0005739472107961774\n",
      "loss\n",
      "0.0867927223443985\n",
      "loss\n",
      "0.0008388814167119563\n",
      "loss\n",
      "0.0008179179858416319\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0008789013954810798\n",
      "loss\n",
      "0.001069450518116355\n",
      "loss\n",
      "0.0004976941272616386\n",
      "loss\n",
      "0.0016221948899328709\n",
      "loss\n",
      "0.00044431351125240326\n",
      "loss\n",
      "0.0019490785198286176\n",
      "loss\n",
      "0.0010580186499282718\n",
      "loss\n",
      "0.003044258337467909\n",
      "loss\n",
      "0.0017691688844934106\n",
      "loss\n",
      "0.0010022860951721668\n",
      "loss\n",
      "0.0004898302140645683\n",
      "loss\n",
      "0.000834117061458528\n",
      "loss\n",
      "0.0032225127797573805\n",
      "loss\n",
      "0.006845948286354542\n",
      "loss\n",
      "0.005816676188260317\n",
      "loss\n",
      "0.0025216706562787294\n",
      "loss\n",
      "0.0019045800436288118\n",
      "loss\n",
      "0.013794328086078167\n",
      "loss\n",
      "0.0015889888163655996\n",
      "loss\n",
      "0.0017301365733146667\n",
      "loss\n",
      "0.0013323486782610416\n",
      "loss\n",
      "0.0006561510381288826\n",
      "loss\n",
      "0.010727358050644398\n",
      "loss\n",
      "0.0008112476789392531\n",
      "loss\n",
      "0.0013940150383859873\n",
      "loss\n",
      "0.0067525296472013\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "1.133132815361023\n",
      "loss\n",
      "0.0005555993411689997\n",
      "loss\n",
      "0.000635183765552938\n",
      "loss\n",
      "0.003932482097297907\n",
      "loss\n",
      "0.0025434307754039764\n",
      "loss\n",
      "0.002568519674241543\n",
      "loss\n",
      "0.0027159492019563913\n",
      "loss\n",
      "0.5777732133865356\n",
      "loss\n",
      "0.0015739921946078539\n",
      "loss\n",
      "0.009132400155067444\n",
      "loss\n",
      "0.0021394239738583565\n",
      "loss\n",
      "0.0009253510506823659\n",
      "loss\n",
      "0.004545712377876043\n",
      "loss\n",
      "0.0010368215152993798\n",
      "loss\n",
      "0.0029525042045861483\n",
      "loss\n",
      "0.0005034133209846914\n",
      "loss\n",
      "0.00035565727739594877\n",
      "loss\n",
      "0.0013454442378133535\n",
      "loss\n",
      "0.0008417400531470776\n",
      "loss\n",
      "0.00573691027238965\n",
      "loss\n",
      "0.0008646087371744215\n",
      "loss\n",
      "0.004140377044677734\n",
      "loss\n",
      "0.002090056659653783\n",
      "loss\n",
      "0.0017508429009467363\n",
      "loss\n",
      "0.0005458295345306396\n",
      "loss\n",
      "0.007576303090900183\n",
      "loss\n",
      "0.002320099389180541\n",
      "loss\n",
      "0.006270022597163916\n",
      "loss\n",
      "0.0028695380315184593\n",
      "loss\n",
      "0.0013546108966693282\n",
      "loss\n",
      "0.0011938833631575108\n",
      "loss\n",
      "0.0004612335760612041\n",
      "loss\n",
      "0.0020207001361995935\n",
      "loss\n",
      "0.0028019000310450792\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0005999195855110884\n",
      "loss\n",
      "0.0004219118563923985\n",
      "loss\n",
      "0.0046008918434381485\n",
      "loss\n",
      "0.002042233245447278\n",
      "loss\n",
      "0.006754186935722828\n",
      "loss\n",
      "0.0007593132322654128\n",
      "loss\n",
      "0.00041547726141288877\n",
      "loss\n",
      "0.0008174415561370552\n",
      "loss\n",
      "0.0022442173212766647\n",
      "loss\n",
      "0.000581572181545198\n",
      "loss\n",
      "0.003397290362045169\n",
      "loss\n",
      "0.0006323245470412076\n",
      "loss\n",
      "0.0019155264599248767\n",
      "loss\n",
      "0.0005765683017671108\n",
      "loss\n",
      "0.0003532739356160164\n",
      "loss\n",
      "0.002211745595559478\n",
      "loss\n",
      "0.03702251985669136\n",
      "loss\n",
      "0.0009205871028825641\n",
      "loss\n",
      "0.008095311000943184\n",
      "loss\n",
      "0.0005775213940069079\n",
      "loss\n",
      "0.0011470888275653124\n",
      "loss\n",
      "0.0011156531982123852\n",
      "loss\n",
      "0.0019378946162760258\n",
      "loss\n",
      "0.003487696871161461\n",
      "loss\n",
      "0.0017197832930833101\n",
      "loss\n",
      "0.0022155519109219313\n",
      "loss\n",
      "0.0010141950333490968\n",
      "loss\n",
      "0.07641874998807907\n",
      "loss\n",
      "0.0013409203384071589\n",
      "loss\n",
      "0.001179595128633082\n",
      "loss\n",
      "0.0014016337227076292\n",
      "loss\n",
      "0.017542092129588127\n",
      "loss\n",
      "0.0032928551081568003\n",
      "loss\n",
      "0.0005334384622983634\n",
      "loss\n",
      "0.0008803306263871491\n",
      "loss\n",
      "0.0032828745897859335\n",
      "loss\n",
      "0.0023209319915622473\n",
      "loss\n",
      "0.0004161922261118889\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.0006065912893973291\n",
      "loss\n",
      "0.0072313714772462845\n",
      "loss\n",
      "0.0024629279505461454\n",
      "loss\n",
      "0.0021594080608338118\n",
      "loss\n",
      "0.0022205475252121687\n",
      "loss\n",
      "0.0010596857173368335\n",
      "loss\n",
      "0.001319729257375002\n",
      "loss\n",
      "0.009429323486983776\n",
      "loss\n",
      "0.002771110739558935\n",
      "loss\n",
      "0.001105888863094151\n",
      "loss\n",
      "0.006941134110093117\n",
      "loss\n",
      "0.0014033003244549036\n",
      "loss\n",
      "0.00024911639047786593\n",
      "loss\n",
      "0.00403613829985261\n",
      "loss\n",
      "0.0013616346986964345\n",
      "loss\n",
      "0.002226970624178648\n",
      "loss\n",
      "0.0029974314384162426\n",
      "loss\n",
      "0.0010115751065313816\n",
      "loss\n",
      "0.0009703694959171116\n",
      "loss\n",
      "0.0023328252136707306\n",
      "loss\n",
      "0.0012416280806064606\n",
      "loss\n",
      "0.008411219343543053\n",
      "loss\n",
      "0.0014676999999210238\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000969535845797509\n",
      "loss\n",
      "0.00026556302327662706\n",
      "loss\n",
      "0.00041261743172071874\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.001522930571809411\n",
      "loss\n",
      "0.0008607972995378077\n",
      "loss\n",
      "0.001702408422715962\n",
      "loss\n",
      "0.00075049843871966\n",
      "loss\n",
      "0.0002951186615973711\n",
      "loss\n",
      "0.0009332115878351033\n",
      "loss\n",
      "0.0014611531514674425\n",
      "loss\n",
      "0.0015103134792298079\n",
      "loss\n",
      "0.0012911563972011209\n",
      "loss\n",
      "0.000649956171400845\n",
      "loss\n",
      "0.002857889048755169\n",
      "loss\n",
      "0.001141611486673355\n",
      "loss\n",
      "0.0008865240379236639\n",
      "loss\n",
      "0.004283539019525051\n",
      "loss\n",
      "0.0007805161876603961\n",
      "loss\n",
      "0.0007681279676035047\n",
      "loss\n",
      "0.0013560395454987884\n",
      "loss\n",
      "0.0003937899600714445\n",
      "loss\n",
      "0.002218644367530942\n",
      "loss\n",
      "0.0014017528155818582\n",
      "loss\n",
      "0.0006868863711133599\n",
      "loss\n",
      "0.026876559481024742\n",
      "loss\n",
      "0.002644615015015006\n",
      "loss\n",
      "0.0009517907164990902\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.003852209774777293\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.00044764988706447184\n",
      "loss\n",
      "0.001259963377378881\n",
      "loss\n",
      "0.002960229991003871\n",
      "loss\n",
      "0.001384491566568613\n",
      "loss\n",
      "0.0019179059891030192\n",
      "loss\n",
      "0.0013440155889838934\n",
      "loss\n",
      "0.0005482124397531152\n",
      "loss\n",
      "0.005588384345173836\n",
      "loss\n",
      "0.0002812943421304226\n",
      "loss\n",
      "0.0008534126682206988\n",
      "loss\n",
      "0.0009110590908676386\n",
      "loss\n",
      "0.0007183355046436191\n",
      "loss\n",
      "0.0003687655262183398\n",
      "loss\n",
      "0.0008818790083751082\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.002041638595983386\n",
      "loss\n",
      "0.002261463785544038\n",
      "loss\n",
      "0.0010599239030852914\n",
      "loss\n",
      "0.0013654442736878991\n",
      "loss\n",
      "0.004441278520971537\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.010698935016989708\n",
      "loss\n",
      "0.000696654780767858\n",
      "loss\n",
      "0.0009991897968575358\n",
      "loss\n",
      "0.00774192251265049\n",
      "loss\n",
      "0.0011189873330295086\n",
      "loss\n",
      "0.0007273888913914561\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.00035232058144174516\n",
      "loss\n",
      "0.003114256775006652\n",
      "loss\n",
      "0.005950829479843378\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.00035398892941884696\n",
      "loss\n",
      "0.0009184433147311211\n",
      "loss\n",
      "0.00034850722295232117\n",
      "loss\n",
      "0.0016182672698050737\n",
      "loss\n",
      "0.0009114163694903255\n",
      "loss\n",
      "0.010211721062660217\n",
      "loss\n",
      "0.0008002892718650401\n",
      "loss\n",
      "0.0028330450877547264\n",
      "loss\n",
      "0.000977038755081594\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0009078433504328132\n",
      "loss\n",
      "0.0004047528200317174\n",
      "loss\n",
      "0.0012709167785942554\n",
      "loss\n",
      "0.0021843877620995045\n",
      "loss\n",
      "0.0014002051902934909\n",
      "loss\n",
      "0.001053255284205079\n",
      "loss\n",
      "0.0003840185818262398\n",
      "loss\n",
      "0.0006049233488738537\n",
      "loss\n",
      "0.0006123098428361118\n",
      "loss\n",
      "0.0007838514284230769\n",
      "loss\n",
      "0.005385533440858126\n",
      "loss\n",
      "0.0015470929211005569\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.002130145439878106\n",
      "loss\n",
      "0.000745018885936588\n",
      "loss\n",
      "0.0004693360242526978\n",
      "loss\n",
      "0.0018774517811834812\n",
      "loss\n",
      "0.0003051292151212692\n",
      "loss\n",
      "0.0018917298875749111\n",
      "loss\n",
      "0.0010977915953844786\n",
      "loss\n",
      "0.001210909802466631\n",
      "loss\n",
      "0.0024791003670543432\n",
      "loss\n",
      "0.002086844528093934\n",
      "loss\n",
      "0.0011520899133756757\n",
      "loss\n",
      "0.00046957432641647756\n",
      "loss\n",
      "0.0006668727728538215\n",
      "loss\n",
      "0.0006725909770466387\n",
      "loss\n",
      "0.0010973153403028846\n",
      "loss\n",
      "0.0013992529129609466\n",
      "loss\n",
      "0.0011435167398303747\n",
      "loss\n",
      "0.0009629856795072556\n",
      "loss\n",
      "0.009789552539587021\n",
      "loss\n",
      "0.00024577934527769685\n",
      "loss\n",
      "0.0004914983292110264\n",
      "loss\n",
      "0.00042906138696707785\n",
      "loss\n",
      "0.005800202023237944\n",
      "loss\n",
      "0.0008683010237291455\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0016599221853539348\n",
      "loss\n",
      "0.0010418231831863523\n",
      "loss\n",
      "0.0007952864980325103\n",
      "loss\n",
      "0.0007488307310268283\n",
      "loss\n",
      "0.00200130813755095\n",
      "loss\n",
      "0.0012532960390672088\n",
      "loss\n",
      "0.0005787128466181457\n",
      "loss\n",
      "0.0008704449282959104\n",
      "loss\n",
      "0.0008831891464069486\n",
      "loss\n",
      "0.00041976699139922857\n",
      "loss\n",
      "0.0006168370018713176\n",
      "loss\n",
      "0.0010122895473614335\n",
      "loss\n",
      "0.000797192333266139\n",
      "loss\n",
      "0.001840565470047295\n",
      "loss\n",
      "0.0009284476400353014\n",
      "loss\n",
      "0.002931585069745779\n",
      "loss\n",
      "0.00023958197562023997\n",
      "loss\n",
      "0.0002543602604418993\n",
      "loss\n",
      "0.0008665143977850676\n",
      "loss\n",
      "0.0005872909096069634\n",
      "loss\n",
      "0.0009951406391337514\n",
      "loss\n",
      "0.0030088413041085005\n",
      "loss\n",
      "4.661959648132324\n",
      "loss\n",
      "0.012682733125984669\n",
      "loss\n",
      "0.0011266082292422652\n",
      "loss\n",
      "0.0011711412807926536\n",
      "loss\n",
      "0.0006866481271572411\n",
      "loss\n",
      "0.005445883143693209\n",
      "loss\n",
      "0.0044141001999378204\n",
      "loss\n",
      "0.0148676922544837\n",
      "loss\n",
      "0.0033791130408644676\n",
      "loss\n",
      "0.01798722706735134\n",
      "loss\n",
      "0.0019363479223102331\n",
      "loss\n",
      "0.0010003806091845036\n",
      "loss\n",
      "0.001105412608012557\n",
      "loss\n",
      "0.000695463502779603\n",
      "loss\n",
      "0.0007178590167313814\n",
      "loss\n",
      "0.058921705931425095\n",
      "loss\n",
      "0.0010342017048969865\n",
      "loss\n",
      "0.002141089178621769\n",
      "loss\n",
      "0.0011211306555196643\n",
      "loss\n",
      "0.06521012634038925\n",
      "loss\n",
      "0.0007843278581276536\n",
      "loss\n",
      "0.0017260904423892498\n",
      "loss\n",
      "0.0006530536338686943\n",
      "loss\n",
      "0.002709410386160016\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.007267585955560207\n",
      "loss\n",
      "0.005277866963297129\n",
      "loss\n",
      "0.0009411911014467478\n",
      "loss\n",
      "0.0005138983833603561\n",
      "loss\n",
      "0.0004433602443896234\n",
      "loss\n",
      "0.002880473854020238\n",
      "loss\n",
      "0.026496661826968193\n",
      "loss\n",
      "0.001375206047669053\n",
      "loss\n",
      "0.002098502591252327\n",
      "loss\n",
      "0.37235745787620544\n",
      "loss\n",
      "0.0003971264814026654\n",
      "loss\n",
      "0.0007009433466009796\n",
      "loss\n",
      "0.006032118573784828\n",
      "loss\n",
      "0.0004629017203114927\n",
      "loss\n",
      "0.001023483811877668\n",
      "loss\n",
      "0.00044550508027896285\n",
      "loss\n",
      "0.0014649622607976198\n",
      "loss\n",
      "0.0023430532310158014\n",
      "loss\n",
      "0.0051649706438183784\n",
      "loss\n",
      "0.0011328000109642744\n",
      "loss\n",
      "0.012277033179998398\n",
      "loss\n",
      "0.0009029601933434606\n",
      "loss\n",
      "0.0016807490028440952\n",
      "loss\n",
      "0.0008746135863475502\n",
      "loss\n",
      "0.0005560758872888982\n",
      "loss\n",
      "0.0037347583565860987\n",
      "loss\n",
      "0.0024947968777269125\n",
      "loss\n",
      "0.005204224959015846\n",
      "loss\n",
      "0.0009472650708630681\n",
      "loss\n",
      "0.000876757490914315\n",
      "loss\n",
      "0.0005915798828937113\n",
      "loss\n",
      "0.0004741021548397839\n",
      "loss\n",
      "0.0028621682431548834\n",
      "loss\n",
      "0.0012935374397784472\n",
      "loss\n",
      "0.0007281036232598126\n",
      "loss\n",
      "0.001086598145775497\n",
      "loss\n",
      "0.0012180536286905408\n",
      "loss\n",
      "0.0010181248653680086\n",
      "loss\n",
      "0.0006835508393123746\n",
      "loss\n",
      "0.0008760428754612803\n",
      "loss\n",
      "0.001959786517545581\n",
      "loss\n",
      "0.0006316096987575293\n",
      "loss\n",
      "0.005911367479711771\n",
      "loss\n",
      "0.001192573574371636\n",
      "loss\n",
      "0.0007014198345132172\n",
      "loss\n",
      "0.001320443581789732\n",
      "loss\n",
      "0.0007500219508074224\n",
      "loss\n",
      "0.0008264940115623176\n",
      "loss\n",
      "0.0005551227368414402\n",
      "loss\n",
      "0.0007950482540763915\n",
      "loss\n",
      "0.002404181519523263\n",
      "loss\n",
      "0.0028921226039528847\n",
      "loss\n",
      "0.06963595002889633\n",
      "loss\n",
      "0.0018892312655225396\n",
      "loss\n",
      "0.002776341512799263\n",
      "loss\n",
      "1.097870945930481\n",
      "loss\n",
      "0.0005777596961706877\n",
      "loss\n",
      "0.0008172033121809363\n",
      "loss\n",
      "0.009694874286651611\n",
      "loss\n",
      "0.006388006266206503\n",
      "loss\n",
      "0.0017229963559657335\n",
      "loss\n",
      "0.001210671616718173\n",
      "loss\n",
      "0.0014313939027488232\n",
      "loss\n",
      "0.008590529672801495\n",
      "loss\n",
      "0.00047743841423653066\n",
      "loss\n",
      "0.0011382774682715535\n",
      "loss\n",
      "0.0026242840103805065\n",
      "loss\n",
      "0.0003644755925051868\n",
      "loss\n",
      "0.0015656605828553438\n",
      "loss\n",
      "0.00157803890760988\n",
      "loss\n",
      "0.0009129646932706237\n",
      "loss\n",
      "0.0011154150124639273\n",
      "loss\n",
      "0.001625765347853303\n",
      "loss\n",
      "2.8665642738342285\n",
      "loss\n",
      "0.012164924293756485\n",
      "loss\n",
      "0.0004950728034600616\n",
      "loss\n",
      "0.0003947432560380548\n",
      "loss\n",
      "0.005643506534397602\n",
      "loss\n",
      "0.00040451448876410723\n",
      "loss\n",
      "0.00030298411729745567\n",
      "loss\n",
      "0.04739883169531822\n",
      "loss\n",
      "0.036705754697322845\n",
      "loss\n",
      "0.00945920031517744\n",
      "loss\n",
      "0.00024720950750634074\n",
      "loss\n",
      "0.061474040150642395\n",
      "loss\n",
      "0.0006235085893422365\n",
      "loss\n",
      "0.007738137152045965\n",
      "loss\n",
      "0.00026246439665555954\n",
      "loss\n",
      "0.04900194704532623\n",
      "loss\n",
      "0.00022837892174720764\n",
      "loss\n",
      "0.021302001550793648\n",
      "loss\n",
      "0.07682917267084122\n",
      "loss\n",
      "0.0003003622987307608\n",
      "loss\n",
      "0.03357384353876114\n",
      "loss\n",
      "0.0002033503697020933\n",
      "loss\n",
      "0.00017331528943032026\n",
      "loss\n",
      "0.0002858230145648122\n",
      "loss\n",
      "0.012821382842957973\n",
      "loss\n",
      "0.016321489587426186\n",
      "loss\n",
      "0.08305186033248901\n",
      "loss\n",
      "0.0014668668154627085\n",
      "loss\n",
      "0.00033158526639454067\n",
      "loss\n",
      "0.018014157190918922\n",
      "loss\n",
      "0.00038211196078918874\n",
      "loss\n",
      "0.05664663016796112\n",
      "loss\n",
      "0.0001652104256208986\n",
      "loss\n",
      "0.0010937429033219814\n",
      "loss\n",
      "0.0409710630774498\n",
      "loss\n",
      "0.040607012808322906\n",
      "loss\n",
      "0.0007090438157320023\n",
      "loss\n",
      "0.005363598000258207\n",
      "loss\n",
      "0.00033968876232393086\n",
      "loss\n",
      "0.0037461596075445414\n",
      "loss\n",
      "0.000534868217073381\n",
      "loss\n",
      "0.00042715485324151814\n",
      "loss\n",
      "0.005437346640974283\n",
      "loss\n",
      "0.0005265279905870557\n",
      "loss\n",
      "0.018740614876151085\n",
      "loss\n",
      "0.005633667577058077\n",
      "loss\n",
      "0.002406084444373846\n",
      "loss\n",
      "0.0005832401220686734\n",
      "loss\n",
      "0.0005103239673189819\n",
      "loss\n",
      "0.0033570146188139915\n",
      "loss\n",
      "0.007710220292210579\n",
      "loss\n",
      "0.0002157455455744639\n",
      "loss\n",
      "0.004921702668070793\n",
      "loss\n",
      "0.00028391621890477836\n",
      "loss\n",
      "0.009648121893405914\n",
      "loss\n",
      "0.0002779574424494058\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.00031704644788987935\n",
      "loss\n",
      "0.002370406873524189\n",
      "loss\n",
      "0.0003496989083942026\n",
      "loss\n",
      "0.0037905762437731028\n",
      "loss\n",
      "0.003954449202865362\n",
      "loss\n",
      "0.00046695294440723956\n",
      "loss\n",
      "0.00443332688882947\n",
      "loss\n",
      "0.0001833270798670128\n",
      "loss\n",
      "0.0022333934903144836\n",
      "loss\n",
      "0.003699603257700801\n",
      "loss\n",
      "0.0019722788129001856\n",
      "loss\n",
      "0.010431881994009018\n",
      "loss\n",
      "0.0002743821241892874\n",
      "loss\n",
      "6.502380847930908\n",
      "loss\n",
      "0.0034477810841053724\n",
      "loss\n",
      "0.0003575639275368303\n",
      "loss\n",
      "0.0002674698771443218\n",
      "loss\n",
      "0.0030373651534318924\n",
      "loss\n",
      "0.0006556744920089841\n",
      "loss\n",
      "0.002501931507140398\n",
      "loss\n",
      "0.0031947072129696608\n",
      "loss\n",
      "0.0007147617870941758\n",
      "loss\n",
      "0.0033837463706731796\n",
      "loss\n",
      "0.001416394836269319\n",
      "loss\n",
      "0.0024562685284763575\n",
      "loss\n",
      "0.00471147894859314\n",
      "loss\n",
      "0.0022033003624528646\n",
      "loss\n",
      "0.002617269055917859\n",
      "loss\n",
      "0.00243831193074584\n",
      "loss\n",
      "0.0016720612766221166\n",
      "loss\n",
      "0.12599332630634308\n",
      "loss\n",
      "0.0017636949196457863\n",
      "loss\n",
      "0.0017951102927327156\n",
      "loss\n",
      "0.00887355301529169\n",
      "loss\n",
      "0.007288059685379267\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.0016561138909310102\n",
      "loss\n",
      "0.005817742552608252\n",
      "loss\n",
      "0.0010451575508341193\n",
      "loss\n",
      "0.0009803733555600047\n",
      "loss\n",
      "0.022383779287338257\n",
      "loss\n",
      "0.0032427129335701466\n",
      "loss\n",
      "0.0009266611887142062\n",
      "loss\n",
      "0.019285082817077637\n",
      "loss\n",
      "0.0009420248097740114\n",
      "loss\n",
      "0.0016465928638353944\n",
      "loss\n",
      "0.0023178397677838802\n",
      "loss\n",
      "0.002938360208645463\n",
      "loss\n",
      "0.08495984971523285\n",
      "loss\n",
      "0.0014054430648684502\n",
      "loss\n",
      "0.0009221353684552014\n",
      "loss\n",
      "0.00436816830188036\n",
      "loss\n",
      "0.0030544791370630264\n",
      "loss\n",
      "0.005048622377216816\n",
      "loss\n",
      "0.0060799880884587765\n",
      "loss\n",
      "0.016540072858333588\n",
      "loss\n",
      "0.2260076254606247\n",
      "loss\n",
      "0.007220128085464239\n",
      "loss\n",
      "0.004577396437525749\n",
      "loss\n",
      "0.0031294680666178465\n",
      "loss\n",
      "0.0011528043542057276\n",
      "loss\n",
      "0.008339227177202702\n",
      "loss\n",
      "0.0013830630341544747\n",
      "loss\n",
      "0.0027998790610581636\n",
      "loss\n",
      "0.001560185570269823\n",
      "loss\n",
      "0.0018848287872970104\n",
      "loss\n",
      "0.030961912125349045\n",
      "loss\n",
      "0.0022762122098356485\n",
      "loss\n",
      "0.003579401643946767\n",
      "loss\n",
      "0.0034686895087361336\n",
      "loss\n",
      "0.002000237349420786\n",
      "loss\n",
      "0.001800346071831882\n",
      "loss\n",
      "0.0035957936197519302\n",
      "loss\n",
      "0.014768090099096298\n",
      "loss\n",
      "0.0046996138989925385\n",
      "loss\n",
      "0.0067700534127652645\n",
      "loss\n",
      "0.0009396428358741105\n",
      "loss\n",
      "0.0017360866768285632\n",
      "loss\n",
      "0.001462581567466259\n",
      "loss\n",
      "0.0004752936656586826\n",
      "loss\n",
      "0.0007898071780800819\n",
      "loss\n",
      "0.0008774721063673496\n",
      "loss\n",
      "0.0011689979583024979\n",
      "loss\n",
      "0.004408997017890215\n",
      "loss\n",
      "0.01529852394014597\n",
      "loss\n",
      "0.0017256144201382995\n",
      "loss\n",
      "0.0005324853118509054\n",
      "loss\n",
      "0.00284944917075336\n",
      "loss\n",
      "0.3101171851158142\n",
      "loss\n",
      "0.0016018429305404425\n",
      "loss\n",
      "0.0008465044084005058\n",
      "loss\n",
      "0.00684417225420475\n",
      "loss\n",
      "0.0010065733222290874\n",
      "loss\n",
      "0.001503290724940598\n",
      "loss\n",
      "0.01349575724452734\n",
      "loss\n",
      "0.0005980133428238332\n",
      "loss\n",
      "0.005173865240067244\n",
      "loss\n",
      "0.0010562323732301593\n",
      "loss\n",
      "0.04587842524051666\n",
      "loss\n",
      "0.001095886342227459\n",
      "loss\n",
      "0.0011753087164834142\n",
      "loss\n",
      "0.000433112756581977\n",
      "loss\n",
      "0.0035584955476224422\n",
      "loss\n",
      "0.03456712141633034\n",
      "loss\n",
      "0.01112897228449583\n",
      "loss\n",
      "0.19578249752521515\n",
      "loss\n",
      "0.0008631794480606914\n",
      "loss\n",
      "0.0010082405060529709\n",
      "loss\n",
      "0.0028364923782646656\n",
      "loss\n",
      "0.0018275955226272345\n",
      "loss\n",
      "0.0023895539343357086\n",
      "loss\n",
      "0.000523430178873241\n",
      "loss\n",
      "0.00041214076918549836\n",
      "loss\n",
      "0.0006101653561927378\n",
      "loss\n",
      "0.0008522216230630875\n",
      "loss\n",
      "0.0012179345358163118\n",
      "loss\n",
      "0.0008753282018005848\n",
      "loss\n",
      "0.0016235039802268147\n",
      "loss\n",
      "0.0009777533123269677\n",
      "loss\n",
      "0.001444011926651001\n",
      "loss\n",
      "0.0035663354210555553\n",
      "loss\n",
      "0.01225253939628601\n",
      "loss\n",
      "0.000615407363511622\n",
      "loss\n",
      "0.002785851713269949\n",
      "loss\n",
      "0.0010449193650856614\n",
      "loss\n",
      "0.0018184330547228456\n",
      "loss\n",
      "0.0007669368060305715\n",
      "loss\n",
      "0.0027184458449482918\n",
      "loss\n",
      "0.001904699020087719\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "0.0010415849974378943\n",
      "loss\n",
      "0.000568228424526751\n",
      "loss\n",
      "0.0031387372873723507\n",
      "loss\n",
      "0.00719101307913661\n",
      "loss\n",
      "0.0006353028584271669\n",
      "loss\n",
      "0.000486970558995381\n",
      "loss\n",
      "0.0007688426994718611\n",
      "loss\n",
      "0.0007824220228940248\n",
      "loss\n",
      "0.0013804440386593342\n",
      "loss\n",
      "0.0009111781837418675\n",
      "loss\n",
      "0.0014972201315686107\n",
      "loss\n",
      "0.0006042085005901754\n",
      "loss\n",
      "0.0012735360069200397\n",
      "loss\n",
      "0.0013263961300253868\n",
      "loss\n",
      "0.0005981324939057231\n",
      "loss\n",
      "0.0006130246329121292\n",
      "loss\n",
      "0.0023889592848718166\n",
      "loss\n",
      "0.0011750705307349563\n",
      "loss\n",
      "0.0005891970940865576\n",
      "loss\n",
      "0.0004895919119007885\n",
      "loss\n",
      "0.0005679901223629713\n",
      "loss\n",
      "0.5016879439353943\n",
      "loss\n",
      "0.0017500099493190646\n",
      "loss\n",
      "0.000506511190906167\n",
      "loss\n",
      "0.0005539313424378633\n",
      "loss\n",
      "0.003807914676144719\n",
      "loss\n",
      "0.028101855888962746\n",
      "loss\n",
      "0.0008391196606680751\n",
      "loss\n",
      "0.0009672730811871588\n",
      "loss\n",
      "0.00046433156239800155\n",
      "loss\n",
      "0.1355494260787964\n",
      "loss\n",
      "0.000809818331617862\n",
      "loss\n",
      "0.005297314375638962\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.0007186928996816278\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.0024447336327284575\n",
      "loss\n",
      "0.0013624681159853935\n",
      "loss\n",
      "0.000847933697514236\n",
      "loss\n",
      "0.0033197076991200447\n",
      "loss\n",
      "0.0002531684876885265\n",
      "loss\n",
      "0.0006457865820266306\n",
      "loss\n",
      "0.00021288513380568475\n",
      "loss\n",
      "0.0028309053741395473\n",
      "loss\n",
      "0.0006840273272246122\n",
      "loss\n",
      "5.774600982666016\n",
      "loss\n",
      "0.0007955246837809682\n",
      "loss\n",
      "0.0029955299105495214\n",
      "loss\n",
      "1.699110507965088\n",
      "loss\n",
      "0.07665915042161942\n",
      "loss\n",
      "0.0006833125371485949\n",
      "loss\n",
      "0.0008222059695981443\n",
      "loss\n",
      "0.001655756845138967\n",
      "loss\n",
      "0.000842692912556231\n",
      "loss\n",
      "0.001057184999808669\n",
      "loss\n",
      "0.0017527469899505377\n",
      "loss\n",
      "0.0015608996618539095\n",
      "loss\n",
      "0.0008507922757416964\n",
      "loss\n",
      "0.001906721736304462\n",
      "loss\n",
      "0.0029006809927523136\n",
      "loss\n",
      "0.0015824426664039493\n",
      "loss\n",
      "0.006690955720841885\n",
      "loss\n",
      "0.026949666440486908\n",
      "loss\n",
      "0.005086457822471857\n",
      "loss\n",
      "0.0062572285532951355\n",
      "loss\n",
      "0.0013733012601733208\n",
      "loss\n",
      "0.015596204437315464\n",
      "loss\n",
      "0.004771632142364979\n",
      "loss\n",
      "0.010210777632892132\n",
      "loss\n",
      "0.0018556771101430058\n",
      "loss\n",
      "0.0012656782055273652\n",
      "loss\n",
      "0.001777260797098279\n",
      "loss\n",
      "0.0009158230968751013\n",
      "loss\n",
      "0.00970514491200447\n",
      "loss\n",
      "0.0015851801726967096\n",
      "loss\n",
      "0.007525546941906214\n",
      "loss\n",
      "0.0019395602867007256\n",
      "loss\n",
      "0.14365465939044952\n",
      "loss\n",
      "0.006396060809493065\n",
      "loss\n",
      "0.0021193204447627068\n",
      "loss\n",
      "0.007879721000790596\n",
      "loss\n",
      "0.03223589062690735\n",
      "loss\n",
      "0.0014585343888029456\n",
      "loss\n",
      "0.12759247422218323\n",
      "loss\n",
      "0.0017432268941774964\n",
      "loss\n",
      "0.0009039129945449531\n",
      "loss\n",
      "0.02098698727786541\n",
      "loss\n",
      "0.0011002921964973211\n",
      "loss\n",
      "0.015343839302659035\n",
      "loss\n",
      "0.0017241863533854485\n",
      "loss\n",
      "0.012848097831010818\n",
      "loss\n",
      "0.016196461394429207\n",
      "loss\n",
      "0.007988644763827324\n",
      "loss\n",
      "0.0008741371566429734\n",
      "loss\n",
      "0.0033289750572293997\n",
      "loss\n",
      "0.002584690460935235\n",
      "loss\n",
      "0.3376161456108093\n",
      "loss\n",
      "0.17622306942939758\n",
      "loss\n",
      "0.005285219289362431\n",
      "loss\n",
      "0.0030551922973245382\n",
      "loss\n",
      "0.0048184944316744804\n",
      "loss\n",
      "0.004540016409009695\n",
      "loss\n",
      "0.004049673210829496\n",
      "loss\n",
      "0.004293984733521938\n",
      "loss\n",
      "0.0020384264644235373\n",
      "loss\n",
      "0.6701012849807739\n",
      "loss\n",
      "0.006541150622069836\n",
      "loss\n",
      "0.0017422748496755958\n",
      "loss\n",
      "0.03246106579899788\n",
      "loss\n",
      "0.0052717006765306\n",
      "loss\n",
      "0.005346760619431734\n",
      "loss\n",
      "0.0010355116100981832\n",
      "loss\n",
      "0.0005392765742726624\n",
      "loss\n",
      "0.001001809723675251\n",
      "loss\n",
      "0.0026057357899844646\n",
      "loss\n",
      "0.003418674925342202\n",
      "loss\n",
      "0.0008293526479974389\n",
      "loss\n",
      "0.009370513260364532\n",
      "loss\n",
      "0.0006006343755871058\n",
      "loss\n",
      "0.003137667663395405\n",
      "loss\n",
      "0.0011179156135767698\n",
      "loss\n",
      "0.01869744248688221\n",
      "loss\n",
      "0.0004447901446837932\n",
      "loss\n",
      "0.001640761154703796\n",
      "loss\n",
      "0.0004435985756572336\n",
      "loss\n",
      "0.004437480587512255\n",
      "loss\n",
      "0.0004259632551111281\n",
      "loss\n",
      "0.02258472703397274\n",
      "loss\n",
      "0.0006771179032512009\n",
      "loss\n",
      "0.0015863704029470682\n",
      "loss\n",
      "0.0030540036968886852\n",
      "loss\n",
      "0.0007546676206402481\n",
      "loss\n",
      "0.002878334140405059\n",
      "loss\n",
      "0.0005322470096871257\n",
      "loss\n",
      "0.026510940864682198\n",
      "loss\n",
      "0.009304258041083813\n",
      "loss\n",
      "0.021776961162686348\n",
      "loss\n",
      "0.001404728856869042\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0008524598088115454\n",
      "loss\n",
      "0.0027340196538716555\n",
      "loss\n",
      "0.002498839981853962\n",
      "loss\n",
      "0.0013705631718039513\n",
      "loss\n",
      "0.00899465661495924\n",
      "loss\n",
      "0.0012816318776458502\n",
      "loss\n",
      "0.001966211013495922\n",
      "loss\n",
      "0.0018600797047838569\n",
      "loss\n",
      "0.0019061268540099263\n",
      "loss\n",
      "0.0014174662064760923\n",
      "loss\n",
      "0.003535688389092684\n",
      "loss\n",
      "0.0014075858052819967\n",
      "loss\n",
      "0.0005794276366941631\n",
      "loss\n",
      "0.0028871302492916584\n",
      "loss\n",
      "0.0061088986694812775\n",
      "loss\n",
      "0.00042572495294734836\n",
      "loss\n",
      "0.000428108120104298\n",
      "loss\n",
      "0.0028246049769222736\n",
      "loss\n",
      "0.007277645170688629\n",
      "loss\n",
      "0.0010798105504363775\n",
      "loss\n",
      "0.0030859727412462234\n",
      "loss\n",
      "0.0006139777251519263\n",
      "loss\n",
      "0.0005782362422905862\n",
      "loss\n",
      "0.010593726299703121\n",
      "loss\n",
      "0.0012159105390310287\n",
      "loss\n",
      "0.0005053196800872684\n",
      "loss\n",
      "0.001134705264121294\n",
      "loss\n",
      "0.00047267231275327504\n",
      "loss\n",
      "0.000846266164444387\n",
      "loss\n",
      "0.0016955060418695211\n",
      "loss\n",
      "0.001279369811527431\n",
      "loss\n",
      "0.00779361417517066\n",
      "loss\n",
      "0.0016464737709611654\n",
      "loss\n",
      "0.003351786872372031\n",
      "loss\n",
      "0.005628214683383703\n",
      "loss\n",
      "0.0035672858357429504\n",
      "loss\n",
      "0.0005370128201320767\n",
      "loss\n",
      "0.0006885541952215135\n",
      "loss\n",
      "0.0004690977220889181\n",
      "loss\n",
      "0.0005258131423033774\n",
      "loss\n",
      "0.0017419178038835526\n",
      "loss\n",
      "0.001593511551618576\n",
      "loss\n",
      "0.0007316772826015949\n",
      "loss\n",
      "0.000707971747033298\n",
      "loss\n",
      "0.0006911749369464815\n",
      "loss\n",
      "0.002665896899998188\n",
      "loss\n",
      "0.0006254147156141698\n",
      "loss\n",
      "0.0015022194711491466\n",
      "loss\n",
      "0.009392124600708485\n",
      "loss\n",
      "0.012946008704602718\n",
      "loss\n",
      "0.0010157431242987514\n",
      "loss\n",
      "0.0014280608156695962\n",
      "loss\n",
      "0.003851853543892503\n",
      "loss\n",
      "0.000559173640795052\n",
      "loss\n",
      "0.0003511289251036942\n",
      "loss\n",
      "0.004013104364275932\n",
      "loss\n",
      "0.0014513921923935413\n",
      "loss\n",
      "0.030644262209534645\n",
      "loss\n",
      "0.0011582816950976849\n",
      "loss\n",
      "0.0014397265622392297\n",
      "loss\n",
      "0.0021531034726649523\n",
      "loss\n",
      "0.001191621064208448\n",
      "loss\n",
      "0.0016000575851649046\n",
      "loss\n",
      "0.0017477489309385419\n",
      "loss\n",
      "0.008792140521109104\n",
      "loss\n",
      "0.0028020190075039864\n",
      "loss\n",
      "0.0006766413571313024\n",
      "loss\n",
      "0.0018878034316003323\n",
      "loss\n",
      "0.0017471539322286844\n",
      "loss\n",
      "0.001256510615348816\n",
      "loss\n",
      "0.001255677198059857\n",
      "loss\n",
      "0.0038440159987658262\n",
      "loss\n",
      "0.0017959432443603873\n",
      "loss\n",
      "0.0016683719586580992\n",
      "loss\n",
      "0.0008314966107718647\n",
      "loss\n",
      "0.0022165034897625446\n",
      "loss\n",
      "0.0023671959061175585\n",
      "loss\n",
      "0.0002982171718031168\n",
      "loss\n",
      "0.014729914255440235\n",
      "loss\n",
      "0.0027272433508187532\n",
      "loss\n",
      "0.0008842610404826701\n",
      "loss\n",
      "0.0004936429904773831\n",
      "loss\n",
      "0.0007914748275652528\n",
      "loss\n",
      "0.0013392536202445626\n",
      "loss\n",
      "0.0038484097458422184\n",
      "loss\n",
      "0.0009913297835737467\n",
      "loss\n",
      "0.03474237769842148\n",
      "loss\n",
      "0.0005641775787808001\n",
      "loss\n",
      "0.0003849719068966806\n",
      "loss\n",
      "0.0013365155318751931\n",
      "loss\n",
      "0.0008220868767239153\n",
      "loss\n",
      "0.0014574630185961723\n",
      "loss\n",
      "0.0016183863626793027\n",
      "loss\n",
      "0.00040356122190132737\n",
      "loss\n",
      "0.0005911033367738128\n",
      "loss\n",
      "0.0027939353603869677\n",
      "loss\n",
      "0.00039641151670366526\n",
      "loss\n",
      "0.0027140469755977392\n",
      "loss\n",
      "0.0018346159486100078\n",
      "loss\n",
      "0.002303329762071371\n",
      "loss\n",
      "0.0006030171643942595\n",
      "loss\n",
      "0.009124013595283031\n",
      "loss\n",
      "0.0012161486083641648\n",
      "loss\n",
      "0.0027712297160178423\n",
      "loss\n",
      "0.0002739054325502366\n",
      "loss\n",
      "0.0006935574929229915\n",
      "loss\n",
      "0.0003129946126136929\n",
      "loss\n",
      "0.000476246903417632\n",
      "loss\n",
      "0.00044288364006206393\n",
      "loss\n",
      "0.0018682897789403796\n",
      "loss\n",
      "0.0003418338019400835\n",
      "loss\n",
      "0.0012497241841629148\n",
      "loss\n",
      "0.0015631611458957195\n",
      "loss\n",
      "0.0023768290411680937\n",
      "loss\n",
      "0.02574142999947071\n",
      "loss\n",
      "0.0017401328077539802\n",
      "loss\n",
      "0.004134322516620159\n",
      "loss\n",
      "0.0008989107445813715\n",
      "loss\n",
      "0.0005607224884442985\n",
      "loss\n",
      "0.001046705641783774\n",
      "loss\n",
      "0.0007827793597243726\n",
      "loss\n",
      "0.0006280356901697814\n",
      "loss\n",
      "0.02786920592188835\n",
      "loss\n",
      "0.0008538890979252756\n",
      "loss\n",
      "0.0010851691477000713\n",
      "loss\n",
      "0.00240013818256557\n",
      "loss\n",
      "0.0021174170542508364\n",
      "loss\n",
      "0.002991488901898265\n",
      "loss\n",
      "0.0014588914345949888\n",
      "loss\n",
      "0.004241281189024448\n",
      "loss\n",
      "0.003736658487468958\n",
      "loss\n",
      "0.0022800182923674583\n",
      "loss\n",
      "0.0007152383332140744\n",
      "loss\n",
      "0.000924993772059679\n",
      "loss\n",
      "0.003994819708168507\n",
      "loss\n",
      "0.000894146622158587\n",
      "loss\n",
      "0.0027646913658827543\n",
      "loss\n",
      "0.0007594323833473027\n",
      "loss\n",
      "0.0021627387031912804\n",
      "loss\n",
      "0.0012949660886079073\n",
      "loss\n",
      "0.002144657773897052\n",
      "loss\n",
      "0.0016444505890831351\n",
      "loss\n",
      "0.0005864569102413952\n",
      "loss\n",
      "0.00039188333903439343\n",
      "loss\n",
      "0.0029439465142786503\n",
      "loss\n",
      "0.0022638426162302494\n",
      "loss\n",
      "0.0009483369067311287\n",
      "loss\n",
      "0.0116086695343256\n",
      "loss\n",
      "0.0008537700050510466\n",
      "loss\n",
      "0.001016814960166812\n",
      "loss\n",
      "0.0008986725588329136\n",
      "loss\n",
      "0.000701658078469336\n",
      "loss\n",
      "0.0017089537577703595\n",
      "loss\n",
      "0.0011794761521741748\n",
      "loss\n",
      "0.0004247716860845685\n",
      "loss\n",
      "0.0005683475756086409\n",
      "loss\n",
      "0.0006723527330905199\n",
      "loss\n",
      "0.0010266992030665278\n",
      "loss\n",
      "0.0019143365789204836\n",
      "loss\n",
      "0.0005214046686887741\n",
      "loss\n",
      "0.001148994080722332\n",
      "loss\n",
      "0.004422408062964678\n",
      "loss\n",
      "0.0005122303264215589\n",
      "loss\n",
      "0.0011468507582321763\n",
      "loss\n",
      "0.00022957073815632612\n",
      "loss\n",
      "0.0027576773427426815\n",
      "loss\n",
      "0.000952267087996006\n",
      "loss\n",
      "0.016635630279779434\n",
      "loss\n",
      "0.0007250064518302679\n",
      "loss\n",
      "0.0006623458466492593\n",
      "loss\n",
      "0.0007408496458083391\n",
      "loss\n",
      "0.002483262214809656\n",
      "loss\n",
      "0.0021470370702445507\n",
      "loss\n",
      "0.0009137984015978873\n",
      "loss\n",
      "0.00023457636416424066\n",
      "loss\n",
      "0.0023837266489863396\n",
      "loss\n",
      "0.0011623300379142165\n",
      "loss\n",
      "0.0013184197014197707\n",
      "loss\n",
      "0.001476865611039102\n",
      "loss\n",
      "0.003979858942329884\n",
      "loss\n",
      "0.0005212855176068842\n",
      "loss\n",
      "0.0005154472892172635\n",
      "loss\n",
      "0.00042965717148035765\n",
      "loss\n",
      "0.000764792668633163\n",
      "loss\n",
      "0.0009112972766160965\n",
      "loss\n",
      "0.0008796160109341145\n",
      "loss\n",
      "0.0008474572678096592\n",
      "loss\n",
      "0.0005744237569160759\n",
      "loss\n",
      "0.0019026764202862978\n",
      "loss\n",
      "0.002003092784434557\n",
      "loss\n",
      "0.42928773164749146\n",
      "loss\n",
      "0.0008839037618599832\n",
      "loss\n",
      "0.001987983239814639\n",
      "loss\n",
      "0.014038268476724625\n",
      "loss\n",
      "0.0011518517276272178\n",
      "loss\n",
      "0.00032658010604791343\n",
      "loss\n",
      "0.0008891443139873445\n",
      "loss\n",
      "0.000750736624468118\n",
      "loss\n",
      "0.013025554828345776\n",
      "loss\n",
      "0.004036019556224346\n",
      "loss\n",
      "0.005081120412796736\n",
      "loss\n",
      "0.0002636561985127628\n",
      "loss\n",
      "0.003246871754527092\n",
      "loss\n",
      "0.00031418632715940475\n",
      "loss\n",
      "0.005515950731933117\n",
      "loss\n",
      "0.0016540905926376581\n",
      "loss\n",
      "0.00022194306075107306\n",
      "loss\n",
      "0.0026214304380118847\n",
      "loss\n",
      "0.016800694167613983\n",
      "loss\n",
      "0.004386090207844973\n",
      "loss\n",
      "0.0003225283289793879\n",
      "loss\n",
      "0.000758955895435065\n",
      "loss\n",
      "0.00035613393993116915\n",
      "loss\n",
      "0.00037448544753715396\n",
      "loss\n",
      "0.0017446548445150256\n",
      "loss\n",
      "0.00015090756642166525\n",
      "loss\n",
      "0.0002574589161667973\n",
      "loss\n",
      "0.0007256020326167345\n",
      "loss\n",
      "0.001990600721910596\n",
      "loss\n",
      "0.00012468514614738524\n",
      "loss\n",
      "0.0026844439562410116\n",
      "loss\n",
      "0.0004638549580704421\n",
      "loss\n",
      "0.000788258679676801\n",
      "loss\n",
      "0.00028796817059628665\n",
      "loss\n",
      "0.0013338964199647307\n",
      "loss\n",
      "0.00020621081057470292\n",
      "loss\n",
      "0.05171101912856102\n",
      "loss\n",
      "0.0030126445926725864\n",
      "loss\n",
      "0.005566809326410294\n",
      "loss\n",
      "0.00016234986833296716\n",
      "loss\n",
      "0.00017736769223120064\n",
      "loss\n",
      "0.0002821285743266344\n",
      "loss\n",
      "0.0006914132391102612\n",
      "loss\n",
      "0.0004648081958293915\n",
      "loss\n",
      "0.0008348317351192236\n",
      "loss\n",
      "0.0019875073339790106\n",
      "loss\n",
      "0.0002169373765354976\n",
      "loss\n",
      "0.0012260308722034097\n",
      "loss\n",
      "0.00018523407925385982\n",
      "loss\n",
      "0.00041571559268049896\n",
      "loss\n",
      "0.0006481691962108016\n",
      "loss\n",
      "0.0010397987207397819\n",
      "loss\n",
      "0.0006696127820760012\n",
      "loss\n",
      "0.011480340734124184\n",
      "loss\n",
      "0.0003594706067815423\n",
      "loss\n",
      "0.0024949158541858196\n",
      "loss\n",
      "0.011587812565267086\n",
      "loss\n",
      "0.0006804534932598472\n",
      "loss\n",
      "0.0003280101518612355\n",
      "loss\n",
      "0.0011869773734360933\n",
      "loss\n",
      "0.0009833505610004067\n",
      "loss\n",
      "0.00016330339713022113\n",
      "loss\n",
      "0.0008563903393223882\n",
      "loss\n",
      "0.0004122599493712187\n",
      "loss\n",
      "0.0005706112715415657\n",
      "loss\n",
      "0.003935806918889284\n",
      "loss\n",
      "0.00036638224264606833\n",
      "loss\n",
      "0.0002915434306487441\n",
      "loss\n",
      "0.00020215852418914437\n",
      "loss\n",
      "0.00126008247025311\n",
      "loss\n",
      "0.0009531007381156087\n",
      "loss\n",
      "0.00030632095877081156\n",
      "loss\n",
      "0.0005411829333752394\n"
     ]
    }
   ],
   "source": [
    "gating, gating_optimizer = init_gating()\n",
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "for i in range(100):\n",
    "    for seqs, seqs_len in train_dls[2]:\n",
    "\n",
    "        gating.train()\n",
    "\n",
    "        gating_optimizer.zero_grad()\n",
    "\n",
    "        outputs = gating(seqs, seqs_len)\n",
    "\n",
    "        #print(\"seq\")\n",
    "        #print(seqs)\n",
    "        #print(\"outputs\")\n",
    "        #print(outputs)\n",
    "\n",
    "        if seqs[1] == 7:\n",
    "            trgts = torch.tensor([0])\n",
    "        else:\n",
    "            trgts = torch.tensor([1])\n",
    "\n",
    "        loss = compute_loss(outputs, trgts, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        gating_optimizer.step()\n",
    "\n",
    "        print(\"loss\")\n",
    "        print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ONMUcvfWeipD"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                        reduction=\"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcqtDHZcJah8",
    "outputId": "a48e44cc-67ab-474d-8e5b-5267ec501dac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(30, 10, bidirectional=True)\n",
      "        (fc): Linear(in_features=20, out_features=10, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=30, out_features=10, bias=True)\n",
      "          (v): Linear(in_features=10, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 30)\n",
      "        (rnn): GRU(50, 10)\n",
      "        (fc_out): Linear(in_features=60, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.7, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "gating, gating_optimizer = init_gating()\n",
    "model = DynaMoE(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "print(model.apply(init_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "UYduyRNYJolw"
   },
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARkJQvgNgU27",
    "outputId": "0012ec60-7122-4876-8fb5-5a863be5f25f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 5, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 7, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 2])\n",
      "1 - tensor([1, 5, 3, 4, 6, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 3, 4, 7, 4, 6, 2])\n",
      "1 - tensor([1, 5, 4, 6, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kO7XbXIIgjWN",
    "outputId": "67eb26e2-95b1-4844-e801-752078589da4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - tensor([1, 7, 4, 6, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 2])\n",
      "1 - tensor([1, 7, 3, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 2])\n",
      "1 - tensor([1, 7, 3, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 6, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 4, 6, 2])\n",
      "1 - tensor([1, 7, 4, 5, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(model, train_dls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "XQStSnwP-bwf"
   },
   "outputs": [],
   "source": [
    "gating_optimizer = optim.Adam(model.gating.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xtz-JxQ--n-U",
    "outputId": "a4164e4b-10f8-4a69-acb9-d454881779b5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3362e-14,\n",
      "          0.0000e+00,  0.0000e+00, -1.0627e-14,  1.2231e-14,  0.0000e+00],\n",
      "        [ 9.5526e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.5729e-09,\n",
      "         -8.0322e-09,  0.0000e+00, -1.3070e-08,  2.0716e-08,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00, -1.2082e-14, -4.8358e-12,  6.6748e-12, -6.2617e-12,\n",
      "         -5.3810e-12,  7.6332e-12, -2.3784e-12,  0.0000e+00, -7.3044e-12],\n",
      "        [-4.1353e-13, -4.9267e-13, -3.5623e-13,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  5.5728e-13,  0.0000e+00,  4.3358e-13, -5.3819e-13],\n",
      "        [ 0.0000e+00,  0.0000e+00, -8.9321e-11,  5.7395e-14,  0.0000e+00,\n",
      "         -1.0017e-10,  1.4144e-10,  0.0000e+00,  5.0259e-14, -6.3196e-14],\n",
      "        [ 0.0000e+00, -1.0079e-14,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         -8.3384e-15,  1.0995e-14, -6.3370e-15,  0.0000e+00, -1.1091e-14]])\n",
      "rnn.weight_ih_l0\n",
      "tensor([[-1.2741e-11, -4.1343e-15, -1.2010e-12,  5.5348e-14, -6.1908e-12,\n",
      "         -2.3396e-11,  1.1962e-12,  1.5775e-12,  1.0668e-11, -6.6139e-14],\n",
      "        [-1.7986e-11, -6.1930e-15, -1.7422e-12,  8.5238e-14, -8.7462e-12,\n",
      "         -3.3065e-11,  1.7365e-12,  2.2192e-12,  1.5059e-11, -1.0169e-13],\n",
      "        [-1.7355e-11, -5.7870e-15, -1.6031e-12,  7.9681e-14, -8.4371e-12,\n",
      "         -3.1844e-11,  1.5982e-12,  2.1442e-12,  1.4531e-11, -9.5055e-14],\n",
      "        [-1.8238e-11, -6.5488e-15, -1.8412e-12,  8.8614e-14, -8.8711e-12,\n",
      "         -3.3589e-11,  1.8348e-12,  2.2480e-12,  1.5270e-11, -1.0583e-13],\n",
      "        [-9.1758e-12, -2.9753e-15, -9.0931e-13,  3.4315e-14, -4.4529e-12,\n",
      "         -1.6883e-11,  9.0369e-13,  1.1421e-12,  7.6826e-12, -4.1435e-14],\n",
      "        [-1.2727e-17, -2.0106e-19, -7.9940e-19,  1.0168e-20, -3.3603e-17,\n",
      "         -2.3203e-17,  1.0065e-18, -1.8089e-17,  2.2752e-17, -2.1822e-19],\n",
      "        [-2.1118e-11, -6.7207e-15, -1.9434e-12,  9.6933e-14, -1.0267e-11,\n",
      "         -3.8743e-11,  1.9376e-12,  2.6091e-12,  1.7681e-11, -1.1530e-13],\n",
      "        [-4.2235e-12, -1.5118e-15, -4.4800e-13,  1.6007e-14, -2.0497e-12,\n",
      "         -7.7942e-12,  4.4502e-13,  5.2544e-13,  3.5362e-12, -1.9453e-14],\n",
      "        [-1.2104e-11, -4.2045e-15, -1.2392e-12,  4.8817e-14, -5.8774e-12,\n",
      "         -2.2303e-11,  1.2321e-12,  1.5028e-12,  1.0134e-11, -5.8929e-14],\n",
      "        [-4.7408e-12, -2.2792e-15, -5.7470e-13,  2.1609e-14, -2.3041e-12,\n",
      "         -8.8055e-12,  5.7121e-13,  5.8587e-13,  3.9693e-12, -2.6509e-14],\n",
      "        [ 6.6286e-13,  7.3984e-16, -8.5020e-14, -1.5201e-15,  3.2065e-13,\n",
      "          1.0991e-12,  8.3078e-14, -8.3417e-14, -5.5514e-13,  2.3342e-15],\n",
      "        [ 8.7248e-13,  8.9356e-16, -1.1475e-13, -1.8182e-15,  4.2183e-13,\n",
      "          1.4442e-12,  1.1224e-13, -1.1005e-13, -7.3065e-13,  2.8197e-15],\n",
      "        [ 8.4736e-13,  8.5097e-16, -1.0343e-13, -1.5827e-15,  4.0949e-13,\n",
      "          1.4090e-12,  1.0117e-13, -1.0709e-13, -7.0961e-13,  2.5146e-15],\n",
      "        [ 1.0759e-12,  9.2496e-16, -1.3016e-13, -2.1993e-15,  5.2013e-13,\n",
      "          1.7902e-12,  1.2728e-13, -1.3580e-13, -9.0093e-13,  3.2712e-15],\n",
      "        [ 5.6700e-13,  8.6599e-16, -8.4782e-14, -2.0148e-15,  2.7489e-13,\n",
      "          9.3052e-13,  8.2685e-14, -7.0587e-14, -4.7490e-13,  2.9716e-15],\n",
      "        [ 3.2364e-17,  1.4174e-16,  3.2161e-17, -3.8775e-17,  2.1664e-16,\n",
      "          1.3156e-16, -1.3289e-16,  2.2662e-16, -1.2378e-16,  1.6023e-16],\n",
      "        [ 1.1006e-12,  8.1159e-16, -1.1776e-13, -1.7806e-15,  5.3170e-13,\n",
      "          1.8437e-12,  1.1522e-13, -1.3944e-13, -9.2167e-13,  2.7080e-15],\n",
      "        [ 3.1208e-13,  9.5127e-16, -5.4686e-14, -1.5326e-15,  1.5156e-13,\n",
      "          5.0549e-13,  5.3179e-14, -3.8384e-14, -2.6151e-13,  2.4646e-15],\n",
      "        [ 6.8898e-13,  1.1269e-15, -1.1249e-13, -2.6129e-15,  3.3411e-13,\n",
      "          1.1229e-12,  1.0977e-13, -8.5628e-14, -5.7703e-13,  3.8803e-15],\n",
      "        [ 4.0820e-13,  1.7776e-15, -8.1602e-14, -2.9815e-15,  1.9877e-13,\n",
      "          6.5251e-13,  7.9180e-14, -4.9381e-14, -3.4215e-13,  4.7378e-15],\n",
      "        [-1.1690e-10, -4.0569e-14, -9.9090e-12,  5.7459e-13, -5.6869e-11,\n",
      "         -2.1379e-10,  9.9007e-12,  1.4401e-11,  9.7877e-11, -6.8428e-13],\n",
      "        [ 1.5179e-10,  5.5814e-14,  1.3358e-11, -8.0338e-13,  7.3899e-11,\n",
      "          2.7800e-10, -1.3354e-11, -1.8637e-11, -1.2709e-10,  9.5584e-13],\n",
      "        [ 1.4691e-10,  5.1973e-14,  1.2321e-11, -7.5753e-13,  7.1503e-11,\n",
      "          2.6858e-10, -1.2322e-11, -1.8059e-11, -1.2300e-10,  9.0062e-13],\n",
      "        [ 1.5032e-10,  5.7241e-14,  1.3668e-11, -8.0311e-13,  7.3187e-11,\n",
      "          2.7565e-10, -1.3659e-11, -1.8447e-11, -1.2585e-10,  9.5701e-13],\n",
      "        [ 7.5015e-11,  2.6564e-14,  6.6023e-12, -3.2660e-13,  3.6450e-11,\n",
      "          1.3737e-10, -6.5820e-12, -9.2864e-12, -6.2807e-11,  3.9260e-13],\n",
      "        [-4.9563e-15, -4.4676e-18, -2.3274e-16,  6.1259e-18, -2.7919e-15,\n",
      "         -8.9162e-15,  2.3483e-16,  3.4348e-16,  4.3254e-15, -1.1415e-17],\n",
      "        [-1.7836e-10, -5.9934e-14, -1.4637e-11,  8.8339e-13, -8.6773e-11,\n",
      "         -3.2581e-10,  1.4634e-11,  2.1964e-11,  1.4933e-10, -1.0495e-12],\n",
      "        [-3.8141e-11, -1.4796e-14, -3.5249e-12,  1.7314e-13, -1.8539e-11,\n",
      "         -6.9977e-11,  3.5140e-12,  4.7135e-12,  3.1934e-11, -2.0884e-13],\n",
      "        [ 9.0623e-11,  3.4656e-14,  8.3105e-12, -4.2677e-13,  4.4064e-11,\n",
      "          1.6622e-10, -8.2893e-12, -1.1184e-11, -7.5875e-11,  5.1302e-13],\n",
      "        [-3.8155e-11, -2.0677e-14, -4.1315e-12,  2.1317e-13, -1.8584e-11,\n",
      "         -7.0482e-11,  4.1218e-12,  4.6718e-12,  3.1946e-11, -2.5969e-13]])\n",
      "rnn.weight_hh_l0\n",
      "tensor([[ 3.2406e-11, -3.2207e-11, -3.2240e-11, -3.2168e-11, -3.2604e-11,\n",
      "          3.2936e-11,  3.2080e-11,  3.2774e-11, -3.2534e-11,  3.2767e-11],\n",
      "        [ 4.5892e-11, -4.5609e-11, -4.5657e-11, -4.5555e-11, -4.6172e-11,\n",
      "          4.6641e-11,  4.5431e-11,  4.6412e-11, -4.6072e-11,  4.6402e-11],\n",
      "        [ 4.4040e-11, -4.3768e-11, -4.3814e-11, -4.3715e-11, -4.4309e-11,\n",
      "          4.4760e-11,  4.3596e-11,  4.4539e-11, -4.4213e-11,  4.4530e-11],\n",
      "        [ 4.6769e-11, -4.6481e-11, -4.6530e-11, -4.6426e-11, -4.7053e-11,\n",
      "          4.7530e-11,  4.6300e-11,  4.7297e-11, -4.6952e-11,  4.7287e-11],\n",
      "        [ 2.3477e-11, -2.3332e-11, -2.3357e-11, -2.3304e-11, -2.3620e-11,\n",
      "          2.3859e-11,  2.3241e-11,  2.3742e-11, -2.3569e-11,  2.3737e-11],\n",
      "        [ 3.1307e-17, -3.1110e-17, -3.1140e-17, -3.1068e-17, -3.1490e-17,\n",
      "          3.1876e-17,  3.0985e-17,  3.1657e-17, -3.1424e-17,  3.1647e-17],\n",
      "        [ 5.3566e-11, -5.3236e-11, -5.3291e-11, -5.3171e-11, -5.3893e-11,\n",
      "          5.4443e-11,  5.3027e-11,  5.4174e-11, -5.3777e-11,  5.4162e-11],\n",
      "        [ 1.0898e-11, -1.0831e-11, -1.0842e-11, -1.0818e-11, -1.0964e-11,\n",
      "          1.1075e-11,  1.0789e-11,  1.1021e-11, -1.0940e-11,  1.1018e-11],\n",
      "        [ 3.1093e-11, -3.0902e-11, -3.0934e-11, -3.0865e-11, -3.1282e-11,\n",
      "          3.1599e-11,  3.0781e-11,  3.1444e-11, -3.1215e-11,  3.1437e-11],\n",
      "        [ 1.2456e-11, -1.2380e-11, -1.2393e-11, -1.2366e-11, -1.2531e-11,\n",
      "          1.2657e-11,  1.2332e-11,  1.2595e-11, -1.2505e-11,  1.2593e-11],\n",
      "        [-1.2247e-12,  1.2160e-12,  1.2177e-12,  1.2142e-12,  1.2331e-12,\n",
      "         -1.2481e-12, -1.2107e-12, -1.2407e-12,  1.2299e-12, -1.2403e-12],\n",
      "        [-1.6030e-12,  1.5915e-12,  1.5938e-12,  1.5892e-12,  1.6139e-12,\n",
      "         -1.6337e-12, -1.5846e-12, -1.6239e-12,  1.6098e-12, -1.6235e-12],\n",
      "        [-1.5819e-12,  1.5707e-12,  1.5729e-12,  1.5684e-12,  1.5927e-12,\n",
      "         -1.6119e-12, -1.5639e-12, -1.6024e-12,  1.5886e-12, -1.6020e-12],\n",
      "        [-2.0123e-12,  1.9981e-12,  2.0009e-12,  1.9952e-12,  2.0260e-12,\n",
      "         -2.0505e-12, -1.9894e-12, -2.0384e-12,  2.0208e-12, -2.0378e-12],\n",
      "        [-1.0099e-12,  1.0026e-12,  1.0041e-12,  1.0011e-12,  1.0169e-12,\n",
      "         -1.0296e-12, -9.9817e-13, -1.0233e-12,  1.0142e-12, -1.0230e-12],\n",
      "        [-2.7163e-16,  2.6840e-16,  2.6669e-16,  2.6575e-16,  2.6862e-16,\n",
      "         -3.1099e-16, -2.6572e-16, -2.7043e-16,  2.6912e-16, -2.6736e-16],\n",
      "        [-2.1069e-12,  2.0921e-12,  2.0950e-12,  2.0891e-12,  2.1210e-12,\n",
      "         -2.1463e-12, -2.0831e-12, -2.1339e-12,  2.1157e-12, -2.1333e-12],\n",
      "        [-5.3080e-13,  5.2684e-13,  5.2764e-13,  5.2603e-13,  5.3453e-13,\n",
      "         -5.4151e-13, -5.2447e-13, -5.3800e-13,  5.3309e-13, -5.3782e-13],\n",
      "        [-1.1973e-12,  1.1885e-12,  1.1903e-12,  1.1867e-12,  1.2056e-12,\n",
      "         -1.2209e-12, -1.1832e-12, -1.2134e-12,  1.2024e-12, -1.2130e-12],\n",
      "        [-6.6248e-13,  6.5742e-13,  6.5846e-13,  6.5637e-13,  6.6724e-13,\n",
      "         -6.7629e-13, -6.5440e-13, -6.7170e-13,  6.6538e-13, -6.7146e-13],\n",
      "        [ 2.7702e-10, -2.7530e-10, -2.7560e-10, -2.7497e-10, -2.7872e-10,\n",
      "          2.8157e-10,  2.7422e-10,  2.8018e-10, -2.7811e-10,  2.8012e-10],\n",
      "        [-3.5715e-10,  3.5494e-10,  3.5531e-10,  3.5451e-10,  3.5933e-10,\n",
      "         -3.6301e-10, -3.5355e-10, -3.6121e-10,  3.5856e-10, -3.6113e-10],\n",
      "        [-3.4445e-10,  3.4232e-10,  3.4268e-10,  3.4190e-10,  3.4656e-10,\n",
      "         -3.5012e-10, -3.4097e-10, -3.4838e-10,  3.4581e-10, -3.4830e-10],\n",
      "        [-3.5395e-10,  3.5176e-10,  3.5213e-10,  3.5134e-10,  3.5611e-10,\n",
      "         -3.5975e-10, -3.5038e-10, -3.5797e-10,  3.5534e-10, -3.5789e-10],\n",
      "        [-1.7851e-10,  1.7741e-10,  1.7759e-10,  1.7719e-10,  1.7960e-10,\n",
      "         -1.8144e-10, -1.7671e-10, -1.8054e-10,  1.7921e-10, -1.8050e-10],\n",
      "        [ 1.1874e-14, -1.1799e-14, -1.1812e-14, -1.1784e-14, -1.1948e-14,\n",
      "          1.2074e-14,  1.1752e-14,  1.2012e-14, -1.1921e-14,  1.2009e-14],\n",
      "        [ 4.1527e-10, -4.1270e-10, -4.1313e-10, -4.1219e-10, -4.1782e-10,\n",
      "          4.2211e-10,  4.1107e-10,  4.2001e-10, -4.1691e-10,  4.1992e-10],\n",
      "        [ 9.2472e-11, -9.1901e-11, -9.1998e-11, -9.1790e-11, -9.3037e-11,\n",
      "          9.3986e-11,  9.1541e-11,  9.3522e-11, -9.2836e-11,  9.3501e-11],\n",
      "        [-2.1470e-10,  2.1337e-10,  2.1360e-10,  2.1312e-10,  2.1601e-10,\n",
      "         -2.1821e-10, -2.1254e-10, -2.1714e-10,  2.1554e-10, -2.1709e-10],\n",
      "        [ 9.3705e-11, -9.3130e-11, -9.3226e-11, -9.3019e-11, -9.4273e-11,\n",
      "          9.5226e-11,  9.2767e-11,  9.4760e-11, -9.4072e-11,  9.4740e-11]])\n",
      "rnn.bias_ih_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.9870e-10, -3.8939e-10, -3.7496e-10, -3.8699e-10, -1.9244e-10,\n",
      "         1.3200e-14,  4.5421e-10,  9.8371e-11, -2.3352e-10,  1.0031e-10])\n",
      "rnn.bias_hh_l0\n",
      "tensor([ 3.2936e-11,  4.6642e-11,  4.4761e-11,  4.7531e-11,  2.3860e-11,\n",
      "         1.0899e-16,  5.4443e-11,  1.1075e-11,  3.1599e-11,  1.2657e-11,\n",
      "        -1.2491e-12, -1.6347e-12, -1.6129e-12, -2.0514e-12, -1.0307e-12,\n",
      "        -1.0369e-15, -2.1472e-12, -5.4274e-13, -1.2221e-12, -6.7793e-13,\n",
      "         2.8158e-10, -3.6301e-10, -3.5012e-10, -3.5975e-10, -1.8144e-10,\n",
      "         1.2938e-14,  4.2211e-10,  9.3988e-11, -2.1822e-10,  9.5228e-11])\n",
      "rnn.weight_ih_l0_reverse\n",
      "tensor([[-8.4594e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.0747e-11,\n",
      "         -1.4896e-10,  0.0000e+00,  1.0874e-11,  7.0829e-11,  0.0000e+00],\n",
      "        [-2.9883e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4394e-11,\n",
      "         -5.2622e-11,  0.0000e+00,  3.8413e-12,  2.5020e-11,  0.0000e+00],\n",
      "        [-5.3683e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.5858e-12,\n",
      "         -9.4532e-12,  0.0000e+00,  6.9007e-13,  4.4948e-12,  0.0000e+00],\n",
      "        [-1.1488e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5334e-11,\n",
      "         -2.0229e-10,  0.0000e+00,  1.4767e-11,  9.6184e-11,  0.0000e+00],\n",
      "        [-1.3635e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.5675e-11,\n",
      "         -2.4010e-10,  0.0000e+00,  1.7527e-11,  1.1416e-10,  0.0000e+00],\n",
      "        [-1.1582e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -5.5790e-12,\n",
      "         -2.0396e-11,  0.0000e+00,  1.4889e-12,  9.6977e-12,  0.0000e+00],\n",
      "        [-5.7277e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.7589e-12,\n",
      "         -1.0086e-11,  0.0000e+00,  7.3627e-13,  4.7957e-12,  0.0000e+00],\n",
      "        [ 1.6456e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  7.9265e-12,\n",
      "          2.8978e-11,  0.0000e+00, -2.1153e-12, -1.3778e-11,  0.0000e+00],\n",
      "        [-9.6630e-12,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.6545e-12,\n",
      "         -1.7016e-11,  0.0000e+00,  1.2421e-12,  8.0906e-12,  0.0000e+00],\n",
      "        [-4.0936e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.9718e-11,\n",
      "         -7.2086e-11,  0.0000e+00,  5.2621e-12,  3.4275e-11,  0.0000e+00],\n",
      "        [ 8.6252e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1546e-10,\n",
      "          1.5188e-09,  0.0000e+00, -1.1087e-10, -7.2217e-10,  0.0000e+00],\n",
      "        [ 6.7716e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.2618e-11,\n",
      "          1.1924e-10,  0.0000e+00, -8.7046e-12, -5.6697e-11,  0.0000e+00],\n",
      "        [ 3.7308e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.7971e-10,\n",
      "          6.5697e-10,  0.0000e+00, -4.7958e-11, -3.1237e-10,  0.0000e+00],\n",
      "        [ 8.7162e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.1984e-10,\n",
      "          1.5349e-09,  0.0000e+00, -1.1204e-10, -7.2979e-10,  0.0000e+00],\n",
      "        [ 8.1539e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  3.9276e-10,\n",
      "          1.4359e-09,  0.0000e+00, -1.0482e-10, -6.8271e-10,  0.0000e+00],\n",
      "        [ 5.1821e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  2.4961e-10,\n",
      "          9.1254e-10,  0.0000e+00, -6.6614e-11, -4.3389e-10,  0.0000e+00],\n",
      "        [-5.1632e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -2.4870e-11,\n",
      "         -9.0921e-11,  0.0000e+00,  6.6371e-12,  4.3231e-11,  0.0000e+00],\n",
      "        [-1.8575e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.9473e-11,\n",
      "         -3.2710e-10,  0.0000e+00,  2.3878e-11,  1.5553e-10,  0.0000e+00],\n",
      "        [-2.9914e-11,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4409e-11,\n",
      "         -5.2677e-11,  0.0000e+00,  3.8454e-12,  2.5047e-11,  0.0000e+00],\n",
      "        [ 1.2659e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  6.0975e-11,\n",
      "          2.2291e-10,  0.0000e+00, -1.6272e-11, -1.0599e-10,  0.0000e+00],\n",
      "        [ 8.4579e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00,  4.0740e-10,\n",
      "          1.4894e-09,  0.0000e+00, -1.0872e-10, -7.0816e-10,  0.0000e+00],\n",
      "        [-1.3305e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.4088e-10,\n",
      "         -2.3429e-09,  0.0000e+00,  1.7103e-10,  1.1140e-09,  0.0000e+00],\n",
      "        [-1.0231e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.9283e-11,\n",
      "         -1.8017e-10,  0.0000e+00,  1.3152e-11,  8.5665e-11,  0.0000e+00],\n",
      "        [ 1.1314e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4499e-10,\n",
      "          1.9924e-09,  0.0000e+00, -1.4544e-10, -9.4733e-10,  0.0000e+00],\n",
      "        [-1.4078e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.7809e-10,\n",
      "         -2.4790e-09,  0.0000e+00,  1.8096e-10,  1.1787e-09,  0.0000e+00],\n",
      "        [-1.6939e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -8.1590e-11,\n",
      "         -2.9828e-10,  0.0000e+00,  2.1774e-11,  1.4182e-10,  0.0000e+00],\n",
      "        [ 1.1274e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.4306e-10,\n",
      "          1.9853e-09,  0.0000e+00, -1.4493e-10, -9.4398e-10,  0.0000e+00],\n",
      "        [-7.6532e-10,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.6864e-10,\n",
      "         -1.3477e-09,  0.0000e+00,  9.8378e-11,  6.4078e-10,  0.0000e+00],\n",
      "        [ 1.0906e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  5.2532e-10,\n",
      "          1.9205e-09,  0.0000e+00, -1.4019e-10, -9.1313e-10,  0.0000e+00],\n",
      "        [-1.3831e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00, -6.6623e-10,\n",
      "         -2.4356e-09,  0.0000e+00,  1.7780e-10,  1.1581e-09,  0.0000e+00]])\n",
      "rnn.weight_hh_l0_reverse\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "rnn.bias_ih_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.9353e-09,  3.0444e-09,  2.3411e-10, -2.5889e-09,  3.2212e-09,\n",
      "         3.8758e-10, -2.5797e-09,  1.7512e-09, -2.4954e-09,  3.1648e-09])\n",
      "rnn.bias_hh_l0_reverse\n",
      "tensor([ 1.9356e-10,  6.8377e-11,  1.2284e-11,  2.6286e-10,  3.1198e-10,\n",
      "         2.6502e-11,  1.3106e-11, -3.7654e-11,  2.2110e-11,  9.3668e-11,\n",
      "        -1.9736e-09, -1.5495e-10, -8.5366e-10, -1.9944e-09, -1.8657e-09,\n",
      "        -1.1858e-09,  1.1814e-10,  4.2503e-10,  6.8449e-11, -2.8965e-10,\n",
      "        -1.4298e-09,  1.0445e-09,  2.2012e-10, -1.7819e-09,  2.0395e-09,\n",
      "         3.5252e-10, -7.8725e-10,  4.3729e-10, -7.7968e-10,  1.1508e-09])\n",
      "fc_out.weight\n",
      "tensor([[-9.4410e-09,  9.3946e-09,  9.4020e-09,  9.3900e-09,  9.4874e-09,\n",
      "         -9.5659e-09, -9.3705e-09, -9.5267e-09,  9.4705e-09, -9.5252e-09,\n",
      "          5.9604e-09, -2.1056e-10, -8.9559e-09,  5.1718e-09, -4.0032e-09,\n",
      "         -8.5015e-09, -1.5177e-10,  5.1033e-10, -8.8546e-11, -4.0043e-10],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [-1.2325e-08,  1.2264e-08,  1.2274e-08,  1.2258e-08,  1.2385e-08,\n",
      "         -1.2488e-08, -1.2233e-08, -1.2437e-08,  1.2363e-08, -1.2435e-08,\n",
      "          7.7809e-09, -2.7487e-10, -1.1691e-08,  6.7515e-09, -5.2260e-09,\n",
      "         -1.1098e-08, -1.9813e-10,  6.6621e-10, -1.1559e-10, -5.2274e-10]])\n",
      "fc_out.bias\n",
      "tensor([-9.5659e-09,  0.0000e+00, -1.2488e-08])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.gating.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "        print(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgyhIfAtNrto"
   },
   "outputs": [],
   "source": [
    "gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aKTw6jmppP8y",
    "outputId": "6161027d-7195-48ea-952c-005a6461f81b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 238,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gating_criterion(torch.tensor([[0, 1.0000e+10, 0],\n",
    "        [0, 1.0000e+10, 0]]),\n",
    "        torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbNMxqrZp_S-",
    "outputId": "7d2a76f9-b577-4a74-bb77-4af9d2f93c01",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([2])\n",
      ">> Gating Loss\n",
      "tensor(1.0986, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[-2.3803e-05]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0837, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0650, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0255]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0419, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0131, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9796, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0922]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.9351, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1283]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8878, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1678]], grad_fn=<SliceBackward>)\n",
      "0.82974424213171\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.8212, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2259]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.7432, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2976]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.6585, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.5479, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.4752, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.5942]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.3468, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.7856]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.2496, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[0.9747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.1614, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.2153]], grad_fn=<SliceBackward>)\n",
      "0.9061286374926567\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0966, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.4879]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0580, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.7517]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0428, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[1.9076]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0243, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.1951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0156, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.4174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0122, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.5413]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0075, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0071, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.8154]], grad_fn=<SliceBackward>)\n",
      "0.9074996039271355\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0053, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[2.9616]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0032, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2070]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0021, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.4383]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0030, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.2398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0023, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3716]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0025, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6711]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6474]], grad_fn=<SliceBackward>)\n",
      "0.9339357018470764\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0014, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6148]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0013, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.6527]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2055]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9430]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1320]], grad_fn=<SliceBackward>)\n",
      "0.8914671167731285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0007, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.9443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3627]], grad_fn=<SliceBackward>)\n",
      "0.9092058017849922\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2314]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4475]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4427]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4040]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0009, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[3.8662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4927]], grad_fn=<SliceBackward>)\n",
      "0.9086853638291359\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7027]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5852]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4688]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8051]], grad_fn=<SliceBackward>)\n",
      "0.9540341049432755\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5801]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5563]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3491]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7696]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5286]], grad_fn=<SliceBackward>)\n",
      "0.9324938133358955\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3866]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5487]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6850]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7848]], grad_fn=<SliceBackward>)\n",
      "0.9254426285624504\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7740]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8104e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2422]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2047]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3928]], grad_fn=<SliceBackward>)\n",
      "0.8661048337817192\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6382]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7866e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9646]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4085]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6154]], grad_fn=<SliceBackward>)\n",
      "0.8831527382135391\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9931]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7738]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4841]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6868]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4436]], grad_fn=<SliceBackward>)\n",
      "0.880535438656807\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4818]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0006, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.0234]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7940]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4468]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8329]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4648e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9808]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5697]], grad_fn=<SliceBackward>)\n",
      "0.8675431832671165\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6845]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4355]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6103]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9230]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6864]], grad_fn=<SliceBackward>)\n",
      "0.9496602267026901\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6443]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4083]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7828]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7782]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6642]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3674]], grad_fn=<SliceBackward>)\n",
      "0.9153327122330666\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4941]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9431]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0011]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9210]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7318]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6719]], grad_fn=<SliceBackward>)\n",
      "0.9284641966223717\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2209]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7534]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6434]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5050]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4922]], grad_fn=<SliceBackward>)\n",
      "0.9609360843896866\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6555e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4290e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7235]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1793]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7433]], grad_fn=<SliceBackward>)\n",
      "0.9041190296411514\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3575e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9867]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6078e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6897]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6544]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7483]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8843]], grad_fn=<SliceBackward>)\n",
      "0.903328500688076\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8428]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7444]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2502e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4540]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2109]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7608]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7668]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3372]], grad_fn=<SliceBackward>)\n",
      "0.9555044546723366\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8617]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5408]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8898]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7394]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8731]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2313]], grad_fn=<SliceBackward>)\n",
      "0.9302210956811905\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2986]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3337e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9876]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5304]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4039e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0407]], grad_fn=<SliceBackward>)\n",
      "0.8923574611544609\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5188]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7251]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1046e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7108]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6158]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5218e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0967]], grad_fn=<SliceBackward>)\n",
      "0.9684783220291138\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8901e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1406]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4026e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9114]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8457]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0005, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.1415]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.2771]], grad_fn=<SliceBackward>)\n",
      "0.9525922313332558\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7273]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5614]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6529e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9403e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0092]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0997]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0004, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3187]], grad_fn=<SliceBackward>)\n",
      "0.8704231753945351\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5723]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0595e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0026]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5881]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7264]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5248]], grad_fn=<SliceBackward>)\n",
      "0.8824385702610016\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5601e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9758]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4923]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7087]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1972]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8807e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0128]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0074]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2956]], grad_fn=<SliceBackward>)\n",
      "0.9546280577778816\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3962]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3098e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9884]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0331e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1300]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7753]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6634]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9485]], grad_fn=<SliceBackward>)\n",
      "0.9295287430286407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.1761e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1197]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3370]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9178]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1839]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.9654e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2145e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9944]], grad_fn=<SliceBackward>)\n",
      "0.8718064799904823\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6463]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7837]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8458]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5933e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0913]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5339]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8529]], grad_fn=<SliceBackward>)\n",
      "0.8350213691592216\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3111]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.8939e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1571]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1373]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0569e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1284]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6946]], grad_fn=<SliceBackward>)\n",
      "0.941309466958046\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6921]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7919]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3554]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1072e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8343]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8788]], grad_fn=<SliceBackward>)\n",
      "0.9300222471356392\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.6980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2356]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3648]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2035]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2303]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2370e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3549e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1079]], grad_fn=<SliceBackward>)\n",
      "0.9206623733043671\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2090]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2973]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9398]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1965]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8679]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6243]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7694]], grad_fn=<SliceBackward>)\n",
      "0.9505183324217796\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3803]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1271]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6526]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2224]], grad_fn=<SliceBackward>)\n",
      "0.8610163629055023\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8198e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1655]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.0953e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0000]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8999]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.8926e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0792]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5534]], grad_fn=<SliceBackward>)\n",
      "0.9559815302491188\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3432]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5469e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9510]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8998]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7809]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.7747e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9638]], grad_fn=<SliceBackward>)\n",
      "0.982656717300415\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7947e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1464]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1478]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0726]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1562]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1804]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1315]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1703]], grad_fn=<SliceBackward>)\n",
      "0.9076273813843727\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7778]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7198]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1668e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9961]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4522]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0199e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0213]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9854e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0903]], grad_fn=<SliceBackward>)\n",
      "0.9130949825048447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2036]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6566]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7200]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9442]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.1178e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0568]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3338]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2357e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1155]], grad_fn=<SliceBackward>)\n",
      "0.9232373982667923\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2943]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5479]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5495]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3715]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9175]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4886e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3081]], grad_fn=<SliceBackward>)\n",
      "0.9844113141298294\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3553]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7245e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7457e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.2013e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0521]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1420]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0777]], grad_fn=<SliceBackward>)\n",
      "0.906353659927845\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2683]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.4951]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9352]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4980e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1621]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5701]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "0.9535688981413841\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.0689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3641]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7138e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0211]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4322]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4635e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0359]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2119e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1173]], grad_fn=<SliceBackward>)\n",
      "0.97125643491745\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9165e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0093]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.5485]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6330]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.4252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1748]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5840e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9730]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3423]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1825]], grad_fn=<SliceBackward>)\n",
      "1.0099817141890526\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5900]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1847]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1950]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2959]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4402]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3417e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2336]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1123]], grad_fn=<SliceBackward>)\n",
      "0.9761429280042648\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0663e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7194]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4567]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1948e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5258]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3700]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6055]], grad_fn=<SliceBackward>)\n",
      "0.9192996472120285\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6065e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0276]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4312]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3450]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0003, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.3697]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2895]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2797]], grad_fn=<SliceBackward>)\n",
      "0.9878424927592278\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4064]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8269]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.6423e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0256]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7722]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1844]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.2106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1912]], grad_fn=<SliceBackward>)\n",
      "0.9360367059707642\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2252]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3832]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3678]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2162]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3179]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1429e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9967]], grad_fn=<SliceBackward>)\n",
      "0.9009067863225937\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9006]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0940e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0581]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4516e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0364]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1126e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3992]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.2383e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8675e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0727]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3520]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5749e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6362]], grad_fn=<SliceBackward>)\n",
      "0.9399682730436325\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5874]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3456e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9861]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4212]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3954]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5671]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0737]], grad_fn=<SliceBackward>)\n",
      "0.9046507701277733\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2467]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4278]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3298e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1814]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9299]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9921e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0054e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4131]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0379]], grad_fn=<SliceBackward>)\n",
      "0.8922909200191498\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.3813e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9843]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2239]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9311]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2146]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4288]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2247]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9376]], grad_fn=<SliceBackward>)\n",
      "0.9415184482932091\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2638]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4342]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.6291e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0882]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1603]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.2715e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1120]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4346]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5226]], grad_fn=<SliceBackward>)\n",
      "0.9168690741062164\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7180]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4277e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0374]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9205]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6203]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2857]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3545]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8505]], grad_fn=<SliceBackward>)\n",
      "0.9186825603246689\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8924]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8905]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2817]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6391]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5944]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6252e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3396]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1430]], grad_fn=<SliceBackward>)\n",
      "1.0298770293593407\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3640]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8706]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6598]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4381]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2565]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.5695e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0920]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8133e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3518]], grad_fn=<SliceBackward>)\n",
      "0.9632200226187706\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3154]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2023]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4719]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3268]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1830]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1365e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3953]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8894]], grad_fn=<SliceBackward>)\n",
      "0.8725502789020538\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1332]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4833]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6502]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9735e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6266]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5859]], grad_fn=<SliceBackward>)\n",
      "0.9114488065242767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9960e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2089]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5629]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3777]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5034]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7095]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2518]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3901]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3826]], grad_fn=<SliceBackward>)\n",
      "0.997397854924202\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8003]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3097]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4366]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8341]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4170]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1381]], grad_fn=<SliceBackward>)\n",
      "0.8821030855178833\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.3788e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1053]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7115]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.9641e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0062]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1152e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1991]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.5363e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9755]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.8124]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8623e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4294]], grad_fn=<SliceBackward>)\n",
      "0.9253259226679802\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5151]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6894]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8625]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5305]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3835]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6994]], grad_fn=<SliceBackward>)\n",
      "0.826819896697998\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2869]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3514]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2914e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3774]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7934e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3588]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6805]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3285e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3351e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9637]], grad_fn=<SliceBackward>)\n",
      "0.9356672465801239\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6941e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6112]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5946e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0275]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1360]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2666]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8146e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4350]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6202]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2711]], grad_fn=<SliceBackward>)\n",
      "0.9173995330929756\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5826]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7444e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3270]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7982]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7323]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8838]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.7734e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4181]], grad_fn=<SliceBackward>)\n",
      "0.9404812529683113\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1484e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2214]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7173]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2763]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6613]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5537e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5192e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2514]], grad_fn=<SliceBackward>)\n",
      "0.908241368830204\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5550e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2469]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5391e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6407]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4078]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1784]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6141]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2168]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6144]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4947]], grad_fn=<SliceBackward>)\n",
      "0.960708424448967\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2662]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4815]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9057]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.9271e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0684]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4498]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3351]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6400]], grad_fn=<SliceBackward>)\n",
      "0.9716712683439255\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9403]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4854]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.4741e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0977]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8236]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9031]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4747]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.3085e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6464e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6196]], grad_fn=<SliceBackward>)\n",
      "0.9475336000323296\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6060]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6597]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9364e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2132]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4424e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9261]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9193e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6303]], grad_fn=<SliceBackward>)\n",
      "0.9496114104986191\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.3762e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2637]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6014e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3421]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.9377e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1358]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8172e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2233]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8546]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4791]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0001, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.9064]], grad_fn=<SliceBackward>)\n",
      "0.9413476660847664\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4557]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6966]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4809e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4819]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9695]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8497]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7643e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8241]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8081]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8603]], grad_fn=<SliceBackward>)\n",
      "0.8706775084137917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2751]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.0582e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0599]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6306]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2411e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7039]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7073]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3102]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5034e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3231]], grad_fn=<SliceBackward>)\n",
      "0.9316414818167686\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6714]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0994e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5410]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6669]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4859]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2053e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7125]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.0067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2988]], grad_fn=<SliceBackward>)\n",
      "0.9206378161907196\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7277]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3717]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2903]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1855e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2824]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.7470e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1489]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.4822e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3551]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5657]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3291]], grad_fn=<SliceBackward>)\n",
      "0.8711266815662384\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0027e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7609]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5503]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.1842e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3892]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3987e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3649]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5139]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7206e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3286]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6760]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3847]], grad_fn=<SliceBackward>)\n",
      "0.9439148530364037\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9087e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5720]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1352e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5354]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0398e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5509]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8880]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3775e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1769]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5106]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8602]], grad_fn=<SliceBackward>)\n",
      "0.9201693758368492\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.1868e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1925]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6908]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5094]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8918]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.7721e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6689e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8538]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2318e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3829]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5983]], grad_fn=<SliceBackward>)\n",
      "0.9166541695594788\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1577e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7206]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3248]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.8662e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1399]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.5801e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1612]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2305e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5201]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5670]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3129]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.6542]], grad_fn=<SliceBackward>)\n",
      "0.9151469171047211\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0888e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4002]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9099]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.5418e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3490]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9325e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5680]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7409]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0769e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4017]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1219e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "0.8847057297825813\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.2080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3875]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1543]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.6040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1602]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9150]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9603e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5619]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5259e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8996]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7073e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4501]], grad_fn=<SliceBackward>)\n",
      "0.9680259078741074\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5604]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4186e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9340]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7770]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6166]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0650e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4043]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7885]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8000e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8142]], grad_fn=<SliceBackward>)\n",
      "0.9062342792749405\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.5669e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2459]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.9841e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2098]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8056]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9116]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5405e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4743]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7908e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4388]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.0795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2013]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8037]], grad_fn=<SliceBackward>)\n",
      "0.8556824177503586\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.3272e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3724]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6071]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7881e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8161]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7696e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2281]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8358e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8051]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6226e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6260]], grad_fn=<SliceBackward>)\n",
      "0.8968541398644447\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6239e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4615]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7179e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9842]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4981]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8386]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6171]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3722e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6759]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.5510e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6369]], grad_fn=<SliceBackward>)\n",
      "0.9463448226451874\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.9815e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4136]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7524e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8018]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7656e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5963]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8466]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.7338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2308]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(0.0002, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[4.7012]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2570e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2749]], grad_fn=<SliceBackward>)\n",
      "0.9251849204301834\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6106e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6263]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6928e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8456]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.0530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4059]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4978]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.8716e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7937]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9686]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.6559e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1310]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.6954e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4523]], grad_fn=<SliceBackward>)\n",
      "0.9190128594636917\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2278e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0058]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4835e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2537]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(6.3060e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.1821]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.5047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4788]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.4212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4912]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4795e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6536]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2755e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9889]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9957]], grad_fn=<SliceBackward>)\n",
      "0.9369889944791794\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6675]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6451e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8579]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6681]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4080e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6682]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8702]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7166e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8357]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6698]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.4175e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1353]], grad_fn=<SliceBackward>)\n",
      "0.8986769914627075\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3961e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6694]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.7431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4452]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2517e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9970]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.2093e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2790]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7287]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.8636e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3134]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5072]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5080]], grad_fn=<SliceBackward>)\n",
      "0.8923800513148308\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.4477e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2570]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.2292e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7044]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.7047e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8404]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(9.1791e-06, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.1504]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1086e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0575]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.5827e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0272]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.8530e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2201]], grad_fn=<SliceBackward>)\n",
      "0.8838892206549644\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.8861e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4253]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.7683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3228]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2782e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5107]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0254]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6332e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8626]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2544e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9010]], grad_fn=<SliceBackward>)\n",
      "0.8662857860326767\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.3974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.4945]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6371e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3378]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0040e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5552]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.2067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5242]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5974e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8772]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9683e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5620]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.1563e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0331]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6600]], grad_fn=<SliceBackward>)\n",
      "0.8832837343215942\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5020e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9028]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.6610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3345]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.6212e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8647]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1113e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5368]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7733]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.4438e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6592]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0967e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0618]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1338e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7280]], grad_fn=<SliceBackward>)\n",
      "0.9058996215462685\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5497e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8928]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9312e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7773]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1378e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2865]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0848e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0713]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.1100e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7309]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(5.1139e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.2887]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.3590e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9529]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.9431e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7744]], grad_fn=<SliceBackward>)\n",
      "0.9235536307096481\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.0279e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5516]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.0610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[6.0756]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.2875e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9810]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4543e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9174]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.6703e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6135]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.0385e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.7508]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(8.4873e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0328]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.7895e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5926]], grad_fn=<SliceBackward>)\n",
      "0.8536463901400566\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(4.9828e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.3014]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(7.8437e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.0728]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.3007e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.6915]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.9802e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5596]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.5736e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.8787]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(1.4067e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.9365]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(2.8610e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5812]], grad_fn=<SliceBackward>)\n",
      "Target\n",
      "tensor([0])\n",
      "Output\n",
      "tensor([0])\n",
      ">> Gating Loss\n",
      "tensor(3.1232e-05, grad_fn=<NllLossBackward>)\n",
      "-- Masked Gating\n",
      "tensor([[5.5363]], grad_fn=<SliceBackward>)\n",
      "0.9422603696584702\n"
     ]
    }
   ],
   "source": [
    "for x in range(100):\n",
    "    print( train_dynamoe_gating(model, train_dls[1], gating_criterion,\n",
    "                                expert_criterion_unreduced,\n",
    "                                CLIP, verbose=True) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8fcaN82bJ-f5",
    "outputId": "2c94f843-e801-4dfb-d79c-89ab704be8fd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.630012683570385\n",
      "0.47744666039943695\n",
      "0.44646283984184265\n",
      "0.4113902524113655\n",
      "0.4127246364951134\n",
      "0.37290962785482407\n",
      "0.43906231224536896\n",
      "0.385328508913517\n",
      "0.37053677439689636\n",
      "0.3609902262687683\n",
      "0.3726344630122185\n",
      "0.3744181916117668\n",
      "0.36561162024736404\n",
      "0.3524796664714813\n",
      "0.39585812389850616\n",
      "0.32904189825057983\n",
      "0.3342840299010277\n",
      "0.3753737062215805\n",
      "0.40356797724962234\n",
      "0.3463684171438217\n",
      "0.35821671038866043\n",
      "0.36753934621810913\n",
      "0.31112828850746155\n",
      "0.3395487293601036\n",
      "0.32247451692819595\n",
      "0.31689079105854034\n",
      "0.3214147612452507\n",
      "0.3431061953306198\n",
      "0.32723822444677353\n",
      "0.3066282793879509\n",
      "0.2927127256989479\n",
      "0.3012479245662689\n",
      "0.2619609013199806\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-d7e8f4aa73e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_dynamoe_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgating_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_criterion_unreduced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-00addaa6fd57>\u001b[0m in \u001b[0;36mtrain_dynamoe_both\u001b[0;34m(model, iterator, gating_criterion, expert_criterion, clip)\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# Train newly initialized model on new train examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mreduced_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mreduced_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpert_optimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for x in range(400):\n",
    "    print( train_dynamoe_both(model, train_dls[1], gating_criterion, expert_criterion_unreduced, CLIP) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer G: Adaptive Mixture of Experts - adamoe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_adamoe\n",
    "Experts only get trained on sequences gating choses for them\n",
    "\n",
    "Variant 1:\n",
    "1. Compute Model Error\n",
    "2. Compute Gating choice\n",
    "3. \"Question\" Gating choice depending on model error\n",
    "4. Train experts depending on resulting choice\n",
    "5. Train gating on resulting error\n",
    "\n",
    "Variant 2:\n",
    "1. Train gating (pre-expert-train), byproduct: gating error, gating choices\n",
    "2. Depending on gating error, \"question\" gating choices\n",
    "3. Train experts depending on resulting choice\n",
    "\n",
    "Variant 3: (has an inf-loop)\n",
    "1. Compute Gating choices\n",
    "2. Depending result train expert, byproduct expert losses\n",
    "3. train Gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adamoe(model, iterator, gating_criterion, expert_criterion,\n",
    "                         expert_criterion_unreduced, clip, SIGNAL=False):\n",
    "\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "\n",
    "        batch_size = seqs.shape[1]\n",
    "        \n",
    "        # 1. Compute model error\n",
    "        model_outputs = model.forward(seqs, seqs_len, seqs)\n",
    "        \n",
    "        model_loss = compute_loss(model_outputs, seqs,\n",
    "                                  expert_criterion,\n",
    "                                  cutFirstInSequence=True)\n",
    "        \n",
    "        \n",
    "        # 2. Gating choices\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "        \n",
    "        gating_choices = torch.argmax(gating_masked, dim=1)\n",
    "        # gating_choices = [batch_size]\n",
    "        \n",
    "        if SIGNAL:\n",
    "            print(\"gating_out\", gating_outputs)\n",
    "            print(\"choices\", gating_choices)\n",
    "        # 3. \"Question\" choices\n",
    "        # Roll dice depending on error for every batch content\n",
    "        gating_override = torch.empty(batch_size)\n",
    "        for b in range(batch_size):\n",
    "            rand = random.random()\n",
    "            # print(\"rand\", rand)\n",
    "            if SIGNAL:\n",
    "                print(\"model_loss\", model_loss)\n",
    "            if rand < model_loss * 0.75 and model.n_active_experts > 1:\n",
    "                randchoice = random.randint(0, model.n_active_experts - 2)\n",
    "                # cannot choose the original choice\n",
    "                gating_override[b] = randchoice\n",
    "                if randchoice >= gating_choices[b]:\n",
    "                    gating_override[b] += 1\n",
    "            else:\n",
    "                gating_override[b] = gating_choices[b]\n",
    "        if SIGNAL:\n",
    "            print(\"override\", gating_override)\n",
    "        \n",
    "        # 4. Train expert on resulting choice\n",
    "        # Compute loss for all experts at the same time\n",
    "        loss_experts = torch.empty((batch_size, model.n_active_experts))\n",
    "        # loss_experts = [batch_size, n_active_experts]\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            \n",
    "            train_id = int(gating_override[b])\n",
    "            \n",
    "            for e_id in range(model.n_active_experts):\n",
    "                \n",
    "                seq_curr = seqs[:,b].unsqueeze(1)\n",
    "                seq_len_curr = seqs_len[b].unsqueeze(0)\n",
    "\n",
    "                #passage = True\n",
    "                #if SIGNAL and train_id == 0:\n",
    "                #   passage = False\n",
    "            \n",
    "                if train_id == e_id:\n",
    "                    model.experts[e_id].train()\n",
    "                    model.expert_optimizers[e_id].zero_grad()\n",
    "\n",
    "                    # Get model prediction\n",
    "                    expert_outputs = model.experts[e_id](seq_curr,\n",
    "                                                         seq_len_curr,\n",
    "                                                         seq_curr)\n",
    "\n",
    "                    loss = compute_loss(expert_outputs, seq_curr,\n",
    "                                        expert_criterion,\n",
    "                                        cutFirstInSequence=True)\n",
    "\n",
    "                    # Log loss to train gating\n",
    "                    loss_experts[b,e_id] = loss\n",
    "\n",
    "                    # Train newly initialized model on new train examples\n",
    "                    reduced_loss = loss.mean()\n",
    "                    reduced_loss.backward()\n",
    "                    model.expert_optimizers[e_id].step()\n",
    "\n",
    "                else:\n",
    "                    model.experts[e_id].eval()\n",
    "\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        # Get model prediction\n",
    "                        expert_outputs = model.experts[e_id](seq_curr,\n",
    "                                                             seq_len_curr,\n",
    "                                                             seq_curr)\n",
    "\n",
    "                        loss = compute_loss(expert_outputs, seq_curr,\n",
    "                                            expert_criterion,\n",
    "                                            cutFirstInSequence=True)\n",
    "#                         if SIGNAL:\n",
    "#                             print(f\"e_id:{e_id} loss:{loss}\")\n",
    "\n",
    "                        # Log loss to train gating\n",
    "                        loss_experts[b,e_id] = loss\n",
    "            \n",
    "            model.n_train_on_experts[train_id] += 1\n",
    "            model.expert_schedulers[train_id].step()\n",
    "\n",
    "        # 5. Train Gating\n",
    "        # Compute expert which should have been chosen\n",
    "        gating_trgs = loss_experts.argmin(dim=1)\n",
    "        # gating_trgs = [batch_size]\n",
    "        \n",
    "        if SIGNAL:\n",
    "            print(\"loss_experts\", loss_experts)\n",
    "            print(\"gating_trgs\", gating_trgs)\n",
    "\n",
    "        gating_trgs = gating_trgs.unsqueeze(0)\n",
    "        # gating_trgs = [[batch_size]]\n",
    "        gating_outputs = gating_outputs.unsqueeze(0)\n",
    "        # gating_ouputs = [[batch_size, n_max_experts]]\n",
    "\n",
    "        gating_loss = compute_loss(gating_outputs, gating_trgs, gating_criterion,\n",
    "                            cutFirstInSequence=False)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        # Get loss of model gating chose\n",
    "        loss_chosen_experts = loss_experts[:,gating_choices]\n",
    "        \n",
    "        loss_chosen_experts = loss_chosen_experts.mean()\n",
    "        \n",
    "        epoch_loss += loss_chosen_experts.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replay_adamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_adamoe(model, task_id, gating_criterion, expert_criterion_unreduced, clip,SIGNAL):\n",
    "    for other_id in range(task_id):\n",
    "        train_dynamoe_gating(model, train_dls[other_id],\n",
    "                              gating_criterion,\n",
    "                              expert_criterion_unreduced,\n",
    "                              clip,SIGNAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_adamoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_adamoe(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id,\n",
    "    n_task_epochs,\n",
    "    step_size_evaluation,\n",
    "    gating_criterion,\n",
    "    expert_criterion,\n",
    "    expert_criterion_unreduced,\n",
    "    clip=1,\n",
    "    repetition=None,\n",
    "    SIGNAL=False\n",
    "):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((n_tasks_total, 3, n_task_epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "    \n",
    "    for epoch in range(n_task_epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, 0, expert_criterion)\n",
    "        \n",
    "        # If training does not lead anywhere, init new expert\n",
    "        if (model.epoch > model.allowed_until_check\n",
    "            and model.n_active_experts < model.n_max_experts\n",
    "           ):\n",
    "            init_expert = False\n",
    "            # Lookahead check whether data from another task is coming in\n",
    "            loss_ahead = evaluate_extra(model, valid_dls[task_id],\n",
    "                                        allOrNoneLoss)\n",
    "            last_loss = model.loss_tracker[-1]\n",
    "            if ((loss_ahead - PERFORMANCE_DIFFERENCE_NEW_TASK > last_loss)):\n",
    "                init_expert = True\n",
    "            \n",
    "            # Check whether learning is stagnating\n",
    "            # (difference to N_EPOCHS_UNTIL_NEW_EXPERT is small enough)\n",
    "            if (((model.loss_tracker[model.epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                     last_loss) < ALLOWED_ERROR_VARIANCE)\n",
    "                and \n",
    "                (model.valid_loss > PERFORMANCE_TRESHHOLD_START)\n",
    "               ):\n",
    "                init_expert = True\n",
    "                        \n",
    "            if init_expert:\n",
    "                # Add new expert\n",
    "                model.status = \"train_gating_uninitialized_expert\"\n",
    "                model.allowed_until_check = model.epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                model.add_expert()\n",
    "\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"--------- Init new Expert ---------\")\n",
    "                print(\"-----------------------------------\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train model\n",
    "        train_loss = train_adamoe(model, train_dls[task_id],\n",
    "                                  gating_criterion, expert_criterion,\n",
    "                                  expert_criterion_unreduced,\n",
    "                                  clip,SIGNAL)\n",
    "\n",
    "        model.valid_loss = evaluate(model, valid_dls[task_id],\n",
    "                                    expert_criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Log hits\n",
    "        model.loss_tracker.append(evaluate_extra(model, valid_dls[task_id],\n",
    "                                                 allOrNoneLoss))\n",
    "            \n",
    "#         if model.valid_loss < best_valid_loss:\n",
    "#             best_valid_loss = model.valid_loss\n",
    "#             torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        # Log performance AFTER training\n",
    "        if epoch % STEP_SIZE_EVALUATION == 0 and epoch != 0:\n",
    "            idx = epoch//STEP_SIZE_EVALUATION\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, idx, expert_criterion)\n",
    "            \n",
    "        if epoch % STEP_SIZE_REPLAY == 0 and epoch != 0:\n",
    "            print(\"-Replay\")\n",
    "            replay_adamoe(model, task_id, gating_criterion, expert_criterion_unreduced, clip,SIGNAL)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {model.valid_loss:.3f} |  Val. PPL: {math.exp(model.valid_loss):7.3f}')\n",
    "        \n",
    "        model.epoch += 1\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adamoe():\n",
    "    # Initialize DynaMoE\n",
    "    model = DynaMoE(expert_decay=True)\n",
    "    print(model.apply(init_weights))\n",
    "    \n",
    "    # Need some extra variables for the model\n",
    "    model.epoch = 0\n",
    "    model.loss_tracker = []\n",
    "    model.allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    # gating_criterion = CosineLoss(N_MAX_EXPERTS, ignore_index=None)\n",
    "    # Cosine loss is inpractical for Gating because result vectors are low dimensional\n",
    "    # Cosine loss works better for small datasets, thus used for experts\n",
    "    gating_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "    expert_criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "    expert_criterion_unreduced = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN,\n",
    "                                            reduction=\"none\")\n",
    "    \n",
    "    return (model, gating_criterion, expert_criterion, expert_criterion_unreduced)\n",
    "\n",
    "def repeat_adamoe(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model, gating_criterion, expert_criterion, expert_criterion_unreduced = pass_on_variables\n",
    "    SIGNAL = False\n",
    "    hist_loss, hist_hits = fit_adamoe(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        gating_criterion,\n",
    "        expert_criterion,\n",
    "        expert_criterion_unreduced,\n",
    "        repetition=repetition,\n",
    "        SIGNAL=SIGNAL\n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment AdaMoE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 15\n",
    "N_GATING_EMBED_DIM = 9\n",
    "LEARNING_RATE_GATING = 0.008\n",
    "SCHEDULE = not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.05\n",
    "PERFORMANCE_TRESHHOLD_START = 0.4\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example\n",
    "\n",
    "STEP_SIZE_REPLAY = 5\n",
    "STEP_SIZE_DECAY = 20\n",
    "GAMMA_DECAY = 0.95\n",
    "LEARNING_RATE = 0.05\n",
    "# need seperate learning rates for gating and experts\n",
    "\n",
    "TEST_ALL_TASKS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 9)\n",
      "    (rnn): GRU(9, 15, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=30, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: AdaMoE-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.620 | Train PPL:   1.858\n",
      "\t Val. Loss: 0.465 |  Val. PPL:   1.592\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.592\n",
      "\t Val. Loss: 0.397 |  Val. PPL:   1.487\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.372 |  Val. PPL:   1.450\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.439\n",
      "\t Val. Loss: 0.407 |  Val. PPL:   1.502\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.462\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.449\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.469\n",
      "\t Val. Loss: 0.374 |  Val. PPL:   1.454\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.457\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.281 |  Val. PPL:   1.324\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.286 |  Val. PPL:   1.331\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.232\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.223\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.205 |  Val. PPL:   1.227\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.204 |  Val. PPL:   1.227\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.152 | Train PPL:   1.164\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.131 |  Val. PPL:   1.140\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.137\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.128 |  Val. PPL:   1.136\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.162 | Train PPL:   1.176\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.141 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.148 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.127\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.124 |  Val. PPL:   1.132\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.134\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.125 |  Val. PPL:   1.133\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.122 |  Val. PPL:   1.130\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.115 |  Val. PPL:   1.122\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.118\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.110 |  Val. PPL:   1.116\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.109 |  Val. PPL:   1.115\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.108 |  Val. PPL:   1.114\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.132\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.109\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.107\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.108\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.131 | Train PPL:   1.140\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.112\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.111\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.105\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.105\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.099 |  Val. PPL:   1.104\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.102\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.101\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.158\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.117\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.124 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.169\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.128\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "\n",
      "SCHEDULE: AdaMoE-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "--------- Init new Expert ---------\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.621 | Train PPL:   1.860\n",
      "\t Val. Loss: 0.627 |  Val. PPL:   1.872\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.629 | Train PPL:   1.876\n",
      "\t Val. Loss: 0.627 |  Val. PPL:   1.871\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.607 | Train PPL:   1.834\n",
      "\t Val. Loss: 0.560 |  Val. PPL:   1.750\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.562 | Train PPL:   1.755\n",
      "\t Val. Loss: 0.626 |  Val. PPL:   1.869\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.475 | Train PPL:   1.608\n",
      "\t Val. Loss: 0.459 |  Val. PPL:   1.583\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.419 | Train PPL:   1.520\n",
      "\t Val. Loss: 0.427 |  Val. PPL:   1.533\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.638 | Train PPL:   1.892\n",
      "\t Val. Loss: 0.625 |  Val. PPL:   1.868\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.456 | Train PPL:   1.578\n",
      "\t Val. Loss: 0.436 |  Val. PPL:   1.547\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.399 | Train PPL:   1.490\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.504\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.387 | Train PPL:   1.473\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.509\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.394 | Train PPL:   1.483\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.357 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.408 |  Val. PPL:   1.505\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.393 |  Val. PPL:   1.481\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.412 |  Val. PPL:   1.510\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.360\n",
      "\t Val. Loss: 0.411 |  Val. PPL:   1.509\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.535\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.392\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.441\n",
      "\t Val. Loss: 0.417 |  Val. PPL:   1.517\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.388\n",
      "\t Val. Loss: 0.402 |  Val. PPL:   1.494\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.401 |  Val. PPL:   1.494\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.474\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.331 |  Val. PPL:   1.392\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.330 |  Val. PPL:   1.391\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.379\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.396 |  Val. PPL:   1.486\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.475\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.498\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.536\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.500\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.461\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.353 |  Val. PPL:   1.424\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.362 |  Val. PPL:   1.436\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.312 |  Val. PPL:   1.366\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.375\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.384\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.282 |  Val. PPL:   1.326\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.324\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.319 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.404\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.345\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.345\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.340\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.258\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.308\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.210 |  Val. PPL:   1.233\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.156 | Train PPL:   1.168\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.295\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.221\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.218\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.159 |  Val. PPL:   1.173\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.152\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.157 | Train PPL:   1.171\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.139\n",
      "\t Val. Loss: 0.123 |  Val. PPL:   1.131\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.120 |  Val. PPL:   1.128\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.144 | Train PPL:   1.155\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.126\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.124\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.117 |  Val. PPL:   1.125\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.118 |  Val. PPL:   1.125\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.137 | Train PPL:   1.147\n",
      "\t Val. Loss: 0.119 |  Val. PPL:   1.126\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.116 |  Val. PPL:   1.123\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.151 | Train PPL:   1.163\n",
      "\t Val. Loss: 0.114 |  Val. PPL:   1.121\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.133 | Train PPL:   1.142\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.136 | Train PPL:   1.146\n",
      "\t Val. Loss: 0.112 |  Val. PPL:   1.119\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.107 |  Val. PPL:   1.113\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.106 |  Val. PPL:   1.112\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.110\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.103 |  Val. PPL:   1.109\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.139 | Train PPL:   1.149\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.120 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.105 |  Val. PPL:   1.111\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.123 | Train PPL:   1.131\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.104 |  Val. PPL:   1.110\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.112 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.114 | Train PPL:   1.121\n",
      "\t Val. Loss: 0.098 |  Val. PPL:   1.103\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.097 |  Val. PPL:   1.102\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.100\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.099\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.099\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.098\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.093 |  Val. PPL:   1.097\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.096\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.110 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.096\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.116\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.134 | Train PPL:   1.143\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.094\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.092\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.088 |  Val. PPL:   1.091\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.146 | Train PPL:   1.157\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.091\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.086 |  Val. PPL:   1.089\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.089\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.088\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.084 |  Val. PPL:   1.087\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.111\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.161\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.087\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.132 | Train PPL:   1.141\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.083 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.147 | Train PPL:   1.159\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.088\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.105 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.150 | Train PPL:   1.162\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.093\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.085 | Train PPL:   1.089\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.089 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.086 | Train PPL:   1.090\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.088 | Train PPL:   1.092\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.087 | Train PPL:   1.091\n",
      "\t Val. Loss: 0.082 |  Val. PPL:   1.085\n",
      "\n",
      "SCHEDULE: AdaMoE-1.s2.t2.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.111 | Train PPL:   1.118\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.129 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.121 | Train PPL:   1.129\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.126 | Train PPL:   1.134\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.127 | Train PPL:   1.135\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.106\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.106 | Train PPL:   1.112\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.108\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.115 | Train PPL:   1.122\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.109 | Train PPL:   1.115\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.125 | Train PPL:   1.133\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.119\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.116 | Train PPL:   1.123\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.113 | Train PPL:   1.120\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.107 | Train PPL:   1.113\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.173\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.099 | Train PPL:   1.104\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.104 | Train PPL:   1.110\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.101 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.135 | Train PPL:   1.144\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.100 | Train PPL:   1.105\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.142 | Train PPL:   1.153\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.102 | Train PPL:   1.107\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.102\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.095\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.130 | Train PPL:   1.138\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.097 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.108 | Train PPL:   1.114\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.126\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.118 | Train PPL:   1.125\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.145 | Train PPL:   1.156\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.119 | Train PPL:   1.127\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.090 | Train PPL:   1.094\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.103 | Train PPL:   1.109\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.092 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.122 | Train PPL:   1.130\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.101\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.096 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.095 | Train PPL:   1.100\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.098\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "-Replay\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.117 | Train PPL:   1.124\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.098 | Train PPL:   1.103\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.091 | Train PPL:   1.096\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.094 | Train PPL:   1.099\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.093 | Train PPL:   1.097\n",
      "\t Val. Loss: 0.087 |  Val. PPL:   1.090\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_G, hist_all_hitsss_G, models_G = experiment(\n",
    "    \"AdaMoE-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_adamoe,\n",
    "    repeat_adamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_G, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - tensor([1, 7, 6, 5, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 3, 6, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 6, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 6, 4, 2])\n",
      "0 - tensor([1, 7, 6, 5, 4, 2])\n",
      "0 - tensor([1, 7, 3, 6, 4, 2])\n",
      "0 - tensor([1, 7, 3, 6, 5, 4, 6, 5, 2])\n",
      "0 - tensor([1, 7, 3, 6, 4, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert(models_G[2], train_dls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expertiment AdaMoE interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "SCHEDULE = medium_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD_START\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.05\n",
    "PERFORMANCE_TRESHHOLD_START = 0.4\n",
    "\n",
    "# Difference between replicated sequence accuracy of training sequence\n",
    "# before and training sequence coming in to decide to consolidate for\n",
    "# a new expert.\n",
    "PERFORMANCE_DIFFERENCE_NEW_TASK = 0.3 # 0.5 for bad example\n",
    "\n",
    "STEP_SIZE_REPLAY = 5\n",
    "STEP_SIZE_DECAY = 40\n",
    "GAMMA_DECAY = 0.95\n",
    "\n",
    "TEST_ALL_TASKS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: AdaMoE-2.s0.t0.e100\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.109 | Train PPL:   3.032\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.941\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.885\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.910\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.934\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.999\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.914\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.849\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.102 | Train PPL:   3.011\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.119 | Train PPL:   3.061\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.921\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.850\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.948\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.066\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.105 | Train PPL:   3.020\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.075 | Train PPL:   2.931\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.826\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.078 | Train PPL:   2.938\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.919\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.942\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.869\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.925\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.002\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.955\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.107 | Train PPL:   3.025\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.634 | Train PPL:   1.885\n",
      "\t Val. Loss: 0.531 |  Val. PPL:   1.701\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.465 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.443 | Train PPL:   1.557\n",
      "\t Val. Loss: 0.449 |  Val. PPL:   1.567\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.509\n",
      "\t Val. Loss: 0.445 |  Val. PPL:   1.560\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.476\n",
      "\t Val. Loss: 0.433 |  Val. PPL:   1.542\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.405 |  Val. PPL:   1.499\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.398 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.515\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.499\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.347 | Train PPL:   1.415\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.464\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.380 |  Val. PPL:   1.463\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.369 |  Val. PPL:   1.447\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.454\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.396\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.341 |  Val. PPL:   1.406\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.375\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.421\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.319 |  Val. PPL:   1.376\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-256-2a30c12a8692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_repetitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m hist_all_losses_G, hist_all_hitsss_G, models_G = experiment(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m\"AdaMoE-2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mn_repetitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSCHEDULE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-4fb69d5f2fa2>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(experiment_name, n_repetitions, schedule, init_func, repeat_func, step_size_evaluation)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# Call task specific repeat function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             hist_loss, hist_hits, model = repeat_func(n_tasks_total,\n\u001b[0m\u001b[1;32m     34\u001b[0m                                                       \u001b[0mn_task_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                                       \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-162acd104393>\u001b[0m in \u001b[0;36mrepeat_dynamoe\u001b[0;34m(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrepeat_dynamoe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tasks_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_task_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepetition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpass_on_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgating_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_criterion_unreduced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpass_on_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     hist_loss, hist_hits = fit_dynamoe(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mn_tasks_total\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-154-e0d602caf7ad>\u001b[0m in \u001b[0;36mfit_dynamoe\u001b[0;34m(n_tasks_total, model, task_id, epochs, step_size_evaluation, gating_criterion, expert_criterion, expert_criterion_unreduced, clip, repetition)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;31m# First train gating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 train_loss, gatingChoices = train_dynamoe_gating(model,\n\u001b[0m\u001b[1;32m     77\u001b[0m                                                                  \u001b[0mtrain_dls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                                                  \u001b[0mgating_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-51297943eede>\u001b[0m in \u001b[0;36mtrain_dynamoe_gating\u001b[0;34m(model, iterator, gating_criterion, expert_criterion_unreduced, clip, verbose, returnGatingChoices)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;31m# Get model prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 expert_outputs = model.experts[e_id](seqs, seqs_len,\n\u001b[0m\u001b[1;32m     41\u001b[0m                                                      expert_trgs)\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-73ff751608d6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m#  and mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#receive output tensor (predictions) and new hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m#place predictions in a tensor holding predictions for each token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-739fe8ad33ea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#prediction = [batch size, output dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_G, hist_all_hitsss_G, models_G = experiment(\n",
    "    \"AdaMoE-2\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_dynamoe,\n",
    "    repeat_dynamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_G, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAASoarxMs3k"
   },
   "source": [
    "## Transfer F: Ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gating_Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gating_Ensembler(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_gating_hidden, n_experts,\n",
    "                 n_max_experts, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "\n",
    "        self.n_max_experts = n_max_experts\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embed_dim, n_gating_hidden, bidirectional=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(n_gating_hidden * 2, n_max_experts)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, seqs, seqs_len, verbose=False):\n",
    "        \n",
    "        # seqs = [seq len, batch_size]\n",
    "        # seqs_len = [batch_size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(seqs))\n",
    "        \n",
    "        # embedded = [seq len, batch_size, embed_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, seqs_len.to(\"cpu\"))\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "\n",
    "        # outputs = [seq len, batch_size, n_experts * num directions]\n",
    "        # hidden = [n layers * num directions, batch size, n_experts]\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # hidden = [batch_size, n_max_experts]\n",
    "\n",
    "        outputs = outputs[-1]\n",
    "\n",
    "        outputs = self.fc_out(outputs)\n",
    "\n",
    "        # outputs = [batch_size, n_max_experts]\n",
    "        if verbose:\n",
    "            print(\"gating_before_relu\", outputs )\n",
    "        return torch.sigmoid(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gating_ensembler():\n",
    "    gating = Gating_Ensembler(INPUT_DIM, N_GATING_EMBED_DIM,\n",
    "                              N_GATING_HIDDEN_DIM, N_EXPERTS_START,\n",
    "                              N_MAX_EXPERTS, GATE_DROPOUT)\n",
    "    gating.to(device)\n",
    "    gating_optimizer = optim.Adam(gating.parameters(), lr=LEARNING_RATE)\n",
    "    return gating, gating_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "id": "ztDsiI2WM98Y"
   },
   "outputs": [],
   "source": [
    "class Ensembler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gating=None,\n",
    "        gating_optimizer=None,\n",
    "        experts=None,\n",
    "        expert_optimizers=None,\n",
    "        status=\"train_gating_initialized_expert\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        gating: nn.Module\n",
    "            Gating module\n",
    "        gating_optimizer: optim\n",
    "            optimizer for passed Gating module\n",
    "        expert: list of nn.Module\n",
    "            list of task experts\n",
    "        expert_optimizers: list of optim\n",
    "            list of optimizer for the expert at the same index\n",
    "        \"\"\"\n",
    "        super(Ensembler, self).__init__()\n",
    "\n",
    "        assert (gating is None) == (gating_optimizer is None), \"gating needs gating_optimizer, and vice versa\"\n",
    "        \n",
    "        if gating is None:\n",
    "            gating, gating_optimizer = init_gating_ensembler()\n",
    "        self.gating = gating\n",
    "        self.gating_optimizer = gating_optimizer\n",
    "        \n",
    "        if experts is None:\n",
    "            expert, expert_optimizer = init_expert()\n",
    "            experts = [expert,]\n",
    "            expert_optimizers = [expert_optimizer,]\n",
    "        \n",
    "        assert len(experts) == len(expert_optimizers), \"unequal amount of experts and expert_optimizers\"\n",
    "        \n",
    "        self.experts = nn.ModuleList(experts)\n",
    "        self.expert_optimizers = expert_optimizers\n",
    "        \n",
    "        self.n_train_on_experts = [0 for _ in experts]   # List how often expert has been trained\n",
    "        self.n_active_experts = len(experts)\n",
    "        self.n_max_experts = gating.n_max_experts\n",
    "        \n",
    "        self.status = status\n",
    "        \n",
    "        self.epoch = 0\n",
    "        self.loss_tracker = []\n",
    "        self.allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "\n",
    "    def forward(self, seqs, seqs_len, trgs, teacher_forcing_ratio=0.5, verbose=False):\n",
    "        #seqs = [seqs len, batch size]\n",
    "        #seqs_len = [batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "\n",
    "        vocab_size = self.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "        \n",
    "        # Decide which expert to use\n",
    "        gatings = self.gating(seqs, seqs_len, verbose)\n",
    "\n",
    "        # gatings = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gatings[:,:self.n_active_experts]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"gating_masked: {gating_masked}\")\n",
    "            experts_loss = torch.empty((self.n_active_experts, batch_size))\n",
    "\n",
    "        expert_outputs = torch.empty((self.n_active_experts, seq_len, batch_size, vocab_size))\n",
    "        for e_id in range(self.n_active_experts):\n",
    "            expert_output = self.experts[e_id](seqs, seqs_len, seqs, teacher_forcing_ratio)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                if verbose:\n",
    "                    experts_loss[e_id,b] = compute_loss(expert_output, seqs, criterion, cutFirstInSequence=True)\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                # expert_outputs[e_id,:,b] = expert_output[:,b]\n",
    "            \n",
    "            # print(\"expert_out\")\n",
    "            # print(expert_output)\n",
    "            # print(\"gating_masked\")\n",
    "            # print(gating_masked)\n",
    "\n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "        if verbose:\n",
    "            print(\"expert_loss\", experts_loss)\n",
    "\n",
    "        return weighted_outputs\n",
    "\n",
    "    def add_expert(self):\n",
    "        # Get new expert\n",
    "        expert, expert_optimizer = init_expert()\n",
    "        self.experts.append(expert)\n",
    "        self.expert_optimizers.append(expert_optimizer)\n",
    "        self.n_active_experts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_ensembler_gating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler_gating(model, iterator, criterion, clip, verbose=False):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    model.gating.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        #seqs = [seq len, batch size]\n",
    "        #output = [seq len, batch size, vocab sizen]\n",
    "        vocab_size = model.gating.input_dim\n",
    "        seq_len, batch_size = seqs.shape\n",
    "\n",
    "        model.gating_optimizer.zero_grad()\n",
    "        \n",
    "        gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "        # gating_outputs = [batch_size, n_max_experts]\n",
    "        \n",
    "        gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "        ## Compute best choice for gating network\n",
    "        # Compute loss for each expert network\n",
    "        expert_outputs = torch.empty((model.n_active_experts, seq_len,\n",
    "                                      batch_size, vocab_size))\n",
    "        for e_id in range(model.n_active_experts):\n",
    "\n",
    "            model.experts[e_id].eval()\n",
    "\n",
    "            expert_output = model.experts[e_id](seqs, seqs_len, seqs)\n",
    "            # expert_output = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "\n",
    "\n",
    "            # Weigh every experts output with respective gating weight\n",
    "            for b in range(batch_size):\n",
    "                #     expert_output[:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "                expert_outputs[e_id,:,b] = expert_output[:,b] * gating_masked[b,e_id]\n",
    "            \n",
    "        weighted_outputs = expert_outputs.sum(dim=0)\n",
    "        # weighted_outputs = [seqs_len, batch_size, vocab_size]\n",
    "\n",
    "        # Gating Loss just is total loss\n",
    "        gating_loss = compute_loss(weighted_outputs, seqs, criterion,\n",
    "                            cutFirstInSequence=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\">> Gating Loss\")\n",
    "            print(gating_loss)\n",
    "\n",
    "        gating_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        model.gating_optimizer.step()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"-- Masked Gating\")\n",
    "            print(gating_masked)\n",
    "        \n",
    "        epoch_loss += gating_loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train_ensembler_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_ensembler_both(model, iterator, criterion, clip):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    \n",
    "    model.eval()\n",
    "    model.experts[model.n_active_experts - 1].train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        # model.gating_optimizer.zero_grad()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].zero_grad()\n",
    "        \n",
    "        outputs = model(seqs, seqs_len, seqs)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # model.gating_optimizer.step()\n",
    "        model.expert_optimizers[model.n_active_experts - 1].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler_expert(model, iterator, train_id, criterion, clip):\n",
    "    assert isinstance(model, Ensembler)\n",
    "    \n",
    "    model.experts[train_id].train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        # model.gating_optimizer.zero_grad()\n",
    "        model.expert_optimizers[train_id].zero_grad()\n",
    "        \n",
    "        outputs = model.experts[train_id](seqs, seqs_len, seqs)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        model.expert_optimizers[train_id].step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensembler(model, optimizer, iterator, criterion, clip, verbose=False):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for seqs, seqs_len in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(seqs, seqs_len, seqs, verbose=verbose)\n",
    "        \n",
    "        loss = compute_loss(outputs, seqs, criterion, cutFirstInSequence=True)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_ensembler(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"gating\" in name:\n",
    "            if \"weight\" in name:\n",
    "                nn.init.uniform_(param.data, a=0.0, b=1.0)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "        else:\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensembler(\n",
      "  (gating): Gating_Ensembler(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "model = Ensembler()\n",
    "m_optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model.apply(init_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_backup = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.508172423811629e-06, -0.00041902376688085496, -0.00015881884610280395, -2.4202134227380157e-05, 0.0003325603320263326, 0.000978443888016045, -0.0004651243216358125, 0.0009709399309940636], [-0.0001899793860502541, -0.000222160539124161, 0.00027672317810356617, -0.0001702979498077184, 0.00025136719341389835, 3.7281206459738314e-06, 1.7927406588569283e-05, 0.0007821590406820178], [-0.00021046228357590735, 0.00024208836839534342, 0.0005469521274790168, 0.000171741092344746, 0.0004685086023528129, 0.0005818973295390606, -0.00011901764082722366, -2.6262918254360557e-05], [-0.00018430400814395398, -0.00022479146718978882, 0.00027763901744037867, -0.00017314677825197577, 0.0002612153475638479, 4.811336111743003e-06, 2.1315645426511765e-05, 0.0007766126655042171], [-0.00020764520741067827, 0.00024078571004793048, 0.0005473484634421766, 0.0001703549933154136, 0.0004734025860670954, 0.0005824442487210035, -0.00011734259896911681, -2.9060785891488194e-05], [-0.00018290600564796478, -0.00022543626255355775, 0.0002778064226731658, -0.00017382139049004763, 0.0002636473800521344, 5.087240424472839e-06, 2.214417327195406e-05, 0.000775201537180692], [-0.0002069516049232334, 0.00024046641192398965, 0.0005474168574437499, 0.00017002702224999666, 0.00047461106441915035, 0.0005825837142765522, -0.00011693310807459056, -2.97726655844599e-05], [-0.00018256196926813573, -0.00022559429635293782, 0.00027783302357420325, -0.000173980908584781, 0.0002642478502821177, 5.157671694178134e-06, 2.2346677724272013e-05, 0.0007748423959128559]]\n",
      "tensor([[7],\n",
      "        [3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [6],\n",
      "        [5],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for s, sl in train_dls[0]:\n",
    "    print((model.experts[1](s, sl, s, 0)[1:])[:,0].tolist())\n",
    "    print(s[1:])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gating_before_relu tensor([[ 1.3537, -0.9929,  0.0215]])\n",
      "gating_masked: tensor([[0.7947, 0.2703]])\n",
      "expert_loss tensor([[0.0929],\n",
      "        [0.6914]])\n",
      "pred = [7, 3, 6, 5, 4, 6, 5, 2] \n",
      "trg  = [7, 3, 6, 5, 4, 6, 5, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.5565, -1.1279,  0.0224]])\n",
      "gating_masked: tensor([[0.8259, 0.2445]])\n",
      "expert_loss tensor([[0.1154],\n",
      "        [0.5721]])\n",
      "pred = [7, 6, 5, 4, 2] \n",
      "trg  = [7, 6, 5, 4, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.1979, -0.8526,  0.0144]])\n",
      "gating_masked: tensor([[0.7682, 0.2989]])\n",
      "expert_loss tensor([[0.1415],\n",
      "        [0.7368]])\n",
      "pred = [7, 6, 4, 2] \n",
      "trg  = [7, 6, 4, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.5534, -1.1237,  0.0221]])\n",
      "gating_masked: tensor([[0.8254, 0.2453]])\n",
      "expert_loss tensor([[0.1328],\n",
      "        [0.5809]])\n",
      "pred = [7, 3, 6, 4, 2] \n",
      "trg  = [7, 3, 6, 4, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.8999, -1.2998,  0.0283]])\n",
      "gating_masked: tensor([[0.8699, 0.2142]])\n",
      "expert_loss tensor([[0.1045],\n",
      "        [0.6851]])\n",
      "pred = [7, 6, 4, 6, 5, 2] \n",
      "trg  = [7, 6, 4, 6, 5, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.0491, -0.8141,  0.0179]])\n",
      "gating_masked: tensor([[0.7406, 0.3070]])\n",
      "expert_loss tensor([[0.1153],\n",
      "        [0.6239]])\n",
      "pred = [7, 3, 6, 4, 6, 2] \n",
      "trg  = [7, 3, 6, 4, 6, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.5244, -1.0906,  0.0235]])\n",
      "gating_masked: tensor([[0.8212, 0.2515]])\n",
      "expert_loss tensor([[0.0969],\n",
      "        [0.5304]])\n",
      "pred = [7, 3, 6, 4, 6, 5, 2] \n",
      "trg  = [7, 3, 6, 4, 6, 5, 2]\n",
      "-\n",
      "gating_before_relu tensor([[ 1.4568, -1.0521,  0.0227]])\n",
      "gating_masked: tensor([[0.8110, 0.2588]])\n",
      "expert_loss tensor([[0.0989],\n",
      "        [0.4440]])\n",
      "pred = [7, 6, 5, 4, 6, 5, 2] \n",
      "trg  = [7, 6, 5, 4, 6, 5, 2]\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "visual_eval(model, train_dls[0], cutEndToken=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "train up, test down\n",
      "Task 0: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model, train_dls)\n",
    "print(\"train up, test down\")\n",
    "accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model.experts[0], train_dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gating_before_relu tensor([[-2.8803,  2.8071, -0.0069]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0531, 0.9431]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2834],\n",
      "        [0.3674]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-3.2180,  3.2752, -0.0049]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0385, 0.9636]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2701],\n",
      "        [0.2669]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.9362,  2.9575, -0.0040]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0504, 0.9506]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.6912],\n",
      "        [0.7696]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.8273,  2.8278, -0.0048]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0559, 0.9442]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.7673],\n",
      "        [0.5674]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.8531,  3.0069, -0.0062]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0545, 0.9529]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.5668],\n",
      "        [0.6961]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.5374,  2.6025, -0.0072]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0733, 0.9310]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.5885],\n",
      "        [0.6965]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.7467,  3.0782, -0.0073]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0603, 0.9560]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3908],\n",
      "        [0.6033]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.3426,  2.6440, -0.0047]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0877, 0.9336]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.8733],\n",
      "        [0.7101]], grad_fn=<CopySlices>)\n",
      "0.5682560727000237\n",
      "gating_before_relu tensor([[-2.1851,  2.5655, -0.0063]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1011, 0.9286]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3635],\n",
      "        [0.7295]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.2343,  2.8584, -0.0033]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.0967, 0.9457]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2419],\n",
      "        [0.4849]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.0045,  2.7368, -0.0030]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1187, 0.9392]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.5218],\n",
      "        [0.9676]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.6925,  2.2450, -0.0040]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1554, 0.9042]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.6696],\n",
      "        [0.5721]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.4309,  2.3166, -0.0041]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.1930, 0.9102]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2925],\n",
      "        [0.2803]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.2046,  2.3297, -0.0037]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.2307, 0.9113]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3677],\n",
      "        [0.6012]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-1.1347e+00,  2.3919e+00, -1.4505e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.2433, 0.9162]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2100],\n",
      "        [0.4992]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-8.8122e-01,  2.3645e+00, -1.7280e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.2929, 0.9141]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1890],\n",
      "        [0.6425]], grad_fn=<CopySlices>)\n",
      "0.49113180488348007\n",
      "gating_before_relu tensor([[-7.2881e-01,  2.0784e+00, -3.2354e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.3255, 0.8888]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1772],\n",
      "        [0.3921]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-3.4440e-01,  2.0593e+00, -1.1747e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.4147, 0.8869]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3006],\n",
      "        [0.7049]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-2.5110e-01,  2.0951e+00,  1.6691e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.4376, 0.8904]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1997],\n",
      "        [0.6493]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[-6.0284e-03,  1.9817e+00,  7.2929e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.4985, 0.8789]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1957],\n",
      "        [0.4550]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 0.3568,  1.9462, -0.0021]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.5883, 0.8750]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.4015],\n",
      "        [0.9136]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[3.4678e-01, 1.8087e+00, 3.8315e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.5858, 0.8592]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.4377],\n",
      "        [0.6772]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 1.0478e+00,  2.1423e+00, -1.1698e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.7404, 0.8949]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1748],\n",
      "        [0.8947]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[9.5378e-01, 1.7798e+00, 5.7311e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.7219, 0.8557]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1809],\n",
      "        [0.3087]], grad_fn=<CopySlices>)\n",
      "0.3428385481238365\n",
      "gating_before_relu tensor([[0.8217, 1.1666, 0.0035]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.6946, 0.7625]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1888],\n",
      "        [0.7070]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 2.1702,  1.6768, -0.0039]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.8975, 0.8425]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1728],\n",
      "        [0.3039]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 2.0957e+00,  1.7332e+00, -1.1982e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.8905, 0.8498]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1313],\n",
      "        [0.6689]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 2.3055,  1.1178, -0.0044]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9093, 0.7536]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2625],\n",
      "        [0.7401]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.1345,  0.9742, -0.0079]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9583, 0.7260]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1709],\n",
      "        [0.4048]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.5878,  1.2512, -0.0051]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9731, 0.7775]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1900],\n",
      "        [0.7602]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.4201,  1.1684, -0.0055]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9683, 0.7629]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1385],\n",
      "        [0.3242]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.0775,  0.7181, -0.0045]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9560, 0.6722]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2430],\n",
      "        [0.6304]], grad_fn=<CopySlices>)\n",
      "0.20110375434160233\n",
      "gating_before_relu tensor([[ 2.9470e+00,  6.3874e-01, -9.4138e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9501, 0.6545]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1353],\n",
      "        [0.4221]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.5710,  0.3203, -0.0037]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9726, 0.5794]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1691],\n",
      "        [0.3096]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.3134,  0.1256, -0.0105]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9868, 0.5314]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1652],\n",
      "        [0.7693]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.3010e+00,  3.2702e-01, -2.3065e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9645, 0.5810]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1697],\n",
      "        [0.5888]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.3775,  0.4142, -0.0085]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9876, 0.6021]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1397],\n",
      "        [0.7431]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9318,  0.1616, -0.0052]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9808, 0.5403]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1870],\n",
      "        [0.2777]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.1598, -0.5044, -0.0099]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9846, 0.3765]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1601],\n",
      "        [0.4203]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7081,  0.1388, -0.0099]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9911, 0.5347]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1650],\n",
      "        [0.6726]], grad_fn=<CopySlices>)\n",
      "0.15688258409500122\n",
      "gating_before_relu tensor([[ 3.9808, -0.4242, -0.0054]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9817, 0.3955]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1476],\n",
      "        [0.8689]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7996, -0.3806, -0.0100]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9918, 0.4060]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.3116],\n",
      "        [0.6119]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.3954, -0.2169, -0.0049]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9878, 0.4460]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1521],\n",
      "        [0.2826]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8863, -0.6735, -0.0134]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9972, 0.3377]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1286],\n",
      "        [0.6042]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3624, -0.9349, -0.0179]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9953, 0.2819]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1181],\n",
      "        [0.6653]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.0272e+00, -4.3498e-01,  5.3743e-04]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9538, 0.3929]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1571],\n",
      "        [0.4951]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7524, -0.5961, -0.0136]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9968, 0.3552]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1181],\n",
      "        [0.3581]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8598, -0.9543, -0.0142]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9923, 0.2780]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1611],\n",
      "        [0.8116]], grad_fn=<CopySlices>)\n",
      "0.15238194167613983\n",
      "gating_before_relu tensor([[ 4.8372, -0.6463, -0.0067]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9921, 0.3438]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1139],\n",
      "        [0.3070]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8928, -1.0277, -0.0136]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9926, 0.2635]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1647],\n",
      "        [0.7107]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4027, -0.8453, -0.0114]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9955, 0.3004]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1281],\n",
      "        [0.9591]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9422, -0.6725, -0.0117]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9929, 0.3379]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1116],\n",
      "        [0.7515]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7669, -0.9021, -0.0152]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9969, 0.2886]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1185],\n",
      "        [0.4883]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.4282, -0.9574, -0.0076]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9882, 0.2774]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1017],\n",
      "        [0.6672]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.2421e+00, -5.0256e-01, -1.4962e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9858, 0.3769]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1003],\n",
      "        [0.3662]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4785, -0.2970, -0.0087]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.4263]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1804],\n",
      "        [0.5933]], grad_fn=<CopySlices>)\n",
      "0.12723933160305023\n",
      "gating_before_relu tensor([[ 5.6101, -0.8207, -0.0106]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.3056]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1303],\n",
      "        [0.3773]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7864, -0.9902, -0.0122]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9969, 0.2709]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0987],\n",
      "        [0.5954]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7247, -0.7913, -0.0048]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9912, 0.3119]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1319],\n",
      "        [0.9183]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5325, -1.1846, -0.0119]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9961, 0.2342]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1119],\n",
      "        [0.7143]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.6682e+00, -1.2028e-01,  1.9920e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9751, 0.4700]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1387],\n",
      "        [0.5548]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4066, -1.2279, -0.0120]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9955, 0.2266]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1097],\n",
      "        [0.3920]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2503, -1.3537, -0.0136]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9948, 0.2053]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1052],\n",
      "        [0.3631]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0512, -1.6280, -0.0186]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9977, 0.1641]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1008],\n",
      "        [0.6289]], grad_fn=<CopySlices>)\n",
      "0.11735443025827408\n",
      "gating_before_relu tensor([[ 5.5197, -0.3026, -0.0103]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9960, 0.4249]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0966],\n",
      "        [0.9131]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8691, -1.6335, -0.0122]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1634]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1143],\n",
      "        [0.3652]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.4047e+00, -1.2325e+00, -2.7984e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9679, 0.2257]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1979],\n",
      "        [0.5706]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.1269, -1.9127, -0.0204]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9978, 0.1287]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1021],\n",
      "        [0.3284]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7837, -1.4804, -0.0095]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9917, 0.1854]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0962],\n",
      "        [0.7189]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0208, -1.8715, -0.0168]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9976, 0.1334]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0959],\n",
      "        [0.5846]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8713, -1.6144, -0.0086]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1660]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1147],\n",
      "        [0.9038]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6091, -1.7570, -0.0149]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9963, 0.1472]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1447],\n",
      "        [0.8630]], grad_fn=<CopySlices>)\n",
      "0.11937637627124786\n",
      "gating_before_relu tensor([[ 4.0150e+00, -4.3933e-01, -2.8834e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9823, 0.3919]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0978],\n",
      "        [0.2730]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3942, -1.9291, -0.0132]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9955, 0.1269]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1020],\n",
      "        [0.3879]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8935, -1.9420, -0.0107]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9926, 0.1254]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1240],\n",
      "        [0.4038]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.4006, -1.8298, -0.0096]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9879, 0.1383]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1015],\n",
      "        [0.4522]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6472, -2.1136, -0.0205]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9965, 0.1078]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1025],\n",
      "        [0.7544]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6900, -2.1106, -0.0170]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9966, 0.1081]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0904],\n",
      "        [0.6723]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4903, -2.0535, -0.0177]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9959, 0.1137]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1273],\n",
      "        [0.4805]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6204, -1.7929, -0.0150]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.1427]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1520],\n",
      "        [0.3475]], grad_fn=<CopySlices>)\n",
      "0.10887506604194641\n",
      "gating_before_relu tensor([[ 4.8902, -1.9732, -0.0092]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9925, 0.1220]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1177],\n",
      "        [0.6760]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.6659, -1.9476, -0.0141]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9907, 0.1248]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0952],\n",
      "        [0.7221]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3869, -1.9410, -0.0168]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9983, 0.1255]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0944],\n",
      "        [0.3171]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5905, -2.0551, -0.0181]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9963, 0.1135]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2283],\n",
      "        [0.7493]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3466, -1.9887, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9953, 0.1204]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1099],\n",
      "        [0.5703]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.2610, -2.0704, -0.0171]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9981, 0.1120]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1042],\n",
      "        [0.4146]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8683, -1.8911, -0.0120]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1311]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1410],\n",
      "        [0.5482]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8821, -1.9857, -0.0132]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9925, 0.1207]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0938],\n",
      "        [0.3281]], grad_fn=<CopySlices>)\n",
      "0.12340371310710907\n",
      "gating_before_relu tensor([[ 5.7049, -2.0258, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9967, 0.1165]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1027],\n",
      "        [0.3901]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9093, -2.1354, -0.0185]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9973, 0.1057]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.2103],\n",
      "        [0.6216]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8734, -1.7630, -0.0104]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1464]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1038],\n",
      "        [0.4297]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.2601, -1.7241, -0.0071]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9861, 0.1513]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1120],\n",
      "        [0.7103]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4747, -1.6799, -0.0117]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1571]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0970],\n",
      "        [0.2786]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8363, -2.0369, -0.0151]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9971, 0.1154]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1545],\n",
      "        [0.6722]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.1577, -1.3526, -0.0069]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9846, 0.2054]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1387],\n",
      "        [0.5406]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8899, -2.1616, -0.0137]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9972, 0.1032]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1254],\n",
      "        [0.4305]], grad_fn=<CopySlices>)\n",
      "0.1300085484981537\n",
      "gating_before_relu tensor([[ 5.8122, -2.1787, -0.0175]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9970, 0.1017]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1275],\n",
      "        [0.5175]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.5104e+00, -4.8284e-01, -3.6801e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9891, 0.3816]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1475],\n",
      "        [0.2445]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9104, -2.1110, -0.0147]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9973, 0.1080]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0964],\n",
      "        [0.7215]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.4744e+00, -9.7075e-01, -4.4670e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9887, 0.2747]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1047],\n",
      "        [0.3222]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4580, -2.0903, -0.0115]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1100]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0833],\n",
      "        [0.3798]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9202e+00, -9.4392e-01, -3.2549e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9805, 0.2801]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1249],\n",
      "        [0.7018]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5679, -2.0358, -0.0158]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9962, 0.1155]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1007],\n",
      "        [0.6918]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6233, -2.0834, -0.0181]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.1107]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1096],\n",
      "        [0.3725]], grad_fn=<CopySlices>)\n",
      "0.10891195386648178\n",
      "gating_before_relu tensor([[ 6.3783, -2.1003, -0.0169]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9983, 0.1091]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1078],\n",
      "        [0.4325]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.5472, -2.2130, -0.0178]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9986, 0.0986]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1276],\n",
      "        [0.4442]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.5784, -1.7973, -0.0090]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9898, 0.1422]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1040],\n",
      "        [0.6390]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2876, -2.0597, -0.0134]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9950, 0.1131]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0948],\n",
      "        [0.5889]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.8707, -2.0082, -0.0111]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9924, 0.1183]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1200],\n",
      "        [0.7572]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5865, -2.1226, -0.0154]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9963, 0.1069]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1118],\n",
      "        [0.4081]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.8871, -1.9478, -0.0142]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9972, 0.1248]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1030],\n",
      "        [0.7047]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.1900, -1.9555, -0.0112]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9945, 0.1240]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1045],\n",
      "        [0.8730]], grad_fn=<CopySlices>)\n",
      "0.10898962616920471\n",
      "gating_before_relu tensor([[ 5.6283, -2.1225, -0.0146]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9964, 0.1069]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0902],\n",
      "        [0.4069]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9432, -1.5345, -0.0062]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9810, 0.1773]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1230],\n",
      "        [0.5389]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5266, -2.1160, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9960, 0.1076]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1065],\n",
      "        [0.4408]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3798, -2.0649, -0.0162]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9983, 0.1126]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1131],\n",
      "        [0.5456]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.4678, -1.7749, -0.0055]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9698, 0.1449]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0847],\n",
      "        [0.6666]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6865, -2.0792, -0.0135]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9966, 0.1111]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0884],\n",
      "        [0.8131]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2607, -1.2293, -0.0084]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9948, 0.2263]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1252],\n",
      "        [0.4275]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.3414, -1.9268, -0.0088]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9952, 0.1271]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1059],\n",
      "        [0.3135]], grad_fn=<CopySlices>)\n",
      "0.10481567680835724\n",
      "gating_before_relu tensor([[ 5.3203, -2.1454, -0.0128]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9951, 0.1048]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1178],\n",
      "        [0.4481]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.1691, -2.0340, -0.0140]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9943, 0.1157]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0736],\n",
      "        [0.6726]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.7277, -2.0166, -0.0057]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9912, 0.1175]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1222],\n",
      "        [0.6592]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4268, -2.0541, -0.0119]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9956, 0.1136]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0912],\n",
      "        [0.7418]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2668, -2.1001, -0.0119]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9949, 0.1091]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1270],\n",
      "        [0.6202]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.2770e+00, -1.3069e+00, -2.4708e-03]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9863, 0.2130]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1051],\n",
      "        [0.2654]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3183, -2.4359, -0.0176]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9982, 0.0805]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0928],\n",
      "        [0.3757]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.3225, -1.9076, -0.0163]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9982, 0.1293]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1054],\n",
      "        [0.4984]], grad_fn=<CopySlices>)\n",
      "0.10430794209241867\n",
      "gating_before_relu tensor([[ 5.9653, -2.3388, -0.0162]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9974, 0.0880]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1504],\n",
      "        [0.7407]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.0999, -1.9738, -0.0090]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9939, 0.1220]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1034],\n",
      "        [0.4247]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.5672, -2.4973, -0.0168]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9986, 0.0760]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1071],\n",
      "        [0.3651]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7082, -2.1570, -0.0163]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9967, 0.1037]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0896],\n",
      "        [0.7222]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.4118, -2.3532, -0.0174]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9984, 0.0868]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0988],\n",
      "        [0.6141]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.1170, -2.1155, -0.0146]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9978, 0.1076]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1202],\n",
      "        [0.5060]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9597, -1.6855, -0.0085]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9930, 0.1564]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0955],\n",
      "        [0.6996]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9963, -2.2361, -0.0116]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9933, 0.0966]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0976],\n",
      "        [0.3798]], grad_fn=<CopySlices>)\n",
      "0.10654933005571365\n",
      "gating_before_relu tensor([[ 5.4211, -2.2857, -0.0163]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9956, 0.0923]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0853],\n",
      "        [0.3621]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.5705, -2.2467, -0.0108]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9962, 0.0956]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1183],\n",
      "        [0.5434]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.6544, -2.2266, -0.0133]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9965, 0.0974]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1173],\n",
      "        [0.5849]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7937, -2.3670, -0.0160]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9970, 0.0857]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0851],\n",
      "        [0.6538]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4660, -2.1800, -0.0112]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1016]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0870],\n",
      "        [0.2786]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9553, -2.3741, -0.0130]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9974, 0.0852]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1050],\n",
      "        [0.5791]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.6036, -1.9152, -0.0056]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9735, 0.1284]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0860],\n",
      "        [0.6645]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.4579, -2.1051, -0.0134]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9958, 0.1086]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0969],\n",
      "        [0.4386]], grad_fn=<CopySlices>)\n",
      "0.09807850420475006\n",
      "gating_before_relu tensor([[ 6.0137, -2.4012, -0.0148]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9976, 0.0831]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0972],\n",
      "        [0.7445]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.4148, -2.2248, -0.0165]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9984, 0.0975]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0858],\n",
      "        [0.2812]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.9518, -2.1956, -0.0098]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9930, 0.1001]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1018],\n",
      "        [0.4027]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0322, -2.4296, -0.0160]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9976, 0.0809]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0989],\n",
      "        [0.2928]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7349, -2.3788, -0.0132]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9968, 0.0848]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1142],\n",
      "        [0.6184]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.4275, -2.4694, -0.0169]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9984, 0.0780]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0976],\n",
      "        [0.4950]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9684, -2.1594, -0.0065]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9814, 0.1035]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0764],\n",
      "        [0.5498]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9642, -2.5048, -0.0137]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9974, 0.0755]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1127],\n",
      "        [0.3727]], grad_fn=<CopySlices>)\n",
      "0.09878010302782059\n",
      "gating_before_relu tensor([[ 5.4276, -2.5226, -0.0157]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9956, 0.0743]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1060],\n",
      "        [0.7565]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 3.9657, -2.1877, -0.0081]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9814, 0.1009]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0753],\n",
      "        [0.3372]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.0549, -2.5996, -0.0157]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9977, 0.0692]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0896],\n",
      "        [0.4811]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 4.6823, -2.3437, -0.0084]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9908, 0.0876]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1269],\n",
      "        [0.5763]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.9247, -2.5884, -0.0149]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9973, 0.0699]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0896],\n",
      "        [0.7571]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.2451, -2.1245, -0.0094]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9948, 0.1067]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1059],\n",
      "        [0.5705]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 5.7418, -2.0439, -0.0118]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9968, 0.1147]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.0891],\n",
      "        [0.4872]], grad_fn=<CopySlices>)\n",
      "gating_before_relu tensor([[ 6.2929, -2.6697, -0.0149]], grad_fn=<AddmmBackward>)\n",
      "gating_masked: tensor([[0.9982, 0.0648]], grad_fn=<SliceBackward>)\n",
      "expert_loss tensor([[0.1104],\n",
      "        [0.4871]], grad_fn=<CopySlices>)\n",
      "0.09925767779350281\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(train_ensembler(model, m_optimizer, train_dls[0], criterion, clip=1, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6819222569465637\n",
      "0.5159699842333794\n",
      "0.4478273540735245\n",
      "0.44442009925842285\n",
      "0.4017461687326431\n",
      "0.4108700156211853\n",
      "0.36904872953891754\n",
      "0.3939468562602997\n",
      "0.380849227309227\n",
      "0.3801833763718605\n",
      "0.32542800158262253\n",
      "0.3442196324467659\n",
      "0.2935721278190613\n",
      "0.35802025347948074\n",
      "0.3127453699707985\n",
      "0.35975559800863266\n",
      "0.3624192997813225\n",
      "0.3340729847550392\n",
      "0.3143194168806076\n",
      "0.31856175512075424\n",
      "0.290366567671299\n",
      "0.33430249243974686\n",
      "0.3475809320807457\n",
      "0.35768380761146545\n",
      "0.32783298194408417\n",
      "0.3088560923933983\n",
      "0.27582738548517227\n",
      "0.2882274463772774\n",
      "0.3500097244977951\n",
      "0.27082621306180954\n",
      "0.2905246168375015\n",
      "0.2647992670536041\n",
      "0.25673429667949677\n",
      "0.2832217290997505\n",
      "0.25846124440431595\n",
      "0.2937106564640999\n",
      "0.24560310691595078\n",
      "0.21725226193666458\n",
      "0.2346200793981552\n",
      "0.24897794425487518\n",
      "0.22636029869318008\n",
      "0.2215658202767372\n",
      "0.21642332524061203\n",
      "0.22373760491609573\n",
      "0.20710919052362442\n",
      "0.2318643182516098\n",
      "0.29033371806144714\n",
      "0.2234899252653122\n",
      "0.21376769244670868\n",
      "0.20557651668787003\n",
      "0.2603313699364662\n",
      "0.25770843774080276\n",
      "0.19829197227954865\n",
      "0.19657956063747406\n",
      "0.23009366542100906\n",
      "0.23028745502233505\n",
      "0.19156234711408615\n",
      "0.194068543612957\n",
      "0.24114757031202316\n",
      "0.18336236476898193\n",
      "0.20552147924900055\n",
      "0.18832410871982574\n",
      "0.24301724135875702\n",
      "0.18124248832464218\n",
      "0.17453563958406448\n",
      "0.2280053123831749\n",
      "0.21842745691537857\n",
      "0.17402775585651398\n",
      "0.18187490850687027\n",
      "0.21305087953805923\n",
      "0.19077467173337936\n",
      "0.2044442892074585\n",
      "0.1522967591881752\n",
      "0.1890232190489769\n",
      "0.14997000992298126\n",
      "0.14673472940921783\n",
      "0.17841757833957672\n",
      "0.17327819764614105\n",
      "0.18703081458806992\n",
      "0.18066978454589844\n",
      "0.2540165185928345\n",
      "0.19356969743967056\n",
      "0.15471993386745453\n",
      "0.189193956553936\n",
      "0.17900065332651138\n",
      "0.15990224480628967\n",
      "0.1817990243434906\n",
      "0.1485789194703102\n",
      "0.15646616369485855\n",
      "0.15941256284713745\n",
      "0.14891498535871506\n",
      "0.13769111782312393\n",
      "0.15419796854257584\n",
      "0.19168980419635773\n",
      "0.13267996162176132\n",
      "0.15432817488908768\n",
      "0.13190093636512756\n",
      "0.17842616140842438\n",
      "0.1523570716381073\n",
      "0.22105004638433456\n",
      "0.17641431838274002\n",
      "0.1648062989115715\n",
      "0.16122521460056305\n",
      "0.1476520374417305\n",
      "0.12822704762220383\n",
      "0.12085522711277008\n",
      "0.15077093988656998\n",
      "0.12596354633569717\n",
      "0.12806012481451035\n",
      "0.12584073841571808\n",
      "0.11973564326763153\n",
      "0.13707377016544342\n",
      "0.12780658155679703\n",
      "0.12644672393798828\n",
      "0.15050291270017624\n",
      "0.13269007205963135\n",
      "0.1205056682229042\n",
      "0.1421618089079857\n",
      "0.11885038018226624\n",
      "0.13633455336093903\n",
      "0.12476804107427597\n",
      "0.17574070394039154\n",
      "0.1280074566602707\n",
      "0.1590150147676468\n",
      "0.11913776397705078\n",
      "0.11645939946174622\n",
      "0.14410822093486786\n",
      "0.12144742161035538\n",
      "0.11631747335195541\n",
      "0.11219434440135956\n"
     ]
    }
   ],
   "source": [
    "train_id = 0\n",
    "for _ in range(130):\n",
    "    print(train_ensembler_expert(model, train_dls[0], train_id, criterion, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6047074869275093\n",
      "0.4525083675980568\n",
      "0.41188517212867737\n",
      "0.3863498419523239\n",
      "0.40757525712251663\n",
      "0.37751537561416626\n",
      "0.38592784851789474\n",
      "0.35213592648506165\n",
      "0.347373366355896\n",
      "0.33041082322597504\n",
      "0.33090417832136154\n",
      "0.3478125035762787\n",
      "0.30645691603422165\n",
      "0.3227307200431824\n",
      "0.2872186452150345\n",
      "0.2924410551786423\n",
      "0.3570890426635742\n",
      "0.31722015142440796\n",
      "0.3560579642653465\n",
      "0.3351268768310547\n",
      "0.3220672607421875\n",
      "0.2739083617925644\n",
      "0.2896508276462555\n",
      "0.35112224519252777\n",
      "0.29314666241407394\n",
      "0.30075741559267044\n",
      "0.28139810264110565\n",
      "0.2736208587884903\n",
      "0.2606027275323868\n",
      "0.3058577924966812\n",
      "0.26708005368709564\n",
      "0.2969105839729309\n",
      "0.3008725270628929\n",
      "0.24462435394525528\n",
      "0.2775457799434662\n",
      "0.27789808064699173\n",
      "0.32739755511283875\n",
      "0.2805725708603859\n",
      "0.2496674880385399\n",
      "0.2798337861895561\n",
      "0.2792147099971771\n",
      "0.28302282840013504\n",
      "0.30447782576084137\n",
      "0.25157085061073303\n",
      "0.23309019953012466\n",
      "0.2473481521010399\n",
      "0.24102695286273956\n",
      "0.2451634630560875\n",
      "0.2204611748456955\n",
      "0.2948429808020592\n",
      "0.23440247029066086\n",
      "0.24506614357233047\n",
      "0.2135242521762848\n",
      "0.27413423359394073\n",
      "0.2258775606751442\n",
      "0.21091344207525253\n",
      "0.2159041240811348\n",
      "0.21692225337028503\n",
      "0.2871958017349243\n",
      "0.2256965935230255\n",
      "0.21657780557870865\n",
      "0.1938825398683548\n",
      "0.20536161214113235\n",
      "0.1733112782239914\n",
      "0.208919920027256\n",
      "0.1945476233959198\n",
      "0.21343356370925903\n",
      "0.180329829454422\n",
      "0.17605005204677582\n",
      "0.17278148978948593\n",
      "0.1825346052646637\n",
      "0.19215259701013565\n",
      "0.17223940044641495\n",
      "0.186446875333786\n",
      "0.1975068897008896\n",
      "0.167712040245533\n",
      "0.18060285598039627\n",
      "0.17173895984888077\n",
      "0.19365082681179047\n",
      "0.1457042619585991\n",
      "0.1513429880142212\n",
      "0.15268734842538834\n",
      "0.15046487003564835\n",
      "0.1243046224117279\n",
      "0.1859157532453537\n",
      "0.1279527246952057\n",
      "0.1380903273820877\n",
      "0.1375075802206993\n",
      "0.14353765547275543\n",
      "0.15004614740610123\n",
      "0.12130044400691986\n",
      "0.12802797555923462\n",
      "0.1309652030467987\n",
      "0.11807587742805481\n",
      "0.14408136159181595\n",
      "0.13316359370946884\n",
      "0.150610513985157\n",
      "0.1284170299768448\n",
      "0.15144933760166168\n",
      "0.13879839330911636\n",
      "0.11879581958055496\n",
      "0.12150575965642929\n",
      "0.11645175516605377\n",
      "0.1601027473807335\n",
      "0.12986896187067032\n",
      "0.12831342220306396\n",
      "0.13970103859901428\n",
      "0.12347128987312317\n",
      "0.14781255275011063\n",
      "0.11918802559375763\n",
      "0.12662296742200851\n",
      "0.10951507091522217\n",
      "0.10373636335134506\n",
      "0.10222604125738144\n",
      "0.10630770772695541\n",
      "0.09814052283763885\n",
      "0.10288447141647339\n",
      "0.10063834488391876\n",
      "0.10184124857187271\n",
      "0.09595887362957001\n",
      "0.09773872047662735\n",
      "0.10060756653547287\n",
      "0.10395079851150513\n",
      "0.09868147224187851\n",
      "0.10205966234207153\n",
      "0.10005279630422592\n",
      "0.09298700839281082\n",
      "0.0952887088060379\n",
      "0.0946991965174675\n",
      "0.09883996844291687\n"
     ]
    }
   ],
   "source": [
    "train_id = 1\n",
    "for _ in range(130):\n",
    "    print(train_ensembler_expert(model, train_dls[1], train_id, criterion, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model.experts[1], train_dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Gating Loss\n",
      "tensor(0.4611, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2177, 0.0018]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.5026, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2379, 0.0298]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.4410, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2804, 0.0831]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.3156, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3703, 0.2269]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.2597, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3285, 0.3424]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1952, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3177, 0.5010]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0973, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2785, 0.8670]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0825, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1595, 0.9603]], grad_fn=<SliceBackward>)\n",
      "loss: 0.29437927156686783\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.0666, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0425, 1.4590]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1020, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0928,  1.5980]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0907, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.1921,  1.6714]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0706, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3137,  2.3131]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0910, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3090,  2.0946]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1039, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3618,  2.6810]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0780, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3650,  3.1779]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1171, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.3101,  3.3351]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08999010920524597\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.1128, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.2386,  3.5002]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.2102,  3.4089]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0708, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.1384,  3.5113]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0894, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0050, 4.2805]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1021, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.1488,  2.4901]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0766, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0660,  3.0221]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0784, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2013, 4.2764]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2115, 4.2603]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08283793181180954\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.0631, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2477, 4.1452]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1100, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3315, 4.4296]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0893, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0884, 3.3466]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0764, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0052,  2.8615]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0670, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0651, 3.1657]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0719, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3593, 4.7786]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1009, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3644, 4.4734]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0808, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.4588, 5.3684]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08241797983646393\n",
      "\n",
      ">> Gating Loss\n",
      "tensor(0.0809, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3727, 4.9117]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0690, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.2877, 4.6521]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.1099, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.3911, 5.3859]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0701, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0252, 3.6559]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0765, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0759,  3.1983]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0991, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.0481, 3.9903]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0611, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[-0.0931,  3.2485]], grad_fn=<SliceBackward>)\n",
      ">> Gating Loss\n",
      "tensor(0.0895, grad_fn=<MeanBackward0>)\n",
      "-- Masked Gating\n",
      "tensor([[0.1304, 5.0337]], grad_fn=<SliceBackward>)\n",
      "loss: 0.08200984448194504\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(f\"loss: {train_ensembler_gating(model, train_dls[1], criterion, 1, verbose=True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 1.0% | Gr acc 1.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model, train_dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ensembler(\n",
    "    n_tasks_total,\n",
    "    model,\n",
    "    task_id,\n",
    "    epochs,\n",
    "    step_size_evaluation,\n",
    "    criterion,\n",
    "    clip,\n",
    "    repetition=None\n",
    "):\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    total_hits = torch.zeros((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    total_loss = torch.zeros((n_tasks_total, 3, epochs//step_size_evaluation,))\n",
    "    # [:,0,:] = train, [:,1,:] = test, [:,2,:] = test_ugr\n",
    "    # [task_id, dataset, evaluations]\n",
    "\n",
    "    model.allowed_until_check = N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "    \n",
    "    # Train model depending on its status\n",
    "    for epoch in range(epochs):\n",
    "        # First Epoch log performance BEFORE training\n",
    "        if epoch == 0:\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, epoch, criterion)\n",
    "\n",
    "        if model.status == \"train_gating_initialized_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log hits\n",
    "            loss_tracker[epoch] = evaluate_extra(model, train_dls[task_id], allOrNoneLoss)\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > model.allowed_until_check:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                ):\n",
    "                    # Case of no improvement:\n",
    "                    # Switch to train the expert and gating\n",
    "\n",
    "                    model.status = \"train_gating_train_expert\"\n",
    "                    model.allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"------Switch to training both------\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if model.status == \"train_gating_train_expert\":\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_both(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "        if model.status == \"train_gating_uninitialized_expert\":\n",
    "            assert len(model.experts) > 0, \"Need at least one expert\"\n",
    "            start_time = time.time()\n",
    "            \n",
    "            train_loss = train_ensembler_gating(model, train_dls[task_id],\n",
    "                                               criterion, clip)\n",
    "            valid_loss = evaluate(model, valid_dls[task_id], criterion)\n",
    "            \n",
    "            end_time = time.time()\n",
    "\n",
    "            # Log loss\n",
    "            loss_tracker[epoch] = valid_loss\n",
    "\n",
    "            # Check for improvement in loss\n",
    "            if epoch > N_EPOCHS_UNTIL_NEW_EXPERT:\n",
    "                if (((loss_tracker[epoch - N_EPOCHS_UNTIL_NEW_EXPERT] - \n",
    "                         loss_tracker[epoch]) < ALLOWED_ERROR_VARIANCE)\n",
    "                    and \n",
    "                    (valid_loss > PERFORMANCE_TRESHHOLD)\n",
    "                   ):\n",
    "                    # Case of no improvement:\n",
    "                    # Initiate new expert and train gating and new expert on it\n",
    "                    model.add_expert()\n",
    "\n",
    "                    model.status = \"train_gating_initialized_expert\"\n",
    "                    model.allowed_until_check = epoch + N_EPOCHS_UNTIL_NEW_EXPERT\n",
    "                    print(\"-----------------------------------\")\n",
    "                    print(\"-----Added Expert-train Gating-----\")\n",
    "                    print(\"-----------------------------------\")\n",
    "\n",
    "            \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), SAVENAME)\n",
    "\n",
    "        if epoch % step_size_evaluation == 0:\n",
    "            idx = epoch//step_size_evaluation\n",
    "            eval_all_tasks(model, total_loss, total_hits, n_tasks_total, epoch, criterion)\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if repetition is not None:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | R{repetition} T{task_id}')\n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s | T{task_id}')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "        \n",
    "        model.epoch += 1\n",
    "        \n",
    "    return total_loss, total_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show_expert2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_expert2(model, iterator):\n",
    "    model.gating.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            seqs, seqs_len = batch\n",
    "\n",
    "            batch_size = seqs.shape[1]\n",
    "\n",
    "            gating_outputs = model.gating(seqs, seqs_len)\n",
    "\n",
    "            gating_masked = gating_outputs[:,:model.n_active_experts]\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                print(f\"{gating_masked[b]} - {seqs[:,b]}\")            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_ensembler():\n",
    "    # Initialize DynaMoE\n",
    "    model = Ensembler()\n",
    "    print(model.apply(init_weights))\n",
    "    \n",
    "    # Cosine loss works better for small datasets, thus used for experts\n",
    "    criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)\n",
    "    model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    return (model, criterion, model_otpimizer)\n",
    "\n",
    "def repeat_ensembler(n_tasks_total, n_task_epochs, task_id, step_size_evaluation, repetition, pass_on_variables):\n",
    "    model, criterion, model_optimizer = pass_on_variables\n",
    "    hist_loss, hist_hits = fit_ensembler(\n",
    "        n_tasks_total,\n",
    "        model,\n",
    "        task_id,\n",
    "        n_task_epochs,\n",
    "        step_size_evaluation,\n",
    "        criterion,\n",
    "        repetition=repetition\n",
    "    )\n",
    "    return hist_loss, hist_hits, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Ensembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "CvFmPpKQozmz"
   },
   "outputs": [],
   "source": [
    "N_EXPERTS_START = 1\n",
    "N_MAX_EXPERTS = 3\n",
    "GATE_DROPOUT = 0.5\n",
    "N_GATING_HIDDEN_DIM = 10\n",
    "N_GATING_EMBED_DIM = 10\n",
    "SCHEDULE = not_interleaved\n",
    "\n",
    "# If the amount of missed hits is bigger then PERFORMANCE_TRESHHOLD\n",
    "# and it stays within ALLOWED_ERROR_VARIANCE for\n",
    "# N_EPOCHS_UNTIL_NEW_EXPERT epochs, then a new\n",
    "# expert is initialized\n",
    "N_EPOCHS_UNTIL_NEW_EXPERT = 30\n",
    "ALLOWED_ERROR_VARIANCE = 0.1\n",
    "PERFORMANCE_TRESHHOLD = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "id": "dUUt4knHYfDj"
   },
   "outputs": [],
   "source": [
    "SEED = 54321\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRoGUVZcfgMg",
    "outputId": "2c57ceea-2add-4c03-f0b3-5a9252f522d0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@@@@@@@@ Repetition   0 @@@@@@@@@\n",
      "DynaMoE(\n",
      "  (gating): Gating(\n",
      "    (embedding): Embedding(8, 10)\n",
      "    (rnn): GRU(10, 10, bidirectional=True)\n",
      "    (fc_out): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (experts): ModuleList(\n",
      "    (0): Seq2Seq(\n",
      "      (encoder): Encoder(\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(19, 9, bidirectional=True)\n",
      "        (fc): Linear(in_features=18, out_features=9, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "      (decoder): Decoder(\n",
      "        (attention): Attention(\n",
      "          (attn): Linear(in_features=27, out_features=9, bias=True)\n",
      "          (v): Linear(in_features=9, out_features=1, bias=False)\n",
      "        )\n",
      "        (embedding): Embedding(8, 19)\n",
      "        (rnn): GRU(37, 9)\n",
      "        (fc_out): Linear(in_features=46, out_features=8, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "SCHEDULE: Ensembler-1.s0.t0.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.109 | Train PPL:   3.032\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 02 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.941\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 03 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.060 | Train PPL:   2.885\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 04 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 05 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.068 | Train PPL:   2.910\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 06 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.934\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 07 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.098 | Train PPL:   2.999\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 08 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.877\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 09 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.070 | Train PPL:   2.914\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 10 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.849\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 11 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.102 | Train PPL:   3.011\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 12 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.119 | Train PPL:   3.061\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 13 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.072 | Train PPL:   2.921\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 14 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.047 | Train PPL:   2.850\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 15 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.081 | Train PPL:   2.948\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 16 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.120 | Train PPL:   3.066\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 17 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.105 | Train PPL:   3.020\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 18 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.019 | Train PPL:   2.770\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 19 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.075 | Train PPL:   2.931\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 20 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.039 | Train PPL:   2.826\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 21 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.059 | Train PPL:   2.883\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 22 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.078 | Train PPL:   2.938\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 23 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.071 | Train PPL:   2.919\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 24 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.957\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 25 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.079 | Train PPL:   2.942\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 26 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.054 | Train PPL:   2.869\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 27 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.073 | Train PPL:   2.925\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 28 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.936\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 29 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.099 | Train PPL:   3.002\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 30 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.084 | Train PPL:   2.955\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "Epoch: 31 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 1.107 | Train PPL:   3.025\n",
      "\t Val. Loss: 1.005 |  Val. PPL:   2.731\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.615 | Train PPL:   1.849\n",
      "\t Val. Loss: 0.477 |  Val. PPL:   1.611\n",
      "Epoch: 33 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.445 | Train PPL:   1.560\n",
      "\t Val. Loss: 0.446 |  Val. PPL:   1.563\n",
      "Epoch: 34 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.439 | Train PPL:   1.551\n",
      "\t Val. Loss: 0.404 |  Val. PPL:   1.497\n",
      "Epoch: 35 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.428 | Train PPL:   1.534\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.501\n",
      "Epoch: 36 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.390 | Train PPL:   1.477\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 37 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.409 |  Val. PPL:   1.505\n",
      "Epoch: 38 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.451\n",
      "\t Val. Loss: 0.406 |  Val. PPL:   1.500\n",
      "Epoch: 39 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 40 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.387 |  Val. PPL:   1.473\n",
      "Epoch: 41 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470\n",
      "Epoch: 42 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.466\n",
      "Epoch: 43 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.321 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.332 |  Val. PPL:   1.394\n",
      "Epoch: 44 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.363 | Train PPL:   1.438\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 45 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452\n",
      "\t Val. Loss: 0.376 |  Val. PPL:   1.456\n",
      "Epoch: 46 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 47 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.298 |  Val. PPL:   1.347\n",
      "Epoch: 48 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 49 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.322 | Train PPL:   1.380\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 50 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
      "Epoch: 51 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 52 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 53 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 54 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 55 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 56 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.271 |  Val. PPL:   1.311\n",
      "Epoch: 57 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 58 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 59 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.356 |  Val. PPL:   1.428\n",
      "Epoch: 60 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.351 |  Val. PPL:   1.421\n",
      "Epoch: 61 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.357 |  Val. PPL:   1.428\n",
      "Epoch: 62 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 63 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.309\n",
      "Epoch: 64 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 65 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.275 | Train PPL:   1.316\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.404\n",
      "Epoch: 66 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.285\n",
      "Epoch: 67 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 68 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.337\n",
      "Epoch: 69 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.321\n",
      "Epoch: 70 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.315 | Train PPL:   1.370\n",
      "\t Val. Loss: 0.291 |  Val. PPL:   1.338\n",
      "Epoch: 71 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 72 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 73 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.238\n",
      "Epoch: 74 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 75 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.229\n",
      "Epoch: 76 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "Epoch: 77 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 78 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 79 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 80 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.235\n",
      "Epoch: 81 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.211\n",
      "Epoch: 82 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 83 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 84 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 85 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 86 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.201\n",
      "Epoch: 87 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 88 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 89 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 90 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.173\n",
      "Epoch: 91 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 92 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 93 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 94 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.165 | Train PPL:   1.179\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.158\n",
      "Epoch: 95 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 96 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 97 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "Epoch: 98 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 99 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 100 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 101 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.166\n",
      "Epoch: 102 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 103 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.192 |  Val. PPL:   1.212\n",
      "Epoch: 104 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 105 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 106 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.256\n",
      "Epoch: 107 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 108 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 109 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.243\n",
      "Epoch: 110 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 111 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.160\n",
      "Epoch: 112 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.202\n",
      "Epoch: 113 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 114 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.194\n",
      "Epoch: 115 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 116 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.357\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 117 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 118 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.354 |  Val. PPL:   1.424\n",
      "Epoch: 119 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 120 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 121 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.347 |  Val. PPL:   1.414\n",
      "Epoch: 122 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.392 |  Val. PPL:   1.480\n",
      "Epoch: 123 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 124 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.320 |  Val. PPL:   1.377\n",
      "Epoch: 125 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.309 | Train PPL:   1.362\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.349\n",
      "Epoch: 126 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.345\n",
      "Epoch: 127 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 128 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.372\n",
      "Epoch: 129 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 130 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 131 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 132 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.207 |  Val. PPL:   1.230\n",
      "Epoch: 133 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 134 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 135 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.178\n",
      "Epoch: 136 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.312\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.222\n",
      "Epoch: 137 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 138 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 139 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 140 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 141 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.266 |  Val. PPL:   1.304\n",
      "Epoch: 142 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 143 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 144 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.162 |  Val. PPL:   1.176\n",
      "Epoch: 145 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 146 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.201 |  Val. PPL:   1.222\n",
      "Epoch: 147 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 148 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.199 |  Val. PPL:   1.220\n",
      "Epoch: 149 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 150 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.276 |  Val. PPL:   1.317\n",
      "Epoch: 151 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 152 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 153 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 154 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.255 | Train PPL:   1.290\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 155 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 156 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 157 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 158 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 159 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 160 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.228 |  Val. PPL:   1.257\n",
      "Epoch: 161 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 162 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 163 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 164 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.229 |  Val. PPL:   1.257\n",
      "Epoch: 165 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.221 |  Val. PPL:   1.247\n",
      "Epoch: 166 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.167\n",
      "Epoch: 167 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.154 |  Val. PPL:   1.166\n",
      "Epoch: 168 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.192\n",
      "Epoch: 169 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.156 |  Val. PPL:   1.169\n",
      "Epoch: 170 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.152 |  Val. PPL:   1.164\n",
      "Epoch: 171 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 172 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.151 |  Val. PPL:   1.163\n",
      "Epoch: 173 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 174 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.263\n",
      "Epoch: 175 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 176 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 177 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.220 |  Val. PPL:   1.246\n",
      "Epoch: 178 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 179 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.191 |  Val. PPL:   1.210\n",
      "Epoch: 180 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 181 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "Epoch: 182 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.224 |  Val. PPL:   1.251\n",
      "Epoch: 183 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.219 |  Val. PPL:   1.245\n",
      "Epoch: 184 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.212 |  Val. PPL:   1.237\n",
      "Epoch: 185 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "Epoch: 186 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.205\n",
      "Epoch: 187 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.167\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 188 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.199\n",
      "Epoch: 189 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 190 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 191 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 192 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.187 |  Val. PPL:   1.206\n",
      "Epoch: 193 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.206\n",
      "Epoch: 194 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.188 |  Val. PPL:   1.207\n",
      "Epoch: 195 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.153\n",
      "Epoch: 196 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.154 | Train PPL:   1.166\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 197 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.186 |  Val. PPL:   1.205\n",
      "Epoch: 198 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.167 | Train PPL:   1.182\n",
      "\t Val. Loss: 0.173 |  Val. PPL:   1.189\n",
      "Epoch: 199 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.208 |  Val. PPL:   1.231\n",
      "Epoch: 200 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.197\n",
      "Epoch: 201 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.232\n",
      "Epoch: 202 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 203 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.249\n",
      "Epoch: 204 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 205 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 206 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 207 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 208 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 209 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.141 |  Val. PPL:   1.152\n",
      "Epoch: 210 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 211 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.186\n",
      "Epoch: 212 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.143 | Train PPL:   1.154\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 213 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 214 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.231 |  Val. PPL:   1.260\n",
      "Epoch: 215 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.254\n",
      "Epoch: 216 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.243\n",
      "Epoch: 217 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 218 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.186\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 219 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.167\n",
      "Epoch: 220 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.268\n",
      "Epoch: 221 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 222 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 223 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.161 | Train PPL:   1.175\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.150\n",
      "Epoch: 224 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 225 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 226 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.164 | Train PPL:   1.178\n",
      "\t Val. Loss: 0.138 |  Val. PPL:   1.148\n",
      "Epoch: 227 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.136 |  Val. PPL:   1.146\n",
      "Epoch: 228 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.143 |  Val. PPL:   1.154\n",
      "Epoch: 229 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.152\n",
      "Epoch: 230 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.177 |  Val. PPL:   1.194\n",
      "Epoch: 231 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 232 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 233 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.198\n",
      "Epoch: 234 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 235 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.321\n",
      "Epoch: 236 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 237 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 238 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 239 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.202 |  Val. PPL:   1.224\n",
      "Epoch: 240 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 241 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.196 |  Val. PPL:   1.217\n",
      "Epoch: 242 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 243 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 244 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.303\n",
      "Epoch: 245 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.267 |  Val. PPL:   1.305\n",
      "Epoch: 246 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 247 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.292\n",
      "Epoch: 248 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 249 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 250 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 251 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.268 |  Val. PPL:   1.307\n",
      "Epoch: 252 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.287\n",
      "\t Val. Loss: 0.275 |  Val. PPL:   1.316\n",
      "Epoch: 253 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.273 |  Val. PPL:   1.314\n",
      "Epoch: 254 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.270 |  Val. PPL:   1.310\n",
      "Epoch: 255 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 256 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.309\n",
      "Epoch: 257 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.265 |  Val. PPL:   1.304\n",
      "Epoch: 258 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 259 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 260 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 261 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 262 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 263 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 264 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 265 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.300\n",
      "Epoch: 266 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 267 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 268 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 269 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.290\n",
      "Epoch: 270 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.291\n",
      "Epoch: 271 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 272 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 273 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 274 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 275 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 276 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 277 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 278 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.287\n",
      "Epoch: 279 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 280 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 281 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 282 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 283 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.296\n",
      "Epoch: 284 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 285 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.282\n",
      "Epoch: 286 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 287 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 288 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 289 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 290 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.263 |  Val. PPL:   1.301\n",
      "Epoch: 291 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.302\n",
      "Epoch: 292 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 293 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 294 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 295 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 296 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 297 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 298 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 299 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.281\n",
      "Epoch: 300 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 301 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.288\n",
      "Epoch: 302 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 303 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.277 |  Val. PPL:   1.319\n",
      "Epoch: 304 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 305 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.296\n",
      "Epoch: 306 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 307 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 308 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 309 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 310 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 311 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 312 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 313 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 314 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 315 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 316 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.214 |  Val. PPL:   1.239\n",
      "Epoch: 317 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.258 |  Val. PPL:   1.294\n",
      "Epoch: 318 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.330\n",
      "Epoch: 319 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.274\n",
      "Epoch: 320 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.272\n",
      "Epoch: 321 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 322 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 323 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.282\n",
      "Epoch: 324 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 325 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.265\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 326 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 327 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 328 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.249 |  Val. PPL:   1.283\n",
      "Epoch: 329 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 330 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 331 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 332 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 333 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.278\n",
      "Epoch: 334 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 335 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 336 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 337 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 338 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 339 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 340 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 341 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.181 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 342 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 343 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.270\n",
      "Epoch: 344 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 345 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 346 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "Epoch: 347 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 348 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.316 |  Val. PPL:   1.371\n",
      "Epoch: 349 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 350 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 351 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 352 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 353 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 354 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.267\n",
      "Epoch: 355 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.213 |  Val. PPL:   1.237\n",
      "Epoch: 356 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.211 |  Val. PPL:   1.234\n",
      "Epoch: 357 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 358 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 359 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 360 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.272\n",
      "Epoch: 361 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.238 |  Val. PPL:   1.269\n",
      "Epoch: 362 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 363 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 364 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 365 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 366 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 367 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.233 |  Val. PPL:   1.262\n",
      "Epoch: 368 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 369 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 370 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.234 |  Val. PPL:   1.263\n",
      "Epoch: 371 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 372 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 373 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.239 |  Val. PPL:   1.269\n",
      "Epoch: 374 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 375 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.264\n",
      "Epoch: 376 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 377 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.298\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 378 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 379 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 380 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.267\n",
      "Epoch: 381 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 382 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 383 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.285\n",
      "Epoch: 384 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 385 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.253 |  Val. PPL:   1.287\n",
      "Epoch: 386 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 387 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.251 |  Val. PPL:   1.286\n",
      "Epoch: 388 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 389 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.283\n",
      "Epoch: 390 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 391 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.235 |  Val. PPL:   1.265\n",
      "Epoch: 392 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.241 |  Val. PPL:   1.273\n",
      "Epoch: 393 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 394 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.276\n",
      "Epoch: 395 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.252 | Train PPL:   1.286\n",
      "\t Val. Loss: 0.248 |  Val. PPL:   1.281\n",
      "Epoch: 396 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 397 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 398 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "Epoch: 399 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 400 | Time: 0m 0s | R0 T0\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.247 |  Val. PPL:   1.280\n",
      "\n",
      "SCHEDULE: Ensembler-1.s1.t1.e400\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 01 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.523 | Train PPL:   1.687\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 02 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.651\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 03 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.528 | Train PPL:   1.695\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 04 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.483 | Train PPL:   1.621\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 05 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.493 | Train PPL:   1.637\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 06 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.731\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 07 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.531 | Train PPL:   1.701\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 08 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.459 | Train PPL:   1.583\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 09 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.529 | Train PPL:   1.697\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 10 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.466 | Train PPL:   1.593\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 11 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.615\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 12 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 13 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.517 | Train PPL:   1.677\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 14 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 15 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.510 | Train PPL:   1.666\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 16 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.526 | Train PPL:   1.692\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 17 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.536 | Train PPL:   1.709\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 18 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.617\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 19 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 20 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 21 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.512 | Train PPL:   1.669\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 22 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.519 | Train PPL:   1.680\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 23 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.502 | Train PPL:   1.651\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 24 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.509 | Train PPL:   1.664\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 25 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 26 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.530 | Train PPL:   1.699\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 27 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 28 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.520 | Train PPL:   1.681\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 29 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.496 | Train PPL:   1.642\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 30 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.548 | Train PPL:   1.729\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 31 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.452 | Train PPL:   1.572\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 32 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.547 | Train PPL:   1.728\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 33 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.538 | Train PPL:   1.713\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 34 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.552 | Train PPL:   1.737\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 35 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 36 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.494 | Train PPL:   1.638\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 37 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.488 | Train PPL:   1.629\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 38 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.522 | Train PPL:   1.686\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 39 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.462 | Train PPL:   1.588\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 40 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 41 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.521 | Train PPL:   1.684\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 42 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.491 | Train PPL:   1.634\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 43 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.489 | Train PPL:   1.630\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 44 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.498 | Train PPL:   1.646\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 45 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.534 | Train PPL:   1.706\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 46 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.504 | Train PPL:   1.655\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 47 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.602\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 48 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.518 | Train PPL:   1.679\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 49 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.624\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 50 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.705\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 51 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 52 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.539 | Train PPL:   1.714\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 53 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.550 | Train PPL:   1.733\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 54 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.471 | Train PPL:   1.601\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 55 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.464 | Train PPL:   1.591\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 56 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.485 | Train PPL:   1.623\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 57 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.561 | Train PPL:   1.752\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 58 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.541 | Train PPL:   1.718\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 59 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.507 | Train PPL:   1.661\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 60 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.503 | Train PPL:   1.654\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 61 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.453 | Train PPL:   1.573\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 62 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.533 | Train PPL:   1.704\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 63 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.678 | Train PPL:   1.969\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 64 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.479 | Train PPL:   1.615\n",
      "\t Val. Loss: 0.475 |  Val. PPL:   1.608\n",
      "Epoch: 65 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.454 | Train PPL:   1.574\n",
      "\t Val. Loss: 0.468 |  Val. PPL:   1.596\n",
      "Epoch: 66 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.423 | Train PPL:   1.526\n",
      "\t Val. Loss: 0.473 |  Val. PPL:   1.604\n",
      "Epoch: 67 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.415 | Train PPL:   1.514\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 68 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.372 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 69 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.422 |  Val. PPL:   1.525\n",
      "Epoch: 70 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.536\n",
      "Epoch: 71 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.429 |  Val. PPL:   1.536\n",
      "Epoch: 72 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.376 | Train PPL:   1.457\n",
      "\t Val. Loss: 0.416 |  Val. PPL:   1.516\n",
      "Epoch: 73 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.381 |  Val. PPL:   1.463\n",
      "Epoch: 74 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.475\n",
      "Epoch: 75 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.379 |  Val. PPL:   1.460\n",
      "Epoch: 76 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "Epoch: 77 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.343 | Train PPL:   1.409\n",
      "\t Val. Loss: 0.318 |  Val. PPL:   1.374\n",
      "Epoch: 78 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.311 |  Val. PPL:   1.364\n",
      "Epoch: 79 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.278 |  Val. PPL:   1.320\n",
      "Epoch: 80 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.333\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 81 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 82 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.368\n",
      "Epoch: 83 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.403\n",
      "\t Val. Loss: 0.333 |  Val. PPL:   1.395\n",
      "Epoch: 84 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.292 | Train PPL:   1.339\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.459\n",
      "Epoch: 85 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 86 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.375\n",
      "\t Val. Loss: 0.289 |  Val. PPL:   1.335\n",
      "Epoch: 87 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "Epoch: 88 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.312\n",
      "Epoch: 89 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.276\n",
      "Epoch: 90 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.253\n",
      "Epoch: 91 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 92 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.300\n",
      "Epoch: 93 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.328\n",
      "\t Val. Loss: 0.218 |  Val. PPL:   1.244\n",
      "Epoch: 94 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.246 |  Val. PPL:   1.279\n",
      "Epoch: 95 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.298\n",
      "Epoch: 96 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.293\n",
      "\t Val. Loss: 0.259 |  Val. PPL:   1.295\n",
      "Epoch: 97 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "Epoch: 98 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.200 |  Val. PPL:   1.221\n",
      "Epoch: 99 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.240\n",
      "Epoch: 100 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.243 |  Val. PPL:   1.275\n",
      "Epoch: 101 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 102 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.253 | Train PPL:   1.288\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 103 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 104 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n",
      "Epoch: 105 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 106 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 107 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 108 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 109 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 110 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.147 |  Val. PPL:   1.158\n",
      "Epoch: 111 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.158 |  Val. PPL:   1.171\n",
      "Epoch: 112 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 113 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 114 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.184 |  Val. PPL:   1.203\n",
      "Epoch: 115 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 116 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 117 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.145 |  Val. PPL:   1.156\n",
      "Epoch: 118 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.140 |  Val. PPL:   1.150\n",
      "Epoch: 119 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.186\n",
      "Epoch: 120 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "Epoch: 121 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.206 |  Val. PPL:   1.228\n",
      "Epoch: 122 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.209 |  Val. PPL:   1.233\n",
      "Epoch: 123 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.189 |  Val. PPL:   1.208\n",
      "Epoch: 124 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.178\n",
      "Epoch: 125 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.199\n",
      "Epoch: 126 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.182 |  Val. PPL:   1.200\n",
      "Epoch: 127 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 128 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 129 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.142 |  Val. PPL:   1.153\n",
      "Epoch: 130 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.166 | Train PPL:   1.181\n",
      "\t Val. Loss: 0.134 |  Val. PPL:   1.144\n",
      "Epoch: 131 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.155 |  Val. PPL:   1.168\n",
      "Epoch: 132 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.195 |  Val. PPL:   1.216\n",
      "Epoch: 133 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.194 |  Val. PPL:   1.214\n",
      "Epoch: 134 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 135 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.175 | Train PPL:   1.191\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 136 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.180\n",
      "Epoch: 137 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.163 |  Val. PPL:   1.177\n",
      "Epoch: 138 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.146 |  Val. PPL:   1.157\n",
      "Epoch: 139 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.180 |  Val. PPL:   1.197\n",
      "Epoch: 140 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.159 | Train PPL:   1.172\n",
      "\t Val. Loss: 0.165 |  Val. PPL:   1.180\n",
      "Epoch: 141 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.169 |  Val. PPL:   1.184\n",
      "Epoch: 142 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 143 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.203 |  Val. PPL:   1.225\n",
      "Epoch: 144 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.175 |  Val. PPL:   1.191\n",
      "Epoch: 145 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.171 |  Val. PPL:   1.187\n",
      "Epoch: 146 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.171 | Train PPL:   1.187\n",
      "\t Val. Loss: 0.179 |  Val. PPL:   1.196\n",
      "Epoch: 147 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.163 | Train PPL:   1.177\n",
      "\t Val. Loss: 0.183 |  Val. PPL:   1.200\n",
      "Epoch: 148 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.204\n",
      "Epoch: 149 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.178 |  Val. PPL:   1.195\n",
      "Epoch: 150 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.181 |  Val. PPL:   1.198\n",
      "Epoch: 151 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.174 | Train PPL:   1.190\n",
      "\t Val. Loss: 0.185 |  Val. PPL:   1.203\n",
      "Epoch: 152 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.193\n",
      "Epoch: 153 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.149 | Train PPL:   1.160\n",
      "\t Val. Loss: 0.170 |  Val. PPL:   1.185\n",
      "Epoch: 154 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.160 | Train PPL:   1.174\n",
      "\t Val. Loss: 0.172 |  Val. PPL:   1.187\n",
      "Epoch: 155 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.166 |  Val. PPL:   1.181\n",
      "Epoch: 156 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.153 | Train PPL:   1.165\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 157 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.140 | Train PPL:   1.150\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "Epoch: 158 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.167 |  Val. PPL:   1.182\n",
      "Epoch: 159 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "Epoch: 160 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.130 |  Val. PPL:   1.139\n",
      "Epoch: 161 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.174 |  Val. PPL:   1.190\n",
      "Epoch: 162 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.227 |  Val. PPL:   1.255\n",
      "Epoch: 163 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 164 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.280 |  Val. PPL:   1.323\n",
      "Epoch: 165 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.324 |  Val. PPL:   1.382\n",
      "Epoch: 166 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471\n",
      "Epoch: 167 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.325 | Train PPL:   1.384\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.357\n",
      "Epoch: 168 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.419\n",
      "\t Val. Loss: 0.279 |  Val. PPL:   1.322\n",
      "Epoch: 169 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.288 | Train PPL:   1.334\n",
      "\t Val. Loss: 0.269 |  Val. PPL:   1.308\n",
      "Epoch: 170 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 171 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 172 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.278\n",
      "Epoch: 173 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 174 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 175 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.232 |  Val. PPL:   1.261\n",
      "Epoch: 176 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.197 |  Val. PPL:   1.217\n",
      "Epoch: 177 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.193 |  Val. PPL:   1.213\n",
      "Epoch: 178 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.198 |  Val. PPL:   1.219\n",
      "Epoch: 179 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 180 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.432\n",
      "Epoch: 181 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.300 |  Val. PPL:   1.351\n",
      "Epoch: 182 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469\n",
      "Epoch: 183 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.400 |  Val. PPL:   1.492\n",
      "Epoch: 184 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 185 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.492\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 186 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.412 | Train PPL:   1.510\n",
      "\t Val. Loss: 0.480 |  Val. PPL:   1.617\n",
      "Epoch: 187 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.282 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 188 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 189 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 190 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.494\n",
      "\t Val. Loss: 0.526 |  Val. PPL:   1.692\n",
      "Epoch: 191 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.420 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 192 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.418\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 193 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.352 | Train PPL:   1.422\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 194 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 195 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 196 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 197 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.421 | Train PPL:   1.523\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 198 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.405 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 199 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.503\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 200 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.350 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 201 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 202 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 203 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.291 | Train PPL:   1.338\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 204 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 205 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.401 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 206 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 207 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.374 | Train PPL:   1.454\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 208 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 209 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.453\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 210 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.332 | Train PPL:   1.394\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 211 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.369 | Train PPL:   1.446\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 212 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 213 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 214 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.370 | Train PPL:   1.448\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 215 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.345 | Train PPL:   1.412\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 216 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "-----------------------------------\n",
      "-----Added Expert-train Gating-----\n",
      "-----------------------------------\n",
      "Epoch: 217 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.377\n",
      "\t Val. Loss: 0.430 |  Val. PPL:   1.538\n",
      "Epoch: 218 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 219 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.348 | Train PPL:   1.416\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 220 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.437\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 221 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.441 | Train PPL:   1.555\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 222 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 223 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.308 | Train PPL:   1.361\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 224 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 225 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.396 | Train PPL:   1.486\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 226 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.310 | Train PPL:   1.363\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 227 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 228 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.329 | Train PPL:   1.390\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 229 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 230 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 231 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.368\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 232 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 233 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.377 | Train PPL:   1.458\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 234 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 235 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 236 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.312 | Train PPL:   1.366\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 237 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.318 | Train PPL:   1.374\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 238 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.313 | Train PPL:   1.367\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 239 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.406 | Train PPL:   1.500\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 240 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 241 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 242 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 243 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.387\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 244 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 245 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.328 | Train PPL:   1.389\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 246 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 247 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.435\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 248 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.403 | Train PPL:   1.497\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "-----------------------------------\n",
      "------Switch to training both------\n",
      "-----------------------------------\n",
      "Epoch: 249 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.647 | Train PPL:   1.909\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 250 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.468 | Train PPL:   1.597\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 251 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 252 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.385 | Train PPL:   1.470\n",
      "\t Val. Loss: 0.335 |  Val. PPL:   1.398\n",
      "Epoch: 253 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.383 | Train PPL:   1.467\n",
      "\t Val. Loss: 0.340 |  Val. PPL:   1.405\n",
      "Epoch: 254 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.431 | Train PPL:   1.539\n",
      "\t Val. Loss: 0.323 |  Val. PPL:   1.381\n",
      "Epoch: 255 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.366 | Train PPL:   1.442\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.386\n",
      "Epoch: 256 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.445\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.355\n",
      "Epoch: 257 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 258 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.371 | Train PPL:   1.450\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.352\n",
      "Epoch: 259 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.331 | Train PPL:   1.393\n",
      "\t Val. Loss: 0.303 |  Val. PPL:   1.355\n",
      "Epoch: 260 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.371\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "Epoch: 261 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.305 |  Val. PPL:   1.356\n",
      "Epoch: 262 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 263 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 264 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.280 | Train PPL:   1.323\n",
      "\t Val. Loss: 0.250 |  Val. PPL:   1.284\n",
      "Epoch: 265 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.294 |  Val. PPL:   1.341\n",
      "Epoch: 266 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 267 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.290\n",
      "Epoch: 268 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.252 |  Val. PPL:   1.286\n",
      "Epoch: 269 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.294\n",
      "Epoch: 270 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.245 |  Val. PPL:   1.277\n",
      "Epoch: 271 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.240 |  Val. PPL:   1.271\n",
      "Epoch: 272 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.294\n",
      "Epoch: 273 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 274 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.257 |  Val. PPL:   1.293\n",
      "Epoch: 275 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 276 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.292\n",
      "\t Val. Loss: 0.262 |  Val. PPL:   1.299\n",
      "Epoch: 277 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.244 |  Val. PPL:   1.277\n",
      "Epoch: 278 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 279 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.230 |  Val. PPL:   1.259\n",
      "Epoch: 280 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "-----------------------------------\n",
      "-- Fix Expert, Train Gating only --\n",
      "-----------------------------------\n",
      "Epoch: 281 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 282 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.273\n",
      "Epoch: 283 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 284 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 285 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 286 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 287 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 288 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 289 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 290 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 291 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 292 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 293 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 294 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 295 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 296 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 297 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 298 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 299 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 300 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 301 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 302 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 303 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 304 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 305 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 306 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.225 |  Val. PPL:   1.252\n",
      "Epoch: 307 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 308 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 309 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 310 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 311 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 312 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 313 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 314 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 315 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 316 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 317 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 318 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 319 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 320 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 321 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 322 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 323 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 324 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 325 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 326 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 327 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 328 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 329 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 330 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 331 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.185 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 332 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 333 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 334 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 335 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 336 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 337 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 338 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 339 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 340 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.203\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 341 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 342 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 343 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 344 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 345 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 346 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 347 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 348 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 349 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 350 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 351 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 352 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 353 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 354 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 355 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 356 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.196\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 357 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 358 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 359 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 360 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 361 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 362 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 363 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 364 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 365 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.199\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 366 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 367 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 368 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 369 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 370 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 371 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 372 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.322 |  Val. PPL:   1.380\n",
      "Epoch: 373 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 374 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 375 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.179 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 376 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 377 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 378 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 379 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 380 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 381 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 382 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 383 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 384 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.168 | Train PPL:   1.183\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 385 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 386 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 387 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 388 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 389 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 390 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.205\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 391 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 392 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 393 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 394 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 395 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 396 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 397 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.238\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 398 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 399 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "Epoch: 400 | Time: 0m 0s | R0 T1\n",
      "\tTrain Loss: 0.172 | Train PPL:   1.188\n",
      "\t Val. Loss: 0.237 |  Val. PPL:   1.268\n",
      "\n",
      "SCHEDULE: Ensembler-1.s2.t2.e400\n",
      "Epoch: 01 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397\n",
      "\t Val. Loss: 0.368 |  Val. PPL:   1.445\n",
      "Epoch: 02 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.407 | Train PPL:   1.502\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 03 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.330 | Train PPL:   1.391\n",
      "\t Val. Loss: 0.350 |  Val. PPL:   1.419\n",
      "Epoch: 04 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.353 | Train PPL:   1.423\n",
      "\t Val. Loss: 0.378 |  Val. PPL:   1.460\n",
      "Epoch: 05 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 06 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.354 | Train PPL:   1.425\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.444\n",
      "Epoch: 07 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.376\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 08 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 09 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 10 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.327 | Train PPL:   1.386\n",
      "\t Val. Loss: 0.364 |  Val. PPL:   1.439\n",
      "Epoch: 11 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.433 | Train PPL:   1.542\n",
      "\t Val. Loss: 0.302 |  Val. PPL:   1.353\n",
      "Epoch: 12 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.344 |  Val. PPL:   1.410\n",
      "Epoch: 13 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.295 |  Val. PPL:   1.343\n",
      "Epoch: 14 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.404 | Train PPL:   1.498\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 15 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 16 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 17 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.308 |  Val. PPL:   1.360\n",
      "Epoch: 18 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.385\n",
      "Epoch: 19 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.359\n",
      "\t Val. Loss: 0.325 |  Val. PPL:   1.385\n",
      "Epoch: 20 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.400 | Train PPL:   1.493\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 21 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 22 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.365\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 23 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404\n",
      "\t Val. Loss: 0.345 |  Val. PPL:   1.412\n",
      "Epoch: 24 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 25 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.364 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 26 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.328 |  Val. PPL:   1.389\n",
      "Epoch: 27 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.431\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 28 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 29 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.346 |  Val. PPL:   1.414\n",
      "Epoch: 30 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.327 |  Val. PPL:   1.387\n",
      "Epoch: 31 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.443\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 32 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.365 | Train PPL:   1.440\n",
      "\t Val. Loss: 0.359 |  Val. PPL:   1.431\n",
      "Epoch: 33 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.388 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.321 |  Val. PPL:   1.378\n",
      "Epoch: 34 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.375 | Train PPL:   1.455\n",
      "\t Val. Loss: 0.296 |  Val. PPL:   1.344\n",
      "Epoch: 35 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.309 |  Val. PPL:   1.362\n",
      "Epoch: 36 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.359 | Train PPL:   1.432\n",
      "\t Val. Loss: 0.315 |  Val. PPL:   1.370\n",
      "Epoch: 37 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.341 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 38 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.368 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.360 |  Val. PPL:   1.433\n",
      "Epoch: 39 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.391 | Train PPL:   1.478\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 40 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.382 | Train PPL:   1.465\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 41 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.389 | Train PPL:   1.475\n",
      "\t Val. Loss: 0.292 |  Val. PPL:   1.339\n",
      "Epoch: 42 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.314\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 43 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 44 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 45 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.335 | Train PPL:   1.398\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 46 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 47 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.337\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 48 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.300 | Train PPL:   1.350\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 49 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.317 | Train PPL:   1.373\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 50 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.311 | Train PPL:   1.364\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 51 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 52 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 53 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 54 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.351 | Train PPL:   1.420\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 55 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.287 |  Val. PPL:   1.332\n",
      "Epoch: 56 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.396\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "Epoch: 57 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 58 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 59 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 60 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 61 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 62 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.331\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 63 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 64 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 65 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 66 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 67 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.316 | Train PPL:   1.372\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 68 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 69 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 70 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 71 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 72 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 73 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 74 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.327\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 75 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.261 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 76 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 77 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 78 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 79 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.271 | Train PPL:   1.311\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 80 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.395 | Train PPL:   1.485\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "Epoch: 81 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.297 |  Val. PPL:   1.346\n",
      "Epoch: 82 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 83 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 84 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.305 | Train PPL:   1.356\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 85 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.383\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 86 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.345\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 87 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.274 |  Val. PPL:   1.315\n",
      "Epoch: 88 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 89 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 90 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.293 |  Val. PPL:   1.340\n",
      "Epoch: 91 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.284 |  Val. PPL:   1.329\n",
      "Epoch: 92 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 93 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 94 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.325\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 95 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 96 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.299 | Train PPL:   1.349\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 97 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 98 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 99 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.302 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 100 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.300\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 101 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 102 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.342\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 103 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 104 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.263 | Train PPL:   1.301\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 105 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.353\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 106 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 107 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 108 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 109 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.287 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 110 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.267 | Train PPL:   1.306\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 111 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.272 |  Val. PPL:   1.313\n",
      "Epoch: 112 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 113 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 114 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.258 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.260 |  Val. PPL:   1.297\n",
      "Epoch: 115 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.294 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 116 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 117 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 118 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.303 | Train PPL:   1.354\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 119 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.273\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 120 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 121 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.277 | Train PPL:   1.319\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 122 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 123 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.284 | Train PPL:   1.329\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 124 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 125 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 126 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 127 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 128 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 129 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 130 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.264 | Train PPL:   1.302\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 131 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.261 |  Val. PPL:   1.299\n",
      "Epoch: 132 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.296 | Train PPL:   1.344\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 133 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 134 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 135 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 136 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 137 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 138 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.268\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 139 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 140 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 141 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.322\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 142 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.272 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 143 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.274 | Train PPL:   1.315\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 144 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 145 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 146 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.283 | Train PPL:   1.326\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 147 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 148 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.295 | Train PPL:   1.343\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 149 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 150 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 151 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.269 | Train PPL:   1.309\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 152 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 153 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 154 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 155 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 156 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.307\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 157 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 158 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.289 | Train PPL:   1.335\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 159 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 160 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.256 | Train PPL:   1.291\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 161 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.270\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 162 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.305\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 163 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.240 | Train PPL:   1.271\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 164 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 165 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 166 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 167 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.257 | Train PPL:   1.294\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 168 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.247 | Train PPL:   1.280\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 169 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 170 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 171 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 172 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.296\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 173 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 174 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 175 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 176 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 177 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.260 | Train PPL:   1.297\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 178 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 179 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.298 | Train PPL:   1.347\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 180 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 181 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.250 | Train PPL:   1.284\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 182 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 183 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 184 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.268 | Train PPL:   1.308\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 185 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 186 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 187 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 188 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 189 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 190 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 191 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 192 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 193 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 194 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 195 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 196 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 197 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 198 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 199 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 200 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.251\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 201 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.320 | Train PPL:   1.378\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 202 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 203 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.281 | Train PPL:   1.324\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 204 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.246 | Train PPL:   1.279\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 205 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.265 | Train PPL:   1.303\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 206 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.306 | Train PPL:   1.358\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 207 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 208 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 209 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.239 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 210 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.254 |  Val. PPL:   1.289\n",
      "Epoch: 211 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.205 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 212 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.262 | Train PPL:   1.299\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 213 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.211\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 214 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.207\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 215 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.278 | Train PPL:   1.320\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 216 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.297 | Train PPL:   1.346\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 217 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 218 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 219 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 220 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.251 | Train PPL:   1.285\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 221 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 222 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.270 | Train PPL:   1.310\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 223 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 224 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.233 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 225 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 226 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 227 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 228 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.249 | Train PPL:   1.283\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 229 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 230 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 231 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.290 | Train PPL:   1.336\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 232 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 233 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 234 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.224 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 235 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.326 | Train PPL:   1.385\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 236 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.193\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 237 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.259 | Train PPL:   1.295\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 238 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 239 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.242 | Train PPL:   1.274\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 240 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 241 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 242 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 243 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 244 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.226 | Train PPL:   1.254\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 245 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 246 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 247 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.170 | Train PPL:   1.185\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 248 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 249 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 250 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 251 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.228\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 252 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.266 | Train PPL:   1.304\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 253 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 254 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.340\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 255 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 256 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.258\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 257 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 258 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 259 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.177 | Train PPL:   1.194\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 260 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 261 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 262 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 263 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 264 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 265 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 266 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 267 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 268 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 269 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 270 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.186 | Train PPL:   1.204\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 271 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 272 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 273 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 274 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 275 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 276 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 277 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 278 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 279 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.249\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 280 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.254 | Train PPL:   1.289\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 281 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 282 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 283 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 284 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 285 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 286 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.273 | Train PPL:   1.313\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 287 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.223 | Train PPL:   1.250\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 288 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 289 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 290 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 291 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 292 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 293 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.192 | Train PPL:   1.212\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 294 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 295 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 296 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 297 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.241 | Train PPL:   1.272\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 298 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.217 | Train PPL:   1.242\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 299 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 300 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.230 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 301 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 302 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 303 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 304 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.276 | Train PPL:   1.318\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 305 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 306 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.236 |  Val. PPL:   1.266\n",
      "Epoch: 307 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.232 | Train PPL:   1.261\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 308 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 309 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 310 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 311 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 312 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.247\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 313 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.244\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 314 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.222 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 315 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.263\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 316 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 317 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 318 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.304 | Train PPL:   1.355\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 319 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 320 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 321 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.227\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 322 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.204 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 323 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 324 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.238 | Train PPL:   1.269\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 325 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.286 | Train PPL:   1.332\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 326 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 327 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.224\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 328 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.228 | Train PPL:   1.256\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 329 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 330 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.219 | Train PPL:   1.245\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 331 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 332 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.221 | Train PPL:   1.248\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 333 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.197\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 334 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.278\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 335 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.275\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 336 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 337 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.220 | Train PPL:   1.246\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 338 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.189 | Train PPL:   1.208\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 339 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.234\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 340 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 341 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.200 | Train PPL:   1.221\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 342 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 343 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.211 | Train PPL:   1.235\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 344 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.243 | Train PPL:   1.276\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 345 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 346 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 347 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 348 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.201\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 349 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 350 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.259\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 351 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.178 | Train PPL:   1.195\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 352 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 353 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.235 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 354 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.190 | Train PPL:   1.209\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 355 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.231 | Train PPL:   1.260\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 356 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 357 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.188 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 358 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 359 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.208 | Train PPL:   1.231\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 360 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.176 | Train PPL:   1.192\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 361 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.202 | Train PPL:   1.223\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 362 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 363 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.236 | Train PPL:   1.266\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 364 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.232\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 365 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.209 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 366 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.173 | Train PPL:   1.189\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 367 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.180 | Train PPL:   1.198\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 368 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.199 | Train PPL:   1.220\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 369 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 370 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.195 | Train PPL:   1.215\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 371 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.226\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 372 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.253\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 373 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.218\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 374 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.210 | Train PPL:   1.233\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 375 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.187 | Train PPL:   1.206\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 376 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.245 | Train PPL:   1.277\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 377 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.216 | Train PPL:   1.241\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 378 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.194 | Train PPL:   1.214\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 379 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.225 | Train PPL:   1.252\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 380 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 381 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.182 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 382 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.193 | Train PPL:   1.213\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 383 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.206 | Train PPL:   1.229\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 384 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.281\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 385 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.201 | Train PPL:   1.222\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 386 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.197 | Train PPL:   1.217\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 387 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.293 | Train PPL:   1.341\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 388 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.218 | Train PPL:   1.243\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 389 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.229 | Train PPL:   1.257\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 390 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.184 | Train PPL:   1.202\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 391 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.207 | Train PPL:   1.230\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 392 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 393 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.237 | Train PPL:   1.267\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 394 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.214 | Train PPL:   1.239\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 395 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.215 | Train PPL:   1.240\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 396 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.234 | Train PPL:   1.264\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 397 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.212 | Train PPL:   1.236\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 398 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.196 | Train PPL:   1.216\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 399 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.227 | Train PPL:   1.255\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n",
      "Epoch: 400 | Time: 0m 0s | R0 T2\n",
      "\tTrain Loss: 0.169 | Train PPL:   1.184\n",
      "\t Val. Loss: 0.242 |  Val. PPL:   1.274\n"
     ]
    }
   ],
   "source": [
    "n_repetitions = 1\n",
    "hist_all_losses_F, hist_all_hitsss_F, models_F = experiment(\n",
    "    \"Ensembler-1\",\n",
    "    n_repetitions,\n",
    "    SCHEDULE,\n",
    "    init_dynamoe,\n",
    "    repeat_dynamoe,\n",
    "    STEP_SIZE_EVALUATION\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotAverages(hist_all_hitsss_F, SCHEDULE, STEP_SIZE_EVALUATION, datasets=(0,1), figsize=(12,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_F[2].n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model 0\n",
      "Task 0: Acc 0.88% | Gr acc 0.75 | Ugr acc 1.0\n",
      "Task 1: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 2: Acc 0.69% | Gr acc 0.38 | Ugr acc 1.0\n",
      "\n",
      "Model 1\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n",
      "\n",
      "Model 2\n",
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracyAll(models_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.3983,  5.1391, -1.3611]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[1]:\n",
    "    print(models_F[2].gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22.0830, -0.2584]) - tensor([1, 3, 7, 5, 4, 5, 2])\n",
      "tensor([21.2003, -0.2421]) - tensor([1, 3, 5, 4, 5, 6, 2])\n",
      "tensor([20.5874, -0.2038]) - tensor([1, 3, 5, 6, 4, 2])\n",
      "tensor([21.1779, -0.2220]) - tensor([1, 3, 5, 4, 2])\n",
      "tensor([21.1373, -0.2487]) - tensor([1, 3, 7, 5, 6, 4, 5, 6, 2])\n",
      "tensor([22.0578, -0.2568]) - tensor([1, 3, 7, 5, 4, 2])\n",
      "tensor([22.0680, -0.2647]) - tensor([1, 3, 7, 5, 4, 5, 6, 2])\n",
      "tensor([20.6297, -0.2455]) - tensor([1, 3, 5, 6, 4, 5, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "show_expert2(model, train_dls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bugfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "id": "TCvCl-WOjvYN"
   },
   "outputs": [],
   "source": [
    "expert, expert_optimizer = init_expert()\n",
    "\n",
    "gating, gating_optimizer = init_gating()\n",
    "\n",
    "model2 = Ensembler(gating, gating_optimizer, [expert,], [expert_optimizer,])\n",
    "\n",
    "model_optimizer = optim.Adam(model2.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CosineLoss(OUTPUT_DIM, ignore_index=PAD_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09185882657766342\n",
      "0.09303067624568939\n",
      "0.09172848239541054\n",
      "0.09473882988095284\n",
      "0.09154968708753586\n",
      "0.09308598190546036\n",
      "0.09350227937102318\n",
      "0.09345363080501556\n",
      "0.09185237810015678\n",
      "0.09177808463573456\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(train_ensembler_gating(model, train_dls[2], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: Acc 0.5% | Gr acc 0.0 | Ugr acc 1.0\n",
      "Task 1: Acc 0.75% | Gr acc 0.5 | Ugr acc 1.0\n",
      "Task 2: Acc 0.62% | Gr acc 0.25 | Ugr acc 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [6],\n",
      "        [4],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[1]:\n",
    "    print(model2.gating(seqs, seqs_len))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.2788e+01,  4.3714e-02, -1.4698e-02]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [5],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n",
      "tensor([[1],\n",
      "        [3],\n",
      "        [7],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[0]:\n",
    "    print(model.gating(seqs, seqs_len))\n",
    "    print(model.experts[0](seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print(model.experts[1](seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print(model(seqs, seqs_len, seqs).argmax(dim=2))\n",
    "    print((model.experts[0](seqs, seqs_len, seqs) * model.gating(seqs, seqs_len)[0][0] + model.experts[1](seqs, seqs_len, seqs) * model.gating(seqs, seqs_len)[0][1]).argmax(dim=2) )\n",
    "    print(seqs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_active_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16881714761257172\n",
      "0.15141833573579788\n",
      "0.15692844986915588\n",
      "0.15707916021347046\n",
      "0.15791257470846176\n",
      "0.18426920473575592\n",
      "0.15594960004091263\n",
      "0.2099481225013733\n",
      "0.19191593676805496\n",
      "0.16780265420675278\n",
      "0.16559413820505142\n",
      "0.14507802575826645\n",
      "0.14517706632614136\n",
      "0.15876172482967377\n",
      "0.18021392822265625\n",
      "0.14854232966899872\n",
      "0.17011234909296036\n",
      "0.19770054519176483\n",
      "0.15062608569860458\n",
      "0.16328270733356476\n",
      "0.1630496382713318\n",
      "0.15208538621664047\n",
      "0.1585157811641693\n",
      "0.1476331725716591\n",
      "0.19418861716985703\n",
      "0.14303864538669586\n",
      "0.15178367495536804\n",
      "0.13727358728647232\n",
      "0.14693152904510498\n",
      "0.16229414194822311\n",
      "0.16068880259990692\n",
      "0.13847041130065918\n",
      "0.17334415018558502\n",
      "0.15148300677537918\n",
      "0.16502273082733154\n",
      "0.14539223909378052\n",
      "0.12727899849414825\n",
      "0.1345638856291771\n",
      "0.14078588038682938\n",
      "0.16145791113376617\n",
      "0.1524231731891632\n",
      "0.1419893577694893\n",
      "0.12537699937820435\n",
      "0.14602867513895035\n",
      "0.14317089319229126\n",
      "0.16259494423866272\n",
      "0.17060024291276932\n",
      "0.1443256139755249\n",
      "0.13479702174663544\n",
      "0.15025698393583298\n",
      "0.14225276559591293\n",
      "0.16034740954637527\n",
      "0.12438227236270905\n",
      "0.12518545240163803\n",
      "0.14440368115901947\n",
      "0.13735894113779068\n",
      "0.19062084704637527\n",
      "0.13652129471302032\n",
      "0.17650730162858963\n",
      "0.14275554567575455\n",
      "0.16762355715036392\n",
      "0.14181502908468246\n",
      "0.1351715624332428\n",
      "0.12215232849121094\n",
      "0.1263386607170105\n",
      "0.11908707767724991\n",
      "0.12115544825792313\n",
      "0.1282174363732338\n",
      "0.1253407672047615\n",
      "0.12408994138240814\n",
      "0.14472167938947678\n",
      "0.128596231341362\n",
      "0.12068396806716919\n",
      "0.12849827855825424\n",
      "0.1212557777762413\n",
      "0.1384652704000473\n",
      "0.172116719186306\n",
      "0.11886386573314667\n",
      "0.11503040790557861\n",
      "0.1102377399802208\n",
      "0.11419399827718735\n",
      "0.13094515353441238\n",
      "0.11548421531915665\n",
      "0.12197885662317276\n",
      "0.11363319307565689\n",
      "0.11196141690015793\n",
      "0.11577392369508743\n",
      "0.11174105852842331\n",
      "0.1156032532453537\n",
      "0.1134130209684372\n",
      "0.12646576017141342\n",
      "0.12065742164850235\n",
      "0.11694453656673431\n",
      "0.13224681466817856\n",
      "0.10931586474180222\n",
      "0.13207688927650452\n",
      "0.123417429625988\n",
      "0.10937980562448502\n",
      "0.12781494110822678\n",
      "0.11629518866539001\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    print(train_dynamoe2_both(model2, train_dls[1], criterion, CLIP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "pJE4efjNcUCM"
   },
   "outputs": [],
   "source": [
    "model2.add_expert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVuFGE3vj6K-",
    "outputId": "80edc0df-4dcb-4f7c-f48f-6e818ad0a2c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [5],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [4],\n",
      "        [6],\n",
      "        [7],\n",
      "        [2]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 1.6035e-04, -3.0651e-03, -3.0967e+00,  5.0364e+00, -2.0193e+00,\n",
      "           1.1399e+01, -1.7679e+00,  4.7675e+00]],\n",
      "\n",
      "        [[-1.7267e-02, -1.7725e-02, -2.5892e+00,  8.6882e+00, -3.9250e-02,\n",
      "           2.2423e+00,  2.9604e+00,  1.6000e+00]],\n",
      "\n",
      "        [[-6.3461e-03, -2.2191e-02, -3.4556e+00,  2.0296e-01,  1.4208e-01,\n",
      "          -6.5787e-02,  1.4856e+01,  2.9764e+00]],\n",
      "\n",
      "        [[-1.5827e-02, -6.2566e-03, -2.2252e+00,  1.3526e+00,  6.0539e+00,\n",
      "          -3.2105e-01,  1.5760e+00,  7.5781e+00]],\n",
      "\n",
      "        [[-1.3362e-02,  1.7968e-02,  3.8569e+00, -1.9823e+00,  8.1386e+00,\n",
      "           1.5963e+00, -1.1647e-02,  1.3524e+00]],\n",
      "\n",
      "        [[-5.1290e-03, -1.0336e-02,  3.7318e+00,  1.4661e+00, -1.7547e+00,\n",
      "          -1.2773e+00,  1.0091e+01,  2.9304e+00]],\n",
      "\n",
      "        [[-6.9810e-03,  1.6720e-02,  7.0399e+00, -3.2432e-01, -3.1907e+00,\n",
      "           1.1463e-01,  1.1009e+00,  1.0808e+01]],\n",
      "\n",
      "        [[-1.7517e-02,  9.3911e-03,  1.1679e+01, -1.5853e+00, -6.2468e-01,\n",
      "           1.4161e+00, -1.5484e+00,  6.6679e+00]]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "for seqs, seqs_len in train_dls[0]:\n",
    "    print(seqs)\n",
    "    print(model2(seqs, seqs_len, seqs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "agl_autoencoder.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "fa0c181e15994ab92b77e0a9eeb7815659805982e17e67ed50eb685ba3cdee26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
